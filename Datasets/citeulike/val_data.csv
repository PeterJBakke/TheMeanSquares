,user.id,doc.id,rating,raw.title,raw.abstract
0,0,3298,1,Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings.,"Experimental and computational approaches to estimate solubility and permeability in discovery and development settings are described. In the discovery setting âthe rule of 5â predicts that poor absorption or permeation is more likely when there are more than 5 H---bond donors, 10 H---bond acceptors, the molecular weight (MWT) is greater than 500 and the calculated Log P (CLogP) is greater than 5 (or MlogP > 4.15). Computational methodology for the rule-based Moriguchi Log P (MLogP) calculation is described. Turbidimetric solubility measurement is described and applied to known drugs. High throughput screening (HTS) leads tend to have higher MWT and Log P and lower turbidimetric solubility than leads in the pre-HTS era. In the development setting, solubility calculations focus on exact value prediction and are difficult because of polymorphism. Recent work on linear free energy relationships and Log P approaches are critically reviewed. Useful predictions are possible in closely related analog series when coupled with experimental thermodynamic solubility measurements."
1,0,4663,1,Coarse-grained normal mode analysis in structural biology.,"The realization that experimentally observed functional motions of proteins can be predicted by coarse-grained normal mode analysis has renewed interest in applications to structural biology. Notable applications include the prediction of biologically relevant motions of proteins and supramolecular structures driven by their structure-encoded collective dynamics; the refinement of low-resolution structures, including those determined by cryo-electron microscopy; and the identification of conserved dynamic patterns and mechanically key regions within protein families. Additionally, hybrid methods that couple atomic simulations with deformations derived from coarse-grained normal mode analysis are able to sample collective motions beyond the range of conventional molecular dynamics simulations. Such applications have provided great insight into the underlying principles linking protein structures to their dynamics and their dynamics to their functions."
2,0,6104,1,SCOPPI: a structural classification of protein-protein interfaces,"{SCOPPI,} the structural classification of protein-protein interfaces, is a comprehensive database that classifies and annotates domain interactions derived from all known protein structures. {SCOPPI} applies {SCOP} domain definitions and a distance criterion to determine inter-domain interfaces. Using a novel method based on multiple sequence and structural alignments of {SCOP} families, {SCOPPI} presents a comprehensive geometrical classification of domain interfaces. Various interface characteristics such as number, type and position of interacting amino acids, conservation, interface size, and permanent or transient nature of the interaction are further provided. Proteins in {SCOPPI} are annotated with Gene Ontology terms, and the ontology can be used to quickly browse {SCOPPI.} Screenshots are available for every interface and its participating domains. Here, we describe contents and features of the web-based user interface as well as the underlying methods used to generate {SCOPPI's} data. In addition, we present a number of examples where {SCOPPI} becomes a useful tool to analyze viral mimicry of human interface binding sites, gene fusion events, conservation of interface residues and diversity of interface localizations. {SCOPPI} is available at http://www.scoppi.org."
3,0,10009,1,How many drug targets are there?,"For the past decade, the number of molecular targets for approved drugs has been debated. Here, we reconcile apparently contradictory previous reports into a comprehensive survey, and propose a consensus number of current drug targets for all classes of approved therapeutic drugs. One striking feature is the relatively constant historical rate of target innovation (the rate at which drugs against new targets are launched); however, the rate of developing drugs against new families is significantly lower. The recent approval of drugs that target protein kinases highlights two additional trends: an emerging realization of the importance of polypharmacology, and also the power of a gene-family-led approach in generating novel and important therapies."
4,0,11227,1,Analysing six types of protein-protein interfaces.,"Non-covalent residue side-chain interactions occur in many different types of proteins and facilitate many biological functions. Are these differences manifested in the sequence compositions and/or the residue-residue contact preferences of the interfaces? Previous studies analysed small data sets and gave contradictory answers. Here, we introduced a new data-mining method that yielded the largest high-resolution data set of interactions analysed. We introduced an information theory-based analysis method. On the basis of sequence features, we were able to differentiate six types of protein interfaces, each corresponding to a different functional or structural association between residues. Particularly, we found significant differences in amino acid composition and residue-residue preferences between interactions of residues within the same structural domain and between different domains, between permanent and transient interfaces, and between interactions associating homo-oligomers and hetero-oligomers. The differences between the six types were so substantial that, using amino acid composition alone, we could predict statistically to which of the six types of interfaces a pool of 1000 residues belongs at 63-100% accuracy. All interfaces differed significantly from the background of all residues in SWISS-PROT, from the group of surface residues, and from internal residues that were not involved in non-trivial interactions. Overall, our results suggest that the interface type could be predicted from sequence and that interface-type specific mean-field potentials may be adequate for certain applications."
5,0,12804,1,The origin of protein interactions and allostery in colocalization.,"Two fundamental principles can account for how regulated networks of interacting proteins originated in cells. These are the law of mass action, which holds that the binding of one molecule to another increases with concentration, and the fact that the colocalization of molecules vastly increases their local concentrations. It follows that colocalization can amplify the effect on one protein of random mutations in another protein and can therefore, through natural selection, lead to interactions between proteins and to a startling variety of complex allosteric controls. It also follows that allostery is common and that homologous proteins can have different allosteric mechanisms. Thus, the regulated protein networks of organisms seem to be the inevitable consequence of natural selection operating under physical laws."
6,0,13924,1,Biochemistry. How do proteins interact?,"PERSPECTIVE. 1st paragraph: Interactions between proteins are central to biology and are becoming increasingly important targets for drug design. Upon forming complexes, protein conformations usually change substantially compared to the unbound protein. Two main hypotheses have been advanced to explain these changes (see the figure). According to the ""induced fit"" hypothesis, the initial interaction between a protein and a binding partner induces a conformational change in the protein through a stepwise process (1). In the ""conformational selection"" model, it is assumed that, prior to the binding interaction, the unliganded protein exists as an ensemble of conformations in dynamic equilibrium. The binding partner interacts preferentially with a weakly populated, higher-energy conformation-causing the equilibrium to shift in favor of the selected conformation. This conformation then becomes the major conformation in the complex (2). Although biochemistry textbooks have championed the induced fit mechanism for more than 50 years, there is now growing support for the additional binding mechanism, including the seminal work by Lange, Lakomek, and co-workers on page 1471 of this issue. [Lange, Science (2008), 320, 1471]"
7,0,15283,1,Linking folding and binding.,"Many cellular proteins are intrinsically disordered and undergo folding, in whole or in part, upon binding to their physiological targets. The past few years have seen an exponential increase in papers describing characterization of intrinsically disordered proteins, both free and bound to targets. Although NMR spectroscopy remains the favored tool, a number of new biophysical techniques are proving exceptionally useful in defining the limits of the conformational ensembles. Advances have been made in prediction of the recognition elements in disordered proteins, in elucidating the kinetics and mechanism of the coupled folding and binding process, and in understanding the role of post-translational modifications in tuning the biological response. Here we review these and other recent advances that are providing new insights into the conformational propensities and interactions of intrinsically disordered proteins and are beginning to reveal general principles underlying their biological functions."
8,1,5573,1,404 not found: the stability and persistence of URLs published in MEDLINE,"Motivation: The advent of the World Wide Web has enabled unprecedented supplementation of traditional journal publications, allowing access to resources, such as video, sound, software, databases, datasets too large to publish, and even supplementary information and discussion. However, unlike traditional publications, continued availability of these online resources is not guaranteed. An automated survey was conducted to quantify the growth in Uniform Resource Locators (URLs) published to date in MEDLINE abstracts, their current availability and distribution by journal.  Results: Of 1630 unique URLs identified, formatting and/or spelling errors were detected within 201 (12%) of them as published. After corrections were made, a survey revealed that [~]63% of these URLs were consistently available, and another 19% were available intermittently. The rate of failure was far worse for anonymous login to FTP sites, with only 12 of 33 sites (36%) responding. This survey also shows that journals vary disproportionately in the number of web citations published, suggesting policy implementation among a few could have a profound impact overall. Out of the 306 journals with a URL published in an abstract, Bioinformatics published the most (12% of total).  Availability: URL database and program available by request. 10.1093/bioinformatics/btg465"
9,1,12897,1,Corpus annotation for mining biomedical events from literature.,"ABSTRACT: BACKGROUND: Advanced text-mining (TM) such as semantic enrichment of papers, event or relation extraction, and intelligent question answering have increasingly attracted attention in the bio-medical domain. For such attempts to succeed, text annotation from the biological point of view is indispensable. However, due to the complexity of the task, semantic annotation has never been tried on a large scale, apart from relatively simple term annotation. RESULTS: We have completed a new type of semantic annotation, event annotation, which is an addition to the existing annotations in the GENIA corpus. The corpus has already been annotated with POS (Parts of Speech), syntactic trees, terms, etc. The new annotation was made on half of the GENIA corpus, consisting of 1,000 Medline abstracts. It contains 9,372 sentences in which 36,114 events are identified. The major challenges during event annotation were (1) to design a scheme of annotation which meets specific requirements of text annotation, (2) to achieve biology-oriented annotation which reflect biologists' interpretation of text, and (3) to ensure the homogeneity of annotation quality across annotators. To meet these challenges, we introduced new concepts such as Single-facet Annotation and Semantic Typing, which have collectively contributed to successful completion of a large scale annotation. CONCLUSIONS: The resulting event-annotated corpus is the largest and one of the best in quality among similar annotation efforts. We expect it to become a valuable resource for NLP (Natural Language Processing)-based TM in the bio-medical domain."
10,1,15048,1,Is searching full text more effective than searching abstracts?,"BACKGROUND: With the growing availability of full-text articles online, scientists and other consumers of the life sciences literature now have the ability to go beyond searching bibliographic records (title, abstract, metadata) to directly access full-text content. Motivated by this emerging trend, I posed the following question: is searching full text more effective than searching abstracts? This question is answered by comparing text retrieval algorithms on MEDLINE abstracts, full-text articles, and spans (paragraphs) within full-text articles using data from the TREC 2007 genomics track evaluation. Two retrieval models are examined: bm25 and the ranking algorithm implemented in the open-source Lucene search engine. RESULTS: Experiments show that treating an entire article as an indexing unit does not consistently yield higher effectiveness compared to abstract-only search. However, retrieval based on spans, or paragraphs-sized segments of full-text articles, consistently outperforms abstract-only search. Results suggest that highest overall effectiveness may be achieved by combining evidence from spans and full articles. CONCLUSION: Users searching full text are more likely to find relevant articles than searching only abstracts. This finding affirms the value of full text collections for text retrieval and provides a starting point for future work in exploring algorithms that take advantage of rapidly-growing digital archives. Experimental results also highlight the need to develop distributed text retrieval algorithms, since full-text articles are significantly longer than abstracts and may require the computational resources of multiple machines in a cluster. The MapReduce programming model provides a convenient framework for organizing such computations."
11,1,16023,1,A dictionary to identify small molecules and drugs in free text,"Motivation: From the scientific community, a lot of effort has been spent on the correct identification of gene and protein names in text, while less effort has been spent on the correct identification of chemical names. Dictionary-based term identification has the power to recognize the diverse representation of chemical information in the literature and map the chemicals to their database identifiers.Results: We developed a dictionary for the identification of small molecules and drugs in text, combining information from UMLS, MeSH, ChEBI, DrugBank, KEGG, HMDB and ChemIDplus. Rule-based term filtering, manual check of highly frequent terms and disambiguation rules were applied. We tested the combined dictionary and the dictionaries derived from the individual resources on an annotated corpus, and conclude the following: (i) each of the different processing steps increase precision with a minor loss of recall; (ii) the overall performance of the combined dictionary is acceptable (precision 0.67, recall 0.40 (0.80 for trivial names); (iii) the combined dictionary performed better than the dictionary in the chemical recognizer OSCAR3; (iv) the performance of a dictionary based on ChemIDplus alone is comparable to the performance of the combined dictionary.Availability: The combined dictionary is freely available as an XML file in Simple Knowledge Organization System format on the web site http://www.biosemantics.org/chemlist.Contact: k.hettne@erasmusmc.nlSupplementary information: Supplementary data are available at Bioinformatics online."
12,2,1330,1,Long-lasting potentiation of synaptic transmission in the dentate area of the anaesthetized rabbit following stimulation of the perforant path,"1. The after-effects of repetitive stimulation of the perforant path fibres to the dentate area of the hippocampal formation have been examined with extracellular micro-electrodes in rabbits anaesthetized with urethane.2. In fifteen out of eighteen rabbits the population response recorded from granule cells in the dentate area to single perforant path volleys was potentiated for periods ranging from 30 min to 10 hr after one or more conditioning trains at 10-20/sec for 10-15 sec, or 100/sec for 3-4 sec.3. The population response was analysed in terms of three parameters: the amplitude of the population excitatory post-synaptic potential (e.p.s.p.), signalling the depolarization of the granule cells, and the amplitude and latency of the population spike, signalling the discharge of the granule cells.4. All three parameters were potentiated in 29% of the experiments; in other experiments in which long term changes occurred, potentiation was confined to one or two of the three parameters. A reduction in the latency of the population spike was the commonest sign of potentiation, occurring in 57% of all experiments. The amplitude of the population e.p.s.p. was increased in 43%, and of the population spike in 40%, of all experiments.5. During conditioning at 10-20/sec there was massive potentiation of the population spike (;frequency potentiation'). The spike was suppressed during stimulation at 100/sec. Both frequencies produced long-term potentiation.6. The results suggest that two independent mechanisms are responsible for long-lasting potentiation: (a) an increase in the efficiency of synaptic transmission at the perforant path synapses; (b) an increase in the excitability of the granule cell population."
13,2,5130,1,The episodic buffer: a new component of working memory?,"In 1974, Baddeley and Hitch proposed a three-component model of working memory. Over the years, this has been successful in giving an integrated account not only of data from normal adults, but also neuropsychological, developmental and neuroimaging data. There are, however, a number of phenomena that are not readily captured by the original model. These are outlined here and a fourth component to the model, the episodic buffer, is proposed. It comprises a limited capacity system that provides temporary storage of information held in a multimodal code, which is capable of binding information from the subsidiary systems, and from long-term memory, into a unitary episodic representation. Conscious awareness is assumed to be the principal mode of retrieval from the buffer. The revised model differs from the old principally in focussing attention on the processes of integrating information, rather than on the isolation of the subsystems. In doing so, it provides a better basis for tackling the more complex aspects of executive control in working memory."
14,2,9091,1,Loss of recent memory after bilateral hippocampal lesions,"Bilateral medial temporal lobe resection in man results in a persistent impairment of recent memory whenever the removal is carried far enough posteriorly to damage portions of the anterior hippocampus and hippocampal gyrus. This conclusion is based on formal psychological testing of nine cases (eight psychotic and one epileptic) carried out from one and one-half to four years after operation. The degree of memory loss appears to depend on the extent of hippocampal removal. In two cases in which bilateral resection was carried to a distance of 8 cm posterior to the temporal tips the loss was particularly severe. Removal of only the uncus and amygdala bilaterally does not appear to cause memory impairment. A case of unilateral inferior temporal lobectomy with radical posterior extension to include the major portion of the hippocampus and hippocampal gyrus showed no lasting memory loss. This is consistent with Milner and Penfield's negative findings in a long series of unilateral removals for temporal lobe epilepsy. The memory loss in these cases of medial temporal lobe excision involved both anterograde and some retrograde amnesia, but left early memories and technical skills intact. There was no deterioration in personality or general intelligence, and no complex perceptual disturbance such as is seen after a more complete bilateral temporal lobectomy. It is concluded that the anterior hippocampus and hippocampal gyrus, either separately or together, are critically concerned in the retention of current experience. It is not known whether the amygdala plays any part in this mechanismi, since the hippocampal complex has not been removed alone, but always together with uncus and amygdala."
15,3,896,1,Object-oriented application frameworks,"Computing power and network bandwidth have increased dramatically over the past decade, yet the design and implementation of complex software remain expensive and error-prone. Much of the cost and effort stems from the continuous rediscovery and reinvention of core concepts and components across the software industry. In particular, the growing heterogeneity of hardware architectures and diversity of operating system and communication platforms make it difficult to build correct, portable, efficient, and inexpensive applications from scratch."
16,4,13251,1,The MC-Fold and MC-Sym pipeline infers RNA structure from sequence data,"The classical {RNA} secondary structure model considers {A.U} and {G.C} {Watson-Crick} as well as {G.U} wobble base pairs. Here we substitute it for a new one, in which sets of nucleotide cyclic motifs define {RNA} structures. This model allows us to unify all base pairing energetic contributions in an effective scoring function to tackle the problem of {RNA} folding. We show how pipelining two computer algorithms based on nucleotide cyclic motifs, {MC-Fold} and {MC-Sym,} reproduces a series of experimentally determined {RNA} three-dimensional structures from the sequence. This demonstrates how crucial the consideration of all base-pairing interactions is in filling the gap between sequence and structure. We use the pipeline to define rules of precursor {microRNA} folding in double helices, despite the presence of a number of presumed mismatches and bulges, and to propose a new model of the human immunodeficiency virus-1 -1 frame-shifting element."
17,5,810,1,Understanding availability,"This paper addresses a simple, yet fundamental question in the design of peer-to-peer systems: What does it mean when we say availability and how does this understanding impact the engineering of practical systems? We argue that existing measurements and models do not capture the complex time-varying nature of availability in todays peer-to-peer environments. Further, we show that unforeseen methodological shortcomings have dramatically biased previous analyses of this phenomenon. As the basis of our study, we empirically characterize the availability of a large peer-to-peer system over a period of 7 days, analyze the dependence of the underlying availability distributions, measure host turnover in the system, and discuss how these results may affect the design of high-availability peer-to-peer services."
18,5,3034,1,Transactional Memory: Architectural Support for Lock-Free Data Structures,"A shared data structure is  lock-free  if its operations do not require mutual exclusion. If one process is interrupted in the middle of an operation, other processes will not be prevented from operating on that object. In highly concurrent systems, lock-free data structures avoid common problems associated with conventional locking techniques, including priority inversion, convoying, and difficulty of avoiding deadlock. This paper introduces  transactional memory , a new multiprocessor architecture intended to make lock-free synchronization as efficient (and easy to use) as conventional techniques based on mutual exclusion. Transactional memory allows programmers to define customized read-modify-write operations that apply to multiple, independently-chosen words of memory. It is implemented by straightforward extensions to any multiprocessor cache-coherence protocol. Simulation results show that transactional memory matches or outperforms the best known locking techniques for simple benchmarks, even in the absence of priority inversion, convoying, and deadlock."
19,5,6345,1,Computer Networks,"Computer Networks, Fourth Edition is the ideal introduction to computer networks. Renowned author, educator, and researcher Andrew S. Tanenbaum has updated his classic best seller to reflect the newest technologies, including 802.11, broadband wireless, ADSL, Bluetooth, gigabit Ethernet, the Web, the wireless Web, streaming audio, IPsec, AES, quantum cryptography, and more. Using real-world examples, Tanenbaum explains how networks work on the inside, from underlying physical layer hardware up through today's most popular network applications."
20,6,3293,1,Principles of Neural Science,"{Now in resplendent color, the new edition continues to define the latest in the scientific understanding of the brain, the nervous system, and human behavior. Each chapter is thoroughly revised and includes the impact of molecular biology in the mechanisms underlying developmental processes and in the pathogenesis of disease. Important features to this edition include a new chapter - Genes and Behavior; a complete updating of development of the nervous system; the genetic basis of neurological and psychiatric disease; cognitive neuroscience of perception, planning, action, motivation and memory; ion channel mechanisms; and much more.}"
21,6,8645,1,The {R}ician distribution of noisy {MRI} data,"The image intensity in magnetic resonance magnitude images in the presence of noise is shown to be governed by a Rician distribution. Low signal intensities (SNR &lt; 2) are therefore biased due to the noise. it is shown how the underlying noise can be estimated from the images and a simple correction scheme is provided to reduce the bias. the noise characteristics in phase images are also studied and shown to be very different from those of the magnitude images. Common to both, however, is that the noise distributions are nearly Gaussian for SNR larger than two."
22,7,9423,1,The Many Faces of Protein&#8211;Protein Interactions: A Compendium of Interface Geometry,"A systematic classification of protein&#8211;protein interfaces is a valuable resource for understanding the principles of molecular recognition and for modelling protein complexes. Here, we present a classification of domain interfaces according to their geometry. Our new algorithm uses a hybrid approach of both sequential and structural features. The accuracy is evaluated on a hand-curated dataset of 416 interfaces. Our hybrid procedure achieves 83&#37; precision and 95&#37; recall, which improves the earlier sequence-based method by 5&#37; on both terms. We classify virtually all domain interfaces of known structure, which results in nearly 6,000 distinct types of interfaces. In 40&#37; of the cases, the interacting domain families associate in multiple orientations, suggesting that all the possible binding orientations need to be explored for modelling multidomain proteins and protein complexes. In general, hub proteins are shown to use distinct surface regions (multiple faces) for interactions with different partners. Our classification provides a convenient framework to query genuine gene fusion, which conserves binding orientation in both fused and separate forms. The result suggests that the binding orientations are not conserved in at least one-third of the gene fusion cases detected by a conventional sequence similarity search. We show that any evolutionary analysis on interfaces can be skewed by multiple binding orientations and multiple interaction partners. The taxonomic distribution of interface types suggests that ancient interfaces common to the three major kingdoms of life are enriched by symmetric homodimers. The classification results are online at http://www.scoppi.org."
23,8,2462,1,Inside PageRank,"Although the interest of a Web page is strictly related to its content and to the subjective readers' cultural background, a measure of the page authority can be provided that only depends on the topological structure of the Web. PageRank is a noticeable way to attach a score to Web pages on the basis of the Web connectivity. In this article, we look inside PageRank to disclose its fundamental properties concerning stability, complexity of computational scheme, and critical role of parameters involved in the computation. Moreover, we introduce a circuit analysis that allows us to understand the distribution of the page score, the way different Web communities interact each other, the role of dangling pages (pages with no outlinks), and the secrets for promotion of Web pages."
24,8,5769,1,Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time,"We introduce the  smoothed analysis of algorithms , which continuously interpolates between the worst-case and average-case analyses of algorithms. In smoothed analysis, we measure the maximum over inputs of the expected performance of an algorithm under small random perturbations of that input. We measure this performance in terms of both the input size and the magnitude of the perturbations. We show that the simplex algorithm has  smoothed complexity  polynomial in the input size and the standard deviation of Gaussian perturbations."
25,8,8588,1,Algebraic Topology and Distributed Computing: A Primer,". Models and techniques borrowed from classical algebraic topology have recently yielded a variety of new lower bounds and impossibility results for distributed and concurrent computation. This paper explains the basic concepts underlying this approach, and shows how they apply to a simple distributed problem. 1 Introduction  The problem of coordinating concurrent processes remains one of the central problems of distributed computing. Coordination problems arise at all scales in distributed and ..."
26,9,747,1,The Strength of Weak Ties,"Analysis of social networks is suggested as a tool for linking micro and macro levels of sociological theory. The procedure is illustrated by elaboration of the macro implications of one aspect of small-scale interaction: the strength of dyadic ties. It is argued that the degree of overlap of two individuals' friendship networks varies directly with the strength of their tie to one another. The impact of this principle on diffusion of influence and information, mobility opportunity, and community organization is explored. Stress is laid on the cohesive power of weak ties. Most network models deal, implicitly, with strong ties, thus confining their applicability to small, well-defined groups. Emphasis on weak ties lends itself to discussion of relations between groups and to analysis of segments of social structure not easily defined in terms of primary groups."
27,9,3888,1,"Power, politics, and MIS implementation","Theories of resistance to management information systems {(MIS)} are important because they guide the implementation strategies and tactics chosen by implementors. Three basic theories of the causes of resistance underlie many prescriptions and rules for {MIS} implementation. Simply stated, people resist {MIS} because of their own internal factors, because of poor system design, and because of the interaction of specific system design features with aspects of the organizational context of system use. These theories differ in their basic assumptions about systems, organizations, and resistance; they also differ in predictions that can be derived from them and in their implications for the implementation process. These differences are described and the task of evaluating the theories on the bases of the differences is begun. Data from a case study are used to illustrate the theories and to demonstrate the superiority, for implementors, of the interaction theory."
28,9,8287,1,"Organizations: Rational, Natural, and Open Systems","{<P><B></B> This broad, balanced introduction to organizational studies enables the reader to compare and contrast different approaches to the study of organizations. This book is a valuable tool for the reader, as we are all intertwined with organizations in one form or another. Numerous other disciplines besides sociology are addressed in this book, including economics, political science, strategy and management theory. <B></B> Topic areas discussed in this book are the importance of organizations; defining organizations; organizations as rational, natural, and open systems; environments, strategies, and structures of organizations; and organizations and society. <B></B> For those employed in fields where knowledge of organizational theory is necessary, including sociology, anthropology, cognitive psychology, industrial engineering, managers in corporations and international business, and business strategists.  </P>}"
29,9,14003,1,Building theories from case study research,"This paper describes the process of inducting theory using case studies-from specifying the research questions to reaching closure. Some features of the process, such as problem definition and construct validation, are similar to hypothesis-testing research. Others, such as within-case analysis and replication logic, are unique to the inductive, case-oriented process Overall, the process described here is highly iterative and tightly linked to data. This research approach is especially appropriate in new topic areas. The resultant theory is often novel, testable, and empirically valid finally, framebreaking insights, the tests of good theory (e.g. parsimony, logical coherence), and convincing grounding in the evidence are the key criteria for evaluating this type of research. ABSTRACT FROM AUTHOR Copyright of Academy of Management Review is the property of Academy of Management and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright"
30,10,1239,1,OWL-S: Semantic Markup for Web Services,"The Semantic Web should enable greater access not only to content but also to services on the Web. Users and software agents should be able to discover, invoke, compose, and monitor Web resources offering particular services and having particular properties, and should be able to do so with a high degree of automation if desired. Powerful tools should be enabled by service descriptions, across the Web service lifecycle. OWL-S (formerly DAML-S) is an ontology of services that makes these functionalities possible. In this submission we describe the overall structure of the ontology and its three main parts: the service profile for advertising and discovering services; the process model, which gives a detailed description of a service's operation; and the grounding, which provides details on how to interoperate with a service, via messages.  Following the layered approach to markup language development, the current version of OWL-S builds on the Ontology Web Language (OWL) Recommendation produced by theWeb-Ontology Working Group at the World Wide Web Consortium"
31,10,2993,1,Ontology evolution: Not the same as schema evolution,"As ontology development becomes a more ubiquitous and collaborative process, ontology versioning and evolution becomes an important area of ontology research. The many similarities between database-schema evolution and ontology evolution will allow us to build on the extensive research in schema evolution. However, there are also important differences between database schemas and ontologies. The differences stem from different usage paradigms, the presence of explicit semantics and different knowledge models. A lot of problems that existed only in theory in database research come to the forefront as practical problems in ontology evolution. These differences have important implications for the development of ontology-evolution frameworks: The traditional distinction between versioning and evolution is not applicable to ontologies. There are several dimensions along which compatibility between versions must be considered. The set of change operations for ontologies is different. We must develop automatic techniques for finding similarities and differences between versions."
32,10,10713,1,The Java Context Awareness Framework (JCAF) â A Service Infrastructure and Programming Framework for Context-Aware Applications,"Context-awareness is a key concept in ubiquitous computing. But to avoid developing dedicated context-awareness sub-systems for specific application areas there is a need for more generic programming frameworks. Such frameworks can help the programmer develop and deploy context-aware applications faster. This paper describes the Java Context-Awareness Framework â JCAF, which is a Java-based context-awareness infrastructure and programming API for creating context-aware computer applications. The paper presents the design goals of JCAF, its runtime architecture, and its programming model. The paper presents some applications of using JCAF in three different applications and discusses lessons learned from using JCAF."
33,11,1042,1,No {S}ilver {B}ullet: {E}ssence and {A}ccidents of {S}oftware {E}ngineering,"Of all the monsters that fill the nightmares of our folklore, none terrify more than werewolves, because they transform unexpectedly from the familiar into horrors. For these, one seeks bullets of silver that can magically lay them to rest. The familiar software project, at least as seen by the nontechnical manager, has something of this character; it is usually innocent and straightforward, but is capable of becoming a monster of missed schedules, blown budgets, and flawed products. So we hear desperate cries for a silver bullet--something to make software costs drop as rapidly as computer hardware costs do. But, as we look to the horizon of a decade hence, we see no silver bullet. There is no single development, in either technology or in management technique, that by itself promises even one order-of-magnitude improvement in productivity, in reliability, in simplicity. In this article, I shall try to show why, by examining both the nature of the software problem and the properties of the bullets proposed. Skepticism is not pessimism, however. Although we see no startling breakthroughs--and indeed, I believe such to be inconsistent with the nature of software--many encouraging"
34,11,2452,1,The Active Badge Location System,"A novel system for the location of people in an office environment is described. Members of staff wear badges that transmit signals providing information about their location to a centralized location service, through a network of sensors. The paper also examines alternative location techniques, system design issues and applications, particularly relating to telephone call routing. Location systems raise concerns about the privacy of an individual and these issues are also addressed."
35,11,3077,1,A middleware infrastructure for active spaces,"The authors present an experimental middleware infrastructure called Gaia, which they have used to prototype the resource management of?and to provide the user-oriented interfaces for-physical spaces populated with network-enabled computing resources. The authors focus on physical spaces used for teaching; classrooms, offices, and lecture rooms. The system described is derived from a series of experiments starting in 1996. The authors show how, by applying the concepts of a conventional operating system to middleware, they can manage the resources, devices, and distributed objects in a room, building, or physical space, how a distributed extension of the model-view-controller that is use in personal computers simplifies and structures practical applications for these environments, and how, by driving context-sensitivity into its data storage mechanisms, the system can help satisfy the requirements for user-centricity and mobility."
36,11,4552,1,Towards architecture-based self-healing systems,"Our approach to creating self-healing systems is based on software architecture, where repairs are done at the level of a software system's components and connectors. In our approach, event-based software architectures are targeted because they offer significant benefits for run-time adaptation. Before an automated planning agent can decide how to repair a self-healing system, a significant infrastructure must be in place to support making the planned repair. Specifically, the self-healing system must be built using a framework that allows for run-time adaptation, there must be a language in which to express the repair plan, and there must be a reconfiguration agent that can execute the repair plan once it is created. In this paper, we present tools and methods that implement these infrastructure elements in the context of an overall architecture-based vision for building self-healing systems. The paper concludes with a gap analysis of our current infrastructure vs. the overall vision, and our plans for fulfilling that vision."
37,11,5002,1,"Tangible Bits: Towards Seamless Interfaces between People, Bits and Atoms","{T}his paper presents our vision of {H}uman {C}omputer {I}nteraction ({H}{C}{I}): ""{T}angible {B}its"". {T}angible {B}its allows users to ""grasp & manipulate"" bits in the center of users' attention by coupling the bits with everyday physical objects and architectural surfaces. {T}angilble {B}its also enables users to be aware of background bits at the periphery of human perception using ambient display media such as light, sound, airflow, and water movement in an augmented space. {T}he goal of {T}angible {B}its is to bridge the gaps between both cyberspace and the physical environment, as well as foreground and background of human activities. {T}his paper describes three key concepts of {T}angible {B}its: interactive surfaces; the coupling of bits and graspable physical objects; and ambient media for background awareness. {W}e illustrate these concepts with three protype systems - the meta{D}{E}{S}{K}, trans{B}{O}{A}{R}{D}, and ambient{R}{O}{O}{M} - to identify underlying research issues."
38,11,9025,1,Autonomic Live Adaptation of Virtual Computational Environments in a Multi-Domain Infrastructure,"A shared distributed infrastructure is formed by federating computation resources from multiple domains. Such shared infrastructures are increasing in popularity and are providing massive amounts of aggregated computation resources to large numbers of users. Meanwhile, virtualization technologies, at machine and network levels, are maturing and enabling mutually isolated virtual computation environments for executing arbitrary parallel/distributed applications on top of such a shared physical infrastructure. In this paper, we go one step further by supporting autonomic adaptation of virtual computation environments as active, integrated entities. More specifically, driven by both dynamic availability of infrastructure resources and dynamic application resource demand, a virtual computation environment is able to automatically relocate itself across the infrastructure and scale its share of infrastructural resources. Such autonomic adaptation is transparent to both users of virtual environments and administrators of infrastructures, maintaining the look and feel of a stable, dedicated environment for the user. As our proofof-concept, we present the design, implementation, and evaluation of a system called VIOLIN, which is composed of a virtual network of virtual machines capable of live migration across a multi-domain physical infrastructure. 1"
39,11,11210,1,Human-Machine Reconfigurations: Plans and Situated Actions,"{This book considers how agencies are currently figured at the human-machine interface, and how they might be imaginatively and materially reconfigured. Contrary to the apparent enlivening of objects promised by the sciences of the artificial, the author proposes that the rhetorics and practices of those sciences work to obscure the performative nature of both persons and things. The question then shifts from debates over the status of human-like machines, to that of how humans and machines are enacted as similar or different in practice, and with what theoretical, practical and political consequences. Drawing on recent scholarship across the social sciences, humanities and computing, the author argues for research aimed at tracing the differences within specific sociomaterial arrangements without resorting to essentialist divides. This requires expanding our unit of analysis, while recognizing the inevitable cuts or boundaries through which technological systems are constituted.}"
40,12,1481,1,Balanced inhibition underlies tuning and sharpens spike timing in auditory cortex,"Neurons in the primary auditory cortex are tuned to the intensity and specific frequencies of sounds, but the synaptic mechanisms underlying this tuning remain uncertain. Inhibition seems to have a functional role in the formation of cortical receptive fields, because stimuli often suppress similar or neighbouring responses1, 2, 3, and pharmacological blockade of inhibition broadens tuning curves4, 5. Here we use whole-cell recordings in vivo to disentangle the roles of excitatory and inhibitory activity in the tone-evoked responses of single neurons in the auditory cortex. The excitatory and inhibitory receptive fields cover almost exactly the same areas, in contrast to the predictions of classical lateral inhibition models. Thus, although inhibition is typically as strong as excitation, it is not necessary to establish tuning, even in the receptive field surround. However, inhibition and excitation occurred in a precise and stereotyped temporal sequence: an initial barrage of excitatory input was rapidly quenched by inhibition, truncating the spiking response within a few (1â4) milliseconds. Balanced inhibition might thus serve to increase the temporal precision6 and thereby reduce the randomness of cortical operation, rather than to increase noise as has been proposed previously7."
41,12,3308,1,Duty to disclose in medical genetics: a legal perspective.,"As technical knowledge and public information in medical genetics continue to expand, the geneticist may expect to be held responsible for informing patients and clients about new developments in research and diagnosis. The long legal evolution of the physician's duty to disclose, and more recent findings of a physician's duty to recall former patients to inform them about newly discovered risks of treatment, indicate that medical geneticists may have a duty to disclose both current and future information about conditions that are or could be inherited. Recent case law supports findings of professional liability for both present and future disclosure, even in the absence of an active physician-patient relationship. The requirement of candid and complete disclosure will affect the counseling approach in testing for deleterious genes and in providing medical treatment for minors with hereditary diseases. Finding a duty to recall may impose further professional burdens on the geneticist to reach beyond the immediate counseling arena and to recontact patients, perhaps years after their initial visit to genetics clinic."
42,12,4409,1,Role of Delays in Shaping Spatiotemporal Dynamics of Neuronal Activity in Large Networks,We study the effect of delays on the dynamics of large networks of neurons. We show that delays give rise to a wealth of bifurcations and to a rich phase diagram; which includes oscillatory bumps; traveling waves; lurching waves; standing waves arising via a period-doubling bifurcation; aperiodic regimes; and regimes of multistability. We study the existence and the stability of the various dynamical patterns analytically and numerically in a simplified rate model as a function of the interaction parameters. The results derived in that framework allow us to understand the origin of the diversity of dynamical states observed in large networks of spiking neurons.
43,12,6783,1,Agent-organized networks for dynamic team formation,"Many multi-agent systems consist of a complex network of autonomous yet interdependent agents. Examples of such networked multi-agent systems include supply chains and sensor networks. In these systems, agents have a select set of other agents with whom they interact based on environmental knowledge, cognitive capabilities, resource limitations, and communications constraints. Previous findings have demonstrated that the structure of the artificial social network governing the agent interactions is strongly correlated with organizational performance. As multi-agent systems are typically embedded in dynamic environments, we wish to develop distributed, on-line network adaptation mechanisms for discovering effective network structures. Therefore, within the context of dynamic team formation, we propose several strategies for agent-organized networks (AONs) and evaluate their effectiveness for increasing organizational performance."
44,12,8154,1,"Massively Multiplayer Online Role-Playing Games: The People, the Addiction and the Playing Experience","{This book is about the fastest growing form of electronic game in the world&#151;the Massively Multiplayer Online Role Playing Game (MMORPG). It introduces these self-contained three-dimensional virtual worlds, often inhabited by thousands of players, and describes their evolution and sometimes become addicted to it. It also delves into the psychology of the people who inhabit the game universe and explores the development of the unique cultures, economies, moral codes, and slang in these virtual communities. It explains how the games are built, the spin-offs that players create to enhance their game lives, and peeks at the future of MMORPGs as they evolve from a form of amusement to an educational, scientific, and business tool.  <P>   Based on hundreds of interviews over a three-year period, the work explores reasons people are attracted to and addicted to these games. It also surveys many existing and upcoming games, identifying their unique features and attractions. Two appendices list online addiction organizations and MMORPG information sites.}"
45,12,10266,1,Mixed Methods Research: A Research Paradigm Whose Time Has Come,"The purposes of this article are to position mixed methods research (mixed research is a synonym) as the natural complement to traditional qualitative and quantitative research, to present pragmatism as offering an attractive philosophical partner for mixed methods research, and to provide a framework for designing and conducting mixed methods research. In doing this, we briefly review the paradigm ""wars"" and incompatibility thesis, we show some commonalities between quantitative and qualitative research, we explain the tenets of pragmatism, we explain the fundamental principle of mixed research and how to apply it, we provide specific sets of designs for the two major types of mixed methods research (mixed-model designs and mixed-method designs), and, finally, we explain mixed methods research as following (recursively) an eight-step process. A key feature of mixed methods research is its methodological pluralism or eclecticism, which frequently results in superior research (compared to monomethod research). Mixed methods research will be successful as more investigators study and help advance its concepts and as they regularly practice it. 10.3102/0013189X033007014"
46,12,11568,1,Power-law distributions in empirical data,"Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution -- the part of the distribution representing large but rare events -- and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out.. Comment: 43 pages, 11 figures, 7 tables, 4 appendices; code available at http://www.santafe.edu/~aaronc/powerlaws/"
47,12,13003,1,100% Accuracy in Automatic Face Recognition,"Accurate face recognition is critical for many security applications. Current automatic face-recognition systems are defeated by natural changes in lighting and pose, which often affect face images more profoundly than changes in identity. The only system that can reliably cope with such variability is a human observer who is familiar with the faces concerned. We modeled human familiarity by using image averaging to derive stable face representations from naturally varying photographs. This simple procedure increased the accuracy of an industry standard face-recognition algorithm from 54% to 100%, bringing the robust performance of a familiar human to an automated system. 10.1126/science.1149656"
48,12,16722,1,myExperiment: a repository and social network for the sharing of bioinformatics workflows.,"myExperiment (http://www.myexperiment.org) is an online research environment that supports the social sharing of bioinformatics workflows. These workflows are procedures consisting of a series of computational tasks using web services, which may be performed on data from its retrieval, integration and analysis, to the visualization of the results. As a public repository of workflows, myExperiment allows anybody to discover those that are relevant to their research, which can then be reused and repurposed to their specific requirements. Conversely, developers can submit their workflows to myExperiment and enable them to be shared in a secure manner. Since its release in 2007, myExperiment currently has over 3500 registered users and contains more than 1000 workflows. The social aspect to the sharing of these workflows is facilitated by registered users forming virtual communities bound together by a common interest or research project. Contributors of workflows can build their reputation within these communities by receiving feedback and credit from individuals who reuse their work. Further documentation about myExperiment including its REST web service is available from http://wiki.myexperiment.org. Feedback and requests for support can be sent to bugs@myexperiment.org."
49,13,2590,1,Directed diffusion: A scalable and robust communication paradigm for sensor networks,"Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diusion paradigm for such coordination. Directed diusion is datacentric in that all communication is for named data. All nodes in a directed diusion-based network are applicationaware. This enables diusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diusion for a simple remote-surveillance sensor network.  1 Introduction  In the near future, advances in processor, memory and radio technology will enable small and cheap nodes capable of wireless communication and signicant computation. The addition of sensing capability to such devices will make distributed microsensing an activity in which a collection of ..."
50,13,5041,1,Current Solutions for Web Service Composition,"Web service composition lets developers create applications on top of service-oriented computing's native description, discovery, and communication capabilities. Such applications are rapidly deployable and offer developers reuse possibilities and users seamless access to a variety of complex services.  There are many existing approaches to service composition, ranging from abstract methods to those aiming to be industry standards.  The authors describe four key issues for Web service composition."
51,14,2534,1,STUDYING ONLINE SOCIAL NETWORKS,"Abstract When a computer network connects people or organizations, it is a social network. Yet the study of such computer-supported social networks has not received as much attention as studies of human-computer interaction, online person-to-person interaction, and computer-supported communication within small groups. We argue the usefulness of a social network approach for the study of computer-mediated communication. We review some basic concepts of social network analysis, describe how to collect and analyze social network data, and demonstrate where social network data can be, and have been, used to study computer-mediated communication. Throughout, we show the utility of the social network approach for studying computer-mediated communication, be it in computer-supported cooperative work, in virtual community, or in more diffuse interactions over less bounded systems such as the Internet."
52,14,12572,1,"Social network sites: Definition, history, and scholarship","Social network sites (SNSs) are increasingly attracting the attention of academic and industry researchers intrigued by their affordances and reach. This special theme section of the Journal of Computer-Mediated Communication brings together scholarship on these emergent phenomena. In this introductory article, we describe features of SNSs and propose a comprehensive definition. We then present one perspective on the history of such sites, discussing key changes and developments. After briefly summarizing existing scholarship concerning SNSs, we discuss the articles in this special section and conclude with considerations for future research. ABSTRACT FROM AUTHOR Copyright of Journal of Computer-Mediated Communication is the property of Blackwell Publishing Limited and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts)"
53,15,15114,1,A Principal Component Analysis of 39 Scientific Impact Measures,"<sec> <title>Background</title> <p>The impact of scientific publications has traditionally been expressed in terms of citation counts. However, scientific activity has moved online over the past decade. To better capture scientific impact in the digital era, a variety of new impact measures has been proposed on the basis of social network analysis and usage log data. Here we investigate how these new measures relate to each other, and how accurately and completely they express scientific impact.</p> </sec><sec> <title>Methodology</title> <p>We performed a principal component analysis of the rankings produced by 39 existing and proposed measures of scholarly impact that were calculated on the basis of both citation and usage log data.</p> </sec><sec> <title>Conclusions</title> <p>Our results indicate that the notion of scientific impact is a multi-dimensional construct that can not be adequately measured by any single indicator, although some measures are more suitable than others. The commonly used citation Impact Factor is not positioned at the core of this construct, but at its periphery, and should thus be used with caution.</p> </sec>"
54,16,1182,1,Machine Learning,"Machine Learning is the study of methods for programming computers to learn. Computers are applied to a wide range of tasks, and for most of these it is relatively easy for programmers to design and implement the necessary software. However, there are many tasks for which this is difficult or impossible. These can be divided into four general categories. First, there are problems for which there exist no human experts. For example, in modern automated manufacturing facilities, there is a need to predict machine failures before they occur by analyzing sensor readings. Because the machines are new, there are no human experts who can be interviewed by a programmer to provide the knowledge necessary to build a computer system. A machine learning system can study recorded data and subsequent machine failures and learn prediction rules. Second, there are problems where human experts exist, but where they are unable to explain their expertise. This is the case in many perceptual tasks, such as speech recognition, hand-writing recognition, and natural language understanding. Virtually all humans exhibit expert-level abilities on these tasks, but none of them can describe the detailed steps that they follow as they perform them. Fortunately, humans can provide machines with examples of the inputs and correct outputs for these tasks, so machine learning algorithms can learn to map the inputs to the outputs. Third, there are problems where phenomena are changing rapidly. In finance, for example, people would like to predict the future behavior of the stock market, of consumer purchases, or of exchange rates. These behaviors change frequently, so that even if a programmer could construct a good predictive computer program, it would need to be rewritten frequently. A learning program can relieve the programmer of this burden by constantly modifying and tuning a set of learned prediction rules. Fourth, there are applications that need to be customized for each computer user separately. Consider, for example, a program to filter unwanted electronic mail messages. Different users will need different filters. It is unreasonable to expect each user to program his or her own rules, and it is infeasible to provide every user with a software engineer to keep the rules up-to-date. A machine learning system can learn which mail messages the user rejects and maintain the filtering rules automatically. Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis. Statistics focuses on understanding the phenomena that have generated the data, often with the goal of testing different hypotheses about those phenomena. Data mining seeks to find patterns in the data that are understandable by people. Psychological studies of human learning aspire to understand the mechanisms underlying the various learning behaviors exhibited by people (concept learning, skill acquisition, strategy change, etc.). In contrast, machine learning is primarily concerned with the accuracy and effectiveness of the resulting computer system. To illustrate this, consider the different questions that might be asked about speech data. A machine learning approach focuses on building an accurate and efficient speech recognition system. A statistician might collaborate with a psychologist to test hypotheses about the mechanisms underlying speech recognition. A data mining approach might look for patterns in speech data that could be applied to group speakers according to age, sex, or level of education."
55,16,5850,1,Application of dimensionality reduction in recommender systems--a case study,"We investigate the use of dimensionality reduction to improve performance for a new class of data analysis software called ârecommender systemsâ. Recommender systems apply knowledge discovery techniques to the problem of making product recommendations during a live customer interaction. These systems are achieving widespread success in E-commerce nowadays, especially with the advent of the Internet. The tremendous growth of customers and products poses three key challenges for recommender systems in the E-commerce domain. These are: producing high quality recommendations, performing many recommendations per second for millions of customers and products, and achieving high coverage in the face of data sparsity. One successful recommender system technology is collaborative filtering, which works by matching customer preferences to other customers in making recommendations. Collaborative filtering has been shown to produce high quality recommendations, but the performance degrades with the number of customers and products. New recommender system technologies are needed that can quickly produce high quality recommendations, even for very largescale problems. This paper presents two different experiments where we have explored one technology called Singular Value Decomposition (SVD) to reduce the dimensionality of recommender system databases. Each experiment compares the quality of a recommender system using SVD with the quality of a recommender system using collaborative filtering. The first experiment compares the effectiveness of the two recommender systems at predicting consumer preferences based on a database of explicit ratings of products. The second experiment compares the effectiveness of the two recommender systems at producing Top-N lists based on a real-life customer purchase database from an E-Commerce site. Our experience suggests that SVD has the potential to meet many of the challenges of recommender systems, under certain conditions. 1"
56,17,3971,1,Free Energy Calculations: Applications to Chemical and Biochemical Phenomena,"I will review the applications of free energy calcuIations employing molecular dynamics or Monte Carlo methods to a variety of chemical and biochemical phenomena. The focus is on the applications of such calculations to molecular solvation, molecular associ- ation, macromolecular stability, and enzyme catalysis. The molecules discussed range from monovalent ions and small molecules to proteins and nucleic acids."
57,18,3065,1,Sparse Coding of Sensory Inputs,"Several theoretical, computational, and experimental studies suggest that neurons encode sensory information using a small number of active neurons at any given point in time. This strategy, referred to as [`]sparse coding', could possibly confer several advantages. First, it allows for increased storage capacity in associative memories; second, it makes the structure in natural signals explicit; third, it represents complex data in a way that is easier to read out at subsequent levels of processing; and fourth, it saves energy. Recent physiological recordings from sensory neurons have indicated that sparse coding could be a ubiquitous strategy employed in several different modalities across different organisms."
58,18,9701,1,Action recognition in the premotor cortex,"We recorded electrical activity from 532 neurons in the rostral part of inferior area 6 (area F5) of two macaque monkeys. Previous data had shown that neurons of this area discharge during goal-directed hand and mouth movements. We describe here the properties of a newly discovered set of F5 neurons ( mirror neurons', n = 92) all of which became active both when the monkey performed a given action and when it observed a similar action performed by the experimenter. Mirror neurons, in order to be visually triggered, required an interaction between the agent of the action and the object of it. The sight of the agent alone or of the object alone (three-dimensional objects, food) were ineffective. Hand and the mouth were by far the most effective agents. The actions most represented among those activating mirror neurons were grasping, manipulating and placing. In most mirror neurons (92%) there was a clear relation between the visual action they responded to and the motor response they coded. In [~]30% of mirror neurons the congruence was very strict and the effective observed and executed actions corresponded both in terms of general action (e.g. grasping) and in terms of the way in which that action was executed (e.g. precision grip). We conclude by proposing that mirror neurons form a system for matching observation and execution of motor actions. We discuss the possible role of this system in action recognition and, given the proposed homology between F5 and human Brocca's region, we posit that a matching system, similar to that of mirror neurons exists in humans and could be involved in recognition of actions as well as phonetic gestures. 10.1093/brain/119.2.593"
59,19,12463,1,Does it matter who contributes: a study on featured articles in the german wikipedia,"The considerable high quality of Wikipedia articles is often accredited to the large number of users who contribute to Wikipediaâs encyclopedia articles, who watch articles and correct errors immediately. In this paper, we are in particu- lar interested in a certain type of Wikipedia articles, namely, the featured articles â articles marked by a communityâs vote as being of outstanding quality. The German Wikipedia has the nice property that it has two types of featured articles: excellent and worth reading. We explore on the German Wikipedia whether only the mere number of contributors makes the difference or whether the high quality of featured articles results from having experienced authors contribut- ing with a reputation for high quality contributions. Our results indicate that it does matter who contributes."
60,19,13943,1,Harvana: harvesting community tags to enrich collection metadata,"Collaborative, social tagging and annotation systems have exploded on the Internet as part of the Web 2.0 phenomenon. Systems such as Flickr, Del.icio.us, Technorati, Connotea and LibraryThing, provide a community-driven approach to classifying information and resources on the Web, so that they can be browsed, discovered and re-used. Although social tagging sites provide simple, user-relevant tags, there are issues associated with the quality of the metadata and the scalability compared with conventional indexing systems. In this paper we propose a hybrid approach that enables authoritative metadata generated by traditional cataloguing methods to be merged with community annotations and tags. The HarvANA (Harvesting and Aggregating Networked Annotations) system uses a standardized but extensible RDF model for representing the annotations/tags and OAI-PMH to harvest the annotations/tags from distributed community servers. The harvested annotations are aggregated with the authoritative metadata in a centralized metadata store. This streamlined, interoperable, scalable approach enables libraries, archives and repositories to leverage community enthusiasm for tagging and annotation, augment their metadata and enhance their discovery services. This paper describes the HarvANA system and its evaluation through a collaborative testbed with the National Library of Australia using architectural images from PictureAustralia."
61,20,21,1,Functional discovery via a compendium of expression profiles.,"Ascertaining the impact of uncharacterized perturbations on the cell is a fundamental problem in biology. Here, we describe how a single assay can be used to monitor hundreds of different cellular functions simultaneously. We constructed a reference database or âcompendiumâ of expression profiles corresponding to 300 diverse mutations and chemical treatments in S. cerevisiae , and we show that the cellular pathways affected can be determined by pattern matching, even among very subtle profiles. The utility of this approach is validated by examining profiles caused by deletions of uncharacterized genes: we identify and experimentally confirm that eight uncharacterized open reading frames encode proteins required for sterol metabolism, cell wall function, mitochondrial respiration, or protein synthesis. We also show that the compendium can be used to characterize pharmacological perturbations by identifying a novel target of the commonly used drug dyclonine."
62,20,995,1,Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays,"Oligonucleotide arrays can provide a broad picture of the state of the cell, by monitoring the expression level of thousands of genes at the same time. It is of interest to develop techniques for extracting useful information from the resulting data sets. Here we report the application of a two-way clustering method for analyzing a data set consisting of the expression patterns of different cell types. Gene expression in 40 tumor and 22 normal colon tissue samples was analyzed with an Affymetrix oligonucleotide array complementary to more than 6,500 human genes. An efficient two-way clustering algorithm was applied to both the genes and the tissues, revealing broad coherent patterns that suggest a high degree of organization underlying gene expression in these tissues. Coregulated families of genes clustered together, as demonstrated for the ribosomal proteins. Clustering also separated cancerous from noncancerous tissue and cell lines from in vivo tissues on the basis of subtle distributed patterns of genes even when expression of individual genes varied only slightly between the tissues. Two-way clustering thus may be of use both in classifying genes into functional groups and in classifying tissues based on gene expression."
63,20,1195,1,The Nature of Statistical Learning Theory,"The aim of this book is to discuss the fundamental ideas which lie behind the statistical theory of learning and generalization. It considers learning as a general problem of function estimation based on empirical data. Omitting proofs and technical details, the author concentrates on discussing the main results of learning theory and their connections to fundamental problems in statistics. These include: * the setting of learning problems based on the model of minimizing the risk functional from empirical data * a comprehensive analysis of the empirical risk minimization principle including necessary and sufficient conditions for its consistency * non-asymptotic bounds for the risk achieved using the empirical risk minimization principle * principles for controlling the generalization ability of learning machines using small sample sizes based on these bounds * the Support Vector methods that control the generalization ability when estimating function using small sample size. The second edition of the book contains three new chapters devoted to further development of the learning theory and SVM techniques. These include: * the theory of direct method of learning based on solving multidimensional integral equations for density, conditional probability, and conditional density estimation * a new inductive principle of learning. Written in a readable and concise style, the book is intended for statisticians, mathematicians, physicists, and computer scientists. Vladimir N. Vapnik is Technology Leader AT&T Labs-Research and Professor of London University. He is one of the founders of statistical learning theory, and the author of seven books published in English, Russian, German, and Chinese."
64,20,2167,1,Statistical Pattern Recognition: A Review,"The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field."
65,20,3247,1,From Ukkonen to McCreight and Weiner: A Unifying View of Linear-Time Suffix Tree Construction,". We review the linear time suffix tree constructions by Weiner, McCreight, and Ukkonen. We use the terminology of the most recent algorithm, Ukkonen's online construction, to explain its historic predecessors. This reveals relationships much closer than one would expect, since the three algorithms are based on rather different intuitive ideas. Moreover, it completely explains the differences between these algorithms in terms of simplicity, efficiency, and implementation complexity.  Key Words. ..."
66,20,5045,1,GTM: The Generative Topographic Mapping,"Accepted for publication in Neural Computation. Latent variable models represent the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. A familiar example is factor analysis which is based on a linear transformations between the latent space and the data space. In this paper we introduce a form of non-linear latent variable model called the Generative Topographic Mapping for which the parameters of the model can be determined using the EM algorithm. GTM provides a principled alternative to the widely used Self-Organizing Map (SOM) of Kohonen (1982), and overcomes most of the significant limitations of the SOM. We demonstrate the performance of the GTM algorithm on a toy problem and on simulated data from flow diagnostics for a multi-phase oil pipeline. GTM: The Generative Topographic Mapping 2"
67,20,5620,1,Artificial Ant Colonies in Digital Image Habitats - A Mass Behaviour Effect Study on Pattern Recognition,"Some recent studies have pointed that, the self-organization of neurons into brain-like structures, and the self-organization of ants into a swarm are similar in many respects. If possible to implement, these features could lead to important developments in pattern recognition systems, where perceptive capabilities can emerge and evolve from the interaction of many simple local rules. The principle of the method is inspired by the work of Chialvo and Millonas who developed the first numerical simulation in which swarm cognitive map formation could be explained. From this point, an extended model is presented in order to deal with digital image habitats, in which artificial ants could be able to react to the environment and perceive it. Evolution of pheromone fields point that artificial ant colonies could react and adapt appropriately to any type of digital habitat. KEYWORDS: Swarm Intelligence, Self-Organization, Stigmergy, Artificial Ant Systems, Pattern Recognition and Perception, Image Segmentation, Gestalt Perception Theory, Distributed Computation."
68,20,5628,1,ANTIDS: Self-Organized Ant-based Clustering Model for Intrusion Detection System,"Security of computers and the networks that connect them is increasingly becoming of great significance. Computer security is defined as the protection of computing systems against threats to confidentiality, integrity, and availability. There are two types of intruders: the external intruders who are unauthorized users of the machines they attack, and internal intruders, who have permission to access the system with some restrictions. Due to the fact that it is more and more improbable to a system administrator to recognize and manually intervene to stop an attack, there is an increasing recognition that ID systems should have a lot to earn on following its basic principles on the behavior of complex natural systems, namely in what refers to self-organization, allowing for a real distributed and collective perception of this phenomena. With that aim in mind, the present work presents a self-organized ant colony based intrusion detection system (ANTIDS) to detect intrusions in a network infrastructure. The performance is compared among conventional soft computing paradigms like Decision Trees, Support Vector Machines and Linear Genetic Programming to model fast, online and efficient intrusion detection systems."
69,20,5636,1,"On Self-Regulated Swarms, Societal Memory, Speed and Dynamics","Wasps, bees, ants and termites all make effective use of their environment and resources by displaying collective ""swarm"" intelligence. Termite colonies - for instance - build nests with a complexity far beyond the comprehension of the individual termite, while ant colonies dynamically allocate labor to various vital tasks such as foraging or defense without any central decision-making ability. Recent research suggests that microbial life can be even richer: highly social, intricately networked, and teeming with interactions, as found in bacteria. What strikes from these observations is that both ant colonies and bacteria have similar natural mechanisms based on Stigmergy and Self-Organization in order to emerge coherent and sophisticated patterns of global foraging behavior. Keeping in mind the above characteristics we propose a Self-Regulated Swarm (SRS) algorithm which hybridizes the advantageous characteristics of Swarm Intelligence as the emergence of a societal environmental memory or cognitive map via collective pheromone laying in the landscape (properly balancing the exploration/exploitation nature of our dynamic search strategy), with a simple Evolutionary mechanism that trough a direct reproduction procedure linked to local environmental features is able to self-regulate the above exploratory swarm population, speeding it up globally. In order to test his adaptive response and robustness, we have recurred to different dynamic multimodal complex functions as well as to Dynamic Optimization Control problems, measuring reaction speeds and performance. Final comparisons were made with standard Genetic Algorithms (GAs), Bacterial Foraging strategies (BFOA), as well as with recent Co-Evolutionary approaches. SRS's were able to demonstrate quick adaptive responses, while outperforming the results obtained by the other approaches. Additionally, some successful behaviors were found: SRS was able to maintain a number of different solutions, while adapting to unforeseen situations even when over the same cooperative foraging period, the community is requested to deal with two different and contradictory purposes; the possibility to spontaneously create and maintain different subpopulations on different peaks, emerging different exploratory corridors with intelligent path planning capabilities; the ability to request for new agents (division of labor) over dramatic changing periods, and economizing those foraging resources over periods of intermediate stabilization. Finally, results illustrate that the present SRS collective swarm of bio-inspired ant-like agents is able to track about 65% of moving peaks traveling up to ten times faster than the velocity of a single individual composing that precise swarm tracking system. This emerged behavior is probably one of the most interesting ones achieved by the present work."
70,20,5881,1,Image metrics in the statistical analysis of DNA microarray data,"DNA microarrays represent an important new method for determining the complete expression profile of a cell. In ``spotted'' microarrays, slides carrying spots of target DNA are hybridized to fluorescently labeled cDNA from experimental and control cells and the arrays are imaged at two or more wavelengths. In this paper, we perform statistical analysis on images of microarrays and show that quantitating the amount of fluorescent DNA bound to microarrays is subject to considerable uncertainty because of large and small-scale intensity fluctuations within spots, nonadditive background, and fabrication artifacts. Pixel-by-pixel analysis of individual spots can be used to estimate these sources of error and establish the precision and accuracy with which gene expression ratios are determined. Simple weighting schemes based on these estimates are effective in improving significantly the quality of microarray data as it accumulates in a multiexperiment database. We propose that error estimates from image-based metrics should be one component in an explicitly probabilistic scheme for the analysis of DNA microarray data."
71,20,5889,1,Principal components analysis to summarize microarray experiments: application to sporulation time series,"A series of microarray experiments produces observations of differential expression for thousands of genes across multiple conditions. It is often not clear whether a set of experiments are measuring fundamentally different gene expression states or are measuring similar states created through different mechanisms. It is useful, therefore, to define a core set of independent features for the expression states that allow them to be compared directly. Principal components analysis (PCA) is a statistical technique for determining the key variables in a multidimensional data set that explain the differences in the observations, and can be used to simplify the analysis and visualization of multidimensional data sets. We show that application of PCA to expression data (where the experimental conditions are the variables, and the gene expression measurements are the observations) allows us to summarize the ways in which gene responses vary under different conditions. Examination of the components also provides insight into the underlying factors that are measured in the experiments. We applied PCA to the publicly released yeast sporulation data set (Chu et al. 1998). In that work, 7 different measurements of gene expression were made over time. PCA on the time-points suggests that much of the observed variability in the experiment can be summarized in just 2 components--i.e. 2 variables capture most of the information. These components appear to represent (1) overall induction level and (2) change in induction level over time. We also examined the clusters proposed in the original paper, and show how they are manifested in principal component space. Our results are available on the internet at http: inverted question markwww.smi.stanford.edu/project/helix/PCArray ."
72,20,5905,1,Practical Methods of Optimization,"Fully describes optimization methods that are currently most valuable in solving real-life problems. Since optimization has applications in almost every branch of science and technology, the text emphasizes their practical aspects in conjunction with the heuristics useful in making them perform more reliably and efficiently. To this end, it presents comparative numerical studies to give readers a feel for possibile applications and to illustrate the problems in assessing evidence. Also provides theoretical background which provides insights into how methods are derived. This edition offers revised coverage of basic theory and standard techniques, with updated discussions of line search methods, Newton and quasi-Newton methods, and conjugate direction methods, as well as a comprehensive treatment of restricted step or trust region methods not commonly found in the literature. Also includes recent developments in hybrid methods for nonlinear least squares; an extended discussion of linear programming, with new methods for stable updating of LU factors; and a completely new section on network programming. Chapters include computer subroutines, worked examples, and study questions."
73,20,10364,1,Clustering by Passing Messages Between Data Points,"Clustering data by identifying a subset of representative examples is important for processing sensory signals and detecting patterns in data. Such {\tt{}""{}}exemplars{\tt{}""{}} can be found by randomly choosing an initial subset of data points and then iteratively refining it, but this works well only if that initial choice is close to a good solution. We devised a method called {\tt{}""{}}affinity propagation,{\tt{}""{}} which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges. We used affinity propagation to cluster images of faces, detect genes in microarray data, identify representative sentences in this manuscript, and identify cities that are efficiently accessed by airline travel. Affinity propagation found clusters with much lower error than other methods, and it did so in less than one-hundredth the amount of time."
74,21,1006,1,Cluster analysis and display of genome-wide expression patterns,"A system of cluster analysis for genome-wide expression data from DNA microarray hybridization is described that uses standard statistical algorithms to arrange genes according to similarity in pattern of gene expression. The output is displayed graphically, conveying the clustering and the underlying expression data simultaneously in a form intuitive for biologists. We have found in the budding yeast Saccharomyces cerevisiae that clustering gene expression data groups together efficiently genes of known similar function, and we find a similar tendency in human data. Thus patterns seen in genome-wide expression experiments can be interpreted as indications of the status of cellular processes. Also, coexpression of genes of known function with poorly characterized or novel genes may provide a simple means of gaining leads to the functions of many genes for which information is not available currently."
75,21,1211,1,Bayesian network classifiers,"Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state-of-the-art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we evaluate approaches for inducing classifiers from data, based on the theory of learning Bayesian networks. These networks are factored representations of probability distributions that generalize the naive Bayesian classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes. We experimentally tested these approaches, using problems from the University of California at Irvine repository, and compared them to C4.5, naive Bayes, and wrapper methods for feature selection."
76,21,1665,1,Improving support vector machine classifiers by modifying kernel functions,"We propose a method of modifying a kernel function to improve the performance of a support  vector machine classifier. This is based on the Riemannian geometrical structure induced by the  kernel function. The idea is to enlarge the spatial resolution around the separating boundary  surface by a conformal mapping such that the separability between classes is increased. Examples  are given specifically for modifying Gaussian Radial Basis Function kernels. Simulation results for  both artificial and real data show remarkable improvement of generalization errors, supporting our  idea.  1 Introduction  Support Vector Machine (SVM) is a new promising pattern classification technique proposed recently by Vapnik and co-workers (Boser et al., 1992, Cortes and Vapnik, 1995, and Vapnik, 1995). Unlike traditional methods which minimize the empirical training error, SVM aims at minimizing an upper bound of the generalization error through maximizing the margin between the separating hyperplane and..."
77,21,2417,1,Normalization for cDNA microarray data: a robust composite method addressing single and multiple slide systematic variation,"There are many sources of systematic variation in cDNA microarray experiments which affect the measured gene expression levels (e.g. differences in labeling efficiency between the two fluorescent dyes). The term normalization refers to the process of removing such variation. A constant adjustment is often used to force the distribution of the intensity log ratios to have a median of zero for each slide. However, such global normalization approaches are not adequate in situations where dye biases can depend on spot overall intensity and/or spatial location within the array. This article proposes normalization methods that are based on robust local regression and account for intensity and spatial dependence in dye biases for different types of cDNA microarray experiments. The selection of appropriate controls for normalization is discussed and a novel set of controls (microarray sample pool, MSP) is introduced to aid in intensity-dependent normalization. Lastly, to allow for comparisons of expression levels across slides, a robust method based on maximum likelihood estimation is proposed to adjust for scale differences among slides."
78,21,3368,1,Array comparative genomic hybridization and its applications in cancer.,"Alteration in DNA copy number is one of the many ways in which gene expression and function may be modified. Some variations are found among normal individuals, others occur in the course of normal processes in some species and still others participate in causing various disease states. For example, many defects in human development are due to gains and losses of chromosomes and chromosomal segments that occur before or shortly after fertilization, and DNA dosage-alteration changes occurring in somatic cells are frequent contributors to cancer. Detecting these aberrations and interpreting them in the context of broader knowledge facilitates the identification of crucial genes and pathways involved in biological processes and disease. Over the past several years, array comparative genomic hybridization has proven its value for analyzing DNA copy-number variations. Here, we discuss the state of the art of array comparative genomic hybridization and its applications in cancer, emphasizing general concepts rather than specific results."
79,21,5877,1,DNA methylation and human disease," DNA methylation is a crucial epigenetic modification of the genome that is involved in regulating many cellular processes. These include embryonic development, transcription, chromatin structure, X chromosome inactivation, genomic imprinting and chromosome stability. Consistent with these important roles, a growing number of human diseases have been found to be associated with aberrant DNA methylation. The study of these diseases has provided new and fundamental insights into the roles that DNA methylation and other epigenetic modifications have in development and normal cellular homeostasis."
80,21,5893,1,High density synthetic oligonucleotide arrays,"Experimental genomics involves taking advantage of sequence information to investigate and understand the workings of genes, cells and organisms. We have developed an approach in which sequence information is used directly to design high- density, two-dimensional arrays of synthetic oligonucleotides. The GeneChip(R) probe arrays are made using spatially patterned, light-directed combinatorial chemical synthesis, and contain up to hundreds of thousands of different oligonucleotides on a small glass surface. The arrays have been designed and used for quantitative and highly parallel measurements of gene expression, to discover polymorphic loci and to detect the presence of thousands of alternative alleles. Here, we describe the fabrication of the arrays, their design and some specific: applications to high-throughput genetic and cellular analysis."
81,21,5901,1,{B}ayesian Interpolation,"Although Bayesian analysis has been in use since Laplace, the Bayesian method of model--comparison has only recently been developed in depth.  In this paper, the Bayesian approach to regularisation and model--comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other problems.  Regularising constants are set by examining their posterior probability distribution. Alternative regularisers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. `Occam&#039;s razor&#039; is automatically embodied by this framework.  The way in which Bayes infers the values of regularising constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling.  1 Data modelling and Occam&#039;s razor In science, a central task is to develop and compare models to a..."
82,22,287,1,Freenet: a distributed anonymous information storage and retrieval system,"Abstract. We describe Freenet, an adaptive peer-to-peer network application that permits the publication, replication, and retrieval of data while protecting the anonymity of both authors and readers. Freenet operates as a network of identical nodes that collectively pool their storage space to store data files and cooperate to route requests to the most likely physical location of data. No broadcast search or centralized location index is employed. Files are referred to in a location-independent manner, and are dynamically replicated in locations near requestors and deleted from locations where there is no interest. It is infeasible to discover the true origin or destination of a file passing through the network, and difficult for a node operator to determine or be held responsible for the actual physical contents of her own node. 1"
83,22,911,1,Tor: The Second-Generation Onion Router,"We present Tor, a circuit-based low-latency anonymous communication service. This second-generation Onion Routing system addresses limitations in the original design by adding perfect forward secrecy, congestion control, directory servers, integrity checking, configurable exit policies, and a practical design for location-hidden services via rendezvous points. Tor works on the real-world Internet, requires no special privileges or kernel modifications, requires little synchronization or coordination between nodes, and provides a reasonable tradeoff between anonymity, usability, and efficiency. We briefly describe our experiences with an international network of more than 30 nodes. We close with a list of open problems in anonymous communication."
84,22,1372,1,An Analysis of Internet Content Delivery Systems,"In the span of only a few years, the Internet has experienced an astronomical increase in the use of specialized content delivery systems, such as content delivery networks and peer-to-peer file sharing systems. Therefore, an understanding of content delivery on the Internet now requires a detailed understanding of how these systems are used in practice. This paper examines content delivery from the point of view of four content delivery systems: HTTP web traffic, the Akamai content delivery network, and Kazaa and Gnutella peer-to-peer file sharing traffic. We collected a trace of all incoming and outgoing network traffic at the University of Washington, a large university with over 60,000 students, faculty, and staff. From this trace, we isolated and characterized traffic belonging to each of these four delivery classes. Our results (1) quantify the rapidly increasing importance of new content delivery systems, particularly peerto-peer networks, (2) characterize the behavior of these systems from the perspectives of clients, objects, and servers, and (3) derive implications for caching in these systems. 1"
85,22,2347,1,"Efficient dispersal of information for security, load balancing, and fault tolerance","An Information Dispersal Algorithm (IDA) is developed that breaks a file  F  of length  L  = &uharl;  F &uharr; into  n  pieces  F i , l &le;  i  &le;  n , each of length &uharl; F i &uharr; =  L / m , so that every  m  pieces suffice for reconstructing  F . Dispersal and reconstruction are computationally efficient. The sum of the lengths &uharl; F i &uharr; is ( n / m ) &middot;  L . Since  n / m  can be chosen to be close to l, the IDA is space efficient.  IDA has numerous applications to secure and reliable storage of information in computer networks and even on single disks, to fault-tolerant and efficient transmission of information in networks, and to communications between processors in parallel computers. For the latter problem provably time-efficient and highly fault-tolerant routing on the  n -cube is achieved, using just constant size buffers."
86,22,3227,1,Computer Networking: A Top-Down Approach Featuring the Internet,"{Certain data-communication protocols hog the spotlight, but all of them have a lot in common. <I>Computer Networking: A Top-Down Approach Featuring the Internet</I> explains the engineering problems that are inherent in communicating digital information from point to point. The top-down approach mentioned in the subtitle means that the book starts at the top of the protocol stack--at the application layer--and works its way down through the other layers, until it reaches bare wire. <p>  The authors, for the most part, shun the well-known seven-layer Open Systems Interconnection (OSI) protocol stack in favor of their own five-layer (application, transport, network, link, and physical) model. It's an effective approach that helps clear away some of the hand waving traditionally associated with the more obtuse layers in the OSI model. The approach is definitely theoretical--don't look here for instructions on configuring Windows 2000 or a Cisco router--but it's relevant to reality, and should help anyone who needs to understand networking as a programmer, system architect, or even administration guru.<p>  The treatment of the network layer, at which routing takes place, is typical of the overall style. In discussing routing, authors James Kurose and Keith Ross explain (by way of lots of clear, definition-packed text) what routing protocols need to do: find the best route to a destination. Then they present the mathematics that determine the best path, show some code that implements those algorithms, and illustrate the logic by using excellent conceptual diagrams. Real-life implementations of the algorithms--including Internet Protocol (both IPv4 and IPv6) and several popular IP routing protocols--help you to make the transition from pure theory to networking technologies. <I>--David Wall</I><p>  <B>Topics covered:</B> The theory behind data networks, with thorough discussion of the problems that are posed at each level (the application layer gets plenty of attention). For each layer, there's academic coverage of networking problems and solutions, followed by discussion of real technologies. Special sections deal with network security and transmission of digital multimedia. } { The most up-to-date introduction to the field of computer networking, this book's top-down approach starts at the application layer and works down the protocol stack. It also uses the Internet as the main example of networks.  This all creates a book relevant to those interested in networking today.  By starting at the application-layer and working down the protocol stack, this book provides a relevant introduction of important concepts. Based on the rationale that once a reader understands the applications of networks they can understand the network services needed to support these applications, this book takes a ""top-down"" approach that exposes readers first to a concrete application and then draws into some of the deeper issues surrounding networking.  This book focuses on the Internet as opposed to addressing it as one of many computer network technologies, further motivating the study of the material.  This book is designed for programmers who need to learn the fundamentals of computer networking. It also has extensive material making it of great interest to networking professionals. }"
87,22,4463,1,Making gnutella-like P2P systems scalable,"Napster pioneered the idea of peer-to-peer file sharing, and supported it with a centralized file search facility. Subsequent P2P systems like Gnutella adopted decentralized search algorithms. However, Gnutella&#039;s notoriously poor scaling led some to propose distributed hash table solutions to the wide-area file search problem. Contrary to that trend, we advocate retaining Gnutella&#039;s simplicity while proposing new mechanisms that greatly improve its scalability. Building upon prior research [1, 12, 22], we propose several modifications to Gnutella&#039;s design that dynamically adapt the overlay topology and the search algorithms in order to accommodate the natural heterogeneity present in most peer-to-peer systems. We test our design through simulations and the results show three to five orders of magnitude improvement in total system capacity. We also report on a prototype implementation and its deployment on a testbed. Categories and Subject Descriptors  C.2 [Computer Communication Networks]: Distributed Systems  General Terms  Algorithms, Design, Performance, Experimentation  Keywords  Peer-to-peer, distributed hash tables, Gnutella  1."
88,22,7402,1,"Untraceable electronic mail, return addresses, and digital pseudonyms","A technique based on public key cryptography ispresented that allows an electronic mail system to hidewho a participant communicates with as well as thecontent of the communication--in spite of an unsecuredunderlying telecommunication system. The techniquedoes not require a universally trusted authority. Onecorrespondent can remain anonymous to a second, whileallowing the second to respond via an untraceble returnaddress.The technique can also be used to form rosters ofuntraceable digital pseudonyms from selected applications.Applicants retain the exclusive ability to formdigital signatures corresponding to their pseudonyms.Elections in which any interested party can verify thatthe ballots have been properly counted are possible ifanonymously mailed ballots are signed with pseudonymsfrom a roster of registered voters. Another use allows anindividual to correspond with a record-keeping organizationunder a unique pseudonym which appears in aroster of acceptable clients."
89,22,9760,1,Analyzing and Improving a BitTorrent Network's Performance Mechanisms,"In recent years, BitTorrent has emerged as a very scalable peer-to-peer file distribution mechanism. While early measurement and analytical studies have verified BitTorrentâs performance, they have also raised questions about various metrics (upload utilization, fairness, etc.), particularly in settings other than those measured. In this paper, we present a simulation-based study of BitTorrent. Our goal is to deconstruct the system and evaluate the impact of its core mechanisms, both individually and in combination, on overall system performance under a variety of workloads. Our evaluation focuses on several important metrics, including peer link utilization, file download time, and fairness amongst peers in terms of volume of content served. Our results confirm that BitTorrent performs near-optimally in terms of uplink bandwidth utilization, and download time except under certain extreme conditions. We also show that low bandwidth peers can download more than they upload to the network when high bandwidth peers are present. We find that the rate-based tit-for-tat policy is not effective in preventing unfairness. We show how simple changes to the tracker and a stricter, block-based tit-for-tat policy, greatly improves fairness."
90,23,768,1,Basic local alignment search tool.,"{A new approach to rapid sequence comparison, basic local alignment search tool (BLAST), directly approximates alignments that optimize a measure of local similarity, the maximal segment pair (MSP) score. Recent mathematical results on the stochastic properties of MSP scores allow an analysis of the performance of this method as well as the statistical significance of alignments it generates. The basic algorithm is simple and robust; it can be implemented in a number of ways and applied in a variety of contexts including straightforward DNA and protein sequence database searches, motif searches, gene identification searches, and in the analysis of multiple regions of similarity in long DNA sequences. In addition to its flexibility and tractability to mathematical analysis, BLAST is an order of magnitude faster than existing sequence comparison tools of comparable sensitivity.}"
91,23,2194,1,A Whole-Genome Assembly of Drosophila,"We report on the quality of a whole-genome assembly of Drosophila melanogaster and the nature of the computer algorithms that accomplished it. Three independent external data sources essentially agree with and support the assembly's sequence and ordering of contigs across the euchromatic portion of the genome. In addition, there are isolated contigs that we believe represent nonrepetitive pockets within the heterochromatin of the centromeres. Comparison with a previously sequenced 2.9- megabase region indicates that sequencing accuracy within nonrepetitive segments is greater than 99. 99% without manual curation. As such, this initial reconstruction of the Drosophila sequence should be of substantial value to the scientific community."
92,23,3963,1,Modeling gene and genome duplications in eukaryotes,"10.1073/pnas.0501102102 Recent analysis of complete eukaryotic genome sequences has revealed that gene duplication has been rampant. Moreover, next to a continuous mode of gene duplication, in many eukaryotic organisms the complete genome has been duplicated in their evolutionary past. Such large-scale gene duplication events have been associated with important evolutionary transitions or major leaps in development and adaptive radiations of species. Here, we present an evolutionary model that simulates the duplication dynamics of genes, considering genome-wide duplication events and a continuous mode of gene duplication. Modeling the evolution of the different functional categories of genes assesses the importance of different duplication events for gene families involved in specific functions or processes. By applying our model to the  genome, for which there is compelling evidence for three whole-genome duplications, we show that gene loss is strikingly different for large-scale and small-scale duplication events and highly biased toward certain functional classes. We provide evidence that some categories of genes were almost exclusively expanded through large-scale gene duplication events. In particular, we show that the three whole-genome duplications in  have been directly responsible for >90% of the increase in transcription factors, signal transducers, and developmental genes in the last 350 million years. Our evolutionary model is widely applicable and can be used to evaluate different assumptions regarding small- or large-scale gene duplication events in eukaryotic genomes."
93,23,4649,1,A block-sorting lossless data compression algorithm.,"The charter of SRC is to advance both the state of knowledge and the state of the art in computer systems. From our establishment in 1984, we have performed basic and applied research to support Digitalâs business objectives. Our current work includes exploring distributed personal computing on multiple platforms, networking, programming technology, system modelling and management techniques, and selected applications. Our strategy is to test the technical and practical value of our ideas by building hardware and software prototypes and using them as daily tools. Interesting systems are too complex to be evaluated solely in the abstract; extended use allows us to investigate their properties in depth. This experience is useful in the short term in refining our designs, and invaluable in the long term in advancing our knowledge. Most of the major advances in information systems have come through this strategy, including personal computing, distributed systems, and the Internet. We also perform complementary work of a more mathematical flavor. Some of it is in established fields of theoretical computer science, such as the analysis of algorithms, computational geometry, and logics of programming. Other work explores new ground motivated by problems that arise in our systems research. We have a strong commitment to communicating our results; exposing and testing our ideas in the research and development communities leads to improved understanding. Our research report series supplements publication in professional journals and conferences. We seek users for our prototype systems among those with whom we have common interests, and we encourage collaboration with university researchers."
94,23,5167,1,ARACHNE: A Whole-Genome Shotgun Assembler,"We describe a new computer system, called ARACHNE, for assembling genome sequence using paired-end whole-genome shotgun reads. ARACHNE has several key features, including an efficient and sensitive procedure for finding read overlaps, a procedure for scoring overlaps that achieves high accuracy by correcting errors before assembly, read merger based on forward-reverse links, and detection of repeat contigs by forward-reverse link inconsistency. To test ARACHNE, we created simulated reads providing approximately 10-fold coverage of the genomes of H. influenzae, S. cerevisiae, and D. melanogaster, as well as human chromosomes 21 and 22. The assemblies of these simulated reads yielded nearly complete coverage of the respective genomes, with a small number of contigs joined into a smaller number of supercontigs (or scaffolds). For example, analysis of the D. melanogaster genome yielded approximately 98% coverage with an N50 contig length of 324 kb and an N50 supercontig length of 5143 kb. The assembly accuracy was high, although not perfect: small errors occurred at a frequency of roughly 1 per 1 Mb (typically, deletion of approximately 1 kb in size), with a very small number of other misassemblies. The assembly was rapid: the Drosophila assembly required only 21 hours on a single 667 MHz processor and used 8.4 Gb of memory."
95,23,6274,1,Essential genes of a minimal bacterium.,"Mycoplasma genitalium has the smallest genome of any organism that can be grown in pure culture. It has a minimal metabolism and little genomic redundancy. Consequently, its genome is expected to be a close approximation to the minimal set of genes needed to sustain bacterial life. Using global transposon mutagenesis, we isolated and characterized gene disruption mutants for 100 different nonessential protein-coding genes. None of the 43 RNA-coding genes were disrupted. Herein, we identify 382 of the 482 M. genitalium protein-coding genes as essential, plus five sets of disrupted genes that encode proteins with potentially redundant essential functions, such as phosphate transport. Genes encoding proteins of unknown function constitute 28\% of the essential protein-coding genes set. Disruption of some genes accelerated M. genitalium growth."
96,23,9322,1,Symbiosis insights through metagenomic analysis of a microbial consortium,"Symbioses between bacteria and eukaryotes are ubiquitous, yet our understanding of the interactions driving these associations is hampered by our inability to cultivate most host-associated microbes. Here we use a metagenomic approach to describe four co-occurring symbionts from the marine oligochaete Olavius algarvensis, a worm lacking a mouth, gut and nephridia. Shotgun sequencing and metabolic pathway reconstruction revealed that the symbionts are sulphur-oxidizing and sulphate-reducing bacteria, all of which are capable of carbon fixation, thus providing the host with multiple sources of nutrition. Molecular evidence for the uptake and recycling of worm waste products by the symbionts suggests how the worm could eliminate its excretory system, an adaptation unique among annelid worms. We propose a model that describes how the versatile metabolism within this symbiotic consortium provides the host with an optimal energy supply as it shuttles between the upper oxic and lower anoxic coastal sediments that it inhabits."
97,23,9906,1,Proteorhodopsin phototrophy in the ocean.,"Proteorhodopsin, a retinal-containing integral membrane protein that functions as a light-driven proton pump, was discovered in the genome of an uncultivated marine bacterium; however, the prevalence, expression and genetic variability of this protein in native marine microbial populations remain unknown. Here we report that photoactive proteorhodopsin is present in oceanic surface waters. We also provide evidence of an extensive family of globally distributed proteorhodopsin variants. The protein pigments comprising this rhodopsin family seem to be spectrally tuned to different habitats--absorbing light at different wavelengths in accordance with light available in the environment. Together, our data suggest that proteorhodopsin-based phototrophy is a globally significant oceanic microbial process."
98,23,10569,1,Metaproteomics approach to study the functionality of the microbiota in the human infant gastrointestinal tract.,"A metaproteomics approach comprising two-dimensional gel electrophoresis and matrix-assisted laser desorption ionization-time of flight (mass spectrometry) was applied to the largely uncultured infant fecal microbiota for the first time. The fecal microbial metaproteome profiles changed over time, and one protein spot contained a peptide sequence that showed high similarity to those of bifidobacterial transaldolases."
99,23,10717,1,Identifying bacterial genes and endosymbiont DNA with Glimmer,"Motivation: The Glimmer gene-finding software has been successfully used for finding genes in bacteria, archaea, and viruses representing hundreds of species. We describe several major changes to the Glimmer system, including improved methods for identifying both coding regions and start codons. We also describe a new module of Glimmer that can distinguish host and endosymbiont DNA. This module was developed in response to the discovery that eukaryotic genome sequencing projects sometimes inadvertently capture the DNA of intracellular bacteria living in the host.  Results: The new methods dramatically reduce the rate of false-positive predictions, while maintaining Glimmer's 99% sensitivity rate at detecting genes in most species, and they find substantially more correct start sites, as measured by comparisons to known and well-curated genes. We show that our interpolated Markov model (IMM) DNA discriminator correctly separated 99% of the sequences in a recent genome project that produced a mixture of sequences from the bacterium Prochloron didemni and its sea squirt host, Lissoclinum patella.  Availability: Glimmer is OSI Certified Open Source and available at http://cbcb.umd.edu/software/glimmer 10.1093/bioinformatics/btm009"
100,23,10951,1,Comparative analysis indicates regulatory neofunctionalization of yeast duplicates.,"ABSTRACT: BACKGROUND: Gene duplication provides raw material for the generation of new functions, but most duplicates are rapidly lost due to the initial redundancy in gene function. How gene function diversifies following duplication is largely unclear. Previous studies analyzed the diversification of duplicates by characterizing their coding sequence divergence. However, functional divergence can also be attributed to changes in regulatory properties, such as protein localization or expression, which require only minor changes in gene sequence. RESULTS: We developed a novel method to compare expression profiles from different organisms and applied it to analyze the expression divergence of yeast duplicated genes. The expression profiles of Saccharomyces cerevisiae duplicate pairs were compared with those of their pre-duplication orthologs in Candida albicans. Duplicate pairs were classified into two classes, corresponding to symmetric versus asymmetric rates of expression divergence. The latter class includes 43 duplicate pairs in which only one copy has a significant expression similarity to the C. albicans ortholog. These may present cases of regulatory neofunctionalization, as supported also by their dispensability and variability. CONCLUSION: Duplicated genes may diversify through regulatory neofunctionalization. Notably, the asymmetry of gene sequence evolution and the asymmetry of gene expression evolution are only weakly correlated, underscoring the importance of expression analysis to elucidate the evolution of novel functions."
101,23,11957,1,"Whole-genome sequencing and assembly with high-throughput, short-read technologies.","While recently developed short-read sequencing technologies may dramatically reduce the sequencing cost and eventually achieve the $1000 goal for re-sequencing, their limitations prevent the de novo sequencing of eukaryotic genomes with the standard shotgun sequencing protocol. We present SHRAP (SHort Read Assembly Protocol), a sequencing protocol and assembly methodology that utilizes high-throughput short-read technologies. We describe a variation on hierarchical sequencing with two crucial differences: (1) we select a clone library from the genome randomly rather than as a tiling path and (2) we sample clones from the genome at high coverage and reads from the clones at low coverage. We assume that 200 bp read lengths with a 1% error rate and inexpensive random fragment cloning on whole mammalian genomes is feasible. Our assembly methodology is based on first ordering the clones and subsequently performing read assembly in three stages: (1) local assemblies of regions significantly smaller than a clone size, (2) clone-sized assemblies of the results of stage 1, and (3) chromosome-sized assemblies. By aggressively localizing the assembly problem during the first stage, our method succeeds in assembling short, unpaired reads sampled from repetitive genomes. We tested our assembler using simulated reads from D. melanogaster and human chromosomes 1, 11, and 21, and produced assemblies with large sets of contiguous sequence and a misassembly rate comparable to other draft assemblies. Tested on D. melanogaster and the entire human genome, our clone-ordering method produces accurate maps, thereby localizing fragment assembly and enabling the parallelization of the subsequent steps of our pipeline. Thus, we have demonstrated that truly inexpensive de novo sequencing of mammalian genomes will soon be possible with high-throughput, short-read technologies using our methodology."
102,23,12644,1,Metagenomic and functional analysis of hindgut microbiota of a wood-feeding higher termite,"From the standpoints of both basic research and biotechnology, there is considerable interest in reaching a clearer understanding of the diversity of biological mechanisms employed during lignocellulose degradation. Globally, termites are an extremely successful group of wood-degrading organisms and are therefore important both for their roles in carbon turnover in the environment and as potential sources of biochemical catalysts for efforts aimed at converting wood into biofuels. Only recently have data supported any direct role for the symbiotic bacteria in the gut of the termite in cellulose and xylan hydrolysis. Here we use a metagenomic analysis of the bacterial community resident in the hindgut paunch of a wood-feeding 'higher' Nasutitermes species (which do not contain cellulose-fermenting protozoa) to show the presence of a large, diverse set of bacterial genes for cellulose and xylan hydrolysis. Many of these genes were expressed in vivo or had cellulase activity in vitro, and further analyses implicate spirochete and fibrobacter species in gut lignocellulose degradation. New insights into other important symbiotic functions including H2 metabolism, CO2-reductive acetogenesis and N2 fixation are also provided by this first system-wide gene analysis of a microbial community specialized towards plant lignocellulose degradation. Our results underscore how complex even a 1-microl environment can be."
103,23,13111,1,The impact of next-generation sequencing technology on genetics," If one accepts that the fundamental pursuit of genetics is to determine the genotypes that explain phenotypes, the meteoric increase of DNA sequence information applied toward that pursuit has nowhere to go but up. The recent introduction of instruments capable of producing millions of DNA sequence reads in a single run is rapidly changing the landscape of genetics, providing the ability to answer questions with heretofore unimaginable speed. These technologies will provide an inexpensive, genome-wide sequence readout as an endpoint to applications ranging from chromatin immunoprecipitation, mutation mapping and polymorphism discovery to noncoding RNA discovery. Here I survey next-generation sequencing technologies and consider how they can provide a more complete picture of how the genome shapes the organism."
104,23,13887,1,Historical contingency and the evolution of a key innovation in an experimental population of Escherichia coli,"10.1073/pnas.0803151105 The role of historical contingency in evolution has been much debated, but rarely tested. Twelve initially identical populations of Escherichia coli were founded in 1988 to investigate this issue. They have since evolved in a glucose-limited medium that also contains citrate, which E. coli cannot use as a carbon source under oxic conditions. No population evolved the capacity to exploit citrate for >30,000 generations, although each population tested billions of mutations. A citrate-using (Cit+) variant finally evolved in one population by 31,500 generations, causing an increase in population size and diversity. The long-delayed and unique evolution of this function might indicate the involvement of some extremely rare mutation. Alternately, it may involve an ordinary mutation, but one whose physical occurrence or phenotypic expression is contingent on prior mutations in that population. We tested these hypotheses in experiments that âreplayedâ evolution from different points in that population's history. We observed no Cit+ mutants among 8.4 Ã 1012 ancestral cells, nor among 9 Ã 1012 cells from 60 clones sampled in the first 15,000 generations. However, we observed a significantly greater tendency for later clones to evolve Cit+, indicating that some potentiating mutation arose by 20,000 generations. This potentiating change increased the mutation rate to Cit+ but did not cause generalized hypermutability. Thus, the evolution of this phenotype was contingent on the particular history of that population. More generally, we suggest that historical contingency is especially important when it facilitates the evolution of key innovations that are not easily evolved by gradual, cumulative selection."
105,23,14511,1,Efficient mapping of Applied Biosystems SOLiD sequence data to a reference genome for functional genomic applications,"Summary: Here, we report the development of SOCS (short oligonucleotide color space), a program designed for efficient and flexible mapping of Applied Biosystems SOLiD sequence data onto a reference genome. SOCS performs its mapping within the context of  color space', and it maximizes usable data by allowing a user-specified number of mismatches. Sequence census functions facilitate a variety of functional genomics applications, including transcriptome mapping and profiling, as well as ChIP-Seq.  Availability: Executables, source code, and sample data are available at http://socs.biology.gatech.edu/  Contact: nickbergman@gatech.edu  Supplementary information: Supplementary data are available at Bioinformatics Online. 10.1093/bioinformatics/btn512"
106,23,14704,1,Turning a hobby into a job: How duplicated genes find new functions,"Gene duplication provides raw material for functional innovation. Recent advances have shed light on two fundamental questions regarding gene duplication: which genes tend to undergo duplication? And how does natural selection subsequently act on them? Genomic data suggest that different gene classes tend to be retained after single-gene and whole-genome duplications. We also know that functional differences between duplicate genes can originate in several different ways, including mutations that directly impart new functions, subdivision of ancestral functions and selection for changes in gene dosage. Interestingly, in many cases the 'new' function of one copy is a secondary property that was always present, but that has been co-opted to a primary role after the duplication."
107,23,15031,1,Multiple whole-genome alignments without a reference organism,"10.1101/gr.081778.108 Multiple sequence alignments have become one of the most commonly used resources in genomics research. Most algorithms for multiple alignment of whole genomes rely either on a reference genome, against which all of the other sequences are laid out, or require a one-to-one mapping between the nucleotides of the genomes, preventing the alignment of recently duplicated regions. Both approaches have drawbacks for whole-genome comparisons. In this paper we present a novel symmetric alignment algorithm. The resulting alignments not only represent all of the genomes equally well, but also include all relevant duplications that occurred since the divergence from the last common ancestor. Our algorithm, implemented as a part of the VISTA Genome Pipeline (VGP), was used to align seven vertebrate and six  genomes. The resulting whole-genome alignments demonstrate a higher sensitivity and specificity than the pairwise alignments previously available through the VGP and have higher exon alignment accuracy than comparable public whole-genome alignments. Of the multiple alignment methods tested, ours performed the best at aligning genes from multigene familiesâperhaps the most challenging test for whole-genome alignments. Our whole-genome multiple alignments are available through the VISTA Browser at ."
108,23,15413,1,"Assembling the marine metagenome, one cell at a time.","The difficulty associated with the cultivation of most microorganisms and the complexity of natural microbial assemblages, such as marine plankton or human microbiome, hinder genome reconstruction of representative taxa using cultivation or metagenomic approaches. Here we used an alternative, single cell sequencing approach to obtain high-quality genome assemblies of two uncultured, numerically significant marine microorganisms. We employed fluorescence-activated cell sorting and multiple displacement amplification to obtain hundreds of micrograms of genomic DNA from individual, uncultured cells of two marine flavobacteria from the Gulf of Maine that were phylogenetically distant from existing cultured strains. Shotgun sequencing and genome finishing yielded 1.9 Mbp in 17 contigs and 1.5 Mbp in 21 contigs for the two flavobacteria, with estimated genome recoveries of about 91% and 78%, respectively. Only 0.24% of the assembling sequences were contaminants and were removed from further analysis using rigorous quality control. In contrast to all cultured strains of marine flavobacteria, the two single cell genomes were excellent Global Ocean Sampling (GOS) metagenome fragment recruiters, demonstrating their numerical significance in the ocean. The geographic distribution of GOS recruits along the Northwest Atlantic coast coincided with ocean surface currents. Metabolic reconstruction indicated diverse potential energy sources, including biopolymer degradation, proteorhodopsin photometabolism, and hydrogen oxidation. Compared to cultured relatives, the two uncultured flavobacteria have small genome sizes, few non-coding nucleotides, and few paralogous genes, suggesting adaptations to narrow ecological niches. These features may have contributed to the abundance of the two taxa in specific regions of the ocean, and may have hindered their cultivation. We demonstrate the power of single cell DNA sequencing to generate reference genomes of uncultured taxa from a complex microbial community of marine bacterioplankton. A combination of single cell genomics and metagenomics enabled us to analyze the genome content, metabolic adaptations, and biogeography of these taxa."
109,23,16494,1,A human gut microbial gene catalogue established by metagenomic sequencing,"To understand the impact of gut microbes on human health and well-being it is crucial to assess their genetic potential. Here we describe the Illumina-based metagenomic sequencing, assembly and characterization of 3.3 million non-redundant microbial genes, derived from 576.7 gigabases of sequence, from faecal samples of 124 European individuals. The gene set, approximately 150 times larger than the human gene complement, contains an overwhelming majority of the prevalent (more frequent) microbial genes of the cohort and probably includes a large proportion of the prevalent human intestinal microbial genes. The genes are largely shared among individuals of the cohort. Over 99% of the genes are bacterial, indicating that the entire cohort harbours between 1,000 and 1,150 prevalent bacterial species and each individual at least 160 such species, which are also largely shared. We define and describe the minimal gut metagenome and the minimal gut bacterial genome in terms of functions present in all individuals and most bacteria, respectively."
110,24,29,1,The structure and function of complex networks,"Inspired by empirical studies of networked systems such as the Internet, social networks, and bio- logical networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks."
111,24,10945,1,Quantifying social group evolution,"The rich set of interactions between individuals in society1, 2, 3, 4, 5, 6, 7 results in complex community structure, capturing highly connected circles of friends, families or professional cliques in a social network3, 7, 8, 9, 10. Thanks to frequent changes in the activity and communication patterns of individuals, the associated social and communication network is subject to constant evolution7, 11, 12, 13, 14, 15, 16. Our knowledge of the mechanisms governing the underlying community dynamics is limited, but is essential for a deeper understanding of the development and self-optimization of society as a whole17, 18, 19, 20, 21, 22. We have developed an algorithm based on clique percolation23, 24 that allows us to investigate the time dependence of overlapping communities on a large scale, and thus uncover basic relationships characterizing community evolution. Our focus is on networks capturing the collaboration between scientists and the calls between mobile phone users. We find that large groups persist for longer if they are capable of dynamically altering their membership, suggesting that an ability to change the group composition results in better adaptability. The behaviour of small groups displays the opposite tendencyâthe condition for stability is that their composition remains unchanged. We also show that knowledge of the time commitment of members to a given community can be used for estimating the communityâs lifetime. These findings offer insight into the fundamental differences between the dynamics of small groups and large institutions."
112,24,15349,1,Extracting the multiscale backbone of complex weighted networks,"10.1073/pnas.0808904106 A large number of complex systems find a natural abstraction in the form of weighted networks whose nodes represent the elements of the system and the weighted edges identify the presence of an interaction and its relative strength. In recent years, the study of an increasing number of large-scale networks has highlighted the statistical heterogeneity of their interaction pattern, with degree and weight distributions that vary over many orders of magnitude. These features, along with the large number of elements and links, make the extraction of the truly relevant connections forming the network's backbone a very challenging problem. More specifically, coarse-graining approaches and filtering techniques come into conflict with the multiscale nature of large-scale systems. Here, we define a filtering method that offers a practical procedure to extract the relevant connection backbone in complex multiscale networks, preserving the edges that represent statistically significant deviations with respect to a null model for the local assignment of weights to edges. An important aspect of the method is that it does not belittle small-scale interactions and operates at all scales defined by the weight distribution. We apply our method to real-world network instances and compare the obtained results with alternative backbone extraction techniques."
113,25,827,1,A Practical Guide to Splines,"This book (seventh printing) is based on the author's experience with calculations involving polynomial splines. It presents those parts of the theory which are especially useful in calculations and stresses the representation of splines as linear combinations of B-splines.  After two chapters summarizing polynomial approximation, a rigorous discussion of elementary spline theory is given involving linear, cubic and parabolic splines. The computational handling of piecewise polynomial functions (of one variable) of arbitrary order is the subject of chapters 7 and 8, while chapters 9, 10, and 11 are devoted to B-splines. The distances from splines with fixed and with variable knots is discussed in chapter 12. The remaining five chapters concern specific approximation methods, interpolation, smoothing and least-squares approximation, the solution of an ordinary differential equation by collocation, curve fitting, and surface fitting."
114,25,2212,1,Convex Optimization,"Convex optimization problems arise frequently in many different fields. This book provides a comprehensive introduction to the subject, and shows in detail how such problems can be solved numerically with great efficiency. The book begins with the basic elements of convex sets and functions, and then describes various classes of convex optimization problems. Duality and approximation techniques are then covered, as are statistical estimation techniques. Various geometrical problems are then presented, and there is detailed discussion of unconstrained and constrained minimization problems, and interior-point methods. The focus of the book is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. It contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance and economics.  Ã¢â¬Â¢ Gives comprehensive details on how to recognize convex optimization problems in a wide variety of settings Ã¢â¬Â¢ Provides a broad range of practical algorithms for solving real problems Ã¢â¬Â¢ Contains hundreds of worked examples and homework exercises"
115,25,7556,1,Constrained model predictive control: stability and optimality,"Model predictive control is a form of control in which the current control action is obtained by solving, at each sampling instant, a finite horizon open-loop optimal control problem, using the current state of the plant as the initial state; the optimization yields an optimal control sequence and the first control in this sequence is applied to the plant. An important advantage of this type of control is its ability to cope with hard constraints on controls and states. It has, therefore, been widely applied in petro-chemical and related industries where satisfaction of constraints is particularly important because efficiency demands operating points on or close to the boundary of the set of admissible states and controls. In this review, we focus on model predictive control of constrained systems, both linear and nonlinear and discuss only briefly model predictive control of unconstrained nonlinear and/or time-varying systems. We concentrate our attention on research dealing with stability and optimality; in these areas the subject has developed, in our opinion, to a stage where it has achieved sufficient maturity to warrant the active interest of researchers in nonlinear control. We distill from an extensive literature essential principles that ensure stability and use these to present a concise characterization of most of the model predictive controllers that have been proposed in the literature. In some cases the finite horizon optimal control problem solved on-line is exactly equivalent to the same problem with an infinite horizon; in other cases it is equivalent to a modified infinite horizon optimal control problem. In both situations, known advantages of infinite horizon optimal control accrue."
116,26,5978,1,"GROMACS: Fast, flexible, and free","Abstract 10.1002/jcc.20291.abs This article describes the software suite GROMACS (Groningen MAchine for Chemical Simulation) that was developed at the University of Groningen, The Netherlands, in the early 1990s. The software, written in ANSI C, originates from a parallel hardware project, and is well suited for parallelization on processor clusters. By careful optimization of neighbor searching and of inner loop performance, GROMACS is a very fast program for molecular dynamics simulation. It does not have a force field of its own, but is compatible with GROMOS, OPLS, AMBER, and ENCAD force fields. In addition, it can handle polarizable shell models and flexible constraints. The program is versatile, as force routines can be added by the user, tabulated functions can be specified, and analyses can be easily customized. Nonequilibrium dynamics and free energy determinations are incorporated. Interfaces with popular quantum-chemical packages (MOPAC, GAMES-UK, GAUSSIAN) are provided to perform mixed MM/QM simulations. The package includes about 100 utility and analysis programs. GROMACS is in the public domain and distributed (with source code and documentation) under the GNU General Public License. It is maintained by a group of developers from the Universities of Groningen, Uppsala, and Stockholm, and the Max Planck Institute for Polymer Research in Mainz. Its Web site is http://www.gromacs.org. Â© 2005 Wiley Periodicals, Inc. J Comput Chem 26: 1701â1718, 2005"
117,26,12771,1,Dissipative particle dynamics: Bridging the gap between atomistic and mesoscopic simulation,"We critically review dissipative particle dynamics (DPD) as a mesoscopic simulation method. We have established useful parameter ranges for simulations, and have made a link between these parameters and -parameters in Flory-Huggins-type models. This is possible because the equation of state of the DPD fluid is essentially quadratic in density. This link opens the way to do large scale simulations, effectively describing millions of atoms, by firstly performing simulations of molecular fragments retaining all atomistic details to derive -parameters, then secondly using these results as input to a DPD simulation to study the formation of micelles, networks, mesophases and so forth. As an example application, we have calculated the interfacial tension between homopolymer melts as a function of and N and have found a universal scaling collapse when /kBT0.4 is plotted against N for N>1. We also discuss the use of DPD to simulate the dynamics of mesoscopic systems, and indicate a possible problem with the timescale separation between particle diffusion and momentum diffusion (viscosity). {\\copyright}1997 American Institute of Physics."
118,27,478,1,Emergence of scaling in random networks,"Recently retired as head of the Global Alliance for Vaccines and Immunization (GAVI) secretariat and as a health advisor to leading global entities, Tore Godal is now a Special Advisor to the Norwegian Prime Minister. He is nevertheless continuing to fight for better global health, cogently articulating the needs of the world's poor and disadvantaged. He is a leading leprosy expert, ex-director of the world's premier agency for research and training in tropical diseases, instigator and prime mover of some global innovative public-private health sector partnerships, adept fund mobilizer, and advocate of the `let's get it done' school of leadership. Few individuals are, therefore, more experienced or better suited for such a crucial and much-needed role"
119,27,2823,1,The Pricing of Options and Corporate Liabilities,"If options are correctly priced in the market, it should not be possible to make sure profits by creating portfolios of long and short positions in options and their underlying stocks. Using this principle, a theoretical valuation formula for options is derived. Since almost all corporate liabilities can be viewed as combinations of options, the formula and the analysis that led to it are also applicable to corporate liabilities such as common stock, corporate bonds, and warrants. In particular, the formula can be used to derive the discount that should be applied to a corporate bond because of the possibility of default."
120,27,6807,1,Market Microstructure Theory,"{Written by one of the leading authorities in market microstructure research, this book provides a comprehensive guide to the theoretical work in this important area of finance.After an introduction to the general issues and problems in market microstructure, the book examines the main theoretical models developed to address inventory-based issues. There is then an extensive examination and discussion of the information-based models, with particular attention paid to the linkage with rational expectations model and learning models. The concluding chapters are concerned with price dynamics and with applications of the various models to specific microstructure problems including:- Liquidity.- Multi-market trading.- Market structure.- Market Design Market Microstructure Theory includes extensive appendices developing Bayesian learning and the rational expectations framework.}"
121,27,10935,1,Continuous auctions and insider trading,"A dynamic model of insider trading with sequential auctions, structured to resemble a sequential equilibrium, is used to examine the informational content of prices, the liquidity characteristics of a speculative market, and the value of private information to an insider. The model has three kinds of traders: a single risk neutral insider, random noise traders, and competitive risk neutral market makers. The insider makes positive profits by exploiting his monopoly power optimally in a dynamic context, where noise trading provides camouflage which conceals his trading from market makers. As the time interval between auctions goes to zero, a limiting model of continuous trading is obtained. In this equilibrium, prices follow Brownian motion, the depth of the market is constant over time, and all private information is incorporated into prices by the end of trading."
122,28,4344,1,Named Entity recognition without gazetteers,"It is often claimed that Named Entity  recognition systems need extensive  gazetteers lists of names of people, organisations,  locations, and other named  entities. Indeed, the compilation of such  gazetteers is sometimes mentioned as a  bottleneck in the design of Named Entity  recognition systems.  We report on a Named Entity recognition  system which combines rule-based  grammars with statistical (maximum entropy)  models. We report on the system  &#039;s performance with gazetteers of different  types and dierent sizes, using test  material from the muc{7 competition.  We show that, for the text type and task  of this competition, it is sucient to use  relatively small gazetteers of well-known  names, rather than large gazetteers of  low-frequency names. We conclude with  observations about the domain independence  of the competition and of our experiments.  1 Introduction  Named Entity recognition involves processing a text and identifying certain occurrences of words or expressions ..."
123,29,1916,1,New Directions in Cryptography,"The authors discuss some of the recent results in communications theory that have arisen out of the need for security in the key distribution channels. They concentrate on the use of ciphers to restrict the extraction of information from a communication over an insecure. As is well known, the transmission and distribution is then likely to become a problem, in efficiency if not in security. The authors suggest various possible approaches to avoid these further problems that arise. The first they call a ""public key distribution system"", which has the feature that an unauthorized ""eavesdropper"" will find it computationally infeasible to decipher the message since the enciphering and deciphering are governed by distinct keys. They propose a couple of techniques for implementing the system, but the reviewer was unconvinced. A further system is designed to treat a problem that arises in business transactions, that of transmitting a legally valid signature whose authenticity can be checked by anyone; this necessitates the signature's being message-dependent. Finally, they treat the problem of ""trap-doors"" (a trap-door cipher is one that strongly resists decipherment by anyone unauthorized and not in possession of particular information used in the design of the cipher) and relate their concerns to various questions that arise in problems of computational complexity."
124,29,6928,1,Network information flow,"AbstractÃWe introduce a new class of problems called network information flow which is inspired by computer network applications. Consider a point-to-point communication network on which a number of information sources are to be mulitcast to certain sets of destinations. We assume that the information sources are mutually independent. The problem is to characterize the admissible coding rate region. This model subsumes all previously studied models along the same line. In this paper, we study the problem with one information source, and we have obtained a simple characterization of the admissible coding rate region. Our result can be regarded as the Max-flow Min-cut Theorem for network information flow. Contrary to oneÃs intuition, our work reveals that it is in general not optimal to regard the information to be multicast as a ÃfluidÃ which can simply be routed or replicated. Rather, by employing coding at the nodes, which we refer to as network coding bandwidth can in general be saved. This finding may have significant impact on future design of switching systems. Index TermsÃDiversity coding, multicast, network coding switching, multiterminal source coding."
125,30,1551,1,{Genome-scale reconstruction of the Saccharomyces cerevisiae metabolic network},"10.1101/gr.234503 The metabolic network in the yeast was reconstructed using currently available genomic, biochemical, and physiological information. The metabolic reactions were compartmentalized between the cytosol and the mitochondria, and transport steps between the compartments and the environment were included. A total of 708 structural open reading frames (ORFs) were accounted for in the reconstructed network, corresponding to 1035 metabolic reactions. Further, 140 reactions were included on the basis of biochemical evidence resulting in a genome-scale reconstructed metabolic network containing 1175 metabolic reactions and 584 metabolites. The number of gene functions included in the reconstructed network corresponds to Ã¢ÂÂ¼16% of all characterized ORFs in . Using the reconstructed network, the metabolic capabilities of  were calculated and compared with . The reconstructed metabolic network is the first comprehensive network for a eukaryotic organism, and it may be used as the basis for in silico analysis of phenotypic functions. [Supplemental material is available online at . The detailed genome-scale reconstructed model of can be found at or.]"
126,30,3171,1,Stochastic reaction-diffusion simulation with MesoRD,"Summary: MesoRD is a tool for stochastic simulation of chemical reactions and diffusion. In particular, it is an implementation of the next subvolume method, which is an exact method to simulate the Markov process corresponding to the reaction-diffusion master equation.  Availability: MesoRD is free software, written in C++ and licensed under the GNU general public license (GPL). MesoRD runs on Linux, Mac OS X, NetBSD, Solaris and Windows XP. It can be downloaded from http://mesord.sourceforge.net.  Contact: johan.elf@icm.uu.se; johan.hattne@embl-hamburg.de  Supplementary information:  MesoRD User's Guide' and other documents are available at http://mesord.sourceforge.net. 10.1093/bioinformatics/bti431"
127,30,5234,1,A protein interaction network of the malaria parasite Plasmodium falciparum,"Plasmodium falciparum causes the most severe form of malaria and kills up to 2.7 million people annually1. Despite the global importance of P. falciparum, the vast majority of its proteins have not been characterized experimentally. Here we identify P. falciparum proteinâprotein interactions using a high-throughput version of the yeast two-hybrid assay that circumvents the difficulties in expressing P. falciparum proteins in Saccharomyces cerevisiae. From more than 32,000 yeast two-hybrid screens with P. falciparum protein fragments, we identified 2,846 unique interactions, most of which include at least one previously uncharacterized protein. Informatic analyses of network connectivity, coexpression of the genes encoding interacting fragments, and enrichment of specific protein domains or Gene Ontology annotations2 were used to identify groups of interacting proteins, including one implicated in chromatin modification, transcription, messenger RNA stability and ubiquitination, and another implicated in the invasion of host cells. These data constitute the first extensive description of the protein interaction network for this important human pathogen."
128,31,3101,1,Neuronal Population Coding of Movement Direction,"Although individual neurons in the arm area of the primate motor cortex are only broadly tuned to a particular direction in three-dimensional space, the animal can very precisely control the movement of its arm. The direction of movement was found to be uniquely predicted by the action of a population of motor cortical neurons. When individual cells were represented as vectors that make weighted contributions along the axis of their preferred direction (according to changes in their activity during the movement under consideration) the resulting vector sum of all cell vectors (population vector) was in a direction congruent with the direction of movement. This population vector can be monitored during various tasks, and similar measures in other neuronal populations could be of heuristic value where there is a neural representation of variables with vectorial attributes."
129,31,14106,1,Internal brain state regulates membrane potential synchrony in barrel cortex of behaving mice.,"Internal brain states form key determinants for sensory perception, sensorimotor coordination and learning. A prominent reflection of different brain states in the mammalian central nervous system is the presence of distinct patterns of cortical synchrony, as revealed by extracellular recordings of the electroencephalogram, local field potential and action potentials. Such temporal correlations of cortical activity are thought to be fundamental mechanisms of neuronal computation. However, it is unknown how cortical synchrony is reflected in the intracellular membrane potential (V(m)) dynamics of behaving animals. Here we show, using dual whole-cell recordings from layer 2/3 primary somatosensory barrel cortex in behaving mice, that the V(m) of nearby neurons is highly correlated during quiet wakefulness. However, when the mouse is whisking, an internally generated state change reduces the V(m) correlation, resulting in a desynchronized local field potential and electroencephalogram. Action potential activity was sparse during both quiet wakefulness and active whisking. Single action potentials were driven by a large, brief and specific excitatory input that was not present in the V(m) of neighbouring cells. Action potential initiation occurs with a higher signal-to-noise ratio during active whisking than during quiet periods. Therefore, we show that an internal brain state dynamically regulates cortical membrane potential synchrony during behaviour and defines different modes of cortical processing."
130,32,8421,1,Automated Tag Clustering: Improving search and exploration in the tag space,"prima di tutto, prende solo i tag che hanno una co-occorrenza rilevante, trovando un punto di taglio nella classifica delle co-occorrenze ordinate di un tag con i suoi co-tags per escludere quelli non significativi. (e gli altri? dove li piazzo nel clustering?). poi fa un grafo e successivamente ci implementa un algoritmo di clustering, basato sullo spectral clustering + modularity utile per fare suggest di tag simili. lavora sempre su matrici con pesi unitari, non gestisce i pesi calcolo della modularit{Ã¡}: A(Vc, Vc) = A(V, V) A(Vc, V) A(V', V'') = somma dei pesi degli archi tra i nodi , uno di V' e l'altro di V''"
131,32,10496,1,The Social Structure of Tagging Internet Video on del.icio.us,"The ability to tag resources with uncontrolled metadata or ""folksonomies"" is often characterized as one of the central features of ""Web 2.0"" applications. Folksonomies are said to support emergent classification, where the semantic value of the tags and their relation to one another is worked out through a negotiated process of users applying their selected tags and seeing what others have tagged the same way. Few studies exist to show how folksonomic tagging is actually done, and to what extent users share each otherÃÂ¿s tagging patterns. In this paper, we present the results of a social network analysis of two months worth of tagging Internet video on the social bookmarking system del.icio.us. The analysis reveals that specific videos are tagged in fairly coherent ways by a relatively tight group of users. However, contrary to our expectations, there does not appear to be much re-use of tags across different content, or even very many users tagging more than a few similar items. Overwhelmingly, specific clusters of tags and users are associated with individual video links. This result suggests that tagging bookmarks is highly local, and the overall collection of tags is unlikely to result in a coherent globally navigable classification system."
132,33,10183,1,Semi-supervised methods to predict patient survival from gene expression data.,"An important goal of DNA microarray research is to develop tools to diagnose cancer more accurately based on the genetic profile of a tumor. There are several existing techniques in the literature for performing this type of diagnosis. Unfortunately, most of these techniques assume that different subtypes of cancer are already known to exist. Their utility is limited when such subtypes have not been previously identified. Although methods for identifying such subtypes exist, these methods do not work well for all datasets. It would be desirable to develop a procedure to find such subtypes that is applicable in a wide variety of circumstances. Even if no information is known about possible subtypes of a certain form of cancer, clinical information about the patients, such as their survival time, is often available. In this study, we develop some procedures that utilize both the gene expression data and the clinical data to identify subtypes of cancer and use this knowledge to diagnose future patients. These procedures were successfully applied to several publicly available datasets. We present diagnostic procedures that accurately predict the survival of future patients based on the gene expression profile and survival times of previous patients. This has the potential to be a powerful tool for diagnosing and treating cancer."
133,33,13411,1,Velvet: algorithms for de novo short read assembly using de Bruijn graphs.,"10.1101/gr.074492.107 We have developed a new set of algorithms, collectively called â Velvet,â to manipulate de Bruijn graphs for genomic sequence assembly. A de Bruijn graph is a compact representation based on short words (-mers) that is ideal for high coverage, very short read (25â50 bp) data sets. Applying Velvet to very short reads and paired-ends information only, one can produce contigs of significant length, up to 50-kb N50 length in simulations of prokaryotic data and 3-kb N50 on simulated mammalian BACs. When applied to real Solexa data sets without read pairs, Velvet generated contigs of â¼8 kb in a prokaryote and 2 kb in a mammalian BAC, in close agreement with our simulated results without read-pair information. Velvet represents a new approach to assembly that can leverage very short reads in combination with read pairs to produce useful assemblies."
134,33,15615,1,The Sequence Alignment/Map format and SAMtools,"Summary: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments.Availability: http://samtools.sourceforge.netContact: rd@sanger.ac.uk"
135,34,30,1,Matching words and pictures,"We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-moda  and correspondence extensions to Hofmannâs hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data."
136,34,2501,1,A Unifying Review of Linear Gaussian Models,"Factor analysis, principal component analysis, mixtures of gaussian  clusters, vector quantization, Kalman filter models, and hidden Markov  models can all be unified as variations of unsupervised learning under  a single basic generative model. This is achieved by collecting  together disparate observations and derivations made by many previous  authors and introducing a new way of linking discrete and continuous  state models using a simple nonlinearity. Through the use of other  nonlinearities, we show how independent component analysis is also a  variation of the same basic generative model. We show that factor  analysis and mixtures of gaussians can be implemented in autoencoder  neural networks and learned using squared error plus the same  regularization term. We introduce a new model for static data, known as  sensible principal component analysis, as well as a novel concept of  spatially adaptive observation noise. We also review some of the  literature involving global and local mixtures of the basic models and  provide pseudocode for inference and learning for all the basic models."
137,34,3778,1,Auditory Scene Analysis: The Perceptual Organization of Sound,"{""Bregman has written a major book, a unique and important contribution to the rapidly expanding field of complex auditory perception. This is a big, rich, and fulfilling piece of work that deserves the wide audience it is sure to attract."" -- Stewart H. Hulse, <i>Science</i>  <P>Auditory Scene Analysis addresses the problem of hearing complex auditory environments, using a series of creative analogies to describe the process required of the human auditory system as it analyzes mixtures of sounds to recover descriptions of individual sounds. In a unified and comprehensive way, Bregman establishes a theoretical framework that integrates his findings with an unusually wide range of previous research in psychoacoustics, speech perception, music theory and composition, and computer modeling.}"
138,34,6640,1,Musical Genre Classification of Audio Signals,"Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content- based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61% for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification."
139,34,8158,1,Linear prediction: A tutorial review,"This paper gives an exposition of linear prediction in the analysis of discrete signals. The signal is modeled as a linear combination of its past values and present and past values of a hypothetical input to a system whose output is the given signal. In the frequency domain, this is equivalent to modeling the signal spectrum by a pole-zero spectrum. The major part of the paper is devoted to all-pole models. The model parameters are obtained by a least squares analysis in the time domain. Two methods result, depending on whether the signal is assumed to be stationary or nonstationary. The same results are then derived in the frequency domain. The resulting spectral matching formulation allows for the modeling of selected portions of a spectrum, for arbitrary spectral shaping in the frequency domain, and for the modeling of continuous as well as discrete spectra. This also leads to a discussion of the advantages and disadvantages of the least squares error criterion. A spectral interpretation is given to the normalized minimum prediction error. Applications of the normalized error are given, including the determination of an ""optimal"" number of poles. The use of linear prediction in data compression is reviewed. For purposes of transmission, particular attention is given to the quantization and encoding of the reflection (or partial correlation) coefficients. Finally, a brief introduction to pole-zero modeling is given."
140,34,11209,1,Phoneme recognition using time-delay neural networks,"In this paper we present a Time-Delay Neural Network (TDNN) approach to phoneme recognition which is characterized by two important properties. 1) Using a 3 layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces. The TDNN learns these decision surfaces automatically using error backpropagation [1]. 2) The time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independent of position in time and hence not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes ``B,'' ``D,'' and ``G'' in varying phonetic contexts was chosen. For comparison, several discrete Hidden Markov Models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5 percent correct while the rate obtained by the best of our HMM's was only 93.7 percent. Closer inspection reveals that the network ``invented'' well-known acoustic-phonetic features (e.g., F2-rise, F2-fall, vowel-onset) as useful abstractions. It also developed alternate internal representations to link different acoustic realizations to the same concept."
141,34,13874,1,Object-based auditory and visual attention," Theories of visual attention argue that attention operates on perceptual objects, and thus that interactions between object formation and selective attention determine how competing sources interfere with perception. In auditory perception, theories of attention are less mature and no comprehensive framework exists to explain how attention influences perceptual abilities. However, the same principles that govern visual perception can explain many seemingly disparate auditory phenomena. In particular, many recent studies of âinformational maskingâ can be explained by failures of either auditory object formation or auditory object selection. This similarity suggests that the same neural mechanisms control attention and influence perception across different sensory modalities."
142,35,940,1,"The Power of Identity: The Information Age: Economy, Society and Culture, Volume II (The Information Age) 2nd Edition","{The Power of Identity is the second volume of Manuel Castells&#146;s trilogy, The Information Age: Economy, Society, and Culture. It deals with the social, political, and cultural dynamics associated with the technological transformation of our societies and with the globalization of the economy. It analyzes the importance of cultural, religious, and national identities as sources of meaning for people, and the implications of these identities for social movements. It studies grassroots mobilizations against the unfettered globalization of wealth and power, and considers the formation of alternative projects of social organization, as represented by the environmental movement and the women&#146;s movement. It also analyzes the crisis of the nation-state and its transformation into a network state, and the effects on political democracies of the difficulties of international governance and the submission of political representation to the dictates of media politics and the!  politics of scandal.    <P>This substantially expanded second edition updates and elaborates the analysis of these themes, adding new sections on al-Qaeda and global terrorist networks, on the anti-globalization movement, on American unilateralism and the conflicts of global governance, on the crisis of political legitimacy throughout the world, and on the theory of the network state.}"
143,35,5266,1,Technology as Experience,"{In <i>Technology as Experience</i>, John McCarthy and Peter Wright argue that any account of what is often called the user experience must take into consideration the emotional, intellectual, and sensual aspects of our interactions with technology. We don't just use technology, they point out; we live with it. They offer a new approach to understanding human-computer interaction through examining the felt experience of technology. Drawing on the pragmatism of such philosophers as John Dewey and Mikhail Bakhtin, they provide a framework for a clearer analysis of technology as experience.<br /> <br /> Just as Dewey, in <i>Art as Experience</i>, argued that art is part of everyday lived experience and not isolated in a museum, McCarthy and Wright show how technology is deeply embedded in everyday life. The ""zestful integration"" or transcendent nature of the aesthetic experience, they say, is a model of what human experience with technology might become.<br /> <br /> McCarthy and Wright illustrate their theoretical framework with real-world examples that range from online shopping to ambulance dispatch. Their approach to understanding human computer interaction -- seeing it as creative, open, and relational, part of felt experience -- is a measure of the fullness of technology's potential to be more than merely functional.}"
144,35,10862,1,A large scale study of wireless search behavior: Google mobile search,"We present a large scale study of search patterns on Google's mobile search interface. Our goal is to understand the current state of wireless search by analyzing over 1 Million hits to Google's mobile search sites. Our study also includes the examination of search queries and the general categories under which they fall. We follow users throughout multiple interactions to determine search behavior; we estimate how long they spend inputting a query, viewing the search results, and how often they click on a search result. We also compare and contrast search patterns between 12-key keypad phones (cellphones), phones with QWERTY keyboards (PDAs) and conventional computers."
145,36,1131,1,Workflow Patterns,"Differences in features supported by the various contemporary commercial workflow management systems point to different insights of suitability and different levels of expressive power. The challenge, which we undertake in this paper, is to systematically address workflow requirements, from basic to complex. Many of the more complex requirements identified, recur quite frequently in the analysis phases of workflow projects, however their implementation is uncertain in current products. Requirements for workflow languages are indicated through workflow patterns. In this context, patterns address business requirements in an imperative workflow style expression, but are removed from specific workflow languages. The paper describes a number of workflow patterns addressing what we believe identify comprehensive workflow functionality. These patterns provide the basis for an in-depth comparison of a number of commercially availablework flow management systems. As such, this paper can be seen as the academic response to evaluations made by prestigious consulting companies. Typically, these evaluations hardly consider the workflow modeling language and routing capabilities, and focus more on the purely technical and commercial aspects."
146,36,11978,1,Improving life sciences information retrieval using semantic web technology.,"The ability to retrieve relevant information is at the heart of every aspect of research and development in the life sciences industry. Information is often distributed across multiple systems and recorded in a way that makes it difficult to piece together the complete picture. Differences in data formats, naming schemes and network protocols amongst information sources, both public and private, must be overcome, and user interfaces not only need to be able to tap into these diverse information sources but must also assist users in filtering out extraneous information and highlighting the key relationships hidden within an aggregated set of information. The Semantic Web community has made great strides in proposing solutions to these problems, and many efforts are underway to apply Semantic Web techniques to the problem of information retrieval in the life sciences space. This article gives an overview of the principles underlying a Semantic Web-enabled information retrieval system: creating a unified abstraction for knowledge using the RDF semantic network model; designing semantic lenses that extract contextually relevant subsets of information; and assembling semantic lenses into powerful information displays. Furthermore, concrete examples of how these principles can be applied to life science problems including a scenario involving a drug discovery dashboard prototype called BioDash are provided."
147,37,1746,1,Ambiguity as a resource for design,"Ambiguity is usually considered anathema in Human Computer Interaction. We argue, in contrast, that it is a resource for design that can be used to encourage close personal engagement with systems. We illustrate this with examples from contemporary arts and design practice, and distinguish three broad classes of ambiguity according to where uncertainty is located in the interpretative relationship linking person and artefact. Ambiguity of information finds its source in the artefact itself, ambiguity of context in the sociocultural discourses that are used to interpret it, and ambiguity of relationship in the interpretative and evaluative stance of the individual. For each of these categories, we describe tactics for emphasising ambiguity that may help designers and other practitioners understand and craft its use."
148,37,3311,1,The Uses of Argument,"{This reissue of the modern classic on the study of argumentation features a new Introduction by the author.} {A central theme throughout the impressive series of philosophical books and articles Stephen Toulmin has published since 1948 is the way in which assertions and opinions concerning all sorts of topics, brought up in everyday life or in academic research, can be rationally justified. Is there one universal system of norms, by which all sorts of arguments in all sorts of fields must be judged, or must each sort of argument be judged according to its own norms? In The Uses of Argument (1958) Toulmin sets out his views on these questions for the first time. In spite of initial criticisms from logicians and fellow philosophers, The Uses of Argument has been an enduring source of inspiration and discussion to students of argumentation from all kinds of disciplinary background for more than forty years.}"
149,37,5169,1,Formalising Trust as a Computational Concept,"Trust is a judgement of unquestionable utility â as humans we use it every day of our lives. However, trust has suffered from an imperfect understanding, a plethora of definitions, and informal use in the literature and in everyday life. It is common to say âI trust you, â but what does that mean? This thesis provides a clarification of trust. We present a formalism for trust which provides us with a tool for precise discussion. The formalism is implementable: it can be embedded in an artificial agent, enabling the agent to make trust-based decisions. Its applicability in the domain of Distributed Artificial Intelligence (DAI) is raised. The thesis presents a testbed populated by simple trusting agents which substantiates the utility of the formalism. The formalism provides a step in the direction of a proper understanding and definition of human trust. A contribution of the thesis is its detailed exploration of the possibilities of future work in the area. Summary 1. Overview This thesis presents an overview of trust as a social phenomenon and discusses it formally. It argues that trust is: â¢ A means for understanding and adapting to the complexity of the environment. â¢ A means of providing added robustness to independent agents. â¢ A useful judgement in the light of experience of the behaviour of others. â¢ Applicable to inanimate others. The thesis argues these points from the point of view of artificial agents. Trust in an artificial agent is a means of providing an additional tool for the consideration of other agents and the environment in which it exists. Moreover, a formalisation of trust enables the embedding of the concept into an artificial agent. This has been done, and is documented in the thesis. 2. Exposition There are places in the thesis where it is necessary to give a broad outline before going deeper. In consequence it may seem that the subject is not receiving a thorough treatment, or that too much is being discussed at one time! (This is particularly apparent in the first and second chapters.) To present a thorough understanding of trust, we have proceeded breadth first in the introductory chapters. Chapter 3 expands, depth first, presenting critical views of established researchers."
150,37,9294,1,The Distributed Constraint Satisfaction Problem: Formalization and Algorithms,"In this paper, we develop a formalism called a distributed constraint satisfaction problem (distributed CSP) and algorithms for solving distributed CSPs. A distributed CSP is a constraint satisfaction problem in which variables and constraints are distributed among multiple agents. Various application problems in Distributed Artificial Intelligence can be formalized as distributed CSPs. We present our newly developed technique called asynchronous backtracking that allows agents to act asynchronously and concurrently without any global control, while guaranteeing the completeness of the algorithm. Furthermore, we describe how the asynchronous backtracking algorithm can be modified into a more efficient algorithm called an asynchronous weak-commitment search, which can revise a bad decision without exhaustive search by changing the priority order of agents dynamically. The experimental results on various example problems show that the asynchronous weak-commitment search algorithm is, by far more, efficient than the asynchronous backtracking algorithm and can solve fairly large-scale problems."
151,38,13535,1,A systemic and cognitive view on collaborative knowledge building with wikis,"Abstract&nbsp;&nbsp;Wikis provide new opportunities for learning and for collaborative knowledge building as well as for understanding these processes. This article presents a theoretical framework for describing how learning and collaborative knowledge building take place. In order to understand these processes, three aspects need to be considered: the social processes facilitated by a wiki, the cognitive processes of the users, and how both processes influence each other mutually. For this purpose, the model presented in this article borrows from the systemic approach of Luhmann as well as from Piaget's theory of equilibration and combines these approaches. The model analyzes processes which take place in the social system of a wiki as well as in the cognitive systems of the users. The model also describes learning activities as processes of externalization and internalization. Individual learning happens through internal processes of assimilation and accommodation, whereas changes in a wiki are due to activities of external assimilation and accommodation which in turn lead to collaborative knowledge building. This article provides empirical examples for these equilibration activities by analyzing Wikipedia articles. Equilibration activities are described as being caused by subjectively perceived incongruities between an individuals' knowledge and the information provided by a wiki. Incongruities of medium level cause cognitive conflicts which in turn activate the described processes of equilibration and facilitate individual learning and collaborative knowledge building."
152,39,5625,1,Artificial Neoteny In Evolutionary Image Segmentation,"Neoteny, also spelled Paedomorphosis, can be defined in biological terms as the retention by an organism of juvenile or even larval traits into later life. In some species, all morphological development is retarded; the organism is juvenilized but sexually mature. Such shifts of reproductive capability would appear to have adaptive significance to organisms that exhibit it. In terms of evolutionary theory, the process of paedomorphosis suggests that larval stages and developmental phases of..."
153,39,5633,1,"On Ants, Bacteria and Dynamic Environments","Wasps, bees, ants and termites all make effective use of their environment and resources by displaying collective âswarmâ intelligence. Termite colonies - for instance - build nests with a complexity far beyond the comprehension of the individual termite, while ant colonies dynamically allocate labor to various vital tasks such as foraging or defense without any central decision-making ability. Recent research suggests that microbial life can be even richer: highly social, intricately networked, and teeming with interactions, as found in bacteria. What strikes from these observations is that both ant colonies and bacteria have similar natural mechanisms based on Stigmergy and Self-Organization in order to emerge coherent and sophisticated patterns of global behaviour. Keeping in mind the above characteristics we will present a simple model to tackle the collective adaptation of a social swarm based on real ant colony behaviors (SSA algorithm) for tracking extrema in dynamic environments and highly multimodal complex functions described in the well-know De Jong test suite. Then, for the purpose of comparison, a recent model of artificial bacterial foraging (BFOA algorithm) based on similar stigmergic features is described and analyzed. Final results indicate that the SSA collective intelligence is able to cope and quickly adapt to unforeseen situations even when over the same cooperative foraging period, the community is requested to deal with two different and contradictory purposes, while outperforming BFOA in adaptive speed."
154,40,2420,1,Chat circles,"Although current online chat environments provide new opportunities for communication, they are quite constrained in their ability to convey many important pieces of social information, ranging from the number of participants in a conversation to the subtle nuances of expression that enrich face to face speech. In this paper we present Chat Circles, an abstract graphical interface for synchronous conversa- tion. Here, presence and activity are made manifest by changes in color and form, proximity-based filtering intu- itively breaks large groups into conversational clusters, and the archives of a conversation are made visible through an integrated history interface. Our goal in this work is to cre- ate a richer environment for online discussions. There are currently a wide variety of tools that allow for synchronous communication over a computer network. Internet Relay Chat (IRC), for instance, is one of the Inter- net's most popular applications for interpersonal communi- cation. And, although the World Wide Web's initial protocols were not conducive to live interaction, the advent of Java has made Web-based chatrooms increasingly popu- lar. When email, newsgroups and chatrooms were first devel- oped, ASCII interfaces were the norm: most systems lacked both the power and the infrastructure for more elaborate graphical interfaces. Today, although faster computers and networks as well as support for visual routines make graphi-"
155,40,10099,1,"Revisiting the Commons: Local Lessons, Global Challenges","In a seminal paper, Garrett Hardin argued in 1968 that users of a commons are caught in an inevitable process that leads to the destruction of the resources on which they depend. This article discusses new insights about such problems and the conditions most likely to favor sustainable uses of common-pool resources. Some of the most difficult challenges concern the management of large-scale resources that depend on international cooperation, such as fresh water in international basins or large marine ecosystems. Institutional diversity may be as important as biological diversity for our long-term survival."
156,41,1443,1,Aspects of the Theory of Syntax,"Beginning in the mid-fifties and emanating largely form MIT, and approach was developed to linguistic theory and to the study of the structure of particular languages that diverges in many respects from modern linguistics. Although this approach is connected to the traditional study of languages, it differs enough in its specific conclusions about the structure and in its specific conclusions about the structure of language to warrant a name, ""generative grammar.""  Various deficiencies have been discovered in the first attempts to formulate a theory of transformational generative grammar and in the descriptive analysis of particular languages that motivated these formulations. At the same time, it has become apparent that these formulations can be extended and deepened.  The major purpose of this book is to review these developments and to propose a reformulation of the theory of transformational generative grammar that takes them into account. The emphasis in this study is syntax; semantic and phonological aspects of the language structure are discussed only insofar as they bear on syntactic theory."
157,41,2687,1,A Theory of Justice,"{Since it appeared in 1971, John Rawls's <I>A Theory of Justice</I> has become a classic. The author has now revised the original edition to clear up a number of difficulties he and others have found in the original book. <P> Rawls aims to express an essential part of the common core of the democratic tradition--justice as fairness--and to provide an alternative to utilitarianism, which had dominated the Anglo-Saxon tradition of political thought since the nineteenth century. Rawls substitutes the ideal of the social contract as a more satisfactory account of the basic rights and liberties of citizens as free and equal persons. ""Each person,"" writes Rawls, ""possesses an inviolability founded on justice that even the welfare of society as a whole cannot override."" Advancing the ideas of Rousseau, Kant, Emerson, and Lincoln, Rawls's theory is as powerful today as it was when first published.}"
158,41,7209,1,Collected Papers,"{John Rawls's work on justice has drawn more commentary and aroused wider attention than any other work in moral or political philosophy in the twentieth century. Rawls is the author of two major treatises, <i>A Theory     of Justice</i> (1971) and <i>Political Liberalism</i> (1993); it is said that <i>A Theory of Justice</i> revived political philosophy in the English-speaking world.     But before and after writing his great treatises Rawls produced a steady stream of essays. Some of these essays articulate views of justice and liberalism distinct from those found in the two books. They are important in and of     themselves because of the deep issues about the nature of justice, moral reasoning, and liberalism they raise as well as for the light they shed on the evolution of Rawls's views. Some of the articles tackle issues not addressed in     either book. They help identify some of the paths open to liberal theorists of justice and some of the knotty problems which liberal theorists must seek to resolve. A complete collection of John Rawls's essays is long overdue.}"
159,41,11967,1,Knowledge and its Limits,"Knowledge and Its Limits presents a systematic new conception of knowledge as a fundamental kind of mental state sensitive to the knower's environment. It makes a major contribution to the debate between externalist ad internalist philosophies of mind, and breaks radically with the epistemological tradition of analysing knowledge in terms of true belief. The theory casts light on a wide variety of philosophical issues: the problem of scepticism, the nature of evidence, probability and assertion, the dispute between realism and anti-realism and the paradox of the surprise examination. Williamson relates the new conception to structural limits on knowledge which imply that what can be known never exhausts what is true. The arguments are illustrated by rigorous models based on epistemic logic and probability theory. The result is a new way of doing epistemology for the twenty-first century."
160,42,4134,1,"Feature Models, Grammars, and Propositional Formulas","Feature models are used to specify members of a product-line. Despite years of progress, contemporary tools often provide limited support for feature constraints and offer little or no support for debugging feature models. We integrate prior results to connect feature models, grammars, and propositional formulas. This connection allows arbitrary propositional constraints to be defined among features and enables off-the-shelf satisfiability solvers to debug feature models. We also show how our ideas can generalize recent results on the staged configuration of feature models."
161,42,4792,1,Concern Graphs: Finding and Describing Concerns Using Structural Program Dependencies,"Many maintenance tasks address concerns, or features, that are not well modularized in the source code comprising a system. Existing approaches available to help software developers locate and manage scattered concerns use a representation based on lines of source code, complicating the analysis of the concerns. In this paper, we introduce the Concern Graph representation that abstracts the implementation details of a concern and makes explicit the relationships between different parts of the concern. The abstraction used in a Concern Graph has been designed to allow an obvious and inexpensive mapping back to the corresponding source code. To investigate the practical tradeoffs related to this approach, we have built the Feature Exploration and Analysis tool (FEAT) that allows a developer to manipulate a concern representation extracted from a Java system, and to analyze the relationships of that concern to the code base. We have used this tool to find and describe concerns related to software change tasks. We have performed case studies to evaluate the feasibility, usability, and scalability of the approach. Our results indicate that Concern Graphs can be used to document a concern for change, that developers unfamiliar with Concern Graphs can use them effectively, and that the underlying technology scales to industrial-sized programs."
162,42,8871,1,Agile Modeling: Effective Practices for Extreme Programming and the Unified Process,"{The first book to cover Agile Modeling, a new modeling technique created specifically for XP projects eXtreme Programming (XP) has created a buzz in the software development community-much like Design Patterns did several years ago. Although XP presents a methodology for faster software development, many developers find that XP does not allow for modeling time, which is critical to ensure that a project meets its proposed requirements. They have also found that standard modeling techniques that use the Unified Modeling Language (UML) often do not work with this methodology. In this innovative book, Software Development columnist Scott Ambler presents Agile Modeling (AM)-a technique that he created for modeling XP projects using pieces of the UML and Rational's Unified Process (RUP). Ambler clearly explains AM, and shows readers how to incorporate AM, UML, and RUP into their development projects with the help of numerous case studies integrated throughout the book. <ul> <li>AM was created by the author for modeling XP projects-an element lacking in the original XP design <li>The XP community and its creator have embraced AM, which should give this book strong market acceptance </ul> <p> Companion Web site at www.agilemodeling.com features updates, links to XP and AM resources, and ongoing case studies about agile modeling.}"
163,42,11949,1,Source Code Analysis: A Road Map,"The automated and semi-automated analysis of source code has remained a topic of intense research for more than thirty years. During this period, algorithms and techniques for source-code analysis have changed, sometimes dramatically. The abilities of the tools that implement them have also expanded to meet new and diverse challenges. This paper surveys current work on source-code analysis. It also provides a road map for future work over the next five-year period and speculates on the development of source-code analysis applications, techniques, and challenges over the next 10, 20, and 50 years."
164,43,934,1,A simple rule-based part of speech tagger,"Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods. In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rulebased tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below. 1."
165,43,1677,1,Text Classification using String Kernels,"We propose a novel approach for categorizing text documents based on the use of a special kernel. The kernel is an inner product in the feature space generated by all subsequences of length &lt;em&gt;k&lt;/em&gt;. A subsequence is any ordered sequence of &lt;em&gt;k&lt;/em&gt; characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences that are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of &lt;em&gt;k&lt;/em&gt;, since the dimension of the feature space grows exponentially with &lt;em&gt;k&lt;/em&gt;. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. Experimental comparisons of the performance of the kernel compared with a standard word feature space kernel (Joachims, 1998) show positive results on modestly sized datasets. The case of contiguous subsequences is also considered for comparison with the subsequences kernel with different decay factors. For larger documents and datasets the paper introduces an approximation technique that is shown to deliver good approximations efficiently for large datasets."
166,43,2324,1,Text categorization with support vector machines: learning with many relevant features,"This paper explores the user of Support Vector machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies, why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and they behave robustly over a variety of different learning tasks. Furthermore, they are fully automatic, eliminating the need for manuar parameter tuning."
167,43,3214,1,Making large-scale support vector machine learning practical,"Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, off-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVMlight is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVMlight V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains."
168,43,6881,1,Inductive learning algorithms and representations for text categorization,"Text categorization â the assignment of natural language texts to one or more predefined categories based on their content â is an important component in many information organization and management tasks. We compare the effectiveness of five different automatic learning algorithms for text categorization in terms of learning speed, realtime classification speed, and classification accuracy. We also examine training set size, and alternative document representations. Very accurate text classifiers can be learned automatically from training examples. Linear Support Vector Machines (SVMs) are particularly promising because they are very accurate, quick to train, and quick to evaluate. 1.1 Keywords Text categorization, classification, support vector machines, machine learning, information management."
169,43,7541,1,Fast and effective text mining using linear-time document clustering,"Clustering is a powerful technique for large-scale topic discovery from text. It involves two phases: first, feature extraction maps each document or record to a point in a high dimensional space, then clustering algorithms automatically group the points into a hierarchy of clusters. We describe an unsupervised, near-linear time text clustering system that offers a number of algorithm choices for each phase. We introduce a methodology for measuring the quality of a cluster hierarchy in terms of F-Measure, and present the results of experiments comparing different algorithms. The evaluation considers some feature selection parameters (\\textif{tfidf} and feature vector lenght) but focuses on the clustering algorithms, namely techniques from Scatter/Gather (buckshot, fractionation, and split/join) and \\textit{k}-means. Our experiments suggest that continuos center adjustement contributes more to cluster quality than seed selection does. It follows that using a simpler seed selection algorithm gives a better time/quality tradeoff. We describe a refinement to center adjustement, ``vector average damping'', that further improves cluster quality. We also compare the near-linear time algorithms to a group average greedy agglomerative clustering algorithm to demonstrate the time/quality tradeoff quantitatively."
170,43,8570,1,Accurate methods for the statistics of surprise and coincidence,"Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results. This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text. However, more applicable methods based on likelihood ratio tests are available which yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms, and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical. This paper describes the basis of a measure based on likelihood ratios which can be applied to the analysis of text."
171,43,10355,1,Ontology-based knowledge representation for bioinformatics,"Much of biology works by applying prior knowledge ( what is known') to an unknown entity, rather than the application of a set of axioms that will elicit knowledge. In addition, the complex biological data stored in bioinformatics databases often require the addition of knowledge to specify and constrain the values held in that database. One way of capturing knowledge within bioinformatics applications and databases is the use of ontologies. An ontology is the concrete form of a conceptualisation of a community's knowledge of a domain  This paper aims to introduce the reader to the use of ontologies within bioinformatics. A description of the type of knowledge held in an ontology will be given. the paper will be illustrated throughout with examples taken from bioinformatics and molecular biology, and a survey of current biological ontologies will be presented. From this it will be seen that the use to which the ontology is put largely determines the content of the ontology. Finally, the paper will describe the process of building an ontology, introducing the reader to the techniques and methods currently in use and the open research questions in ontology development. 10.1093/bib/1.4.398"
172,43,11364,1,Frustratingly Easy Domain Adaptation,"We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough âtargetâ data to do slightly better than just using only âsourceâ data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. The technique comes with several simple theoretical guarantees. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains."
173,44,15,1,Collective dynamics of 'small-world' networks.,"Networks of coupled dynamical systems have been used to model biological oscillators1, 2, 3, 4, Josephson junction arrays5,6, excitable media7, neural networks8, 9, 10, spatial games11, genetic control networks12 and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks 'rewired' to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them 'small-world' networks, by analogy with the small-world phenomenon13,14 (popularly known as six degrees of separation15). The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices."
174,44,1201,1,Statistical mechanics of complex networks,"The science of complex networks is a new interdisciplinary branch of science which has arisen recently on the interface of physics, biology, social and computer sciences, and others. Its main goal is to discover general laws governing the creation and growth as well as processes taking place on networks, like e.g. the Internet, transportation or neural networks. It turned out that most real-world networks cannot be simply reduced to a compound of some individual components. Fortunately, the statistical mechanics, being one of pillars of modern physics, provides us with a very powerful set of tools and methods for describing and understanding these systems. In this thesis, we would like to present a consistent approach to complex networks based on statistical mechanics, with the central role played by the concept of statistical ensemble of networks. We show how to construct such a theory and present some practical problems where it can be applied. Among them, we pay attention to the problem of finite-size corrections and the dynamics of a simple model of mass transport on networks."
175,44,3423,1,Stochastic Models for the Web Graph,"The Web may be viewed as a directed graph each of whose vertices is a static HTML Web page, and each of whose edges corresponds to a hyperlink from one Web page to another. We propose and analyze random graph models inspired by a series of empirical observations on the Web. Our graph models differ from the traditional G<sub>n,p</sub> models in two ways: 1. Independently chosen edges do not result in the statistics (degree distributions, clique multitudes) observed on the Web. Thus, edges in our model are statistically dependent on each other. 2. Our model introduces new vertices in the graph as time evolves. This captures the fact that the Web is changing with time. Our results are two fold: we show that graphs generated using our model exhibit the statistics observed on the Web graph, and additionally, that natural graph models proposed earlier do not exhibit them. This remains true even when these earlier models are generalized to account for the arrival of vertices over time. In particular, the sparse random graphs in our models exhibit properties that do not arise in far denser random graphs generated by Erdos-Renyi models"
176,44,11165,1,A Large-scale Evaluation and Analysis of Personalized Search Strategies,"Although personalized search has been proposed for many years and many personalization strategies have been investigated, it is still unclear whether personalization is consistently effective on different queries for different users, and under different search contexts. In this paper, we study this problem and get some preliminary conclusions. We present a large-scale evaluation framework for personalized search based on query logs, and then evaluate five personalized search strategies (including two click-based and three profile-based ones) using 12-day MSN query logs. By analyzing the results, we reveal that personalized search has significant improvement over common web search on some queries but it also has little effect on other queries (e.g., queries with small click entropy). It even harms search accuracy under some situations. Furthermore, we show that straightforward click-based personalization strategies perform consistently and considerably well, while profile-based ones are unstable in our experiments. We also reveal that both long-term and short-term contexts are very important in improving search performance for profile-based personalized search strategies."
177,45,1649,1,Applied bioinformatics for the identification of regulatory elements.," The compilation of multiple metazoan genome sequences and the deluge of large-scale expression data have combined to motivate the maturation of bioinformatics methods for the analysis of sequences that regulate gene transcription. Historically, these bioinformatics methods have been plagued by poor predictive specificity, but new bioinformatics algorithms that accelerate the identification of regulatory regions are drawing disgruntled users back to their keyboards. However, these new approaches and software are not without problems. Here, we introduce the purpose and mechanisms of the leading algorithms, with a particular emphasis on metazoan sequence analysis. We identify key issues that users should take into consideration in interpreting the results and provide an online training example to help researchers who wish to test online tools before taking an independent foray into the bioinformatics of transcription regulation."
178,45,3443,1,Automated ortholog inference from phylogenetic trees and  calculation of orthology reliability,"Motivation: Orthologous proteins in different species are likely   to have similar biochemical function and biological role. When   annotating a newly sequenced genome by sequencehomology, the most precise and reliable functional information can thus be derived from orthologs in other species. A standard method of finding orthologs is to compare the sequence tree with the species tree. However, since the topology of phylogenetic tree is not always reliable one might get incorrect assignments.  Results: Here we present a novel method that resolves this   problem by analyzing a set of bootstrap trees instead of the optimal tree. The frequency of orthology assignments in the bootstrap trees can be interpreted as a support value for the possible orthology of the sequences. Our method is efficient enough to analyze data in the scale of whole genomes. It is implemented in Java and calculates orthology support levels for all pairwise combinations of homologous sequences of two species. The method was tested on simulated datasets and on real data of homologous proteins.  Availability: Downloadable free of charge from ftp://ftp.cgb.ki.se/pub/prog/orthostrapper/or   on request from the authors.  Contact: christian.storm@cgb.ki.se 10.1093/bioinformatics/18.1.92"
179,46,132,1,Advances to Bayesian network inference for generating causal networks from observational biological data,"Motivation: Network inference algorithms are powerful computational tools for identifying putative causal interactions among variables from observational data. Bayesian network inference algorithms hold particular promise in that they can capture linear, non-linear, combinatorial, stochastic and other types of relationships among variables across multiple levels of biological organization. However, challenges remain when applying these algorithms to limited quantities of experimental data collected from biological systems. Here, we use a simulation approach to make advances in our dynamic Bayesian network (DBN) inference algorithm, especially in the context of limited quantities of biological data.  Results: We test a range of scoring metrics and search heuristics to find an effective algorithm configuration for evaluating our methodological advances. We also identify sampling intervals and levels of data discretization that allow the best recovery of the simulated networks. We develop a novel influence score for DBNs that attempts to estimate both the sign (activation or repression) and relative magnitude of interactions among variables. When faced with limited quantities of observational data, combining our influence score with moderate data interpolation reduces a significant portion of false positive interactions in the recovered networks. Together, our advances allow DBN inference algorithms to be more effective in recovering biological networks from experimentally collected data.  Availability: Source code and simulated data are available upon request.  Supplementary information: http://www.jarvislab.net/Bioinformatics/BNAdvances/ 10.1093/bioinformatics/bth448"
180,46,4047,1,BiNGO: a Cytoscape plugin to assess overrepresentation of Gene Ontology categories in Biological Networks,"Summary: The Biological Networks Gene Ontology tool (BiNGO) is an open-source Java tool to determine which Gene Ontology (GO) terms are significantly overrepresented in a set of genes. BiNGO can be used either on a list of genes, pasted as text, or interactively on subgraphs of biological networks visualized in Cytoscape. BiNGO maps the predominant functional themes of the tested gene set on the GO hierarchy, and takes advantage of Cytoscape's versatile visualization environment to produce an intuitive and customizable visual representation of the results. Availability: http://www.psb.ugent.be/cbd/papers/BiNGO/ Contact: martin.kuiper@psb.ugent.be"
181,46,10126,1,Structural Dynamics of Eukaryotic Chromosome Evolution,"Large-scale genome sequencing is providing a comprehensive view of the complex evolutionary forces that have shaped the structure of eukaryotic chromosomes. Comparative sequence analyses reveal patterns of apparently random rearrangement interspersed with regions of extraordinarily rapid, localized genome evolution. Numerous subtle rearrangements near centromeres, telomeres, duplications, and interspersed repeats suggest hotspots for eukaryotic chromosome evolution. This localized chromosomal instability may play a role in rapidly evolving lineage-specific gene families and in fostering large-scale changes in gene order. Computational algorithms that take into account these dynamic forces along with traditional models of chromosomal rearrangement show promise for reconstructing the natural history of eukaryotic chromosomes."
182,46,12517,1,Population Genomics: Whole-Genome Analysis of Polymorphism and Divergence in Drosophila simulans,"The population genetic perspective is that the processes shaping genomic variation can be revealed only through simultaneous investigation of sequence polymorphism and divergence within and between closely related species. Here we present a population genetic analysis of Drosophila simulans based on whole-genome shotgun sequencing of multiple inbred lines and comparison of the resulting data to genome assemblies of the closely related species, D. melanogaster and D. yakuba . We discovered previously unknown, large-scale fluctuations of polymorphism and divergence along chromosome arms, and significantly less polymorphism and faster divergence on the X chromosome. We generated a comprehensive list of functional elements in the D. simulans genome influenced by adaptive evolution. Finally, we characterized genomic patterns of base composition for coding and noncoding sequence. These results suggest several new hypotheses regarding the genetic and biological mechanisms controlling polymorphism and divergence across the Drosophila genome, and provide a rich resource for the investigation of adaptive evolution and functional variation in D. simulans ."
183,46,13411,1,Velvet: algorithms for de novo short read assembly using de Bruijn graphs.,"10.1101/gr.074492.107 We have developed a new set of algorithms, collectively called â Velvet,â to manipulate de Bruijn graphs for genomic sequence assembly. A de Bruijn graph is a compact representation based on short words (-mers) that is ideal for high coverage, very short read (25â50 bp) data sets. Applying Velvet to very short reads and paired-ends information only, one can produce contigs of significant length, up to 50-kb N50 length in simulations of prokaryotic data and 3-kb N50 on simulated mammalian BACs. When applied to real Solexa data sets without read pairs, Velvet generated contigs of â¼8 kb in a prokaryote and 2 kb in a mammalian BAC, in close agreement with our simulated results without read-pair information. Velvet represents a new approach to assembly that can leverage very short reads in combination with read pairs to produce useful assemblies."
184,46,14581,1,Aggressive assembly of pyrosequencing reads with mates,"Motivation: DNA sequence reads from Sanger and pyrosequencing platforms differ in cost, accuracy, typical coverage, average read length and the variety of available paired-end protocols. Both read types can complement one another in a  hybrid' approach to whole-genome shotgun sequencing projects, but assembly software must be modified to accommodate their different characteristics. This is true even of pyrosequencing mated and unmated read combinations. Without special modifications, assemblers tuned for homogeneous sequence data may perform poorly on hybrid data.  Results: Celera Assembler was modified for combinations of ABI 3730 and 454 FLX reads. The revised pipeline called CABOG (Celera Assembler with the Best Overlap Graph) is robust to homopolymer run length uncertainty, high read coverage and heterogeneous read lengths. In tests on four genomes, it generated the longest contigs among all assemblers tested. It exploited the mate constraints provided by paired-end reads from either platform to build larger contigs and scaffolds, which were validated by comparison to a finished reference sequence. A low rate of contig mis-assembly was detected in some CABOG assemblies, but this was reduced in the presence of sufficient mate pair data.  Availability: The software is freely available as open-source from http://wgs-assembler.sf.net under the GNU Public License.  Contact: jmiller@jcvi.org  Supplementary information: Supplementary data are available at Bioinformatics online. 10.1093/bioinformatics/btn548"
185,46,15287,1,"Rare Variants of {IFIH1}, a Gene Implicated in Antiviral Responses, Protect Against Type 1 Diabetes","Genome-wide association studies (GWASs) are regularly used to map genomic regions contributing to common human diseases, but they often do not identify the precise causative genes and sequence variants. To identify causative type 1 diabetes (T1D) variants, we resequenced exons and splice sites of 10 candidate genes in pools of DNA from 480 patients and 480 controls and tested their disease association in over 30,000 participants. We discovered four rare variants that lowered T1D risk independently of each other (odds ratio = 0.51 to 0.74; P = 1.3 x 10(-3) to 2.1 x 10(-16)) in IFIH1 (interferon induced with helicase C domain 1), a gene located in a region previously associated with T1D by GWASs. These variants are predicted to alter the expression and structure of IFIH1 [MDA5 (melanoma differentiation-associated protein 5)], a cytoplasmic helicase that mediates induction of interferon response to viral RNA. This finding firmly establishes the role of IFIH1 in T1D and demonstrates that resequencing studies can pinpoint disease-causing genes in genomic regions initially identified by GWASs."
186,46,15586,1,Genome assembly reborn: recent computational challenges.,"Research into genome assembly algorithms has experienced a resurgence due to new challenges created by the development of next generation sequencing technologies. Several genome assemblers have been published in recent years specifically targeted at the new sequence data; however, the ever-changing technological landscape leads to the need for continued research. In addition, the low cost of next generation sequencing data has led to an increased use of sequencing in new settings. For example, the new field of metagenomics relies on large-scale sequencing of entire microbial communities instead of isolate genomes, leading to new computational challenges. In this article, we outline the major algorithmic approaches for genome assembly and describe recent developments in this domain. 10.1093/bib/bbp026"
187,46,15755,1,Probabilistic resolution of multi-mapping reads in massively parallel sequencing data using MuMRescueLite,"Summary: Multi-mapping sequence tags are a significant impediment to short-read sequencing platforms. These tags are routinely omitted from further analysis, leading to experimental bias and reduced coverage. Here, we present MuMRescueLite, a low-resource requirement version of the MuMRescue software that has been used by several next generation sequencing projects to probabilistically reincorporate multi-mapping tags into mapped short read data.  Availability and implementation: MuMRescueLite is written in Python; executables and documentation are available from http://genome.gsc.riken.jp/osc/english/software/.  Contact: geoff.faulkner@roslin.ed.ac.uk 10.1093/bioinformatics/btp438"
188,46,16065,1,ALLPATHS 2: small genomes assembled accurately and with high continuity from short paired reads,"We demonstrate that genome sequences approaching finished quality can be generated from short paired reads. Using 36 base (fragment) and 26 base (jumping) reads from five microbial genomes of varied GC composition and sizes up to 40 Mb, ALLPATHS2 generated assemblies with long, accurate contigs and scaffolds. Velvet and EULER-SR were less accurate. For example, for Escherichia coli, the fraction of 10-kb stretches that were perfect was 99.8% (ALLPATHS2), 68.7% (Velvet), and 42.1 (EULER-SR)."
189,46,16435,1,Microindel detection in short-read sequence data,"Motivation: Several recent studies have demonstrated the effectiveness of resequencing and single nucleotide variant (SNV) detection by deep short-read sequencing platforms. While several reliable algorithms are available for automated SNV detection, the automated detection of microindels in deep short-read data presents a new bioinformatics challenge.Results: We systematically analyzed how the short-read mapping tools MAQ, Bowtie, Burrows-Wheeler alignment tool (BWA), Novoalign and RazerS perform on simulated datasets that contain indels and evaluated how indels affect error rates in SNV detection. We implemented a simple algorithm to compute the equivalent indel region eir, which can be used to process the alignments produced by the mapping tools in order to perform indel calling. Using simulated data that contains indels, we demonstrate that indel detection works well on short-read data: the detection rate for microindels (<4 bp) is >90%. Our study provides insights into systematic errors in SNV detection that is based on ungapped short sequence read alignments. Gapped alignments of short sequence reads can be used to reduce this error and to detect microindels in simulated short-read data. A comparison with microindels automatically identified on the ABI Sanger and Roche 454 platform indicates that microindel detection from short sequence reads identifies both overlapping and distinct indels.Contact: peter.krawitz@googlemail.com; peter.robinson@charite.deSupplementary information: Supplementary data are available at Bioinformatics online."
190,46,16611,1,Structural variation analysis with strobe reads,"Motivation: Structural variation including deletions, duplications and rearrangements of DNA sequence are an important contributor to genome variation in many organisms. In human, many structural variants are found in complex and highly repetitive regions of the genome making their identification difficult. A new sequencing technology called strobe sequencing generates strobe reads containing multiple subreads from a single contiguous fragment of DNA. Strobe reads thus generalize the concept of paired reads, or mate pairs, that have been routinely used for structural variant detection. Strobe sequencing holds promise for unraveling complex variants that have been difficult to characterize with current sequencing technologies.  Results: We introduce an algorithm for identification of structural variants using strobe sequencing data. We consider strobe reads from a test genome that have multiple possible alignments to a reference genome due to sequencing errors and/or repetitive sequences in the reference. We formulate the combinatorial optimization problem of finding the minimum number of structural variants in the test genome that are consistent with these alignments. We solve this problem using an integer linear program. Using simulated strobe sequencing data, we show that our algorithm has better sensitivity and specificity than paired read approaches for structural variation identification.  Contact: braphael@brown.edu 10.1093/bioinformatics/btq153"
191,46,16744,1,Efficient construction of an assembly string graph using the FM-index,"Motivation: Sequence assembly is a difficult problem whose importance has grown again recently as the cost of sequencing has dramatically dropped. Most new sequence assembly software has started by building a de Bruijn graph, avoiding the overlap-based methods used previously because of the computational cost and complexity of these with very large numbers of short reads. Here, we show how to use suffix array-based methods that have formed the basis of recent very fast sequence mapping algorithms to find overlaps and generate assembly string graphs asymptotically faster than previously described algorithms.Results: Standard overlap assembly methods have time complexity O(N2), where N is the sum of the lengths of the reads. We use the FerraginaÃ¢ÂÂManzini index (FM-index) derived from the BurrowsÃ¢ÂÂWheeler transform to find overlaps of length at least ÃÂ among a set of reads. As well as an approach that finds all overlaps then implements transitive reduction to produce a string graph, we show how to output directly only the irreducible overlaps, significantly shrinking memory requirements and reducing compute time to O(N), independent of depth. Overlap-based assembly methods naturally handle mixed length read sets, including capillary reads or long reads promised by the third generation sequencing technologies. The algorithms we present here pave the way for overlap-based assembly approaches to be developed that scale to whole vertebrate genome de novo assembly.Contact: js18@sanger.ac.uk"
192,47,2928,1,GOstat: find statistically overrepresented Gene Ontologies within a group of genes.,"SUMMARY: Modern experimental techniques, as for example DNA microarrays, as a result usually produce a long list of genes, which are potentially interesting in the analyzed process. In order to gain biological understanding from this type of data, it is necessary to analyze the functional annotations of all genes in this list. The Gene-Ontology (GO) database provides a useful tool to annotate and analyze the functions of a large number of genes. Here, we introduce a tool that utilizes this information to obtain an understanding of which annotations are typical for the analyzed list of genes. This program automatically obtains the GO annotations from a database and generates statistics of which annotations are overrepresented in the analyzed list of genes. This results in a list of GO terms sorted by their specificity. AVAILABILITY: Our program GOstat is accessible via the Internet at http://gostat.wehi.edu.au"
193,47,7597,1,Topological structure analysis of the protein-protein interaction network in budding yeast,"Interaction detection methods have led to the discovery of thousands of interactions between proteins, and discerning relevance within large-scale data sets is important to present-day biology. Here, a spectral method derived from graph theory was introduced to uncover hidden topological structures (i.e. quasi-cliques and quasi-bipartites) of complicated protein-protein interaction networks. Our analyses suggest that these hidden topological structures consist of biologically relevant functional groups. This result motivates a new method to predict the function of uncharacterized proteins based on the classification of known proteins within topological structures. Using this spectral analysis method, 48 quasi-cliques and six quasi-bipartites were isolated from a network involving 11 855 interactions among 2617 proteins in budding yeast, and 76 uncharacterized proteins were assigned functions. 10.1093/nar/gkg340"
194,48,1214,1,Conditional random fields: Probabilistic models for segmenting and labeling sequence data,"We present Â£  Â¤  Â¥  Â¦ Â§Â¨Â§ Â¤  Â¥  Â© ï¿½ï¿½ï¿½ Â©  Â¥  Â¦  Â¤ ï¿½ï¿½ ï¿½ ï¿½ï¿½ Â¦  ï¿½ , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data. 1."
195,48,3442,1,Multiple sequence alignment using partial order graphs,"Motivation: Progressive Multiple Sequence Alignment (MSA) methods depend on reducing an MSA to a linear profile for each alignment step. However, this leads to loss of information needed for accurate alignment, and gap scoring artifacts. Results: We present a graph representation of an MSA that can itself be aligned directly by pairwise dynamic programming, eliminating the need to reduce the MSA to a profile. This enables our algorithm (Partial Order Alignment (POA)) to guarantee that the optimal alignment of each new sequence versus each sequence in the MSA will be considered. Moreover, this algorithm introduces a new edit operator, homologous recombination, important for multidomain sequences. The algorithm has improved speed (linear time complexity) over existing MSA algorithms, enabling construction of massive and complex alignments (e.g. an alignment of 5000 sequences in 4 h on a Pentium II). We demonstrate the utility of this algorithm on a family of multidomain SH2 proteins, and on EST assemblies containing alternative splicing and polymorphism. Availability: The partial order alignment program POA is available at http://www.bioinformatics.ucla.edu/poa. Contact: leec@mbi.ucla.edu"
196,48,7466,1,Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment,"We address the text-to-text generation problem of sentence-level paraphrasing --- a phenomenon distinct from and more difficult than word- or phrase-level paraphrasing. Our approach applies  multiple-sequence alignment  to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by  word lattice  pairs and automatically determines how to apply these patterns to rewrite new sentences. The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems."
197,48,9241,1,A codon-based model of nucleotide substitution for protein-coding DNA sequences.,"A codon-based model for the evolution of protein-coding DNA sequences is presented for use in phylogenetic estimation. A Markov process is used to describe substitutions between codons. Transition/transversion rate bias and codon usage bias are allowed in the model, and selective restraints at the protein level are accommodated using physicochemical distances between the amino acids coded for by the codons. Analyses of two data sets suggest that the new codon-based model can provide a better fit to data than can nucleotide-based models and can produce more reliable estimates of certain biologically important measures such as the transition/transversion rate ratio and the synonymous/nonsynonymous substitution rate ratio."
198,49,51,1,Genomic analysis of regulatory network dynamics reveals large topological changes,"Network analysis has been applied widely, providing a unifying language to describe disparate systems ranging from social interactions to power grids. It has recently been used in molecular biology, but so far the resulting networks have only been analysed statically1, 2, 3, 4, 5, 6, 7, 8. Here we present the dynamics of a biological network on a genomic scale, by integrating transcriptional regulatory information9, 10, 11 and gene-expression data12, 13, 14, 15, 16 for multiple conditions in Saccharomyces cerevisiae. We develop an approach for the statistical analysis of network dynamics, called SANDY, combining well-known global topological measures, local motifs and newly derived statistics. We uncover large changes in underlying network architecture that are unexpected given current viewpoints and random simulations. In response to diverse stimuli, transcription factors alter their interactions to varying degrees, thereby rewiring the network. A few transcription factors serve as permanent hubs, but most act transiently only during certain conditions. By studying sub-network structures, we show that environmental responses facilitate fast signal propagation (for example, with short regulatory cascades), whereas the cell cycle and sporulation direct temporal progression through multiple stages (for example, with highly inter-connected transcription factors). Indeed, to drive the latter processes forward, phase-specific transcription factors inter-regulate serially, and ubiquitously active transcription factors layer above them in a two-tiered hierarchy. We anticipate that many of the concepts presented hereâparticularly the large-scale topological changes and hub transienceâwill apply to other biological networks, including complex sub-systems in higher eukaryotes."
199,49,209,1,Gene regulatory network growth by duplication,"We are beginning to elucidate transcriptional regulatory networks on a large scale and to understand some of the structural principles of these networks, but the evolutionary mechanisms that form these networks are still mostly unknown. {H}ere we investigate the role of gene duplication in network evolution. {G}ene duplication is the driving force for creating new genes in genomes: at least 50\% of prokaryotic genes and over 90\% of eukaryotic genes are products of gene duplication. {T}he transcriptional interactions in regulatory networks consist of multiple components, and duplication processes that generate new interactions would need to be more complex. {W}e define possible duplication scenarios and show that they formed the regulatory networks of the prokaryote {E}scherichia coli and the eukaryote {S}accharomyces cerevisiae. {G}ene duplication has had a key role in network evolution: more than one-third of known regulatory interactions were inherited from the ancestral transcription factor or target gene after duplication, and roughly one-half of the interactions were gained during divergence after duplication. {I}n addition, we conclude that evolution has been incremental, rather than making entire regulatory circuits or motifs by duplication with inheritance of interactions."
200,49,1080,1,Aligning Multiple Genomic Sequences With the Threaded Blockset Aligner,"10.1101/gr.1933104 We define a âthreaded blockset,â which is a novel generalization of the classic notion of a multiple alignment. A new computer program called TBA (for âthreaded blockset alignerâ) builds a threaded blockset under the assumption that all matching segments occur in the same order and orientation in the given sequences; inversions and duplications are not addressed. TBA is designed to be appropriate for aligning many, but by no means all, megabase-sized regions of multiple mammalian genomes. The output of TBA can be projected onto any genome chosen as a reference, thus guaranteeing that different projections present consistent predictions of which genomic positions are orthologous. This capability is illustrated using a new visualization tool to view TBA-generated alignments of vertebrate Hox clusters from both the mammalian and fish perspectives. Experimental evaluation of alignment quality, using a program that simulates evolutionary change in genomic sequences, indicates that TBA is more accurate than earlier programs. To perform the dynamic-programming alignment step, TBA runs a stand-alone program called MULTIZ, which can be used to align highly rearranged or incompletely sequenced genomes. We describe our use of MULTIZ to produce the whole-genome multiple alignments at the Santa Cruz Genome Browser."
201,49,1291,1,A memory-efficient dynamic programming algorithm for optimal alignment of a sequence to an RNA secondary structure.,"Abstract Background Covariance models (CMs) are probabilistic models of RNA secondary structure, analogous to profile hidden Markov models of linear sequence. The dynamic programming algorithm for aligning a CM to an RNA sequence of length N is O(N3) in memory. This is only practical for small RNAs. Results I describe a divide and conquer variant of the alignment algorithm that is analogous to memory-efficient Myers/Miller dynamic programming algorithms for linear sequence alignment. The new algorithm has an O(N2 log N) memory complexity, at the expense of a small constant factor in time. Conclusions Optimal ribosomal RNA structural alignments that previously required up to 150 GB of memory now require less than 270 MB."
202,49,1309,1,Using an {RNA} secondary structure partition function to determine confidence in base pairs predicted by free energy minimization,"A partition function calculation for RNA secondary structure is presented that uses a current set of nearest neighbor parameters for conformational free energy at 37 degrees C, including coaxial stacking. For a diverse database of RNA sequences, base pairs in the predicted minimum free energy structure that are predicted by the partition function to have high base pairing probability have a significantly higher positive predictive value for known base pairs. For example, the average positive predictive value, 65.8%, is increased to 91.0% when only base pairs with probability of 0.99 or above are considered. The quality of base pair predictions can also be increased by the addition of experimentally determined constraints, including enzymatic cleavage, flavin mono-nucleotide cleavage, and chemical modification. Predicted secondary structures can be color annotated to demonstrate pairs with high probability that are therefore well determined as compared to base pairs with lower probability of pairing."
203,49,1337,1,The language of RNA: a formal grammar that includes pseudoknots,"Motivation: In a previous paper, we presented a polynomial time dynamic programming algorithm for predicting optimal RNA secondary structure including pseudoknots. However, a formal grammatical representation for RNA secondary structure with pseudoknots was still lacking.Results: Here we show a one-to-one correspondence between that algorithm and a formal transformational grammar. This grammar class encompasses the context-free grammars and goes beyond to generate pseudoknotted structures. The pseudoknot grammar avoids the use of general context-sensitive rules by introducing a small number of auxiliary symbols used to reorder the strings generated by an otherwise context-free grammar. This formal representation of the residue correlations in RNA structure is important because it means we can build full probabilistic models of RNA secondary structure, including pseudoknots, and use them to optimally parse sequences in polynomial time.Contact: eddy@genetics.wustl.edu"
204,49,1587,1,{A Bayesian networks approach for predicting protein-protein interactions from genomic data},"We have developed an approach using Bayesian networks to predict protein-protein interactions genome-wide in yeast. Our method naturally weights and combines into reliable predictions genomic features only weakly associated with interaction (e.g., messenger RNAcoexpression, coessentiality, and colocalization). In addition to de novo predictions, it can integrate often noisy, experimental interaction data sets. We observe that at given levels of sensitivity, our predictions are more accurate than the existing high-throughput experimental data sets. We validate our predictions with TAP (tandem affinity purification) tagging experiments. Our analysis, which gives a comprehensive view of yeast interactions, is available at genecensus.org/intint. 10.1126/science.1087361"
205,49,1633,1,{Network motifs in the transcriptional regulation network of Escherichia coli},"Little is known about the design principles1, 2, 3, 4, 5, 6, 7, 8, 9, 10 of transcriptional regulation networks that control gene expression in cells. Recent advances in data collection and analysis2, 11, 12, however, are generating unprecedented amounts of information about gene regulation networks. To understand these complex wiring diagrams1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, we sought to break down such networks into basic building blocks2. We generalize the notion of motifs, widely used for sequence analysis, to the level of networks. We define 'network motifs' as patterns of interconnections that recur in many different parts of a network at frequencies much higher than those found in randomized networks. We applied new algorithms for systematically detecting network motifs to one of the best-characterized regulation networks, that of direct transcriptional interactions in Escherichia coli3, 6. We find that much of the network is composed of repeated appearances of three highly significant motifs. Each network motif has a specific function in determining gene expression, such as generating temporal expression programs and governing the responses to fluctuating external signals. The motif structure also allows an easily interpretable view of the entire known transcriptional network of the organism. This approach may help define the basic computational elements of other biological networks."
206,49,1804,1,Probabilistic Inference using {M}arkov Chain {M}onte {C}arlo Methods,"Probabilistic inference is an attractive approach to uncertain reasoning and empirical learning in artificial intelligence. Computational difculties arise, however, because probabilistic models with the necessary realism and flexibility lead to complex distributions over high-dimensional spaces. Related problems in other fields have been tackled using Monte Carlo methods based on sampling using Markov chains, providing a rich array of techniques that can be applied to problems in artificial intelligence. The &#034;Metropolis algorithm&#034; has been used to solve difficult problems in statistical physics for over forty years, and, in the last few years, the related method of &#034;Gibbs sampling&#034; has been applied to problems of statistical inference. Concurrently, an alternative method for solving problems in statistical physics by means of dynamical simulation has been developed as well, and has recently been unified with the Metropolis algorithm to produce the &#034;hybrid Monte Carlo&#034; method. In computer science, Markov chain sampling is the basis of the heuristic optimization technique of &#034;simulated annealing&#034;, and has recently been used in randomized algorithms for approximate counting of large sets. In this review, I outline the role of probabilistic inference in artificial intelligence, and present the theory of Markov chains, and describe various Markov chain Monte Carlo algorithms, along with a number of supporting techniques. I try to present a comprehensive picture of the range of methods that have been developed, including techniques from the varied literature that have not yet seen wide application in artificial intelligence, but which appear relevant. As illustrative examples, I use the problems of probabilitic inference in expert systems, discovery of latent classes from data, and Bayesian learning for neural networks."
207,49,2226,1,Learning Probabilistic Relational Models,"A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with &amp;quot;flat &amp;quot; data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning â the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases. 1"
208,49,3366,1,From signatures to models: understanding cancer using microarrays.,"Genomics has the potential to revolutionize the diagnosis and management of cancer by offering an unprecedented comprehensive view of the molecular underpinnings of pathology. Computational analysis is essential to transform the masses of generated data into a mechanistic understanding of disease. Here we review current research aimed at uncovering the modular organization and function of transcriptional networks and responses in cancer. We first describe how methods that analyze biological processes in terms of higher-level modules can identify robust signatures of disease mechanisms. We then discuss methods that aim to identify the regulatory mechanisms underlying these modules and processes. Finally, we show how comparative analysis, combining human data with model organisms, can lead to more robust findings. We conclude by discussing the challenges of generalizing these methods from cells to tissues and the opportunities they offer to improve cancer diagnosis and management."
209,49,3900,1,Hitchhiking under positive Darwinian selection,"Positive selection can be inferred from its effect on linked neutral variation. In the restrictive case when there is no recombination, all linked variation is removed. If recombination is present but rare, both deterministic and stochastic models of positive selection show that linked variation hitchhikes to either low or high frequencies. While the frequency distribution of variation can be influenced by a number of evolutionary processes, an excess of derived variants at high frequency is a unique pattern produced by hitchhiking (derived refers to the nonancestral state as determined from an outgroup). We adopt a statistic, H, to measure an excess of high compared to intermediate frequency variants. Only a few high-frequency variants are needed to detect hitchhiking since not many are expected under neutrality. This is of particular utility in regions of low recombination where there is not much variation and in regions of normal or high recombination, where the hitchhiking effect can be limited to a small (<1 kb) region. Application of the H test to published surveys of Drosophila variation reveals an excess of high frequency variants that are likely to have been influenced by positive selection."
210,49,4548,1,Spontaneous evolution of modularity and network motifs,"Biological networks have an inherent simplicity: they are modular with a design that can be separated into units that perform almost independently. {F}urthermore, they show reuse of recurring patterns termed network motifs. {L}ittle is known about the evolutionary origin of these properties. {C}urrent models of biological evolution typically produce networks that are highly nonmodular and lack understandable motifs. {H}ere, we suggest a possible explanation for the origin of modularity and network motifs in biology. {W}e use standard evolutionary algorithms to evolve networks. {A} key feature in this study is evolution under an environment (evolutionary goal) that changes in a modular fashion. {T}hat is, we repeatedly switch between several goals, each made of a different combination of subgoals. {W}e find that such ""modularly varying goals"" lead to the spontaneous evolution of modular network structure and network motifs. {T}he resulting networks rapidly evolve to satisfy each of the different goals. {S}uch switching between related goals may represent biological evolution in a changing environment that requires different combinations of a set of basic biological functions. {T}he present study may shed light on the evolutionary forces that promote structural simplicity in biological networks and offers ways to improve the evolutionary design of engineered systems."
211,49,5210,1,Predicting the in vivo signature of human gene regulatory sequences,"Motivation: In the living cell nucleus, genomic DNA is packaged into chromatin. DNA sequences that regulate transcription and other chromosomal processes are associated with local disruptions, or openings', in chromatin structure caused by the cooperative action of regulatory proteins. Such perturbations are extremely specific for cis-regulatory elements and occur over short stretches of DNA (typically [~]250 bp). They can be detected experimentally as DNaseI hypersensitive sites (HSs) in vivo, though the process is extremely laborious and costly. The ability to discriminate DNaseI HSs computationally would have a major impact on the annotation and utilization of the human genome. Results: We found that a supervised pattern recognition algorithm, trained using a set of 280 DNaseI HS and 737 non-HS control sequences from erythroid cells, was capable of de novo prediction of HSs across the human genome with surprisingly high accuracy determined by prospective in vivo validation. Systematic application of this computational approach will greatly facilitate the discovery and analysis of functional non-coding elements in the human and other complex genomes. Availability: Supplementary data is available at noble.gs.washington.edu/proj/hs Contact: noble@gs.washington.edu; jstam@regulome.com 10.1093/bioinformatics/bti1047"
212,49,5468,1,Profile hidden Markov models,The recent literature on profile hidden Markov model (profile HMM) methods and software is reviewed. Profile HMMs turn a multiple sequence alignment into a position-specific scoring system suitable for searching databases for remotely homologous sequences. Profile HMM analyses complement standard pairwise comparison methods for large- scale sequence analysis. Several software implementations and two large libraries of profile HMMs of common protein domains are available. HMM methods performed comparably to threading methods in the CASP2 structure prediction exercise.
213,49,5482,1,The Evolution of Controlled Multitasked Gene Networks: The Role of Introns and Other Noncoding {RNAs} in the Development of Complex Organisms,"Eukaryotic phenotypic diversity arises from multitasking of a core proteome of limited size. Multitasking is routine in computers, as well as in other sophisticated information systems, and requires multiple inputs and outputs to control and integrate network activity. Higher eukaryotes have a mosaic gene structure with a dual output, mRNA (protein-coding) sequences and introns, which are released from the pre-mRNA by posttranscriptional processing. Introns have been enormously successful as a class of sequences and comprise up to 95% of the primary transcripts of protein-coding genes in mammals. In addition, many other transcripts (perhaps more than half) do not encode proteins at all, but appear both to be developmentally regulated and to have genetic function. We suggest that these RNAs (eRNAs) have evolved to function as endogenous network control molecules which enable direct gene-gene communication and multitasking of eukaryotic genomes. Analysis of a range of complex genetic phenomena in which RNA is involved or implicated, including co-suppression, transgene silencing, RNA interference, imprinting, methylation, and transvection, suggests that a higher-order regulatory system based on RNA signals operates in the higher eukaryotes and involves chromatin remodeling as well as other RNA-DNA, RNA-RNA, and RNA-protein interactions. The evolution of densely connected gene networks would be expected to result in a relatively stable core proteome due to the multiple reuse of components, implying that cellular differentiation and phenotypic variation in the higher eukaryotes results primarily from variation in the control architecture. Thus, network integration and multitasking using trans-acting RNA molecules produced in parallel with protein-coding sequences may underpin both the evolution of developmentally sophisticated multicellular organisms and the rapid expansion of phenotypic complexity into uncontested environments such as those initiated in the Cambrian radiation and those seen after major extinction events."
214,49,5502,1,{Profile analysis: detection of distantly related proteins},"{Profile analysis is a method for detecting distantly related proteins by sequence comparison. The basis for comparison is not only the customary Dayhoff mutational-distance matrix but also the results of structural studies and information implicit in the alignments of the sequences of families of similar proteins. This information is expressed in a position-specific scoring table (profile), which is created from a group of sequences previously aligned by structural or sequence similarity. The similarity of any other sequence (target) to the group of aligned sequences (probe) can be tested by comparing the target to the profile using dynamic programming algorithms. The profile method differs in two major respects from methods of sequence comparison in common use: (i) Any number of known sequences can be used to construct the profile, allowing more information to be used in the testing of the target than is possible with pairwise alignment methods. (ii) The profile includes the penalties for insertion or deletion at each position, which allow one to include the probe secondary structure in the testing scheme. Tests with globin and immunoglobulin sequences show that profile analysis can distinguish all members of these families from all other sequences in a database containing 3800 protein sequences.}"
215,49,5788,1,The evolution of transcriptional regulation in eukaryotes,"10.1093/molbev/msg140 Gene expression is central to the genotype-phenotype relationship in all organisms, and it is an important component of the genetic basis for evolutionary change in diverse aspects of phenotype. However, the evolution of transcriptional regulation remains understudied and poorly understood. Here we review the evolutionary dynamics of promoter, or cis-regulatory, sequences and the evolutionary mechanisms that shape them. Existing evidence indicates that populations harbor extensive genetic variation in promoter sequences, that a substantial fraction of this variation has consequences for both biochemical and organismal phenotype, and that some of this functional variation is sorted by selection. As with protein-coding sequences, rates and patterns of promoter sequence evolution differ considerably among loci and among clades for reasons that are not well understood. Studying the evolution of transcriptional regulation poses empirical and conceptual challenges beyond those typically encountered in analyses of coding sequence evolution: promoter organization is much less regular than that of coding sequences, and sequences required for the transcription of each locus reside at multiple other loci in the genome. Because of the strong context-dependence of transcriptional regulation, sequence inspection alone provides limited information about promoter function. Understanding the functional consequences of sequence differences among promoters generally requires biochemical and in vivo functional assays. Despite these challenges, important insights have already been gained into the evolution of transcriptional regulation, and the pace of discovery is accelerating."
216,49,6714,1,Genome-Scale Identification of Nucleosome Positions in S. cerevisiae,"The positioning of nucleosomes along chromatin has been implicated in the regulation of gene expression in eukaryotic cells, because packaging DNA into nucleosomes affects sequence accessibility. We developed a tiled microarray approach to identify at high resolution the translational positions of 2278 nucleosomes over 482 kilobases of Saccharomyces cerevisiae DNA, including almost all of chromosome III and 223 additional regulatory regions. The majority of the nucleosomes identified were well-positioned. We found a stereotyped chromatin organization at Pol II promoters consisting of a nucleosome-free region approximately 200 base pairs upstream of the start codon flanked on both sides by positioned nucleosomes. The nucleosome-free sequences were evolutionarily conserved and were enriched in poly-deoxyadenosine or poly-deoxythymidine sequences. Most occupied transcription factor binding motifs were devoid of nucleosomes, strongly suggesting that nucleosome positioning is a global determinant of transcription factor access."
217,49,7190,1,Positive and Negative Selection on the Human Genome,"The distinction between deleterious, neutral, and adaptive mutations is a fundamental problem in the study of molecular evolution. Two significant quantities are the fraction of DNA variation in natural populations that is deleterious and destined to be eliminated and the fraction of fixed differences between species driven by positive Darwinian selection. We estimate these quantities using the large number of human genes for which there are polymorphism and divergence data. The fraction of amino acid mutations that is neutral is estimated to be 0.20 from the ratio of common amino acid (A) to synonymous (S) single nucleotide polymorphisms (SNPs) at frequencies of [&gt;=]15%. Among the 80% of amino acid mutations that are deleterious at least 20% of them are only slightly deleterious and often attain frequencies of 1-10%. We estimate that these slightly deleterious mutations comprise at least 3% of amino acid SNPs in the average individual or at least 300 per diploid genome. This estimate is not sensitive to human population history. The A/S ratio of fixed differences is greater than that of common SNPs and suggests that a large fraction of protein divergence is adaptive and driven by positive Darwinian selection."
218,49,7755,1,Evidence for stabilizing selection in a eukaryotic enhancer element,"Eukaryotic gene expression is mediated by compact cis-regulatory modules, or enhancers, which are bound by specific sets of transcription factors1. The combinatorial interaction of these bound transcription factors determines time- and tissue-specific gene activation or repression. The even-skipped stripe 2 element controls the expression of the second transverse stripe of even-skipped messenger RNA in Drosophila melanogaster embryos, and is one of the best characterized eukaryotic enhancers2, 3, 4. Although even-skipped stripe 2 expression is strongly conserved in Drosophila, the stripe 2 element itself has undergone considerable evolutionary change in its binding-site sequences and the spacing between them. We have investigated this apparent contradiction, and here we show that two chimaeric enhancers, constructed by swapping the 5' and 3' halves of the native stripe 2 elements of two species, no longer drive expression of a reporter gene in the wild-type pattern. Sequence differences between species have functional consequences, therefore, but they are masked by other co-evolved differences. On the basis of these results, we present a model for the evolution of eukaryotic regulatory sequences."
219,49,8278,1,Stochastic mechanisms in gene expression.,"In cellular regulatory networks, genetic activity is controlled by molecular signals that determine when and how often a given gene is transcribed. {I}n genetically controlled pathways, the protein product encoded by one gene often regulates expression of other genes. {T}he time delay, after activation of the first promoter, to reach an effective level to control the next promoter depends on the rate of protein accumulation. {W}e have analyzed the chemical reactions controlling transcript initiation and translation termination in a single such ""genetically coupled"" link as a precursor to modeling networks constructed from many such links. {S}imulation of the processes of gene expression shows that proteins are produced from an activated promoter in short bursts of variable numbers of proteins that occur at random time intervals. {A}s a result, there can be large differences in the time between successive events in regulatory cascades across a cell population. {I}n addition, the random pattern of expression of competitive effectors can produce probabilistic outcomes in switching mechanisms that select between alternative regulatory paths. {T}he result can be a partitioning of the cell population into different phenotypes as the cells follow different paths. {T}here are numerous unexplained examples of phenotypic variations in isogenic populations of both prokaryotic and eukaryotic cells that may be the result of these stochastic gene expression mechanisms."
220,49,8777,1,A genomic code for nucleosome positioning,"Eukaryotic genomes are packaged into nucleosome particles that occlude the DNA from interacting with most DNA binding proteins. Nucleosomes have higher affinity for particular DNA sequences, reflecting the ability of the sequence to bend sharply, as required by the nucleosome structure. However, it is not known whether these sequence preferences have a significant influence on nucleosome position in vivo, and thus regulate the access of other proteins to DNA. Here we isolated nucleosome-bound sequences at high resolution from yeast and used these sequences in a new computational approach to construct and validate experimentally a nucleosome-DNA interaction model, and to predict the genome-wide organization of nucleosomes. Our results demonstrate that genomes encode an intrinsic nucleosome organization and that this intrinsic organization can explain approximately 50% of the in vivo nucleosome positions. This nucleosome positioning code may facilitate specific chromosome functions including transcription factor binding, transcription initiation, and even remodelling of the nucleosomes themselves."
221,49,9411,1,Evolution of alternative transcriptional circuits with identical logic.,"Evolution of gene regulation is an important contributor to the variety of life. Here, we analyse the evolution of a combinatorial transcriptional circuit composed of sequence-specific DNA-binding proteins that are conserved among all eukaryotes. This circuit regulates mating in the ascomycete yeast lineage. We first identify a group of mating genes that was transcriptionally regulated by an activator in a fungal ancestor, but is now transcriptionally regulated by a repressor in modern bakers' yeast. Despite this change in regulatory mechanism, the logical output of the overall circuit remains the same. By examining the regulation of mating in modern yeasts that are related to different extents, we deduce specific, sequential changes in both cis- and trans-regulatory elements that constitute the transition from positive to negative regulation. These changes indicate specific mechanisms by which fitness barriers were traversed during the transition."
222,49,9842,1,Principles for the Buffering of Genetic Variation,"Most genetic research has used inbred organisms and has not explored the complexity of natural genetic variation present in outbred populations. The translation of genotype to phenotype is complicated by gene interactions observed as epistasis, canalization, robustness, or buffering. Analysis of double mutations in inbred experimental organisms suggests some principles for gene interaction that may apply to natural variation as well. The buffering of variation in one gene is most often due to a small number of other genes that function in the same biochemical process. However, buffering can also result from genes functioning in processes extrinsic to that of the primary gene."
223,49,10362,1,Reconstructing dynamic regulatory maps.,"Even simple organisms have the ability to respond to internal and external stimuli. This response is carried out by a dynamic network of protein-DNA interactions that allows the specific regulation of genes needed for the response. We have developed a novel computational method that uses an input-output hidden Markov model to model these regulatory networks while taking into account their dynamic nature. Our method works by identifying bifurcation points, places in the time series where the expression of a subset of genes diverges from the rest of the genes. These points are annotated with the transcription factors regulating these transitions resulting in a unified temporal map. Applying our method to study yeast response to stress, we derive dynamic models that are able to recover many of the known aspects of these responses. Predictions made by our method have been experimentally validated leading to new roles for Ino4 and Gcn4 in controlling yeast response to stress. The temporal cascade of factors reveals common pathways and highlights differences between master and secondary factors in the utilization of network motifs and in condition-specific regulation."
224,49,11704,1,Beyond the sequence: cellular organization of genome function.,"Genomes are more than linear sequences. In vivo they exist as elaborate physical structures, and their functional properties are strongly determined by their cellular organization. I discuss here the functional relevance of spatial and temporal genome organization at three hierarchical levels: the organization of nuclear processes, the higher-order organization of the chromatin fiber, and the spatial arrangement of genomes within the cell nucleus. Recent insights into the cell biology of genomes have overturned long-held dogmas and have led to new models for many essential cellular processes, including gene expression and genome stability."
225,49,12114,1,Current progress in network research: toward reference networks for key model organisms,"The collection of multiple genome-scale datasets is now routine, and the frontier of research in systems biology has shifted accordingly. Rather than clustering a single dataset to produce a static map of functional modules, the focus today is on data integration, network alignment, interactive visualization and ontological markup. Because of the intrinsic noisiness of high-throughput measurements, statistical methods have been central to this effort. In this review, we briefly survey available datasets in functional genomics, review methods for data integration and network alignment, and describe recent work on using network models to guide experimental validation. We explain how the integration and validation steps spring from a Bayesian description of network uncertainty, and conclude by describing an important near-term milestone for systems biology: the construction of a set of rich reference networks for key model organisms. 10.1093/bib/bbm038"
226,49,12288,1,A high-resolution atlas of nucleosome occupancy in yeast,"We present the first complete high-resolution map of nucleosome occupancy across the whole Saccharomyces cerevisiae genome, identifying over 70,000 positioned nucleosomes occupying 81% of the genome. On a genome-wide scale, the persistent nucleosome-depleted region identified previously in a subset of genes demarcates the transcription start site. Both nucleosome occupancy signatures and overall occupancy correlate with transcript abundance and transcription rate. In addition, functionally related genes can be clustered on the basis of the nucleosome occupancy patterns observed at their promoters. A quantitative model of nucleosome occupancy indicates that DNA structural features may account for much of the global nucleosome occupancy."
227,49,12977,1,Analyzing time series gene expression data,"Motivation: Time series expression experiments are an increasingly popular method for studying a wide range of biological systems. However, when analyzing these experiments researchers face many new computational challenges. Algorithms that are specifically designed for time series experiments are required so that we can take advantage of their unique features (such as the ability to infer causality from the temporal response pattern) and address the unique problems they raise (e.g. handling the different non-uniform sampling rates).  Results: We present a comprehensive review of the current research in time series expression data analysis. We divide the computational challenges into four analysis levels: experimental design, data analysis, pattern recognition and networks. For each of these levels, we discuss computational and biological problems at that level and point out some of the methods that have been proposed to deal with these issues. Many open problems in all these levels are discussed. This review is intended to serve as both, a point of reference for experimental biologists looking for practical solutions for analyzing their data, and a starting point for computer scientists interested in working on the computational problems related to time series expression analysis. 10.1093/bioinformatics/bth283"
228,49,13251,1,The MC-Fold and MC-Sym pipeline infers RNA structure from sequence data,"The classical {RNA} secondary structure model considers {A.U} and {G.C} {Watson-Crick} as well as {G.U} wobble base pairs. Here we substitute it for a new one, in which sets of nucleotide cyclic motifs define {RNA} structures. This model allows us to unify all base pairing energetic contributions in an effective scoring function to tackle the problem of {RNA} folding. We show how pipelining two computer algorithms based on nucleotide cyclic motifs, {MC-Fold} and {MC-Sym,} reproduces a series of experimentally determined {RNA} three-dimensional structures from the sequence. This demonstrates how crucial the consideration of all base-pairing interactions is in filling the gap between sequence and structure. We use the pipeline to define rules of precursor {microRNA} folding in double helices, despite the presence of a number of presumed mismatches and bulges, and to propose a new model of the human immunodeficiency virus-1 -1 frame-shifting element."
229,49,13562,1,Hierarchical structure and the prediction of missing links in networks.,"Networks have in recent years emerged as an invaluable tool for describing and quantifying complex systems in many branches of science1, 2, 3. Recent studies suggest that networks often exhibit hierarchical organization, in which vertices divide into groups that further subdivide into groups of groups, and so forth over multiple scales. In many cases the groups are found to correspond to known functional units, such as ecological niches in food webs, modules in biochemical networks (protein interaction networks, metabolic networks or genetic regulatory networks) or communities in social networks4, 5, 6, 7. Here we present a general technique for inferring hierarchical structure from network data and show that the existence of hierarchy can simultaneously explain and quantitatively reproduce many commonly observed topological properties of networks, such as right-skewed degree distributions, high clustering coefficients and short path lengths. We further show that knowledge of hierarchical structure can be used to predict missing connections in partly known networks with high accuracy, and for more general network structures than competing techniques8. Taken together, our results suggest that hierarchy is a central organizing principle of complex networks, capable of offering insight into many network phenomena."
230,49,14190,1,Metabolic gene regulation in a dynamically changing environment,"Natural selection dictates that cells constantly adapt to dynamically changing environments in a context-dependent manner. Gene-regulatory networks often mediate the cellular response to perturbation1, 2, 3, and an understanding of cellular adaptation will require experimental approaches aimed at subjecting cells to a dynamic environment that mimics their natural habitat4, 5, 6, 7, 8, 9. Here we monitor the response of Saccharomyces cerevisiae metabolic gene regulation to periodic changes in the external carbon source by using a microfluidic platform that allows precise, dynamic control over environmental conditions. We show that the metabolic system acts as a low-pass filter that reliably responds to a slowly changing environment, while effectively ignoring fast fluctuations. The sensitive low-frequency response was significantly faster than in predictions arising from our computational modelling, and this discrepancy was resolved by the discovery that two key galactose transcripts possess half-lives that depend on the carbon source. Finally, to explore how induction characteristics affect frequency response, we compare two S. cerevisiae strains and show that they have the same frequency response despite having markedly different induction properties. This suggests that although certain characteristics of the complex networks may differ when probed in a static environment, the system has been optimized for a robust response to a dynamically changing environment."
231,49,14650,1,Genetic mapping in human disease.,"Genetic mapping provides a powerful approach to identify genes and biological processes underlying any trait influenced by inheritance, including human diseases. We discuss the intellectual foundations of genetic mapping of Mendelian and complex traits in humans, examine lessons emerging from linkage analysis of Mendelian diseases and genome-wide association studies of common diseases, and discuss questions and challenges that lie ahead."
232,49,15502,1,Metatranscriptomics reveals unique microbial small RNAs in the oceanâs water column,"Microbial gene expression in the environment has recently been assessed via pyrosequencing of total RNA extracted directly from natural microbial assemblages. Several such 'metatranscriptomic' studies1, 2 have reported that many complementary DNA sequences shared no significant homology with known peptide sequences, and so might represent transcripts from uncharacterized proteins. Here we report that a large fraction of cDNA sequences detected in microbial metatranscriptomic data sets are comprised of well-known small RNAs (sRNAs)3, as well as new groups of previously unrecognized putative sRNAs (psRNAs). These psRNAs mapped specifically to intergenic regions of microbial genomes recovered from similar habitats, displayed characteristic conserved secondary structures and were frequently flanked by genes that indicated potential regulatory functions. Depth-dependent variation of psRNAs generally reflected known depth distributions of broad taxonomic groups4, but fine-scale differences in the psRNAs within closely related populations indicated potential roles in niche adaptation. Genome-specific mapping of a subset of psRNAs derived from predominant planktonic species such as Pelagibacter revealed recently discovered as well as potentially new regulatory elements. Our analyses show that metatranscriptomic data sets can reveal new information about the diversity, taxonomic distribution and abundance of sRNAs in naturally occurring microbial communities, and indicate their involvement in environmentally relevant processes including carbon metabolism and nutrient acquisition."
233,49,15840,1,Architecture and secondary structure of an entire HIV-1 RNA genome,"Single-stranded RNA viruses encompass broad classes of infectious agents and cause the common cold, cancer, AIDS and other serious health threats. Viral replication is regulated at many levels, including the use of conserved genomic RNA structures. Most potential regulatory elements in viral RNA genomes are uncharacterized. Here we report the structure of an entire HIV-1 genome at single nucleotide resolution using SHAPE, a high-throughput RNA analysis technology. The genome encodes protein structure at two levels. In addition to the correspondence between RNA and protein primary sequences, a correlation exists between high levels of RNA structure and sequences that encode inter-domain loops in HIV proteins. This correlation suggests that RNA structure modulates ribosome elongation to promote native protein folding. Some simple genome elements previously shown to be important, including the ribosomal gag-pol frameshift stem-loop, are components of larger RNA motifs. We also identify organizational principles for unstructured RNA regions, including splice site acceptors and hypervariable regions. These results emphasize that the HIV-1 genome and, potentially, many coding RNAs are punctuated by previously unrecognized regulatory motifs and that extensive RNA structure constitutes an important component of the genetic code."
234,49,16194,1,Synthetic biology: understanding biological design from synthetic circuits,"An important aim of synthetic biology is to uncover the design principles of natural biological systems through the rational design of gene and protein circuits. Here, we highlight how the process of engineering biological systems â from synthetic promoters to the control of cellâcell interactions â has contributed to our understanding of how endogenous systems are put together and function. Synthetic biological devices allow us to grasp intuitively the ranges of behaviour generated by simple biological circuits, such as linear cascades and interlocking feedback loops, as well as to exert control over natural processes, such as gene expression and population dynamics."
235,49,16453,1,Variability in gene expression underlies incomplete penetrance,"The phenotypic differences between individual organisms can often be ascribed to underlying genetic and environmental variation. However, even genetically identical organisms in homogeneous environments vary, indicating that randomness in developmental processes such as gene expression may also generate diversity. To examine the consequences of gene expression variability in multicellular organisms, we studied intestinal specification in the nematode Caenorhabditis elegans in which wild-type cell fate is invariant and controlled by a small transcriptional network. Mutations in elements of this network can have indeterminate effects: some mutant embryos fail to develop intestinal cells, whereas others produce intestinal precursors. By counting transcripts of the genes in this network in individual embryos, we show that the expression of an otherwise redundant gene becomes highly variable in the mutants and that this variation is subjected to a threshold, producing an ON/OFF expression pattern of the master regulatory gene of intestinal differentiation. Our results demonstrate that mutations in developmental networks can expose otherwise buffered stochastic variability in gene expression, leading to pronounced phenotypic variation."
236,49,16589,1,Transcriptome-wide Identification of RNA-Binding Protein and MicroRNA Target Sites by PAR-CLIP,"RNA transcripts are subject to posttranscriptional gene regulation involving hundreds of RNA-binding proteins (RBPs) and microRNA-containing ribonucleoprotein complexes (miRNPs) expressed inÂ a cell-type dependent fashion. We developed a cell-based crosslinking approach to determine at high resolution and transcriptome-wide the binding sites of cellular RBPs and miRNPs. The crosslinked sites are revealed by thymidine to cytidine transitions inÂ the cDNAs prepared from immunopurified RNPs of 4-thiouridine-treated cells. We determined the binding sites and regulatory consequences for several intensely studied RBPs and miRNPs, includingÂ PUM2, QKI, IGF2BP1-3, AGO/EIF2C1-4 and TNRC6A-C. Our study revealed that these factors bind thousands of sites containing defined sequence motifs and have distinct preferences for exonic versus intronic or coding versus untranslated transcript regions. The precise mapping of binding sites across the transcriptome will be critical to the interpretation of the rapidly emerging data on genetic variation between individuals and how these variations contribute to complex genetic diseases. Âº PAR-CLIP is a transcriptome-wide crosslinking method for RNA-binding proteins (RBP) Âº It is based on incorporation of photoactivatable nucleoside analogs into nascent RNA Âº Characteristic sequence transitions in the prepared cDNA reveal the precise binding site Âº We deduced binding motifs and preferences for 5 different RBP families"
237,50,329,1,A tutorial on support vector regression,"In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for regression and function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization and capacity control from a SV point of view."
238,50,1892,1,An introduction to variable and feature selection,"			Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods."
239,50,5901,1,{B}ayesian Interpolation,"Although Bayesian analysis has been in use since Laplace, the Bayesian method of model--comparison has only recently been developed in depth.  In this paper, the Bayesian approach to regularisation and model--comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other problems.  Regularising constants are set by examining their posterior probability distribution. Alternative regularisers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. `Occam&#039;s razor&#039; is automatically embodied by this framework.  The way in which Bayes infers the values of regularising constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling.  1 Data modelling and Occam&#039;s razor In science, a central task is to develop and compare models to a..."
240,51,397,1,Queryfree news search,"Many daily activities present information in the form of a stream of text, and often people can benefit from additional information on the topic discussed. TV broadcast news can be treated as one such stream of text; in this paper we discuss finding news articles on the web that are relevant to news currently being broadcast.We evaluated a variety of algorithms for this problem, looking at the impact of inverse document frequency, stemming, compounds, history, and query length on the relevance and coverage of news articles returned in real time during a broadcast. We also evaluated several postprocessing techniques for improving the precision, including reranking using additional terms, reranking by document similarity, and filtering on document similarity. For the best algorithm, 84%-91% of the articles found were relevant, with at least 64% of the articles being on the exact topic of the broadcast. In addition, a relevant article was found for at least 70% of the topics."
241,51,1343,1,Measuring Similarity between Ontologies,"Ontologies now play an important role for many knowledge-intensive applications for which they provide a source of precisely defined terms. However, with their wide-spread usage there come problems concerning their proliferation. Ontology engineers or users frequently have a core ontology that they use, e.g., for browsing or querying data, but they need to extend it with, adapt it to, or compare it with the large set of other ontologies. For the task of detecting and retrieving relevant ontologies, one needs means for measuring the similarity between ontologies. We present a set of ontology similarity measures and a multiple-phase empirical evaluation."
242,51,2770,1,Information filtering and information retrieval: two sides of the same coin?,"Information filtering systems are designed for unstructured or semistructured data, as opposed to database applications, which use very structured data. The systems also deal primarily with textual information, but they may also entail images, voice, video or other data types that are part of multimedia information systems. Information filtering systems also involve a large amount of data and streams of incoming data, whether broadcast from a remote source or sent directly by other sources. Filtering is based on descriptions of individual or group information preferences, or profiles, that typically represent long-term interests. Filtering also implies removal of data from an incoming stream rather than finding data in the stream; users see only the data that is extracted. Models of information retrieval and filtering, and lessons for filtering from retrieval research are presented."
243,51,3599,1,A Historical View of Context,"{A}bstract. {T}his paper examines a number of the approaches, origins and ideals of context- aware systems design, looking particularly at the way that history inuences what we do in our ongoing activity. {A}s a number of sociologists and philosophers have pointed out, past social interaction, as well as past use of the heterogeneous mix of media, tools and artifacts that we use in our everyday activity, inuence our ongoing interaction with the people and media at hand. {W}e suggest that ones experience and history is thus part of ones current context, with patterns of use temporally and subjectively combining and interconnecting dierent media as well as dierent modes of use of those media. {O}ne such mode of use is transparent use, put forward by {W}eiser as ubicomps design ideal. {O}ne theoretical nding is that this design ideal is unachievable or incomplete because transparent and more focused analytical use are inter- dependent, aecting and feeding into each other through ones experience and history. {U}sing these theoretical points, we discuss a number of context-aware system designs that make good use of history in supporting ongoing user activity."
244,51,4497,1,"{Semantic Annotation, Indexing, and Retrieval}","The Semantic Web realization depends on the availability of a critical mass of metadata for the web content, associated with the respective formal knowledge about the world. We claim that the Semantic Web, at its current stage of development, is in a state of a critical need of metadata generation and usage schemata that are specific, well-defined and easy to understand. This paper introduces our vision for a holistic architecture for semantic annotation, indexing, and retrieval of documents with regard to extensive semantic repositories. A system (called KIM), implementing this concept, is presented in brief and it is used for the purposes of evaluation and demonstration. A particular schema for semantic annotation with respect to real-world entities is proposed. The underlying philosophy is that a practical semantic annotation is impossible without some particular knowledge modelling commitments. Our understanding is that a system for such semantic annotation should be based upon a simple model of real-world entity classes, complemented with extensive instance knowledge. To ensure the efficiency, ease of sharing, and reusability of the metadata, we introduce an upper-level ontology (of about 250 classes and 100 properties), which starts with some basic philosophical distinctions and then goes down to the most common entity types (people, companies, cities, etc.). Thus it encodes many of the domain-independent commonsense concepts and allows straightforward domain-specific extensions. On the basis of the ontology, a large-scale knowledge base of entity descriptions is bootstrapped, and further extended and maintained. Currently, the knowledge bases usually scales between 10 5  and 10 6  descriptions. Finally, this paper presents a semantically enhanced information extraction system, which provides automatic semantic annotation with references to classes in the ontology and to instances. The system has been running over a continuously growing document collection (currently about 0.5 million news articles), so it has been under constant testing and evaluation for some time now. On the basis of these semantic annotations, we perform semantic based indexing and retrieval where users can mix traditional information retrieval (IR) queries and ontology-based ones. We argue that such large-scale, fully automatic methods are essential for the transformation of the current largely textual web into a Semantic Web."
245,51,5212,1,Personalizing search via automated analysis of interests and activities,"We formulate and study search algorithms that consider a user's prior interactions with a wide variety of content to personalize that user's current Web search. Rather than relying on the unrealistic assumption that people will precisely specify their intent when searching, we pursue techniques that leverage implicit information about the user's interests. This information is used to re-rank Web search results within a relevance feedback framework. We explore rich models of user interests, built from both search-related information, such as previously issued queries and previously visited Web pages, and other information about the user such as documents and email the user has read and created. Our research suggests that rich representations of the user and the corpus are important for personalization, but that it is possible to approximate these representations and provide efficient client-side algorithms for personalizing search. We show that such personalization algorithms can significantly improve on current Web search."
246,51,6002,1,The Pragmatic Roots of Context,"When modelling complex systems one can not include all the causal factors, but one has to settle for partial models. This is alright if the factors left out are either so constant that they can be ignored or one is able to recognise the circumstances when they will be such that the partial model applies. The transference of knowledge from the point of application to the point of learning utilises a combination of recognition and inference â a simple model of the important features is learnt and later situations where inferences can be drawn from the model are recognised. Context is an abstraction of the collection of background features that are later recognised. Different heuristics for recognition and model formulation will be effective for different learning tasks. Each of these will lead to a different type of context. Given this, there two ways of modelling context: one can either attempt to investigate the contexts that arise out of the heuristics that a particular agent actua lly applies or one can attempt to model context using the external source of regularity that the heuristics exploit. There are also two basic methodologies for the investigation of context: a top-down approach where one tries to lay down general, a priori principles and a bottom-up approach where one can try and find what sorts of context arise by experiment and simulation. A simulation is exhibited which is designed to illustrate the practicality of the bottom-up approach in elucidating the sorts of internal context that arise in an artificial agent which is attempting to learn simple models of a complex environment."
247,51,6770,1,Fusion via a linear combination of scores,"We present a thorough analysis of the capabilities of the linear combination (LC) model for fusion of information retrieval systems. The LC model combines the results lists of multiple IR systems by scoring each document using a weighted sum of the scores from each of the component systems. We first present both empirical and analytical justification for the hypotheses that such a model should only be used when the systems involved have high performance, a large overlap of relevant documents, and a small overlap of nonrelevant documents. The empirical approach allows us to very accurately predict the performance of a combined system. We also derive a formula for a theoretically optimal weighting scheme for combining 2 systems. We introduce d&mdash;the difference between the average score on relevant documents and the average score on nonrelevant documents&mdash;as a performance measure which not only allows mathematical reasoning about system performance, but also allows the selection of weights which generalize well to new documents. We describe a number of experiments involving large numbers of different IR systems which support these findings."
248,51,8213,1,Retroactive Answering of Search Queries,"Major search engines currently use the history of a user's actions (e.g., queries, clicks) to personalize search results. In this paper, we present a new personalized service,  query-specific web recommendations  (QSRs), that retroactively answers queries from a user's history as new results arise. The QSR system addresses two important subproblems with applications beyond the system itself: (1) Automatic identification of queries in a user's history that represent standing interests and unfulfilled needs. (2) Effective detection of interesting new results to these queries. We develop a variety of heuristics and algorithms to address these problems, and evaluate them through a study of Google history users. Our results strongly motivate the need for automatic detection of standing interests from a user's history, and identifies the algorithms that are most useful in doing so. Our results also identify the algorithms, some which are counter-intuitive, that are most useful in identifying interesting new results for past queries, allowing us to achieve very high precision over our data set."
249,51,8345,1,User Modeling for Adaptive News Access,"We present a framework for adaptive news access, based on machine learning techniques specifically designed for this task. First, we focus on the system's general functionality and system architecture. We then describe the interface and design of two deployed news agents that are part of the described architecture. While the first agent provides personalized news through a web-based interface, the second system is geared towards wireless information devices such as PDAs (personal digital assistants) and cell phones. Based on implicit and explicit user feedback, our agents use a machine learning algorithm to induce individual user models. Motivated by general shortcomings of other user modeling systems for Information Retrieval applications, as well as the specific requirements of news classification, we propose the induction of hybrid user models that consist of separate models for short-term and long-term interests. Furthermore, we illustrate how the described algorithm can be used to address an important issue that has thus far received little attention in the Information Retrieval community: a user's information need changes as a direct result of interaction with information. We empirically evaluate the system's performance based on data collected from regular system users. The goal of the evaluation is not only to understand the performance contributions of the algorithm's individual components, but also to assess the overall utility of the proposed user modeling techniques from a user perspective. Our results provide empirical evidence for the utility of the hybrid user model, and suggest that effective personalization can be achieved without requiring any extra effort from the user."
250,51,9018,1,What makes a query difficult?,"This work tries to answer the question of what makes a query difficult. It addresses a novel model that captures the main components of a topic and the relationship between those components and topic difficulty. The three components of a topic are the textual expression describing the information need (the query or queries), the set of documents relevant to the topic (the Qrels), and the entire collection of documents. We show experimentally that topic difficulty strongly depends on the distances between these components. In the absence of knowledge about one of the model components, the model is still useful by approximating the missing component based on the other components. We demonstrate the applicability of the difficulty model for several uses such as predicting query difficulty, predicting the number of topic aspects expected to be covered by the search results, and analyzing the findability of a specific domain."
251,51,9454,1,IR evaluation methods for retrieving highly relevant documents,"This paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to retrieve highly relevant documents. This is desirable from the user point of view in modern large IR environments. The proposed methods are (1) a novel application of P-R curves and average precision computations based on separate recall bases for documents of different degrees of relevance, and (2) two novel measures computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. We then demonstrate the use of these evaluation methods in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of relevance. The test was run with a best match retrieval system (In-Query 1 ) in a text database consisting of newspaper articles. The results indicate that the tested strong query structures are most effective in retrieving highly relevant documents. The differences between the query types are practically essential and statistically significant. More generally, the novel evaluation methods and the case demonstrate that non-dichotomous relevance assessments are applicable in IR experiments, may reveal interesting phenomena, and allow harder testing of IR methods."
252,51,10368,1,User modeling via stereotypes,"This paper addresses the problems that must be considered if computers are going to treat their users as individuals with distinct personalities, goals, and so forth. It first outlines the issues, and then proposes stereotypes as a useful mechanism for building models of individual users on the basis of a small amount of information about them. In order to build user models quickly, a large amount of uncertain knowledge must be incorporated into the models. The issue of how to resolve the conflicts that will arise among such inferences is discussed. A system, Grundy, is described that builds models of its users, with the aid of stereotypes, and then exploits those models to guide it in its task, suggesting novels that people may find interesting. If stereotypes are to be useful to Grundy, they must accurately characterize the users of the system. Some techniques to modify stereotypes on the basis of experience are discussed. An analysis of Grundy's performance shows that its user models are effective in guiding its performance."
253,51,10634,1,Modeling context through domain ontologies,"Abstract&nbsp;&nbsp;Traditional information retrieval systems aim at satisfying most users for most of their searches, leaving aside the context in which the search takes place. We propose to model two main aspects of context: The themes of the user's information need and the specific data the user is looking for to achieve the task that has motivated his search. Both aspects are modeled by means of ontologies. Documents are semantically indexed according to the context representation and the user accesses information by browsing the ontologies. The model has been applied to a case study that has shown the added value of such a semantic representation of context."
254,51,10987,1,Bayesian adaptive user profiling with explicit & implicit feedback,"Research in information retrieval is now moving into a personalized scenario where a retrieval or filtering system maintains a separate user profile for each user. In this framework, information delivered to the user can be automatically personalized and catered to individual user's information needs. However, a practical concern for such a personalized system is the ""cold start problem"": any user new to the system must endure poor initial performance until sufficient feedback from that user is provided.To solve this problem, we use both explicit and implicit feedback to build a user's profile and use Bayesian hierarchical methods to borrow information from existing users. We analyze the usefulness of implicit feedback and the adaptive performance of the model on two data sets gathered from user studies where users' interaction with a document, or implicit feedback , were recorded along with explicit feedback. Our results are two-fold: first, we demonstrate that the Bayesian modeling approach effectively trades off between shared and user-specific information, alleviating poor initial performance for each user. Second, we find that implicit feedback has very limited unstable predictive value by itself and only marginal value when combined with explicit feedback."
255,51,11204,1,Google news personalization: scalable online collaborative filtering,"Several approaches to collaborative filtering have been studied but seldom have the studies been reported for large (several millions of users and items) and dynamic (the underlying item set is continually changing) settings. In this paper we describe our approach to collaborative filtering for generating personalized recommendations for users of Google News. We generate recommendations using three approaches: collaborative filtering using {MinHash} clustering, Probabilistic Latent Semantic Indexing {(PLSI),} and covisitation counts. We combine recommendations from different algorithms using a linear model. Our approach is content agnostic and consequently domain independent, making it easily adaptible for other applications and languages with minimal effort. This paper will describe our algorithms and system setup in detail, and report results of running the recommendations engine on Google News."
256,51,12681,1,Personalized information retrieval based on context and ontological knowledge,"Context modeling has long been acknowledged as a key aspect in a wide variety of problem domains. In this paper we focus on the combination of contextualization and personalization methods to improve the performance of personalized information retrieval. The key aspects in our proposed approach are (1) the explicit distinction between historic user context and live user context, (2) the use of ontology-driven representations of the domain of discourse, as a common, enriched representational ground for content meaning, user interests, and contextual conditions, enabling the definition of effective means to relate the three of them, and (3) the introduction of fuzzy representations as an instrument to properly handle the uncertainty and imprecision involved in the automatic interpretation of meanings, user attention, and user wishes. Based on a formal grounding at the representational level, we propose methods for the automatic extraction of persistent semantic user preferences, and live, ad-hoc user interests, which are combined in order to improve the accuracy and reliability of personalization for retrieval."
257,51,13862,1,How Do Users Find Things with PubMed? Towards Automatic Utility Evaluation with User Simulations,"In the context of document retrieval in the biomedical domain, this paper explores the complex relationship between the quality of initial query results and the overall utility of an interactive retrieval system. We demonstrate that a content-similarity browsing tool can compensate for poor retrieval results, and that the relationship between retrieval performance and overall utility is non-linear. Arguments are advanced with user simulations, which characterize the relevance of documents that a user might encounter with different browsing strategies. With broader implications to IR, this work provides a case study of how user simulations can be exploited as a formative tool for automatic utility evaluation. Simulation-based studies provide researchers with an additional evaluation tool to complement interactive and Cranfield-style experiments."
258,51,14633,1,Personalized recommendation in social tagging systems using hierarchical clustering,"Intro i tag permettono una navigazione non legata ad una gerarchia concettuale. sono intuitivi per l'utente, danno senso di comunit{Ã¡}, permettono di connettere persone in base agli interessi, assorbono velocemente i trend e flessibili ai cambiamenti di vocabolario. Il sistema quindi pu{Ã³} monitorare gli interessi degli utenti nelle risorse e il vocabolario che usano per il tagging di queste risorse. La libert{Ã¡} di vocabolario porta a ambiguit{Ã¡}: singolo tag ha diversi significati ridondanza: diversi tag hanno lo stesso significato Quindi recommendation personalizzata deve aiutare l'utente a interagire con il sistema abbiamo 3 dimensioni da gestire con il recommender system: utenti; items (risorse - gruppi; film; bookmarks, etc...); tags navigazione nel sistema attraversando tag->utente->risorsa scelgo un tag; vedo recommendations e utenti correlati al tag; vedo un utente e scelgo di vedere il suo profilo; nel profilo vedo delle risorse dei chicago bulls e le vado a vedere. devo essere libero di navigare attraverso queste tre dimensioni della folksonomia. Devo presentare all'utente una serie di avenues (strade?) correlate ai suoi interessi, che possono essere viste come una serie di recommendations Con il clustering si pu{Ã³} superare l'ambiguit{Ã¡} dei tags. L'algoritmo di personalizzazione clustering-based funziona cos{\\'\\i}: INPUT: profilo utente; insieme di clusters; tag selezionato OUTPUT: risorse suggerite l'utente seleziona un tag e vuole un elenco di recommendations Le folksonomies sono un aiuto per capire i bisogni informativi degli utenti. L'importanza delle risorse deriva dagli utenti (o viceversa???) L'importanza degli utenti deriva dalle risorse ??? (Dataset) Obiettivi&Tecniche Algoritmo di personalizzazione per recommendation in folksonomies che si basa su cluster di tags gerarchici Folksonomy: U utenti R risorse T tags A annotazioni D = <U, R, T, A> le annotazioni sono triple con user, tag, resource A = {< u, r, t >} Una folksonomy pu{Ã³} essere vista come ipegrafo tripartito: NODI: users, tags e resources IPERARCHI: le annotazioni, che collegano <un utente con un tag con una risorsa> Possibili metodi per recommendation: recency, authority, linkage, popularity, vector space models ogni utente {Ã©} un vettore sull'insieme di (tutti i) tag, dove ogni peso rappresenta l'importanza del tag u = <w(t1), w(t2), ... , w( T )> anche le risorse possono essere modellate come vettore sull'insieme di tags come si calcolano i pesi dei tags nei vettori? tag frequency: numero di volte che una risorsa {Ã©} stata annotata con quel tag tf(t, r) = cardinalit{Ã¡} delle annotazioni che hanno <u, t1, r1> con t1 = t e r1= r poi possiamo modificare il TF*IDF per le folksonomies tf*idf(t, r) = tf(t, r) * log(N/nt) N numero totale di risorse nt numero di tutte le risorse che hanno il tag t le query e le risorse possono essere rappresentate come dei vettori sui tags ipotizziamo che un utente cominci la navigazione con una query che seleziona un solo tag l'utente seleziona un tag e vuole un elenco di recommendations Somiglianza tra vettori: Coseno-somiglianza (Jaccard similarity coefficient) siccome la query {Ã©} solo un tag, si semplifica l'equazione. Misuriamo la somiglianza tra tutte le risorse e la query del tag e selezioniamo le top n. C'{Ã©} bisogno critico di personalizzazione, altrimenti avremmo lo stesso elenco di recommendations per tutti gli utenti. tag ambiguity e redundancy (due utenti, uno dei white sox, uno dei red sox, dietro il tag sox si otterrebbe la stessa lista di risultati) Due fasi dell'algoritmo di personalizzazione 1) si ottiene la lista di risorse raccomandate; 2) si personalizzano tenedo conto del profilo utente e dei cluster di tags; Il clustering si effettua prima, in modalit{Ã¡} offline Processo di recommendation in dettaglio: 1) coseno similarit{Ã¡} calcolato tra la query e tutte le risorse r. si ottiene un sottoinsieme di risorse R' che hanno una certa somiglianza con la query 2) calcolare rilevanza delle risorse r di R' per l'utente u i cluster sono necessari, in quanto punto di collegamento che mette in relazione gli utenti con le risorse, permettendo di mostrare risorse che rispecchino gli interessi dell'utente. INPUT: profilo utente; risorse in R' OUTPUT: rilevanza di ogni risorsa di R' per l'utente u 2.1) calcola l'interesse dell'utente in ogni cluster {Ã©} il rapporto tra numero di risorse che ha annotato con tags di quel cluster e numero totale di annotazioni dell'utente [0, 1] 2.2) calcola i cluster pi{Ãº} vicini ad ogni risorsa la relazione tra risorsa e un cluster si calcola rapporto tra volte che la risorsa {Ã©} stata annotata con un tag del cluster e totale numero di volte che la risorsa {Ã©} stata annotata [0, 1] 2.3) inferisci gli interessi dell'utente nei confronti di ogni risorsa misura di rilevanza utente-risorsa si fa sa somma dei prodotti (dell'interesse utente-cluster con relazione risorsa-cluster) (ogni cluster = topic) 3) calcolare il punteggio di rank personalizzato somiglianza personalizzata: (utente, query, risorsa) coseno-similarit{Ã¡} * misura di rilevanza abbiamo questo valore per tutte le risorse (ed erano state ordinate) e sono restituite le prime n risorse NB!!! : i pesi cluster-risorsa saranno indipendenti dagli utenti, mentre i pesi che connettono gli utenti ai cluster dipendono dal profilo utente. Modellazione utenti e risorse: vettori su un insieme di tags gli interessi di un utente si capiscono misurando la rilevanza di un cluster per un utente associo anche le risorse con i cluster di tag per vedere la rilevanza di una risorsa per un topic descritto da un cluster Risultato: utente<--->tag cluster<--->risorse Risultati Le folksonomies con un solo argomento sono obiettivi pi{Ãº} facili per la recommendation rispetto a folksonomies con diversi argomenti scorrelati Se usiamo una strategia di recommendation progettata per ridurre lo spazio del topic, riusciamo a trattare abbastanza bene sia i casi single topic che multi topic, ma {Ã©} meglio nel multi topic. La chiave {Ã©} ridurre lo spazio dell'informazione per focalizzarci sugli interessi dell'utente, usando il context-dependent hierarchical clustering approach. filtro i tag ambigui e ridondanti. Discussione La selezione del topic {Ã©} una strategia importante per la recommendation nelle folksonomies multi-topic Futuro Idee Context dependent cluster selection Riduco lo spazio dei topic e uso la recommendation strategy basandomi sul profilo utente che rappresenta la memoria a breve termine i pesi cluster-risorsa saranno indipendenti dagli utenti, mentre i pesi che connettono gli utenti ai cluster dipendono dal profilo utente. ERGO: per il mio scopo dovrei rendere i cluster dipendenti dal profilo utente ed avere cos{\\'\\i} pesi risorsa-cluster diversi per ogni profilo, altrimenti {Ã©} come se usassi tutti gli utenti per formare una folksonomia da scomporre in cluster, invece voglio che ogni utente abbia la sua folksonomia di riferimento e suddividere quella in cluster."
259,51,15105,1,Collaborative Annotation for Context-Aware Retrieval,"We discuss how collaborative annotations can be exploited to simplify and improve the management of context and re- sources in the context-aware retrieval ï¬eld. We apply this approach to our Context Aware Browser, a general purpose solution to Web content perusal by means of mobile devices, based on the userâs context. Instead of relying on a pool of experts and on a rigid categorization, as it is usually done in the context-aware ï¬eld, our solution allows the crowd of users to model, control and manage the contextual knowl- edge through collaboration and participation. We propose two models and we outline an example of application."
260,51,15812,1,Context-aware query classification,"Understanding users'search intent expressed through their search queries is crucial to Web search and online advertisement. Web query classification (QC) has been widely studied for this purpose. Most previous QC algorithms classify individual queries without considering their context information. However, as exemplified by the well-known example on query ""jaguar"", many Web queries are short and ambiguous, whose real meanings are uncertain without the context information. In this paper, we incorporate context information into the problem of query classification by using conditional random field (CRF) models. In our approach, we use neighboring queries and their corresponding clicked URLs (Web pages) in search sessions as the context information. We perform extensive experiments on real world search logs and validate the effectiveness and effciency of our approach. We show that we can improve the F1 score by 52% as compared to other state-of-the-art baselines."
261,52,1404,1,The vision of autonomic computing,"A 2001 IBM manifesto observed that a looming software complexity crisis -caused by applications and environments that number into the tens of millions of lines of code - threatened to halt progress in computing. The manifesto noted the almost impossible difficulty of managing current and planned computing systems, which require integrating several heterogeneous environments into corporate-wide computing systems that extend into the Internet. Autonomic computing, perhaps the most attractive approach to solving this problem, creates systems that can manage themselves when given high-level objectives from administrators. Systems manage themselves according to an administrator's goals. New components integrate as effortlessly as a new cell establishes itself in the human body. These ideas are not science fiction, but elements of the grand challenge to create self-managing computing systems."
262,52,3261,1,Borrowed-virtual-time (BVT) scheduling: supporting latency-sensitive threads in a general-purpose scheduler,"Systems need to run a larger and more diverse set of applications, from real-time to interactive to batch, on uniprocessor and multiprocessor platforms. However, most schedulers either do not address latency requirements or are specialized to complex real-time paradigms, limiting their applicability to general-purpose systems.  In this paper, we present Borrowed-Virtual-Time (BVT) Scheduling, showing that it provides low-latency for realtime and interactive applications yet weighted sharing of the CPU across applications according to system policy, even with thread failure at the real-time level, all with a low-overhead implementation on multiprocessors as well as uniprocessors. It makes minimal demands on application developers, and can be used with a reservation or admission control module for hard real-time applications.  1 Introduction  With modern processor speeds and memory capacities, systems can now run a wide diversity of application tasks, and they need to in order to meet us..."
263,52,4639,1,The dawning of the autonomic computing era,"This issue of the  IBM Systems Journal  explores a broad set of ideas and approaches to autonomic computing--some first steps in what we see as a journey to create more self-managing computing systems. Autonomic computing represents a collection and integration of technologies that enable the creation of an information technology computing infrastructure for IBM's agenda for the next era of computing--e-business on demand. This paper presents an overview of IBM's autonomic computing initiative. It examines the genesis of autonomic computing, the industry and marketplace drivers, the fundamental characteristics of autonomic systems, a framework for how systems will evolve to become more self-managing, and the key role for open industry standards needed to support autonomic behavior in heterogeneous system environments. Technologies explored in each of the papers presented in this issue are introduced for the reader."
264,53,206,1,Transcriptional Regulatory Networks in Saccharomyces cerevisiae,"We have determined how most of the transcriptional regulators encoded in the eukaryote {S}accharomyces cerevisiae associate with genes across the genome in living cells. {J}ust as maps of metabolic networks describe the potential pathways that may be used by a cell to accomplish metabolic processes, this network of regulator-gene interactions describes potential pathways yeast cells can use to regulate global gene expression programs. {W}e use this information to identify network motifs, the simplest units of network architecture, and demonstrate that an automated process can use motifs to assemble a transcriptional regulatory network structure. {O}ur results reveal that eukaryotic cellular functions are highly connected through networks of transcriptional regulators that regulate other transcriptional regulators."
265,53,5327,1,A mutation accumulation assay reveals a broad capacity for rapid evolution of gene expression,"Mutation is the ultimate source of biological diversity because it generates the variation that fuels evolution(1). Gene expression is the first step by which an organism translates genetic information into developmental change. Here we estimate the rate at which mutation produces new variation in gene expression by measuring transcript abundances across the genome during the onset of metamorphosis in 12 initially identical Drosophila melanogaster lines that independently accumulated mutations for 200 generations(2). We find statistically significant mutational variation for 39% of the genome and a wide range of variability across corresponding genes. As genes are upregulated in development their variability decreases, and as they are downregulated it increases, indicating that developmental context affects the evolution of gene expression. A strong correlation between mutational variance and environmental variance shows that there is the potential for widespread canalization(3). By comparing the evolutionary rates that we report here with differences between species(4,5), we conclude that gene expression does not evolve according to strictly neutral models. Although spontaneous mutations have the potential to generate abundant variation in gene expression, natural variation is relatively constrained."
266,53,10283,1,The Quantitative Genetics of Transcription," Quantitative geneticists have become interested in the heritability of transcription and detection of expression quantitative trait loci (eQTLs). Linkage mapping methods have identified major-effect eQTLs for some transcripts and have shown that regulatory polymorphisms in cis and in trans affect expression. It is also clear that these mapping strategies have little power to detect polygenic factors, and some new statistical approaches are emerging that paint a more complex picture of transcriptional heritability. Several studies imply pervasive non-additivity of transcription, transgressive segregation and epistasis, and future studies will soon document the extent of genotypeâenvironment interaction and population structure at the transcriptional level. The implications of these findings for genotypeâphenotype mapping and modeling the evolution of transcription are discussed."
267,53,11838,1,Widely distributed noncoding purifying selection in the human genome,"10.1073/pnas.0705140104 It is widely assumed that human noncoding sequences comprise a substantial reservoir for functional variants impacting gene regulation and other chromosomal processes. Evolutionarily conserved noncoding sequences (CNSs) in the human genome have attracted considerable attention for their potential to simplify the search for functional elements and phenotypically important human alleles. A major outstanding question is whether functionally significant human noncoding variation is concentrated in CNSs or distributed more broadly across the genome. Here, we combine wholegenome sequence data from four nonhuman species (chimp, dog, mouse, and rat) with recently available comprehensive human polymorphism data to analyze selection at single-nucleotide resolution. We show that a substantial fraction of active purifying selection in human noncoding sequences occurs outside of CNSs and is diffusely distributed across the genome. This finding suggests the existence of a large complement of human noncoding variants that may impact gene expression and phenotypic traits, the majority of which will escape detection with current approaches to genome analysis."
268,53,13063,1,Independent effects of cis- and trans-regulatory variation on gene expression in Drosophila melanogaster,"Biochemical interactions between cis-regulatory DNA sequences and trans-regulatory gene products suggest that cis- and trans-acting polymorphisms may interact genetically. Here we present a strategy to test this hypothesis by comparing the relative cis-regulatory activity of two alleles in different genetic backgrounds. Of the eight genes surveyed in this study, five were affected by trans-acting variation that altered total transcript levels, two of which were also affected by differences in cis-regulation. The presence of trans-acting variation had no effect on relative cis-regulatory activity, showing that cis-regulatory polymorphisms can function independently of trans-regulatory variation. The frequency of such independent interactions on a genomic scale is yet to be determined. 10.1534/genetics.107.082032"
269,54,1743,1,The plasticity of aging: insights from long-lived mutants.,"Mutations in genes affecting endocrine signaling, stress responses, metabolism, and telomeres can all increase the life spans of model organisms. These mutations have revealed evolutionarily conserved pathways for aging, some of which appear to extend life span in response to sensory cues, caloric restriction, or stress. Many mutations affecting longevity pathways delay age-related disease, and the molecular analysis of these pathways is leading to a mechanistic understanding of how these two processes--aging and disease susceptibility--are linked."
270,54,3274,1,The Patterns of Natural Variation in Human Genes.,"Currently, more than 10 million DNA sequence variations have been uncovered in the human genome. The most detailed variation discovery efforts have focused on candidate genes involved in cardiovascular disease or in susceptibilities associated with exposure to environmental agents. Here we provide an overviewof natural genetic variation from the literature and in 510 human candidate genes resequenced for variation discovery. The average human gene contains 126 biallelic polymorphisms, 46 of which are common (>/=5% minor allele frequency) and 5 of which are found in coding regions. Using this complete picture of genetic diversity, we explore conservation, signatures of selection, and historical recombination to mine information useful for candidate gene association studies. In general, we find that the patterns of human gene variation suggest that no one approach will be appropriate for genetic association studies across all genes. Therefore, many different approaches may be required to identify the elusive genotypes associated with common human phenotypes. Expected online publication date for the Annual Review of Genomics and Human Genetics Volume 6 is August 30, 2005. Please see http://www.annualreviews.org/catalog/pub_dates.asp for revised estimates."
271,54,5065,1,Genetic association studies.,"We review the rationale behind and discuss methods of design and analysis of genetic association studies. There are similarities between genetic association studies and classic epidemiological studies of environmental risk factors but there are also issues that are specific to studies of genetic risk factors such as the use of particular family-based designs, the need to account for different underlying genetic mechanisms, and the effect of population history. Association differs from linkage (covered elsewhere in this series) in that the alleles of interest will be the same across the whole population. As with other types of genetic epidemiological study, issues of design, statistical analysis, and interpretation are very important."
272,55,768,1,Basic local alignment search tool.,"{A new approach to rapid sequence comparison, basic local alignment search tool (BLAST), directly approximates alignments that optimize a measure of local similarity, the maximal segment pair (MSP) score. Recent mathematical results on the stochastic properties of MSP scores allow an analysis of the performance of this method as well as the statistical significance of alignments it generates. The basic algorithm is simple and robust; it can be implemented in a number of ways and applied in a variety of contexts including straightforward DNA and protein sequence database searches, motif searches, gene identification searches, and in the analysis of multiple regions of similarity in long DNA sequences. In addition to its flexibility and tractability to mathematical analysis, BLAST is an order of magnitude faster than existing sequence comparison tools of comparable sensitivity.}"
273,55,1705,1,{KEGG: Kyoto Encyclopedia of Genes and Genomes},"K{EGG} ({K}yoto {E}ncyclopedia of {G}enes and {G}enomes) is a knowledge base for systematic analysis of gene functions, linking genomic information with higher order functional information. {T}he genomic information is stored in the {GENES} database, which is a collection of gene catalogs for all the completely sequenced genomes and some partial genomes with up-to-date annotation of gene functions. {T}he higher order functional information is stored in the {PATHWAY} database, which contains graphical representations of cellular processes, such as metabolism, membrane transport, signal transduction and cell cycle. {T}he {PATHWAY} database is supplemented by a set of ortholog group tables for the information about conserved subpathways (pathway motifs), which are often encoded by positionally coupled genes on the chromosome and which are especially useful in predicting gene functions. {A} third database in {KEGG} is {LIGAND} for the information about chemical compounds, enzyme molecules and enzymatic reactions. {KEGG} provides {J}ava graphics tools for browsing genome maps, comparing two genome maps and manipulating expression maps, as well as computational tools for sequence comparison, graph comparison and path computation. {T}he {KEGG} databases are daily updated and made freely available (http://www. genome.ad.jp/kegg/)."
274,55,3089,1,Comparative metagenomics of microbial communities.,The species complexity of microbial communities and challenges in culturing representative isolates make it difficult to obtain assembled genomes. Here we characterize and compare the metabolic capabilities of terrestrial and marine microbial communities using largely unassembled sequence data obtained by shotgun sequencing DNA isolated from the various environments. Quantitative gene content analysis reveals habitat-specific fingerprints that reflect known characteristics of the sampled environments. The identification of environment-specific genes through a gene-centric comparative analysis presents new opportunities for interpreting and diagnosing environments. 10.1126/science.1107851
275,55,4841,1,Protein Molecular Function Prediction by Bayesian Phylogenomics,"We present a statistical graphical model to infer specific molecular function for unannotated protein sequences using homology. Based on phylogenomic principles, SIFTER (Statistical Inference of Function Through Evolutionary Relationships) accurately predicts molecular function for members of a protein family given a reconciled phylogeny and available function annotations, even when the data are sparse or noisy. Our method produced specific and consistent molecular function predictions across 100 Pfam families in comparison to the Gene Ontology annotation database, BLAST, GOtcha, and Orthostrapper. We performed a more detailed exploration of functional predictions on the adenosine-5&#8242;-monophosphate/adenosine deaminase family and the lactate/malate dehydrogenase family, in the former case comparing the predictions against a gold standard set of published functional characterizations. Given function annotations for 3&#37; of the proteins in the deaminase family, SIFTER achieves 96&#37; accuracy in predicting molecular function for experimentally characterized proteins as reported in the literature. The accuracy of SIFTER on this dataset is a significant improvement over other currently available methods such as BLAST (75&#37;), GeneQuiz (64&#37;), GOtcha (89&#37;), and Orthostrapper (11&#37;). We also experimentally characterized the adenosine deaminase from Plasmodium falciparum, confirming SIFTER&#39;s prediction. The results illustrate the predictive power of exploiting a statistical model of function evolution in phylogenomic problems. A software implementation of SIFTER is available from the authors."
276,55,6618,1,{Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy},"This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task."
277,55,8401,1,A new measure for functional similarity of gene products based on Gene Ontology,"BACKGROUND:Gene Ontology (GO) is a standard vocabulary of functional terms and allows for coherent annotation of gene products. These annotations provide a basis for new methods that compare gene products regarding their molecular function and biological role.RESULTS:We present a new method for comparing sets of GO terms and for assessing the functional similarity of gene products. The method relies on two semantic similarity measures; simRel and funSim. One measure (simRel) is applied in the comparison of the biological processes found in different groups of organisms. The other measure (funSim) is used to find functionally related gene products within the same or between different genomes. Results indicate that the method, in addition to being in good agreement with established sequence similarity approaches, also provides a means for the identification of functionally related proteins independent of evolutionary relationships. The method is also applied to estimating functional similarity between all proteins in Saccharomyces cerevisiae and to visualizing the molecular function space of yeast in a map of the functional space. A similar approach is used to visualize the functional relationships between protein families.CONCLUSION:The approach enables the comparison of the underlying molecular biology of different taxonomic groups and provides a new comparative genomics tool identifying functionally related gene products independent of homology. The proposed map of the functional space provides a new global view on the functional relationships between gene products or protein families."
278,55,9443,1,"The Connectivity Map: Using Gene-Expression Signatures to Connect Small Molecules, Genes, and Disease","To pursue a systematic approach to the discovery of functional connections among diseases, genetic perturbation, and drug action, we have created the first installment of a reference collection of gene-expression profiles from cultured human cells treated with bioactive small molecules, together with pattern-matching software to mine these data. We demonstrate that this Ã¢ÂÂConnectivity MapÃ¢ÂÂ resource can be used to find connections among small molecules sharing a mechanism of action, chemicals and physiological processes, and diseases and drugs. These results indicate the feasibility of the approach and suggest the value of a large-scale community Connectivity Map project."
279,55,11033,1,"Blast2GO: a universal tool for annotation, visualization and analysis in functional genomics research","Summary: We present here Blast2GO (B2G), a research tool designed with the main purpose of enabling Gene Ontology (GO) based data mining on sequence data for which no GO annotation is yet available. B2G joints in one application GO annotation based on similarity searches with statistical analysis and highlighted visualization on directed acyclic graphs. This tool offers a suitable platform for functional genomics research in non-model species. B2G is an intuitive and interactive desktop application that allows monitoring and comprehension of the whole annotation and analysis process. Availability: Blast2GO is freely available via Java Web Start at http://www.blast2go.de Supplementary material: http://www.blast2go.de -> Evaluation Contact: aconesa@ivia.es; stefang@fis.upv.es"
280,55,13550,1,High-throughput functional annotation and data mining with the Blast2GO suite,"Functional genomics technologies have been widely adopted in the biological research of both model and non-model species. An efficient functional annotation of DNA or protein sequences is a major requirement for the successful application of these approaches as functional information on gene products is often the key to the interpretation of experimental results. Therefore, there is an increasing need for bioinformatics resources which are able to cope with large amount of sequence data, produce valuable annotation results and are easily accessible to laboratories where functional genomics projects are being undertaken. We present the Blast2GO suite as an integrated and biologist-oriented solution for the high-throughput and automatic functional annotation of DNA or protein sequences based on the Gene Ontology vocabulary. The most outstanding Blast2GO features are: (i) the combination of various annotation strategies and tools controlling type and intensity of annotation, (ii) the numerous graphical features such as the interactive GO-graph visualization for gene-set function profiling or descriptive charts, (iii) the general sequence management features and (iv) high-throughput capabilities. We used the Blast2GO framework to carry out a detailed analysis of annotation behaviour through homology transfer and its impact in functional genomics research. Our aim is to offer biologists useful information to take into account when addressing the task of functionally characterizing their sequence data. 10.1093/nar/gkn176"
281,56,706,1,NP-complete Problems and Physical Reality,"Can NP-complete problems be solved efficiently in the physical universe? I survey proposals including soap bubbles, protein folding, quantum computing, quantum advice, quantum adiabatic algorithms, quantum-mechanical nonlinearities, hidden variables, relativistic time dilation, analog computing, Malament-Hogarth spacetimes, quantum gravity, closed timelike curves, and âanthropic computing. â The section on soap bubbles even includes some âexperimental â results. While I do not believe that any of the proposals will let us solve NP-complete problems efficiently, I argue that by studying them, we can learn something not only about computation but also about physics. 1"
282,56,9056,1,Discrete mathematics: methods and challenges,"Combinatorics is a fundamental mathematical discipline as well as an essential component of many mathematical areas, and its study has experienced an impressive growth in recent years. One of the main reasons for this growth is the tight connection between Discrete Mathematics and Theoretical Computer Science, and the rapid development of the latter. While in the past many of the basic combinatorial results were obtained mainly by ingenuity and detailed reasoning, the modern theory has grown out of this early stage, and often relies on deep, well developed tools. This is a survey of two of the main general techniques that played a crucial role in the development of modern combinatorics; algebraic methods and probabilistic methods. Both will be illustrated by examples, focusing on the basic ideas and the connection to other areas."
283,57,6160,1,SLIM: an alternative Web interface for MEDLINE/PubMed searches - a preliminary study.,"Background  With the rapid growth of medical information and the pervasiveness of the Internet, online search and retrieval systems have become indispensable tools in medicine. The progress of Web technologies can provide expert searching capabilities to non-expert information seekers. The objective of the project is to create an alternative search interface for MEDLINE/PubMed searches using JavaScript slider bars. SLIM, or Slider Interface for MEDLINE/PubMed searches, was developed with PHP and JavaScript. Interactive slider bars in the search form controlled search parameters such as limits, filters and MeSH terminologies. Connections to PubMed were done using the Entrez Programming Utilities (E-Utilities). Custom scripts were created to mimic the automatic term mapping process of Entrez. Page generation times for both local and remote connections were recorded.  Results  Alpha testing by developers showed SLIM to be functionally stable. Page generation times to simulate loading times were recorded the first week of alpha and beta testing. Average page generation times for the index page, previews and searches were 2.94 milliseconds, 0.63 seconds and 3.84 seconds, respectively. Eighteen physicians from the US, Australia and the Philippines participated in the beta testing and provided feedback through an online survey. Most users found the search interface user-friendly and easy to use. Information on MeSH terms and the ability to instantly hide and display abstracts were identified as distinctive features.  Conclusion  SLIM can be an interactive time-saving tool for online medical literature research that improves user control and capability to instantly refine and refocus search strategies. With continued development and by integrating search limits, methodology filters, MeSH terms and levels of evidence, SLIM may be useful in the practice of evidence-based medicine."
284,57,8786,1,Scholarly work and the shaping of digital access,"Abstract 10.1002/asi.20204.abs In the cycle of scholarly communication, scholars play the role of both consumer and contributor of intellectual works within the stores of recorded knowledge. In the digital environment scholars are seeking and using information in new ways and generating new types of scholarly products, many of which are specialized resources for access to research information. These practices have important implications for the collection and organization of digital access resources. Drawing on a series of qualitative studies investigating the information work of scientists and humanities scholars, specific information seeking activities influenced by the Internet and two general modes of information access evident in research practice are identified in this article. These conceptual modes of access are examined in relation to the digital access resources currently being developed by researchers in the humanities and neuroscience. Scholars' modes of access and their âworkingâ and âimplicitâ assemblages of information represent what researchers actually do when gathering and working with research materials and therefore provide a useful framework for the collection and organization of access resources in research libraries."
285,57,13742,1,"PolySearch: a web-based text mining system for extracting relationships between human diseases, genes, mutations, drugs and metabolites.","A particular challenge in biomedical text mining is to find ways of handling 'comprehensive' or 'associative' queries such as 'Find all genes associated with breast cancer'. Given that many queries in genomics, proteomics or metabolomics involve these kind of comprehensive searches we believe that a web-based tool that could support these searches would be quite useful. In response to this need, we have developed the PolySearch web server. PolySearch supports >50 different classes of queries against nearly a dozen different types of text, scientific abstract or bioinformatic databases. The typical query supported by PolySearch is 'Given X, find all Y's' where X or Y can be diseases, tissues, cell compartments, gene/protein names, SNPs, mutations, drugs and metabolites. PolySearch also exploits a variety of techniques in text mining and information retrieval to identify, highlight and rank informative abstracts, paragraphs or sentences. PolySearch's performance has been assessed in tasks such as gene synonym identification, protein-protein interaction identification and disease gene identification using a variety of manually assembled 'gold standard' text corpuses. Its f-measure on these tasks is 88, 81 and 79%, respectively. These values are between 5 and 50% better than other published tools. The server is freely available at http://wishart.biology.ualberta.ca/polysearch."
286,57,14269,1,"Towards a cyberinfrastructure for the biological sciences: progress, visions and challenges","Wiki pages and commentingBiology is an information-driven science. Large-scale data sets from genomics, physiology, population genetics and imaging are driving research at a dizzying rate. Simultaneously, interdisciplinary collaborations among experimental biologists, theorists, statisticians and computer scientists have become the key to making effective use of these data sets. However, too many biologists have trouble accessing and using these electronic data sets and tools effectively. A 'cyberinfrastructure' is a combination of databases, network protocols and computational services that brings people, information and computational tools together to perform science in this information-driven world. This article reviews the components of a biological cyberinfrastructure, discusses current and pending implementations, and notes the many challenges that lie ahead."
287,57,15263,1,Semantic web for integrated network analysis in biomedicine,"The Semantic Web technology enables integration of heterogeneous data on the World Wide Web by making the semantics of data explicit through formal ontologies. In this article, we survey the feasibility and state of the art of utilizing the Semantic Web technology to represent, integrate and analyze the knowledge in various biomedical networks. We introduce a new conceptual framework, semantic graph mining, to enable researchers to integrate graph mining with ontology reasoning in network data analysis. Through four case studies, we demonstrate how semantic graph mining can be applied to the analysis of disease-causal genes, Gene Ontology category cross-talks, drug efficacy analysis and herb-drug interactions analysis. 10.1093/bib/bbp002"
288,57,16058,1,"Gestores de referencias de Ãºltima generaciÃ³n: anÃ¡lisis comparativo de RefWorks, EndNote Web y Zotero","Reference managing tools are one of the most useful devices for researchers and librarians due to their ability to compile, store and format information related to different products, sources and types of records. In recent years a new generation of reference-managing software has appeared. These new tools include applications from the new technological context that have contributed to reinforcing their capacity and potential. An overview of these tools and their applications is offered. We also makes a comparative analysis of the different products with a view to highlighting their strengths as well as the elements that could be improved in each tool."
289,57,16484,1,<i>CiteULike</i> y <i>Connotea</i>: herramientas 2.0 para el descubrimiento de la informaciÃ³n cientÃ­fica,"Social reference managers automate repetitive and tedious tasks such as literature management, offering an alternative to search engines and traditional databases for social mediation and scientific discovery. In this study we reflect upon the implications of social tagging processes for personal bibliographic management in the 2.0 environment, and we study two of the most famous applications, although still little known and employed in Spain: CiteULike y Connotea."
290,58,4676,1,Human gaze control during real-world scene perception,"In human vision, acuity and color sensitivity are best at the point of fixation, and the visual-cognitive system exploits this fact by actively controlling gaze to direct fixation towards important and informative scene regions in real time as needed. How gaze control operates over complex real-world scenes has recently become of central concern in several core cognitive science disciplines including cognitive psychology, visual neuroscience, and machine vision. This article reviews current approaches and empirical findings in human gaze control during real-world scene perception."
291,58,11667,1,Cross-frequency coupling between neuronal oscillations," Electrophysiological recordings in animals, including humans, are modulated by oscillatory activities in several frequency bands. Little is known about how oscillations in various frequency bands interact. Recent findings from the human neocortex show that the power of fast gamma oscillations (30â150Â Hz) is modulated by the phase of slower theta oscillations (5â8Â Hz). Given that this coupling reflects a specific interplay between large ensembles of neurons, it is likely to have profound implications for neuronal processing."
292,58,13181,1,Rapid Neural Coding in the Retina with Relative Spike Latencies,"Natural vision is a highly dynamic process. Frequent body, head, and eye movements constantly bring new images onto the retina for brief periods, challenging our understanding of the neural code for vision. We report that certain retinal ganglion cells encode the spatial structure of a briefly presented image in the relative timing of their first spikes. This code is found to be largely invariant to stimulus contrast and robust to noisy fluctuations in response latencies. Mechanistically, the observed response characteristics result from different kinetics in two retinal pathways (""ON"" and ""OFF"") that converge onto ganglion cells. This mechanism allows the retina to rapidly and reliably transmit new spatial information with the very first spikes emitted by a neural population. 10.1126/science.1149639"
293,58,14279,1,Interhemispheric correlations of slow spontaneous neuronal fluctuations revealed in human sensory cortex.,"Animal studies have shown robust electrophysiological activity in the sensory cortex in the absence of stimuli or tasks. Similarly, recent human functional magnetic resonance imaging (fMRI) revealed widespread, spontaneously emerging cortical fluctuations. However, it is unknown what neuronal dynamics underlie this spontaneous activity in the human brain. Here we studied this issue by combining bilateral single-unit, local field potentials (LFPs) and intracranial electrocorticography (ECoG) recordings in individuals undergoing clinical monitoring. We found slow (&lt;0.1 Hz, following 1/f-like profiles) spontaneous fluctuations of neuronal activity with significant interhemispheric correlations. These fluctuations were evident mainly in neuronal firing rates and in gamma (40-100 Hz) LFP power modulations. Notably, the interhemispheric correlations were enhanced during rapid eye movement and stage 2 sleep. Multiple intracranial ECoG recordings revealed clear selectivity for functional networks in the spontaneous gamma LFP power modulations. Our results point to slow spontaneous modulations in firing rate and gamma LFP as the likely correlates of spontaneous fMRI fluctuations in the human sensory cortex."
294,58,16409,1,The Asynchronous State in Cortical Circuits,"Correlated spiking is often observed in cortical circuits, but its functional role is controversial. It is believed that correlations are a consequence of shared inputs between nearby neurons and could severely constrain information decoding. Here we show theoretically that recurrent neural networks can generate an asynchronous state characterized by arbitrarily low mean spiking correlations despite substantial amounts of shared input. In this state, spontaneous fluctuations in the activity of excitatory and inhibitory populations accurately track each other, generating negative correlations in synaptic currents which cancel the effect of shared input. Near-zero mean correlations were seen experimentally in recordings from rodent neocortex in vivo. Our results suggest a reexamination of the sources underlying observed correlations and their functional consequences for information processing. 10.1126/science.1179850"
295,59,1189,1,Pattern Recognition and Neural Networks,"{This book uses tools from statistical decision theory and computational learning theory to create  a rigorous foundation for the theory of neural networks. On the theoretical side, <I>Pattern Recognition and  Neural Networks</I> emphasizes probability and statistics. Almost all the results have proofs that are often  original. On the application side, the emphasis is on pattern recognition. Most of the examples are from real  world problems. In addition to the more common types of networks, the book has chapters on decision trees  and belief networks from the machine-learning field. This book is intended for use in graduate courses that  teach statistics and engineering. A strong background in statistics is needed to fully appreciate the  theoretical developments and proofs. However, undergraduate-level linear algebra, calculus, and  probability knowledge is sufficient to follow the book.} {Ripley brings together two crucial ideas in pattern recognition: statistical methods and machine learning via neural networks. He brings unifying principles to the fore, and reviews the state of the subject. Ripley also includes many examples to illustrate real problems in pattern recognition and how to overcome them.}"
296,59,6638,1,Numerical Recipes in Fortran 77: The Art of Scientific Computing,"This is the greatly revised and greatly expanded Second Edition of the hugely popular Numerical Recipes: The Art of Scientific Computing. The product of a unique collaboration among four leading scientists in academic research and industry Numerical Recipes is a complete text and reference book on scientific computing. In a self-contained manner it proceeds from mathematical and theoretical considerations to actual practical computer routines. With over 100 new routines bringing the total to well over 300, plus upgraded versions of the original routines, this new edition remains the most practical, comprehensive handbook of scientific computing available today. Highlights of the new material include: -A new chapter on integral equations and inverse methods -Multigrid and other methods for solving partial differential equations -Improved random number routines - Wavelet transforms -The statistical bootstrap method -A new chapter on ""less-numerical"" algorithms including compression coding and arbitrary precision arithmetic. The book retains the informal easy-to-read style that made the first edition so popular, while introducing some more advanced topics. It is an ideal textbook for scientists and engineers and an indispensable reference for anyone who works in scientific computing. The Second Edition is availabe in FORTRAN, the traditional language for numerical calculations and in the increasingly popular C language."
297,60,1204,1,{Maximum likelihood from incomplete data via the EM algorithm},"{A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.}"
298,61,248,1,Data clustering: a review,"Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify  cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval."
299,61,11004,1,An Extended Set of Haar-like Features for Rapid Object Detection,"Recently Viola et al. [5] have introduced a rapid object detection scheme based on a boosted cascade of simple features. In this paper we introduce a novel set of rotated haar-like features, which significantly enrich this basic set of simple haar-like features and which can also be calculated very efficiently. At a given hit rate our sample face detector shows off on average a 10 % lower false alarm rate by means of using these additional rotated features. We also present a novel post optimization procedure for a given boosted cascade improving on average the false alarm rate further by 12.5%. Using both enhancements the number of false detections is only 24 at a hit rate of 82.3 % on the CMU face set [7]. 1"
300,62,8205,1,Seeing it differently: visual processing in autism.,"Several recent behavioral and neuroimaging studies have documented an impairment in face processing in individuals with Autism Spectrum Disorder (ASD). It remains unknown, however, what underlying mechanism gives rise to this face processing difficulty. One theory suggests that the difficulty derives from a pervasive problem in social interaction and/or motivation. An alternative view proposes that the face-processing problem is not entirely social in nature and that a visual perceptual impairment might also contribute. The focus of this review is on this latter, perceptual perspective, documenting the psychological and neural alterations that might account for the face processing impairment. The available evidence suggests that perceptual alterations are present in ASD, independent of social function."
301,62,11711,1,Network and intrinsic cellular mechanisms underlying theta phase precession of hippocampal neurons,"Hippocampal `place cells' systematically shift their phase of firing in relation to the theta rhythm as an animal traverses the `place field'. These dynamics imply that the neural ensemble begins each theta cycle at a point in its state-space that might `represent' the current location of the rat, but that the ensemble `looks ahead' during the rest of the cycle. Phase precession could result from intrinsic cellular dynamics involving interference of two oscillators of different frequencies, or from network interactions, similar to Hebb's `phase sequence' concept, involving asymmetric synaptic connections. Both models have difficulties accounting for all of the available experimental data, however. A hybrid model, in which the look-ahead phenomenon implied by phase precession originates in superficial entorhinal cortex by some form of interference mechanism and is enhanced in the hippocampus proper by asymmetric synaptic plasticity during sequence encoding, seems to be consistent with available data, but as yet there is no fully satisfactory theoretical account of this phenomenon. This review is part of the INMED/TINS special issue Physiogenic and pathogenic oscillations: the beauty and the beast, based on presentations at the annual INMED/TINS symposium (http://inmednet.com)."
302,62,13291,1,A central circuit of the mind," The methodologies of cognitive architectures and functional magnetic resonance imaging can mutually inform each other. For example, four modules of the ACT-R (adaptive control of thought â rational) cognitive architecture have been associated with four brain regions that are active in complex tasks. Activity in a lateral inferior prefrontal region reflects retrieval of information in a declarative module; activity in a posterior parietal region reflects changes to problem representations in an imaginal module; activity in the anterior cingulate cortex reflects the updates of control information in a goal module; and activity in the caudate nucleus reflects execution of productions in a procedural module. Differential patterns of activation in such central regions can reveal the time course of different components of complex cognition."
303,63,918,1,Linked: How Everything Is Connected to Everything Else and What It Means,"How is the human brain like the AIDS epidemic? Ask physicist Albert-LÃ¡szlÃ³ BarabÃ¡si and he'll explain them both in terms of networks of individual nodes connected via complex but understandable relationships. _Linked: The New Science of Networks_ is his bright, accessible guide to the fundamentals underlying neurology, epidemiology, Internet traffic, and many other fields united by complexity.  BarabÃ¡si's gift for concrete, nonmathematical explanations and penchant for eccentric humor would make the book thoroughly enjoyable even if the content weren't engaging. But the results of BarabÃ¡si's research into the behavior of networks are deeply compelling. Not all networks are created equal, he says, and he shows how even fairly robust systems like the Internet could be crippled by taking out a few super-connected nodes, or hubs. His mathematical descriptions of this behavior are helping doctors, programmers, and security professionals design systems better suited to their needs. _Linked_ presents the next step in complexity theory--from understanding chaos to practical applications. _--Rob Lightner_"
304,64,13217,1,Neuronal oscillations and visual amplification of speech,"It is widely recognized that viewing a speaker's face enhances vocal communication, although the neural substrates of this phenomenon remain unknown. We propose that the enhancement effect uses the ongoing oscillatory activity of local neuronal ensembles in the primary auditory cortex. Neuronal oscillations reflect rhythmic shifting of neuronal ensembles between high and low excitability states. Our hypothesis holds that oscillations are `predictively' modulated by visual input, so that related auditory input arrives during a high excitability phase and is thus amplified. We discuss the anatomical substrates and key timing parameters that enable and constrain this effect. Our hypothesis makes testable predictions for future studies and emphasizes the idea that `background' oscillatory activity is instrumental to cortical sensory processing."
305,64,15861,1,Experience sampling during fMRI reveals default network and executive system contributions to mind wandering,"10.1073/pnas.0900234106 Although mind wandering occupies a large proportion of our waking life, its neural basis and relation to ongoing behavior remain controversial. We report an fMRI study that used experience sampling to provide an online measure of mind wandering during a concurrent task. Analyses focused on the interval of time immediately preceding experience sampling probes demonstrate activation of default network regions during mind wandering, a finding consistent with theoretical accounts of default network functions. Activation in medial prefrontal default network regions was observed both in association with subjective self-reports of mind wandering and an independent behavioral measure (performance errors on the concurrent task). In addition to default network activation, mind wandering was associated with executive network recruitment, a finding predicted by behavioral theories of off-task thought and its relation to executive resources. Finally, neural recruitment in both default and executive network regions was strongest when subjects were unaware of their own mind wandering, suggesting that mind wandering is most pronounced when it lacks meta-awareness. The observed parallel recruitment of executive and default network regionsÃ¢ÂÂtwo brain systems that so far have been assumed to work in oppositionÃ¢ÂÂsuggests that mind wandering may evoke a unique mental state that may allow otherwise opposing networks to work in cooperation. The ability of this study to reveal a number of crucial aspects of the neural recruitment associated with mind wandering underscores the value of combining subjective self-reports with online measures of brain function for advancing our understanding of the neurophenomenology of subjective experience."
306,65,5685,1,Current practice in measuring usability: Challenges to usability studies and research,"How to measure usability is an important question in HCI research and user interface evaluation. We review current practice in measuring usability by categorizing and discussing usability measures from 180 studies published in core HCI journals and proceedings. The discussion distinguish several problems with the measures, including whether they actually measure usability, if they cover usability broadly, how they are reasoned about, and if they meet recommendations on how to measure usability. In many studies, the choice of and reasoning about usability measures fall short of a valid and reliable account of usability as quality-in-use of the user interface being studied. Based on the review, we discuss challenges for studies of usability and for research into how to measure usability. The challenges are to distinguish and empirically compare subjective and objective measures of usability; to focus on developing and employing measures of learning and retention; to study long-term use and usability; to extend measures of satisfaction beyond post-use questionnaires; to validate and standardize the host of subjective satisfaction questionnaires used; to study correlations between usability measures as a means for validation; and to use both micro and macro tasks and corresponding measures of usability. In conclusion, we argue that increased attention to the problems identified and challenges discussed may strengthen studies of usability and usability research."
307,65,14735,1,How Colorful Was Your Day? Why Questionnaires Cannot Assess Presence in Virtual Environments,"This paper argues that a scientific basis for âpresenceâ as it's usually understood in virtual environments research, can not be established on the basis of postexperience presence questionnaires alone. To illustrate the point, an arbitrary mental attribute called âcolorfulness of the experienceâ is conjured up, and a set of questions administered to 74 respondents with an online questionnaire. The results suggested that colorfulness of yesterday's experiences was associated with the extent to which a person accomplished their tasks, and also associated with yesterday being a âgoodâ, âpleasantâ, but not frustrating day. The meaning lessness of this analysis illustrates that the equivalent methodology used by presence researchers, may, similarly, bring into being the idea of presence in the minds of {VE} participants. However, it is argued that there can be no evidence on this methodological basis that presence played any role in their actual mental activity or behavior at the time of the experience. It is concluded that presence researchers must move away from heavy reliance on questionnaires in order to make any progress in this area."
308,66,693,1,Conditional Independence in Statistical Theory,"Some simple heuristic properties of conditional independence are shown to form a conceptual framework for much of the theory of statistical inference. This framework is illustrated by an examination of the role of conditional independence in several diverse areas of the field of statistics. Topics covered include sufficiency and ancillarity, parameter identification, causal inference, prediction sufficiency, data selection mechanisms, invariant statistical models and a subjectivist approach to model-building."
309,66,1036,1,Mining the Web: Discovering Knowledge from Hypertext Data,"{Mining the Web: Discovering Knowledge from Hypertext Data is the first book devoted entirely to techniques for producing knowledge from the vast body of unstructured Web data. Building on an initial survey of infrastructural issuesincluding Web crawling and indexingChakrabarti examines low-level machine learning techniques as they relate specifically to the challenges of Web mining. He then devotes the final part of the book to applications that unite infrastructure and analysis to bring machine learning to bear on systematically acquired and stored data. Here the focus is on results: the strengths and weaknesses of these applications, along with their potential as foundations for further progress. From Chakrabarti's workpainstaking, critical, and forward-lookingreaders will gain the theoretical and practical understanding they need to contribute to the Web mining effort.<br><br>* A comprehensive, critical exploration of statistics-based attempts to make sense of Web Mining.<br>* Details the special challenges associated with analyzing unstructured and semi-structured data.<br>* Looks at how classical Information Retrieval techniques have been modified for use with Web data.<br>* Focuses on today's dominant learning methods: clustering and classification, hyperlink analysis, and supervised and semi-supervised learning.<br>* Analyzes current applications for resource discovery and social network analysis.<br>* An excellent way to introduce students to especially vital applications of data mining and machine learning technology.</li></ul>}"
310,66,1691,1,Principal component analysis,"Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques it continues to be the subject of much research, ranging from new model- based approaches to algorithmic ideas from neural networks. It is extremely versatile with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is Professor of Statistics at the University of Aberdeen. He is author or co-author of over 60 research papers and three other books. His research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years."
311,66,4303,1,Probabilistic and Statistical Properties of Words: An Overview,"In the following, an overview is given on statistical and probabilistic properties of words, as occurring in the analysis of biological sequences. Counts of occurrence, counts of clumps, and renewal counts are distinguished, and exact distributions as well as normal approximations, Poisson process approximations, and compound Poisson approximations are derived. Here, a sequence is modelled as a stationary ergodic Markov chain; a test for determining the appropriate order of the Markov chain is described. The convergence results take the error made by estimating the Markovian transition probabilities into account. The main tools involved are moment generating functions, martingales, Stein's method, and the Chen-Stein method. Similar results are given for occurrences of multiple patterns, and, as an example, the problem of unique recoverability of a sequence from SBH chip data is discussed. Special emphasis lies on disentangling the complicated dependence structure between word occurrences, due to self-overlap as well as due to overlap between words. The results can be used to derive approximate, and conservative, confidence intervals for tests."
312,66,6449,1,Scientific Reasoning: The Bayesian Approach,"{In this clearly reasoned defense of Bayes's Theorem &#151; that probability can be used to reasonably justify scientific theories &#151; Colin Howson and Peter Urbach examine the way in which scientists appeal to probability arguments, and demonstrate that the classical approach to statistical inference is full of flaws. Arguing the case for the Bayesian method with little more than basic algebra, the authors show that it avoids the difficulties of the classical system. The book also refutes the major criticisms leveled against Bayesian logic, especially that it is too subjective. This newly updated edition of this classic textbook is also suitable for college courses.}"
313,66,9890,1,Factor analysis,"A frequently applied paradigm in analyzing data from multivariate observations is to model the relevant information (represented in a multivariate variable X) as coming from a limited number of latent factors. In a survey on household consumption, for example, the consumption levels, X, of p different goods during one month could be observed. The variations and covariations of the p components of X throughout the survey might in fact be explained by two or three main social behavior factors of the household. For instance, a basic desire of comfort or the willingness to achieve a certain social level or other social latent concepts might explain most of the consumption behavior. These unobserved factors are much more interesting to the social scientist than the observed quantitative measures (X) themselves, because they give a better understanding of the behavior of households. As shown in the examples below, the same kind of factor analysis is of interest in many fields such as psychology, marketing, economics, politic sciences, etc."
314,66,11707,1,The Neural Basis of Decision Making,"Abstract The study of decision making spans such varied fields as neuroscience, psychology, economics, statistics, political science, and computer science. Despite this diversity of applications, most decisions share common elements including deliberation and commitment. Here we evaluate recent progress in understanding how these basic elements of decision formation are implemented in the brain. We focus on simple decisions that can be studied in the laboratory but emphasize general principles likely to extend to other settings."
315,66,13155,1,The hippocampus and memory: insights from spatial processing.,"The hippocampus appears to be crucial for long-term episodic memory, yet its precise role remains elusive. Electrophysiological studies in rodents offer a useful starting point for developing models of hippocampal processing in the spatial domain. Here we review one such model that points to an essential role for the hippocampus in the construction of mental images. We explain how this neural-level mechanistic account addresses some of the current controversies in the field, such as the role of the hippocampus in imagery and short-term memory, and discuss its broader implications for the neural bases of episodic memory."
316,67,616,1,Folksonomies - Cooperative Classification and Communication Through Shared Metadata,"This paper examines user-&#8205;generated metadata as implemented and applied in two web services designed to share and organize digital media to better understand grassroots classification. Metadata - data about data - allows systems to collocate related information, and helps users find relevant information. The creation of metadata has generally been approached in two ways: professional creation and author creation. In libraries and other organizations, creating metadata, primarily in the form of catalog records, has traditionally been the domain of dedicated professionals working with complex, detailed rule sets and vocabularies. The primary problem with this approach is scalability and its impracticality for the vast amounts of content being produced and used, especially on the World Wide Web. The apparatus and tools built around professional cataloging systems are generally too complicated for anyone without specialized training and knowledge. A second approach is for metadata to be created by authors. The movement towards creator described documents was heralded by SGML, the WWW, and the Dublin Core Metadata Initiative. There are problems with this approach as well - often due to inadequate or inaccurate description, or outright deception. This paper examines a third approach: user-&#8205;created metadata, where users of the documents and media create metadata for their own individual use that is also shared throughout a community."
317,67,6348,1,Folksonomies: Tidying up Tags?,"1. Introduction A folksonomy is a type of distributed classification system. It is usually created by a group of individuals, typically the resource users. Users add tags to online items, such as images, videos, bookmarks and text. These tags are then shared and sometimes refined. A general review of social bookmarking tools, one popular use area of folksonomies, was given in the April edition of D-Lib [1]. In the article the authors elaborate on the approach taken by social classification systems and the motivators behind tagging. They write, ""...tags are just one kind of metadata and are not a replacement for formal classification systems such as Dublin Core, MODS, etc.... Rather, they are a supplemental means to organise information and order search results."" In this article we look at what makes folksonomies work. We agree with the premise that tags are no replacement for formal systems, but we see this as being the core quality that makes folksonomy tagging so useful. We begin by looking at the issue of ""sloppy tags"", a problem to which critics of folksonomies are keen to allude, and ask if there are ways the folksonomy community could offset such problems and create systems that are conducive to searching, sorting and classifying. We then go on to question this ""tidying up"" approach and its underlying assumptions, highlighting issues surrounding removal of low-quality, redundant or nonsense metadata, and the potential risks of tidying too neatly and thereby losing the very openness that has made folksonomies so popular."
318,67,8929,1,Annotation and Navigation in Semantic Wikis,"Semantic Wikis allow users to semantically annotate their Wiki content. The particular annotations can differ in expressive power, simplicity, and meaning. We present an elaborate conceptual model for semantic annotations, introduce a unique and rich Wiki syntax for these annotations, and discuss how to best formally represent the augmented Wiki content. We improve existing navigation techniques to automat- ically construct faceted browsing for semistructured data. By utilising the Wiki annotations we provide greatly enhanced information retrieval. Further we report on our ongoing development of these techniques in our prototype SemperWiki."
319,67,11217,1,Citeulike: A Researcher's Social Bookmarking Service,"Describes Citeulike, a fusion of Web-based social bookmarking services and traditional bibliographic management tools. The article begins with a discussion of how Citeulike turns the linear 'gather, collect, share' process inherent in academic research into a circular 'gather, collect, share and network' process, enabling the sharing and discovery of academic literature and research papers. The basic functionality of the tool is simple: when a researcher sees a paper on the Web that interests them, they can click a button and have a link to it added to their personal library. Ultimately Citeulike works because it is useful to its users. It automates a repetitive bibliographic management task and it offers a complimentary alternative to search engines and databases of academic literature through socially mediated retrieval and discovery of papers. Adapted from the source document"
320,67,12449,1,Learning with Semantic Wikis,"Abstract. The knowledge society requires life-long learning and flexible learning environments that allow learners to learn whenever they have time, whereever they are, and according to their own needs and background knowledge. In this article, we investigate how Semantic Wikis â a combination of Wiki and Semantic Web technology â can support learners in such flexible learning environments. We first summarise common features of Wikis and Semantic Wikis and then describe different aspects of Semantic Wikis for learning. We also introduce our Semantic Wiki system called IkeWiki and show why it is particularly promising as a learning tool."
321,68,2075,1,Modeling the sustainability of subsistence farming and hunting in the Ituri forest of Zaire,"We used empirical data to simulate the impacts, over the next 40 years, of subsistence-level agricultural clearing and bushmeat consumption on forest resources within the recently established Okapi Wildlife Reserve in northeastern Zaire. Satellite imagery, human population census data, and field measurements were used to calculate Present and projected impacts of agricultural clearing on forest cove: Data on per capita meat consumption and the species captured by hunters were combined with relevant ecological data to estimate ratios of consumption to production and to assess the sustainability of hunting. Even with projected population growth of nearly 300% among local communities over 40 years, sufficient secondary forest is available that agricultural clearing will have minimal effect on mature forest throughout most of the reserve. Impacts on the reserve's fauna will be more dramatic particularly within 15 km of villages where most hunting currently occurs Subsistence exploitation of forest antelopes may be sustainable in much of the reserve (especially if high estimates of game production are used), but as the human Population continues to increase duikers will likely be over- hunted. Primate populations do not appear to be threatened In the near future in those areas where bow hunters exploit monkeys, but an increase in this specialized activity in other legions of the reserve and growing human populations could change this. Although additional surveys of commonly hunted species throughout the Okapi wildlife Reserve are essential to enhancing the precision of the simulation, our results suggest that mitigation efforts should he designed and implemented note if the long-term effects of domestic bushmeat consumption are to be addressed."
322,68,4390,1,"The geoarchaeology of the prehistoric ditched sites of the upper Mae Nam Mun Valley, NE Thailand, III: Late Holocene vegetation history","The upper Mae Nam (River) Mun Valley of northeast Thailand has been occupied at least since the Bronze Age, but is notable for the rapid expansion of intense town-based Iron Age settlement. The area presently forms the seasonally-arid core of mainland southeast Asia, and is presently dominated by increasingly saline soils, low-productivity rice cultivation and regrowth semi-arid scrub. However, the archaeological evidence for this region indicates a highly-productive natural environment within the last two millennia. Pollen sequences from the infill of Iron Age features provide the first palynological evidence for this part of northeast Thailand, detailing Late Holocene vegetational change. The area around the sites was initially dominated by forest, which then underwent two phases of the replacement by mosaics of grassland, probable rice cultivation, arboriculture and scrub, prior to a subsequent phase of forest and woodland regeneration. Spatial patterning of the study area's palaeovegetation appears to have been complex. While a general progress of landscape change is evident, local compositional differences are also clear. Although the region's archaeological and, especially, geomorphological evidence suggests significant climatic change during this period, the pollen record, as in studies further north in the region for the same period, appears to have been dominated by human influences. Of note are the effects of intensified human settlement and thus increased land and natural resource use. At present this Late Holocene pollen sequence yields no evidence for a direct relationship with climatic change."
323,68,5669,1,Integrating the statistical analysis of spatial data in ecology,"In many areas of ecology there is an increasing emphasis on spatial relationships. Often ecologists are interested in new ways of analyzing data with the objective of quantifying spatial patterns, and in designing surveys and experiments in light of the recognition that there may be underlying spatial pattern in biotic responses. In doing so, ecologists have adopted a number of widely different techniques and approaches derived from different schools of thought, and from other scientific disciplines. While the adaptation of a diverse array of statistical approaches and methodologies for the analysis of spatial data has yielded considerable insight into various ecological problems, this diversity of approaches has sometimes impeded communication and retarded more rapid progress in this emergent area. Many of these different statistical methods provide similar information about spatial characteristics, but the differences among these methods make it difficult to compare the results of studies that employ contrasting approaches. The papers in this mini-series explore possible areas of agreement and synthesis between a diversity of approaches to spatial analysis in ecology."
324,68,6712,1,Mixed biodiversity benefits of agri-environment schemes in five European countries,"Abstract Agri-environment schemes are an increasingly important tool for the maintenance and restoration of farmland biodiversity in Europe but their ecological effects are poorly known. Scheme design is partly based on non-ecological considerations and poses important restrictions on evaluation studies. We describe a robust approach to evaluate agri-environment schemes and use it to evaluate the biodiversity effects of agri-environment schemes in five European countries. We compared species density of vascular plants, birds, bees, grasshoppers and crickets, and spiders on 202 paired fields, one with an agri-environment scheme, the other conventionally managed. In all countries, agri-environment schemes had marginal to moderately positive effects on biodiversity. However, uncommon species benefited in only two of five countries and species listed in Red Data Books rarely benefited from agri-environment schemes. Scheme objectives may need to differentiate between biodiversity of common species that can be enhanced with relatively simple modifications in farming practices and diversity or abundance of endangered species which require more elaborate conservation measures."
325,68,6913,1,Quantifying biodiversity: procedures and pitfalls in the measurement and comparison of species richness,"Species richness is a fundamental measurement of community and regional diversity, and it underlies many ecological models and conservation strategies. In spite of its importance, ecologists have not always appreciated the effects of abundance and sampling effort on richness measures and comparisons. We survey a series of common pitfalls in quantifying and comparing taxon richness. These pitfalls can be largely avoided by using accumulation and rarefaction curves, which may be based on either individuals or samples. These taxon sampling curves contain the basic information for valid richness comparisons, including category2013subcategory ratios (species-to-genus and species-to-individual ratios). Rarefaction methods 2013 both sample-based and individual-based 2013 allow for meaningful standardization and comparison of datasets. Standardizing data sets by area or sampling effort may produce very different results compared to standardizing by number of individuals collected, and it is not always clear which measure of diversity is more appropriate. Asymptotic richness estimators provide lower-bound estimates for taxon-rich groups such as tropical arthropods, in which observed richness rarely reaches an asymptote, despite intensive sampling. Recent examples of diversity studies of tropical trees, stream invertebrates, and herbaceous plants emphasize the importance of carefully quantifying species richness using taxon sampling curves."
326,68,8070,1,The ED strategy: how species-level surrogates indicate general biodiversity patterns through an 'environmental diversity' perspective,"Abstract Biodiversity assessment requires that we use surrogate information in practice to indicate more general biodiversity patterns. 'ED' refers to a surrogates framework that can link species data and environmental information based on a robust relationship of compositional dissimilarities to ordinations that indicate underlying environmental variation. In an example analysis of species and environmental data from Panama, the environmental and spatial variables that correlate with an hybrid multi-dimensional scaling ordination were able to explain 83% of the variation in the corresponding Bray Curtis dissimilarities. The assumptions of ED also provide the rationale for its use of p-median optimization criteria to measure biodiversity patterns among sites in a region. M.B. Araujo, P.J. Densham & P.H. Williams (2004, Journal of Biogeography31, 1) have re-named ED as 'AD' in their evaluation of the surrogacy value of ED based on European species data. Because lessons from previous work on ED options consequently may have been neglected, we use a corroboration framework to investigate the evidence and 'background knowledge' presented in their evaluations of ED. Investigations focus on the possibility that their weak corroboration of ED surrogacy (non-significance of target species recovery relative to a null model) may be a consequence of Araujo et al.'s use of particular evidence and randomizations. We illustrate how their use of discrete ED, and not the recommended continuous ED, may have produced unnecessarily poor species recovery values. Further, possible poor optimization of their MDS ordinations, due to small numbers of simulations and/or low resolution of stress values appears to have provided a possible poor basis for ED application and, consequently, may have unnecessarily favoured non-corroboration results. Consideration of Araujo et al.'s randomizations suggests that acknowledged sampling biases in the European data have not only artefactually promoted the non-significance of ED recovery values, but also artefactually elevated the significance of competing species surrogates recovery values. We conclude that little credence should be given to the comparisons of ED and species-based complementarity sets presented in M.B. Araujo, P.J. Densham & P.H. Williams (2004, Journal of Biogeography31, 1), unless the factors outlined here can be analysed for their effects on results. We discuss the lessons concerning surrogates evaluation emerging from our investigations, calling for better provision in such studies of the background information that can allow (i) critical examination of evidence (both at the initial corroboration and re-evaluation stages), and (ii) greater synthesis of lessons about the pitfalls of different forms of evidence in different contexts."
327,68,8549,1,"Positive feedbacks among forest fragmentation, drought, and climate change in the Amazon","The Amazon basin is experiencing rapid forest loss and fragmentation. Fragmented forests are more prone than intact forests to periodic damage from El Nino-Southern Oscillation (ENSO) droughts, which cause elevated tree mortality, increased litterfall, shifts in plant phenology, and other ecological changes, especially near forest edges. Moreover, positive feedbacks among forest loss, fragmentation, fire, and regional climate change appear increasingly likely. Deforestation reduces plant evapotranspiration, which in turn constrains regional rainfall, increasing the vulnerability of forests to fire. Forest fragments are especially vulnerable because they have dry, fire-prone edges, are logged frequently, and often are adjoined by cattle pastures, which are burned regularly. The net result is that there may be a critical ""deforestation threshold"" above which Amazonian rainforests can no longer be sustained, particularly in relatively seasonal areas of the basin. Global warming could exacerbate this problem if it promotes drier climates or stronger ENSO droughts. Synergisms among many simultaneous environmental changes are posing unprecedented threats to Amazonian forests."
328,68,11379,1,Integrating landscape and metapopulation modeling approaches: viability of the sharp-tailed grouse in a dynamic landscape.,": The lack of management experience at the landscape scale and the limited feasibility of experiments at this scale have increased the use of scenario modeling to analyze the effects of different management actions on focal species. However, current modeling approaches are poorly suited for the analysis of viability in dynamic landscapes. Demographic (e.g., metapopulation) models of species living in these landscapes do not incorporate the variability in spatial patterns of early successional habitats, and landscape models have not been linked to population viability models. We link a landscape model to a metapopulation model and demonstrate the use of this model by analyzing the effect of forest management options on the viability of the Sharp-tailed Grouse ( Tympanuchus phasianellus) in the Pine Barrens region of northwestern Wisconsin (U.S.A.). This approach allows viability analysis based on landscape dynamics brought about by processes such as succession, disturbances, and silviculture. The landscape component of the model (LANDIS) predicts forest landscape dynamics in the form of a time series of raster maps. We combined these maps into a time series of patch structures, which formed the dynamic spatial structure of the metapopulation component (RAMAS). Our results showed that the viability of Sharp-tailed Grouse was sensitive to landscape dynamics and demographic variables such as fecundity and mortality. Ignoring the landscape dynamics gave overly optimistic results, and results based only on landscape dynamics (ignoring demography) lead to a different ranking of the management options than the ranking based on the more realistic model incorporating both landscape and demographic dynamics. Thus, models of species in dynamic landscapes must consider habitat and population dynamics simultaneously."
329,68,11387,1,Agent based simulation of a small catchment water management in northern Thailand description of the CATCHSCAPE model.,"Due to mounting human pressure, stakeholders in northern Thailand are facing crucial natural resources management (NRM) issues. Among others, the impact of upstream irrigation management on downstream agricultural viability is a growing source of conflict, which often has both biophysical and social origins. As multiple rural stakeholders are involved, appropriate solutions should only emerge from negotiation. CATCHSCAPE is a Multi-Agent System (MAS) that enables us to simulate the whole catchment features as well as farmer&rsquo;s individual decisions. The biophysical modules simulate the hydrological system with its distributed water balance, irrigation scheme management and crop and vegetation dynamics. The social dynamics are described as a set of resource management processes (water, land, cash, labour force). Water management is described according to the actual different levels of control (individual, scheme and catchment). Moreover, the model&rsquo;s architecture is presented in a way that emphasises the transparency of the rules and methods implemented. Finally, one simulated scenario is described along with its main results, according to different viewpoints (economy, landscape, water management)."
330,68,11395,1,An improved method for the rapid assessment of forest understorey light environments,"1. The high spatial and temporal variability of forest understorey light environments requires lengthy and/or extensive sampling in order to characterize it by direct measurement. As this is often impractical, a number of surrogate measures have been developed that estimate light availability from assessments of forest canopy structure. 2. The subjective crown illumination index developed by Clark & Clark (1992) was compared with Garrison's (1949) moosehorn and two new methods: (i) the crown illumination ellipses method, which compares the size of canopy gaps with a series of standard area ellipses printed on a transparent screen; and (ii) the canopy-scope that, like the moosehorn, uses an array of 25 dots printed on a transparent screen to assess canopy openness, but is more robust and portable, measuring the largest canopy gap visible from the point of measurement rather than canopy openness overhead. 3. The new measures were more highly correlated with canopy openness in the range 0-30%, measured from hemispherical photographs, than the crown illumination index, and showed lower levels of between-observer variability. 4. The canopy-scope has the potential to be widely used for the simple and rapid assessment of forest understorey light environments. It has the advantage of giving ratio scale measurements that can be used in parametric statistics. The crown illumination ellipses can be used to score the illumination of crowns that are above head height."
331,68,11403,1,Effects of the past and the present on species distribution: land-use history and demography of wintergreen.,"Summary 1 Past land use can have long-term effects on plant speciesâ distributional patterns if alterations in resources and environmental conditions have persistent effects on population demography (environmental change) and/or if plants are intrinsically limited in their colonization ability (historical factors). 2 We evaluated the role of environmental alteration vs. historical factors in controlling distributional patterns of Gaultheria procumbens, a woody, clonal understorey species with a pronounced restriction to areas that have never been ploughed, and near absence from adjoining areas that were ploughed in the 19th century. The demographic study was conducted in scrub oak and hardwood plant communities on an extensive sand plain, where it was possible to control for the effect of variation in environment prior to land use. 3 The observed demographic effects were contrary to the hypothesis that persistent environmental alteration depressed demographic performance and limited the distribution of G. procumbens. We observed no overall effect of land-use history on stem density, stem recruitment or flower production. In fact, some aspects of performance were enhanced in previously ploughed areas. Populations in previously ploughed areas exhibited less stem mortality in scrub oak transitions, an increase in germination, seedling longevity and proportion of potentially reproductive stems in both plant communities, a trend for slower observed rates of population decline in both plant communities, and a higher projected rate of population growth in the scrub oak transitions. Thus, particularly in scrub oak communities, the lower abundance of G. procumbens in formerly ploughed than in unploughed areas contrasted with its performance. 4 The limited occurrence of G. procumbens in formerly farmed areas was explained instead by its slow intrinsic growth rate, coupled with limited seedling establishment. Lateral population extension occurred exclusively through vegetative growth, allowing a maximum expansion of 43 cm year-1. 5 We conclude that inherent limitations in the colonizing ability of some plant species may present a major obstacle in the restoration or recovery of plant communities on intensively disturbed sites, even in the absence of persistent environmental effects that depress population growth."
332,68,11411,1,Assessing biodiversity at landscape level in northern Thailand and Sumatra (Indonesia): the importance of environmental context.,"{Most biodiversity assessment methods tend to sample isolated areas of land cover such as closed forest or local land use mosaics. Contemporary methods of assessing biodiversity are briefly reviewed and focus on the relative roles of the Linnean species and plant functional types (PFTs). Recent case studies from central Sumatra and northern Thailand indicate how the range distributions of many plant and animal species and functional types frequently extend along regional gradients of light, water and nutrient availability and corresponding land use intensity. We show that extending the sampling context to include a broader array of environmental determinants of biodiversity results in a more interpretable pattern of biodiversity. Our results indicate sampling within a limited environmental context has the potential to generate highly truncated range distributions and thus misleading information for land managers and for conservation. In an intensive, multi-taxa survey in lowland Sumatra, vegetational data were collected along a land use intensity gradient using a proforma specifically designed for rapid survey. Each vegetation sample plot was a focal point for faunal survey. Whereas biodiversity pattern from samples within closed canopy rain forest was difficult to interpret, extending the sample base to include a wider variety of land cover and land use greatly improved interpretation of plant and animal distribution. Apart from providing an improved theoretical and practical basis for forecasting land use impact on biodiversity, results illustrate how specific combinations of plant-based variables might be used to predict impacts on specific animal taxa, functional types and above-ground carbon. Implications for regional assessment and monitoring of biodiversity and in improving understanding of the landscape dynamics are briefly discussed. (C) 2004 Elsevier B.V. All rights reserved.}"
333,68,11419,1,Individual-based modelling and ecological theory: synthesis of a workshop.,"Twelve papers featured in a special issue on individual-based modelling in ecology are reviewed in an effort to identify common methodological and theoretical issues. The review focuses on issues related to the question of whether and how individual-based modelling is changing ecological theory. One major hindrance impeding the generation of theory from individual-based models (IBMs) is the fact that IBMs are more or less complex computer simulation models. They are thus hard to develop, hard to communicate, and hard to analyse. Solving this problem requires both software tools which help to implement and communicate IBMs and at least the same effort in analysing the models as is currently put into their development. A new field of application of IBMs is Virtual Ecology, i.e. the comparison of simulated data sets with those obtained by virtual (i.e. simulated) ecologists. This method allows field methods, empirical measures and sampling protocols to be optimised. As far as theoretical issues are concerned, individual variability was by far the most important issue discussed in the papers. Previously most studies concentrated on the mechanisms generating individual variability, but there is now also a growing number of models addressing the consequences of individual variability for population and community dynamics. In order to determine these consequences, a currency is required which allows the model populations to be evaluated. Persistence and other stability properties are proposed as such a unifying currency. The lesson contained in our review is that even with just twelve papers more or less explicitly oriented towards ecological theory, elements of a theory that might emerge form individual-based modelling in the future can already be identified. (C) 1999 Elsevier Science B.V. All rights reserved."
334,68,11427,1,The accuracy of GPS for wildlife telemetry and habitat mapping,"Decision support tools used for vegetation management require accurate information on the spatial array of different plant communities and a herbivore's grazing location. We tested the accuracy and precision of locations derived using the satellite navigation global positioning system (GPS).  Before May 2000, the accuracy and precision of GPS-derived locations were degraded by a process known as selective availability (SA); after May 2000, SA was disabled. In this study we investigated how to handle and improve the quality of data generated both when SA was enabled and when SA was disabled using relative GPS (rGPS). rGPS entails the post-processed correction of the roving GPS module with simultaneously acquired positional errors recorded at a known stationary reference location.  With SA enabled, GPS data were obtained at a fixed known location to obtain baseline information, and from a roving module that essentially mimicked surveying techniques or the movement of a free-ranging animal. The mean accuracy of GPS with SA enabled was 21Â m for the fixed module and 25Â m for the roving module. Use of rGPS and further manipulation of the data improved the mean accuracy of the data to 7Â m for the fixed module and 10Â m for the roving module. With SA disabled, data were similarly recorded from the fixed known location and resulted in a mean location accuracy of 5Â m. The use of rGPS resulted in a significant improvement of this value to 3Â·6Â m and precision measured by the 95% quantile wasÂ &lt;Â 10Â m. For mapping and wildlife tracking, such quality in terms of location accuracy and precision is unprecedented and demonstrates that rGPS may still be useful in many applications.  GPS enables the world-wide collection of accurate and precise location information at 1-second intervals. Furthermore, by programming the GPS receiver to overdetermine location by using information from all visible satellites, many of the limitations that arise in habitats or environments with a limited view of the sky may be overcome.  With SA now disabled, the potential use of GPS will increase. With further miniaturization, surveying of remote featureless landscapes or the tracking of crepuscular or far-ranging animals will become more accurate and more quantifiable than ever before."
335,68,11435,1,"Habitat fragmentation, species loss and biological control.","Fragmentation of habitats in the agricultural landscape is a major threat to biological diversity, which is greatly determined by insects. Isolation of habitat fragments resulted in decreased numbers of species as well as reduced effects of natural enemies. Manually established islands of red clover were colonized by most available herbivore species but few parasitoid species. Thus, herbivores were greatly released from parasitism, experiencing only 19 to 60 percent of the parasitism of nonisolated populations. Species failing to successfully colonize isolated islands were characterized by small and highly variable populations. Accordingly, lack of habitat connectivity released insects from predator control. 10.1126/science.264.5165.1581"
336,68,11451,1,Modelling landscape-scale habitat use using GIS and remote sensing: a case study with great bustards,"1. Many species are adversely affected by human activities at large spatial scales and their conservation requires detailed information on distributions. Intensive ground surveys cannot keep pace with the rate of land-use change over large areas and new methods are needed for regional-scale mapping. 2. We present predictive models for great bustards in central Spain based on readily available advanced very high resolution radiometer (AVHRR) satellite imagery combined with mapped features in the form of geographic information system (GIS) data layers. As AVHRR imagery is coarse-grained, we used a 12-month time series to improve the definition of habitat types. The GIS data comprised measures of proximity to features likely to cause disturbance and a digital terrain model to allow for preference for certain topographies 3. We used logistic regression to model the above data, including an autologistic term to account for spatial autocorrelation. The results from models were combined using Bayesian integration, and model performance was assessed using receiver operating characteristics plots. 4. Sites occupied by bustards had significantly lower densities of roads, buildings, railways and rivers than randomly selected survey points. Bustards also occurred within a narrower range of elevations and at locations with significantly less variable terrain. 5. Logistic regression analysis showed that roads, buildings, rivers and terrain all contributed significantly to the difference between occupied and random sites. The Bayesian integrated probability model showed an excellent agreement with the original census data and predicted suitable areas not presently occupied. 6. The great bustard's distribution is highly fragmented and vacant habitat patches may occur for a variety of reasons, including the species' very strong fidelity to traditional sites through conspecific attraction. This may limit recolonization of previously occupied sites. 7. We conclude that AVHRR satellite imagery and GIS data sets have potential to map distributions at large spatial scales and could be applied to other species. While models based on imagery alone can provide accurate predictions of bustard habitats at some spatial scales, terrain and human influence are also significant predictors and are needed for finer scale modelling."
337,68,11459,1,Mapping the conservation landscape.,"Abstract: Before widespread, informed collaboration can take place in conservation there must be a process of understanding the different approaches employed by different conservation organizations to conserve biodiversity. To begin this process and to help build understanding and collaboration, we provide a conceptual map of 21 approaches currently being implemented by 13 conservation organizations. We examined each of these approaches according to (1) the nature of the conservation target-the object(s) of the conservation action; ( 2 ) whether the question addressed is where conservation should be done or how conservation should be done; ( 3 ) the scale ( both grain and extent ) of the approach; and (4 ) the principles that underlie the approach. These questions provide a good way of distinguishing between most of the approaches and reveal that there is less competition between them than is assumed. We conclude that only with explicit understanding can the conservation community and its supporters critically compare approaches and come to a consensus about a set of metrics for measuring and achieving global conservation. Clasificando los Enfoques Utilizadas en la Conservacion Resumen: Para que una colaboracion bien fundamentada pueda llevarse a cabo, debe haber un proceso de entendimiento de los distintos enfoques utilizadas por diferentes organizaciones de conservacion para preservar la biodiversidad. Para iniciar este proceso y ayudar a fomentar el conocimiento y la colaboracion, proveemos un mapa conceptual de 21 enfoques utilizados actualmente por 13 organizaciones conservacionistas. Examinamos cada uno de estos enfoques segun (1) la naturaleza del objetivo de conservacion-el ( los ) objetos( s ) de las actividades de conservacion; ( 2 ) la naturaleza de la pregunta a contestar, ya sea ""donde se debe llevar a cabo la conservacion"" o ""como se debe llevar a cabo la conservacion""; ( 3 ) la escala ( tanto a nivel de detalle como extension ) del enfoque; (4 ) los principios que constituyen el fundamento del enfoque. Estas preguntas proveen una buena manera de diferenciar la mayoria de las metodologias y muestran que hay menos competencia entre los enfoques de lo que se cree. Concluimos que la comunidad conservacionista y sus seguidores solo podran comparar los diversos enfoques de manera criteriosa si tienen un entendimiento explicito de los mismos, y de esa manera, podra desarrollar, por consenso, una serie de variables para medir y lograr la conservacion a nivel global."
338,68,11467,1,Communicating ecological Indicators to decision makers and the public.,"Ecological assessments and monitoring programs often rely on indicators to evaluate environmental conditions. Such indicators are frequently developed by scientists, expressed in technical language, and target aspects of the environment that scientists consider useful. Yet setting environmental policy priorities and making environmental decisions requires both effective communication of environmental information to decision makers and consideration of what members of the public value about ecosystems. However, the complexity of ecological issues, and the ways in which they are often communicated, make it difficult for these parties to fully engage such a dialogue. This paper describes our efforts to develop a process for translating the indicators of regional ecological condition used by the {U.S.} Environmental Protection Agency into common language for communication with public and decision-making audiences. A series of small-group sessions revealed that people did not want to know what these indicators measured, or how measurements were performed. Rather, respondents wanted to know what such measurements can tell them about environmental conditions. Most positively received were descriptions of the kinds of information that various combinations of indicators provide about broad ecological conditions. Descriptions that respondents found most appealing contained general reference to both the set of indicators from which the information was drawn and aspects of the environment valued by society to which the information could be applied. These findings can assist with future efforts to communicate scientific information to nontechnical audiences, and to represent societal values in ecological programs by improving scientist-public communication"
339,68,11475,1,Conflicts between lesser kestrel conservation and European agricultural policies as identified by habitat use analyses.,"European pseudo-steppes have suffered from extensive changes in agricultural practices during the past decades with the disappearance of field margins and fallow systems and the increase of biocide treatments. The negative effect on wildlife has led to the adoption by the European Union of policies more compatible with environmental conservation, but decisions about optimal land use are difficult to make because of lack of information. We studied habitat use by the Lesser Kestrel (Falco naumanni), a globally vulnerable species, in a Spanish pseudo-steppe (Los Monegros) where traditional agro-grazing systems are still being practiced, and we compared the results with those of another Spanish pseudo-steppe where modern and intensive agriculture has been implemented. We focused on the use by Lesser Kestrels of habitats subject to changes provoked by recent agricultural policies. Habitat availability was determined in a 3-km radius around 11 colonies, where 23 Lesser Kestrels were radio-tracked during the chick-rearing stage. Habitat selection was determined through compositional analysis. The rank of selected habitats was similar for all kestrels, considering both habitats surveyed and habitats where kestrels hunted. Kestrels selected field margins and cereal fields and rejected abandoned crops and scrubland. This selectivity seemed to be due to prey availability. In the intensively cultivated areas the kestrels selected similar habitats but used only small foraging patches and obtained smaller prey than in the traditional agro-grazing systems, probably because of the irregular distribution of prey resources as a result of the intensive biocide treatments. Consequently, in intensively cultivated habitats Lesser Kestrels had larger home ranges (63.65 km2) than in those with traditional systems (12.36 km2). These differences are reflected in the productivity and population trends of both populations. Thus, the best strategy for conserving the Lesser Kestrel seems to be the maintainance of traditional cereal cultures with low biocide treatments and numerous field margins. Both agricultural intensification and marginal land abandonment (with subsequent scrubland invasion) have detrimental consequences for this and probably for other pseudo-steppe species. Positive management steps can be encouraged by recent agro-environmental regulations such as the 2078/92 European Union Reglament, which favors the creation of programs in which agricultural practices accord with wildlife conservation."
340,68,11483,1,Using pattern-oriented modeling for revealing hidden information: a key for reconciling ecological theory and application.,"We suggest that the conscious use of information that is ""hidden"" in distinct structures in nature itself and in data extracted from nature (=pattern) during the process of modeling (=pattern-oriented modeling) can substantially improve models in ecological application and conservation. Observed patterns, such as time-series patterns and spatial patterns of presence/absence in habitat patches, contain a great deal of data on scales, site-history, parameters and processes. Use of these data provides criteria for aggregating the biological information in the model, relates the model explicitly to the relevant scales of the system, facilitates the use of helpful techniques of indirect parameter estimation with independent data, and helps detect underlying ecological processes. Additionally, pattern-oriented models produce comparative predictions that can be tested in the field. We developed a step-by-step protocol for pattern-oriented modeling and illustrate the potential of this protocol by discussing three pattern-oriented population models: (1) a population viability analysis for brown bears (Ursusarctos) in northern Spain using time-series data on females with cubs of the year to adjust unknown model parameters; (2) a savanna model for detecting underlying ecological processes from spatial patterns of tree distribution; and (3) the incidence function model of metapopulation dynamics as an example of process integration and model generalization. We conclude that using the pattern-oriented approach to its full potential will require a major paradigm shift in the strategies of modeling and data collection, and we argue that more emphasis must be placed on observing and documenting relevant patterns in addition to attempts to obtain direct estimates of model parameters."
341,69,15210,1,The Physics of Cosmic Acceleration,"The discovery that the cosmic expansion is accelerating has been followed by an intense theoretical and experimental response in physics and astronomy. The discovery implies that our most basic notions about how gravity works are violated on cosmological distance scales. A simple fix is to introduce a cosmological constant into the field equations for general relativity. However, the extremely small value of the cosmological constant, relative to theoretical expectations, has led theorists to explore numerous alternative explanations that involve the introduction of an exotic negative-pressure fluid or a modification of general relativity. Here we review the evidence for cosmic acceleration. We then survey some of the theoretical attempts to account for it, including the cosmological constant, quintessence and its variants, mass-varying neutrinos, and modifications of general relativity. We discuss experimental and observational tests that may allow us to distinguish among some of the theoretical ideas that have been proposed."
342,70,885,1,Information extraction,"The automatic extraction of information from unstructured sources has opened up new avenues for querying, organizing, and analyzing data by drawing upon the clean semantics of structured databases and the abundance of unstructured data. The field of information extraction has its genesis in the natural language processing community where the primary impetus came from competitions centered around the recognition of named entities like people names and organization from news articles. As society became more data oriented with easy online access to both structured and unstructured data, new applications of structure extraction came around. Now, there is interest in converting our personal desktops to structured databases, the knowledge in scientific publications to structured records, and harnessing the Internet for structured fact finding queries. Consequently, there are many different communities of researchers bringing in techniques from machine learning, databases, information retrieval, and computational linguistics for various aspects of the information extraction problem.  This review is a survey of information extraction research of over two decades from these diverse communities. We create a taxonomy of the field along various dimensions derived from the nature of the extraction task, the techniques used for extraction, the variety of input resources exploited, and the type of output produced. We elaborate on rule-based and statistical methods for entity and relationship extraction. In each case we highlight the different kinds of models for capturing the diversity of clues driving the recognition process and the algorithms for training and efficiently deploying the models. We survey techniques for optimizing the various steps in an information extraction pipeline, adapting to dynamic data, integrating with existing entities and handling uncertainty in the extraction process."
343,70,1707,1,Managing Gigabytes: Compressing and Indexing Documents and Images,"{Of all the tasks programmers are asked to perform, storing, compressing, and retrieving information are some of the most challenging--and critical to many applications. <I>Managing Gigabytes: Compressing and Indexing Documents and Images</I> is a treasure trove of theory, practical illustration, and general discussion in this fascinating technical subject.<p> Ian Witten, Alistair Moffat, and Timothy Bell have updated their original work with this even more impressive second edition. This version adds recent techniques such as block-sorting, new indexing techniques, new lossless compression strategies, and many other elements to the mix. In short, this work is a comprehensive summary of text and image compression, indexing, and querying techniques. The history of relevant algorithm development is woven well with a practical discussion of challenges, pitfalls, and specific solutions.<p> This title is a textbook-style exposition on the topic, with its information organized very clearly into topics such as compression, indexing, and so forth. In addition to diagrams and example text transformations, the authors use ""pseudo-code"" to present algorithms in a language-independent manner wherever possible. They also supplement the reading with <I>mg</I>--their own implementation of the techniques. The mg C language source code is freely available on the Web. <p> Alone, this book is an impressive collection of information. Nevertheless, the authors list numerous titles for further reading in selected topics. Whether you're in the midst of application development and need solutions fast or are merely curious about how top-notch information management is done, this hardcover is an excellent investment. <I>--Stephen W. Plain</I><p> <B>Topics covered</B>: Text compression models, including Huffman, LZW, and their variants; trends in information management; index creation and compression; image compression; performance issues; and overall system implementation.} {<p>In this fully updated second edition of the highly acclaimed <b>Managing Gigabytes</b>, authors Witten, Moffat, and Bell continue to provide unparalleled coverage of state-of-the-art techniques for compressing and indexing data. Whatever your field, if you work with large quantities of information, this book is essential reading--an authoritative theoretical resource and a practical guide to meeting the toughest storage and access challenges. It covers the latest developments in compression and indexing and their application on the Web and in digital libraries. It also details dozens of powerful techniques supported by mg, the authors' own system for compressing, storing, and retrieving text, images, and textual images. mg's source code is freely available on the Web.<br><br>* Up-to-date coverage of new text compression algorithms such as block sorting, approximate arithmetic coding, and fat Huffman coding<br>* New sections on content-based index compression and distributed querying, with 2 new data structures for fast indexing<br>* New coverage of image coding, including descriptions of de facto standards in use on the Web (GIF and PNG), information on CALIC, the new proposed JPEG Lossless standard, and JBIG2<br>* New information on the Internet and WWW, digital libraries, web search engines, and agent-based retrieval<br>* Accompanied by a public domain system called MG which is a fully worked-out operational example of the advanced techniques developed and explained in the book<br>* New appendix on an existing digital library system that uses the MG software}"
344,70,4346,1,Learning Information Extraction Rules for Semi-structured and Free Text,". A wealth of on-line text information can be made available to automatic processing by information extraction (IE) systems. Each IE application needs a separate set of rules tuned to the domain and writing style. WHISK helps to overcome this knowledge-engineering bottleneck by learning text extraction rules automatically. WHISK is designed to handle text styles ranging from highly structured to free text, including text that is neither rigidly formatted nor composed of grammatical sentences. Such semistructured text has largely been beyond the scope of previous systems. When used in conjunction with a syntactic analyzer and semantic tagging, WHISK can also handle extraction from free text such as news stories.  Keywords: natural language processing, information extraction, rule learning 1. Information extraction  As more and more text becomes available on-line, there is a growing need for systems that extract information automatically from text data. An information extraction (IE) sys..."
345,71,2313,1,GENIES: a natural-language processing system for the extraction  of molecular pathways from journal articles,"Systems that extract structured information from natural language passages have been highly successful in specialized domains.  The time is opportune for developing analogous applications for molecular biology and genomics. We present a system, GENIES, that extracts and structures information about cellular pathways from the biological literature in accordance with a knowledge model that we developed earlier. We implemented GENIES by modifying an existing medical natural language processing system, MedLEE, and performed a preliminary evaluation study. Our results demonstrate the value of the underlying techniques for the purpose of acquiring valuable knowledge from biological journals.  Contact: friedman.carol@dmi.columbia.edu 10.1093/bioinformatics/17.suppl_1.S74"
346,71,6111,1,GoPubMed: exploring PubMed with the Gene Ontology,"The biomedical literature grows at a tremendous rate and PubMed comprises already over 15 000 000 abstracts. Finding relevant literature is an important and difficult problem. We introduce GoPubMed, a web server which allows users to explore PubMed search results with the Gene Ontology (GO), a hierarchically structured vocabulary for molecular biology. GoPubMed provides the following benefits: first, it gives an overview of the literature abstracts by categorizing abstracts according to the GO and thus allowing users to quickly navigate through the abstracts by category. Second, it automatically shows general ontology terms related to the original query, which often do not even appear directly in the abstract. Third, it enables users to verify its classification because GO terms are highlighted in the abstracts and as each term is labelled with an accuracy percentage. Fourth, exploring PubMed abstracts with GoPubMed is useful as it shows definitions of GO terms without the need for further look up. GoPubMed is online at www.gopubmed.org. Querying is currently limited to 100 papers per query. 10.1093/nar/gki470"
347,71,9940,1,Discovering evolutionary theme patterns from text: an exploration of temporal text mining,"Temporal Text Mining (TTM) is concerned with discovering temporal patterns in text information collected over time. Since most text information bears some time stamps, TTM has many applications in multiple domains, such as summarizing events in news articles and revealing research trends in scientific literature. In this paper, we study a particular TTM task -- discovering and summarizing the evolutionary patterns of themes in a text stream. We define this new text mining problem and present general probabilistic methods for solving this problem through (1) discovering latent themes from text; (2) constructing an evolution graph of themes; and (3) analyzing life cycles of themes. Evaluation of the proposed methods on two different domains (i.e., news articles and literature) shows that the proposed methods can discover interesting evolutionary theme patterns effectively."
348,72,747,1,The Strength of Weak Ties,"Analysis of social networks is suggested as a tool for linking micro and macro levels of sociological theory. The procedure is illustrated by elaboration of the macro implications of one aspect of small-scale interaction: the strength of dyadic ties. It is argued that the degree of overlap of two individuals' friendship networks varies directly with the strength of their tie to one another. The impact of this principle on diffusion of influence and information, mobility opportunity, and community organization is explored. Stress is laid on the cohesive power of weak ties. Most network models deal, implicitly, with strong ties, thus confining their applicability to small, well-defined groups. Emphasis on weak ties lends itself to discussion of relations between groups and to analysis of segments of social structure not easily defined in terms of primary groups."
349,72,15862,1,Opinion and community formation in coevolving networks,"In human societies, opinion formation is mediated by social interactions, consequently taking place on a network of relationships and at the same time influencing the structure of the network and its evolution. To investigate this coevolution of opinions and social interaction structure, we develop a dynamic agent-based network model by taking into account short range interactions like discussions between individuals, long range interactions like a sense for overall mood modulated by the attitudes of individuals, and external field corresponding to outside influence. Moreover, individual biases can be naturally taken into account. In addition, the model includes the opinion-dependent link-rewiring scheme to describe network topology coevolution with a slower time scale than that of the opinion formation. With this model, comprehensive numerical simulations and mean field calculations have been carried out and they show the importance of the separation between fast and slow time scales resulting in the network to organize as well-connected small communities of agents with the same opinion."
350,73,8926,1,"Where Everybody Knows Your (Screen) Name: Online Games as ""Third Places""","This article examines the form and function of massively multiplayer online games (MMOs) in terms of social engagement. Combining conclusions from media effects research informed by the communication effects literature with those from ethnographic research informed by a sociocultural perspective on cognition and learning, we present a shared theoretical framework for understanding (a) the extent to which such virtual worlds are structurally similar to ""third places"" (Oldenburg, 1999) for informal sociability, and (b) their potential function in terms of social capital (Coleman, 1988; Putnam, 2000). Our conclusion is that by providing spaces for social interaction and relationships beyond the workplace and home, MMOs have the capacity to function as one form of a new ""third place"" for informal sociability. Participation in such virtual ""third places"" appears particularly well suited to the formation of bridging social capital2014social relationships that, while not usually providing deep emotional support, typically function to expose the individual to a diversity of worldviews."
351,74,2583,1,Transcription regulation and animal diversity,"Whole-genome sequence assemblies are now available for seven different animals, including nematode worms, mice and humans. Comparative genome analyses reveal a surprising constancy in genetic content: vertebrate genomes have only about twice the number of genes that invertebrate genomes have, and the increase is primarily due to the duplication of existing genes rather than the invention of new ones. How, then, has evolutionary diversity arisen? Emerging evidence suggests that organismal complexity arises from progressively more elaborate regulation of gene expression."
352,74,11594,1,"What is a gene, post-ENCODE? History and updated definition.","10.1101/gr.6339607 While sequencing of the human genome surprised us with how many protein-coding genes there are, it did not fundamentally change our perspective on what a gene is. In contrast, the complex patterns of dispersed regulation and pervasive transcription uncovered by the ENCODE project, together with non-genic conservation and the abundance of noncoding RNA genes, have challenged the notion of the gene. To illustrate this, we review the evolution of operational definitions of a gene over the past centuryÃ¢ÂÂfrom the abstract elements of heredity of Mendel and Morgan to the present-day ORFs enumerated in the sequence databanks. We then summarize the current ENCODE findings and provide a computational metaphor for the complexity. Finally, we propose a tentative update to the definition of a gene: A gene is a union of genomic sequences encoding a coherent set of potentially overlapping functional products. Our definition sidesteps the complexities of regulation and transcription by removing the former altogether from the definition and arguing that final, functional gene products (rather than intermediate transcripts) should be used to group together entities associated with a single gene. It also manifests how integral the concept of biological function is in defining genes."
353,74,11979,1,Divergence of Transcription Factor Binding Sites Across Related Yeast Species,"Characterization of interspecies differences in gene regulation is crucial for understanding the molecular basis of both phenotypic diversity and evolution. By means of chromatin immunoprecipitation and DNA microarray analysis, the divergence in the binding sites of the pseudohyphal regulators Ste12 and Tec1 was determined in the yeasts Saccharomyces cerevisiae, S. mikatae, and S. bayanus under pseudohyphal conditions. We have shown that most of these sites have diverged across these species, far exceeding the interspecies variation in orthologous genes. A group of Ste12 targets was shown to be bound only in S. mikatae and S. bayanus under pseudohyphal conditions. Many of these genes are targets of Ste12 during mating in S. cerevisiae, indicating that specialization between the two pathways has occurred in this species. Transcription factor binding sites have therefore diverged substantially faster than ortholog content. Thus, gene regulation resulting from transcription factor binding is likely to be a major cause of divergence between related species."
354,74,12931,1,On the relation between promoter divergence and gene expression evolution.,"Recent studies have characterized significant differences in the cis-regulatory sequences of related organisms, but the impact of these differences on gene expression remains largely unexplored. Here, we show that most previously identified differences in transcription factor (TF)-binding sequences of yeasts and mammals have no detectable effect on gene expression, suggesting that compensatory mechanisms allow promoters to rapidly evolve while maintaining a stabilized expression pattern. To examine the impact of changes in cis-regulatory elements in a more controlled setting, we compared the genes induced during mating of three yeast species. This response is governed by a single TF (STE12), and variations in its predicted binding sites can indeed account for about half of the observed expression differences. The remaining unexplained differences are correlated with the increased divergence of the sequences that flank the binding sites and an apparent modulation of chromatin structure. Our analysis emphasizes the flexibility of promoter structure, and highlights the interplay between specific binding sites and general chromatin structure in the control of gene expression."
355,74,15268,1,Biopython: freely available Python tools for computational molecular biology and bioinformatics.,"Summary: The Biopython project is a mature open source international collaboration of volunteer developers, providing Python libraries for a wide range of bioinformatics problems. Biopython includes modules for reading and writing different sequence file formats and multiple sequence alignments, dealing with 3D macro molecular structures, interacting with common tools such as BLAST, ClustalW and EMBOSS, accessing key online databases, as well as providing numerical methods for statistical learning.  Availability: Biopython is freely available, with documentation and source code at www.biopython.org under the Biopython license.  Contact: All queries should be directed to the Biopython mailing lists, see www.biopython.org/wiki/_Mailing_listspeter.cock@scri.ac.uk. 10.1093/bioinformatics/btp163"
356,74,16226,1,De novo sequencing of plant genomes using second-generation technologies,"10.1093/bib/bbp039 The ability to sequence the DNA of an organism has become one of the most important tools in modern biological research. Until recently, the sequencing of even small model genomes required substantial funds and international collaboration. The development of Ã¢ÂÂsecond-generationÃ¢ÂÂ sequencing technology has increased the throughput and reduced the cost of sequence generation by several orders of magnitude. These new methods produce vast numbers of relatively short reads, usually at the expense of read accuracy. Since the first commercial second-generation sequencing system was produced by 454 Technologies and commercialised by Roche, several other companies including Illumina, Applied Biosystems, Helicos Biosciences and Pacific Biosciences have joined the competition. Because of the relatively high error rate and lack of assembly tools, short-read sequence technology has mainly been applied to the re-sequencing of genomes. However, some recent applications have focused on the de novo assembly of these data. De novo assembly remains the greatest challenge for DNA sequencing and there are specific problems for second generation sequencing which produces short reads with a high error rate. However, a number of different approaches for short-read assembly have been proposed and some have been implemented in working software. In this review, we compare the current approaches for second-generation genome sequencing, explore the future direction of this technology and the implications for plant genome research."
357,75,82,1,MIPS: a database for genomes and protein sequences,"The Munich Information Center for Protein Sequences (MIPS-GSF, Neuherberg, Germany) continues to provide genome-related information in a systematic way. MIPS supports both national and European sequencing and functional analysis projects, develops and maintains automatically generated and manually annotated genome-specific databases, develops systematic classification schemes for the functional annotation of protein sequences, and provides tools for the comprehensive analysis of protein sequences. This report updates the information on the yeast genome (CYGD), the Neurospora crassa genome (MNCDB), the databases for the comprehensive set of genomes (PEDANT genomes), the database of annotated human EST clusters (HIB), the database of complete cDNAs from the DHGP (German Human Genome Project), as well as the project specific databases for the GABI (Genome Analysis in Plants) and HNB (Helmholtz-Netzwerk Bioinformatik) networks. The Arabidospsis thaliana database (MATDB), the database of mitochondrial proteins (MITOP) and our contribution to the PIR International Protein Sequence Database have been described elsewhere [Schoof et al. (2002) Nucleic Acids Res., 30, 91-93; Scharfe et al. (2000) Nucleic Acids Res., 28, 155-158; Barker et al. (2001) Nucleic Acids Res., 29, 29-32]. All databases described, the protein analysis tools provided and the detailed descriptions of our projects can be accessed through the MIPS World Wide Web server (http://mips.gsf.de)."
358,75,1589,1,{Lethality and centrality in protein networks},"The most highly connected proteins in the cell are the most important for its survival. Proteins are traditionally identified on the basis of their individual actions as catalysts, signalling molecules, or building blocks in cells and microorganisms. But our post-genomic view is expanding the protein's role into an element in a network of proteinâprotein interactions as well, in which it has a contextual or cellular function within functional modules1, 2. Here we provide quantitative support for this idea by demonstrating that the phenotypic consequence of a single gene deletion in the yeast Saccharomyces cerevisiae is affected to a large extent by the topological position of its protein product in the complex hierarchical web of molecular interactions."
359,75,3203,1,The closest BLAST hit is often not the nearest neighbor.,"It is well known that basing phylogenetic reconstructions on uncorrected genetic distances can lead to errors in their reconstruction. Nevertheless, it is often common practice to report simply the most similar BLAST (Altschul et al. 1997) hit in genomic reports that discuss many genes (Ruepp et al. 2000; Freiberg et al. 1997). This is because BLAST hits can provide a rapid, efficient, and concise analysis of many genes at once. These hits are often interpreted to imply that the gene is most closely related to the gene or protein in the databases that returned the closest BLAST hit. Though these two may coincide, for many genes, particularly genes with few homologs, they may not be the same. There are a number of circumstances that can account for such limitations in accuracy (Eisen 2000). We stress here that genes appearing to be the most similar based on BLAST hits are often not each others closest relative phylogenetically. The extent to which this occurs depends on the availability of close relatives present in the databases. As an example we have chosen the analysis of the genomes of a crenarcheaota species Aeropyrum pernix, an organism with few close relatives fully sequenced, and Escherichia coli, an organism whose closest relative, Salmonella typhimurium, is completely sequenced."
360,75,7184,1,"Ontologies in biology: design, applications and future challenges","Biological knowledge is inherently complex and so cannot readily be integrated into existing databases of molecular (for example, sequence) data. An ontology is a formal way of representing knowledge in which concepts are described both by their meaning and their relationship to each other. Unique identifiers that are associated with each concept in biological ontologies (bio-ontologies) can be used for linking to and querying molecular databases. This article reviews the principal bio-ontologies and the current issues in their design and development: these include the ability to query across databases and the problems of constructing ontologies that describe complex knowledge, such as phenotypes."
361,75,9774,1,Evaluation of clustering algorithms for protein-protein interaction networks,"ABSTRACT: BACKGROUND: Protein interactions are crucial components of all cellular processes. Recently, high-throughput methods have been developed to obtain a global description of the interactome (the whole network of protein interactions for a given organism). In 2002, the yeast interactome was estimated to contain up to 80,000 potential interactions. This estimate is based on the integration of data sets obtained by various methods (mass spectrometry, two-hybrid methods, genetic studies). High-throughput methods are known, however, to yield a non-negligible rate of false positives, and to miss a fraction of existing interactions. The interactome can be represented as a graph where nodes correspond with proteins and edges with pairwise interactions. In recent years clustering methods have been developed and applied in order to extract relevant modules from such graphs. These algorithms require the specification of parameters that may drastically affect the results. In this paper we present a comparative assessment of four algorithms: Markov Clustering (MCL), Restricted Neighborhood Search Clustering (RNSC), Super Paramagnetic Clustering (SPC), and Molecular Complex Detection (MCODE). RESULTS: A test graph was built on the basis of 220 complexes annotated in the MIPS database. To evaluate the robustness to false positives and false negatives, we derived 41 altered graphs by randomly removing edges from or adding edges to the test graph in various proportions. Each clustering algorithm was applied to these graphs with various parameter settings, and the clusters were compared with the annotated complexes. We analysed the sensitivity of the algorithms to the parameters and determined their optimal parameter values. We also evaluated their robustness to alterations of the test graph. We then applied the four algorithms to six graphs obtained from high-throughput experiments and compared the resulting clusters with the annotated complexes. CONCLUSIONS: This analysis shows that MCL is remarkably robust to graph alterations. In the tests of robustness, RNSC is more sensitive to edge deletion but less sensitive to the use of suboptimal parameter values. The other two algorithms are clearly weaker under most conditions. The analysis of high-throughput data supports the superiority of MCL for the extraction of complexes from interaction networks."
362,75,11928,1,The protein network of bacterial motility,"Motility is achieved in most bacterial species by the flagellar apparatus. It consists of dozens of different proteins with thousands of individual subunits. The published literature about bacterial chemotaxis and flagella documented 51 proteinâprotein interactions (PPIs) so far. We have screened whole genome two-hybrid arrays of Treponema pallidum and Campylobacter jejuni for PPIs involving known flagellar proteins and recovered 176 and 140 high-confidence interactions involving 110 and 133 proteins, respectively. To explore the biological relevance of these interactions, we tested an Escherichia coli gene deletion array for motility defects (using swarming assays) and found 159 gene deletion strains to have reduced or no motility. Comparing our interaction data with motility phenotypes from E. coli, Bacillus subtilis, and Helicobacter pylori, we found 23 hitherto uncharacterized proteins involved in motility. Integration of phylogenetic information with our interaction and phenotyping data reveals a conserved core of motility proteins, which appear to have recruited many additional species-specific components over time. Our interaction data also predict 18 110 interactions for 64 flagellated bacteria."
363,75,13398,1,The Landscape of Human Proteins Interacting with Viruses and Other Pathogens,"Infectious diseases result in millions of deaths each year. Mechanisms of infection have been studied in detail for many pathogens. However, many questions are relatively unexplored. What are the properties of human proteins that interact with pathogens? Do pathogens interact with certain functional classes of human proteins? Which infection mechanisms and pathways are commonly triggered by multiple pathogens? In this paper, to our knowledge, we provide the first study of the landscape of human proteins interacting with pathogens. We integrate humanÃ¢â¬âpathogen proteinÃ¢â¬âprotein interactions (PPIs) for 190 pathogen strains from seven public databases. Nearly all of the 10,477 human-pathogen PPIs are for viral systems (98.3%), with the majority belonging to the humanÃ¢â¬âHIV system (77.9%). We find that both viral and bacterial pathogens tend to interact with hubs (proteins with many interacting partners) and bottlenecks (proteins that are central to many paths in the network) in the human PPI network. We construct separate sets of human proteins interacting with bacterial pathogens, viral pathogens, and those interacting with multiple bacteria and with multiple viruses. Gene Ontology functions enriched in these sets reveal a number of processes, such as cell cycle regulation, nuclear transport, and immune response that participate in interactions with different pathogens. Our results provide the first global view of strategies used by pathogens to subvert human cellular processes and infect human cells. Supplementary data accompanying this paper is available at http://staff.vbi.vt.edu/dyermd/publicatiÃ¢â¬â¹ons/dyer2008a.html ."
364,75,14289,1,High-quality binary protein interaction map of the yeast interactome network.,"Current yeast interactome network maps contain several hundred molecular complexes with limited and somewhat controversial representation of direct binary interactions. We carried out a comparative quality assessment of current yeast interactome data sets, demonstrating that high-throughput yeast two-hybrid (Y2H) screening provides high-quality binary interaction information. Because a large fraction of the yeast binary interactome remains to be mapped, we developed an empirically controlled mapping framework to produce a ""second-generation"" high-quality, high-throughput Y2H data set covering [{\\textasciitilde}]20% of all yeast binary interactions. Both Y2H and affinity purification followed by mass spectrometry (AP/MS) data are of equally high quality but of a fundamentally different and complementary nature, resulting in networks with different topological and biological properties. Compared to co-complex interactome models, this binary map is enriched for transient signaling interactions and intercomplex connections with a highly significant clustering between essential proteins. Rather than correlating with essentiality, protein connectivity correlates with genetic pleiotropy."
365,75,15818,1,RNA-MATE: a recursive mapping strategy for high-throughput RNA-sequencing data.,"SUMMARY: Mapping of next-generation sequencing data derived from RNA samples (RNAseq) presents different genome mapping challenges than data derived from DNA. For example, tags that cross exon-junction boundaries will often not map to a reference genome, and the strand specificity of the data needs to be retained. Here we present RNA-MATE, a computational pipeline based on a recursive mapping strategy for placing strand specific RNAseq data onto a reference genome. Maximizing the mappable tags can provide significant savings in the cost of sequencing experiments. This pipe-line provides an automatic and integrated way to align color-space sequencing data, collate this information and generate files for examining gene-expression data in a genomic context. AVAILABILITY: Executables, source code, and exon-junction libraries are available from http://grimmond.imb.uq.edu.au/RNA-MATE/ CONTACT: n.cloonan@imb.uq.edu.au SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics Online."
366,76,4794,1,A Survey of Program Slicing Techniques,"A program slice consists of the parts of a program that (potentially) affect the values computed at some point of interest. Such a point of interest is referred to as a slicing criterion, and is typically specified by a location in the program in combination with a subset of the program's variables. The task of computing program slices is called program slicing. The article presents an overview of program slicing, including the various general approaches used to compute slices, as well as the specific techniques used to address a variety of language features such as procedures, unstructured control flow, composite data types and pointers, and concurrency. Static and dynamic slicing methods for each of these features are compared and classified in terms of their accuracy and efficiency. Moreover, the possibilities for combining solutions for different features are investigated. Recent work on the use of compiler optimization and symbolic execution techniques for obtaining more accurate slices is discussed. The paper concludes with an overview of the applications of program slicing, which include debugging, program integration, dataflow testing, and software maintenance"
367,77,2948,1,A method for simultaneous alignment of multiple protein structures.,"Here, we present MultiProt, a fully automated highly efficient technique to detect multiple structural alignments of protein structures. MultiProt finds the common geometrical cores between input molecules. To date, most methods for multiple alignment start from the pairwise alignment solutions. This may lead to a small overall alignment. In contrast, our method derives multiple alignments from simultaneous superpositions of input molecules. Further, our method does not require that all input molecules participate in the alignment. Actually, it efficiently detects high scoring partial multiple alignments for all possible number of molecules in the input. To demonstrate the power of MultiProt, we provide a number of case studies. First, we demonstrate known multiple alignments of protein structures to illustrate the performance of MultiProt. Next, we present various biological applications. These include: (1) a partial alignment of hinge-bent domains; (2) identification of functional groups of G-proteins; (3) analysis of binding sites; and (4) protein-protein interface alignment. Some applications preserve the sequence order of the residues in the alignment, whereas others are order-independent. It is their residue sequence order-independence that allows application of MultiProt to derive multiple alignments of binding sites and of protein-protein interfaces, making MultiProt an extremely useful structural tool."
368,78,9955,1,The Elements of Typographic Style,"{<div>Renowned typographer and poet Robert Bringhurst brings clarity to the art of typography with this masterful style guide. Combining the practical, theoretical, and historical, this edition is completely updated, with a thorough exploration of the newest innovations in intelligent font technology, and is a must-have for graphic artists, editors, or anyone working with the printed page using digital or traditional methods.</div>} {This lovely, well-written book is concerned foremost with creating beautiful typography and is essential for professionals who regularly work with typographic designs. Author Robert Bringhurst writes about designing with the correct typeface; striving for rhythm, proportion, and harmony; choosing and combining type; designing pages; using section heads, subheads, footnotes, and tables; applying kerning and other type adjustments to improve legibility; and adding special characters, including punctuation and diacritical marks. <I>The Elements of Typographic Style</I> teaches the history of and the artistic and practical perspectives on a variety of type families that are available in Europe and America today.<P> The last section of the book classifies and displays many type families, offers a glossary of typography terms, and lists type designers and type foundries. The book briefly mentions digital typography, but otherwise ignores it, focusing instead on general typography and page- and type-design issues. Its examples include text in a variety of languages--including English, Russian, German, and Greek--which is particularly helpful if your work has a multinational focus.}"
369,78,13634,1,Panama Fever: The Epic Story of One of the Greatest Human Achievements of All Time-- the Building of the Panama Canal,"{ The Surprising Story Behind the Massive Construction Feat that Changed the Face of the World.  <P>The building of the Panama Canal was one of the greatest engineering feats in human history.  A tale of exploration, conquest, money, politics, and medicine, <i>Panama Fever</i> charts the challenges that marked the long, labyrinthine road to the building of the canal.  Drawing on a wealth of new materials and sources, Matthew Parker brings to life the men who recognized the impact a canal would have on global politics and economics, and adds new depth to the familiar story of Teddy Roosevelt's remarkable triumph in making the waterway a reality.  As thousands of workers succumbed to dysentery, yellow fever, and malaria, scientists raced to stop the deadly epidemics so that work could continue.  The treatments they developed changed the course of medical history. The opening of the Panama Canal in 1914 spelled the end of the Victorian Age and the beginning of the ""American Century.""  <i>Panama Fever</i> brilliantly captures the innovative thinking and backbreaking labor, as well as the commercial and political interests, that helped make America a global power. <P>Presented unabridged on 14 CDs, narrated by William Dufris.}"
370,78,13650,1,The Revolution: A Manifesto,"{<STRONG>This Much Is True: You Have Been Lied To.</STRONG><BR><BR><BR/><UL type=disc><BR/><LI MARGIN: 0in 1.5in 0pt 0in; TEXT-ALIGN: center; mso-list: l0 level1 lfo1; tab-stops: list .5in?>The government is expanding. <BR/><LI MARGIN: 0in 1.5in 0pt 0in; TEXT-ALIGN: center; mso-list: l0 level1 lfo1; tab-stops: list .5in?>Taxes are increasing. <BR/><LI MARGIN: 0in 1.5in 0pt 0in; TEXT-ALIGN: center; mso-list: l0 level1 lfo1; tab-stops: list .5in?>More senseless wars are being planned. <BR/><LI MARGIN: 0in 1.5in 0pt 0in; TEXT-ALIGN: center; mso-list: l0 level1 lfo1; tab-stops: list .5in?>Inflation is ballooning. <BR/><LI MARGIN: 0in 1.5in 0pt 0in; TEXT-ALIGN: center; mso-list: l0 level1 lfo1; tab-stops: list .5in?>Our basic freedoms are disappearing.</LI></UL><BR>The Founding Fathers didn't want any of this. In fact, they said so quite clearly in the Constitution of the <st1:place w:st=""on""><st1:country-region w:st=""on"">United States of America</st1:country-region></st1:place>. Unfortunately, that beautiful, ingenious, and revolutionary document is being ignored more and more in <st1:State w:st=""on""><st1:place w:st=""on"">Washington</st1:place></st1:State>. If we are to enjoy peace, freedom, and prosperity once again, we absolutely must return to the principles upon which <st1:country-region w:st=""on""><st1:place w:st=""on"">America</st1:place></st1:country-region> was founded. But finally, there is hope . . . <BR><BR>In THE REVOLUTION,<I></I><st1:State w:st=""on"">Texas</st1:State> congressman and presidential candidate Ron Paul has exposed the core truths behind everything threatening <st1:country-region w:st=""on""><st1:place w:st=""on"">America</st1:place></st1:country-region>, from the <I>real </I>reasons behind the collapse of the dollar and the looming financial crisis, to terrorism and the loss of our precious civil liberties. In this book, Ron Paul provides answers to questions that few even dare to ask. <BR><BR>Despite a media blackout, this septuagenarian physician-turned-congressman sparked a movement that has attracted a legion of young, dedicated, enthusiastic supporters . . . a phenomenon that has amazed veteran political observers and made more than one political rival envious. Candidates across <st1:country-region w:st=""on""><st1:place w:st=""on"">America</st1:place></st1:country-region> are already running as ""Ron Paul Republicans.""<BR><BR>""Dr. Paul cured my apathy,"" says a popular campaign sign. THE REVOLUTION may cure yours as well.}"
371,78,13658,1,The Whole Truth,"{""Matt, I need a war.""<BR><BR>So begins David Baldacci's new book--a thriller unlike any he's written before. ""Matt"" is Mathew Pender, of Pender Associates--a shadowy organization that specializes in managing seemingly impossible situations for its clients. Sometimes, those services extend to managing--and creating--armed conflict. When Matt Pender is asked by his client--the largest defense contractor in the world--to manipulate two nations against each other, a shocking and surprising series of events are set in motion that will possibly bring the world to the brink of World War III.*<BR><BR>In this epic thriller with a global backdrop, David Baldacci delivers all the twists and turns, compelling characters, and can't-put-it-down pacing that readers have come to expect from this master storyteller.}"
372,78,13666,1,Smart Women Finish Rich,"{Many women, whether they've managed million-dollar budgets at work or managed kids, bills, and car payments at home, feel at a loss when it comes to figuring out their financial futures. Often, they leave the ""big"" money matters to the men in their lives, or they put off making important financial decisions that could affect their lifestyles down the road.<br><br>Now, in <b>Smart Women Finish Rich</b>, David Bach finally gives women the tools and the program they need to create rich futures. A renowned financial advisor who has coached thousands of people looking for financial security, Bach has developed a proven system for women that can make them better financial planners than men.<br><br>Bach has found that women tend to be naturally great investors when they learn to tap into their values. He has discovered that while men tend to focus on earning money for ""things""--cars, boats, golf gear, etc.--and frequently switch stocks in their portfolios, women who prioritize their ""values""--security, freedom, and education--are able to commit easily to a long-term savings and investment plan to build futures they really want.<br><br>Whether you're working with a few dollars a week, a significant inheritance, or a savings account that's been earning just 3 percent, the <b>Smart Women Finish Rich</b> approach toward saving and investing can provide a huge payoff.Â Â Secure your financial future and achieve your dreams by following these seven steps:<br><br>1. Learn the facts and myths about your money.<br>2. Put your money where your values are.<br>3. Figure out where you stand financially... and where you want to go.<br>4. Use the power of the lattÃ© factor to create massive wealth on just a few dollars a week.<br>5. Practice Grandma's three-basket approach to financial security.<br>6. Learn the nine biggest mistakes investors make and how to avoid them.<br>7. Follow the twelve commandments of attracting greater wealth.<br><br>These seven easy steps will transform your relationship with money, and <b>Smart Women Finish Rich</b> will put you in control of your finances and your future.} {David Bach's <I>Smart Women Finish Rich</I> is a homage to the financial wisdom of his grandmother; it's also an excellent foundation for women who are starting to get their financial lives in order. Bach's approach to money management is rooted in years of investment seminars for women and his work as senior vice president of investments at Morgan Stanley Dean Witter. During that time he recognized that ""people rarely know what is truly driving them emotionally when it comes to money."" In response, Bach has written a guide to money management for women based on his belief that ""financial planning is as much an emotional issue as it is an intellectual one."" Are you considering your values in your work and investing? What part of your daily work is driven by your goals in life? Is your latte habit preventing you from accumulating substantial wealth? Bach addresses tax strategies, wills, insurance, retirement plans, and investments in a highly accessible manner. <I>Smart Women Finish Rich</I> ably bridges the gap between simple saving strategies and preparing for widowhood and financial independence. <i>--Brad Doll</i>}"
373,78,13674,1,Love You Forever,"{<p>  A young woman holds her newborn son<br>And looks at him lovingly.  </p><blockquote>  Softly she sings to him:<br />  ""I'll love you forever<br />  I'll like you for always<br />  As long as I'm living<br />  My baby you'll be.""  </blockquote><p>  So begins the story that has touched the hearts of millions worldwide. Since publication in l986, <i>Love You Forever</i> has sold more than 15 million copies in paperback and the regular hardcover edition (as well as hundreds of thousands of copies in Spanish and French).  </p><p>  Firefly Books is proud to offer this sentimental favorite in a variety of editions and sizes:  </p><p>  We offer a trade paper and laminated hardcover edition in a 8"" x 8"" size.  </p><p>  In gift editions we carry:<br />  a slipcased edition (8 1/2"" x 8 1/4""), with a laminated box and a cloth binding on the book<br />  and a 10"" x 10"" laminated hardcover with jacket.  </p><p>  And a Big Book Edition, 16"" x 16"" with a trade paper binding.  </p> (20050426)} {The mother sings to her sleeping baby: ""I'll love you forever / I'll  love you for always / As long as I'm living / My baby you'll be."" She still sings the same song when her baby has turned into a fractious 2-year-old, a slovenly 9-year-old, and then a raucous teen. So far so ordinary--but this is one persistent lady. When her son grows up and leaves home, she takes to driving across town with a ladder on the car roof, climbing through her grown son's window, and rocking the sleeping man in the same way. Then, inevitably, the day comes when she's too old and sick to hold him, and the roles are at last reversed. Each stage is illustrated by one of Sheila McGraw's comic and yet poignant pastels. (Ages 4 to 8) <I>--Richard Farr</I>}"
374,78,13682,1,"Alexander and the Terrible, Horrible, No Good, Very Bad Day","{The story is about Alexander and his very bad day. Read about his best friend that deserted him, and no dessert in his lunch bag and lima beans for dinner and kissing on TV. Paperback.} {""I went to sleep with gum in my mouth and now there's  gum in my hair and when I got out of bed this morning I tripped on  the skateboard and by mistake I dropped my sweater in the sink  while the water was running and I could tell it was going to be a  terrible, horrible, no good, very bad day.""   <P>So begin the trials and tribulations of the irascible  Alexander, who has been earning the sympathy of readers since  1972. People of all ages have terrible, horrible days, and  Alexander offers us the cranky commiseration we crave as well as a  reminder that things may not be all that bad. As Alexander's day  progresses, he faces a barrage of bummers worthy of a country- western song: getting smushed in the middle seat of the car, a  dessertless lunch sack, a cavity at the dentist's office,  stripeless sneakers, witnessing kissing on television, and being  forced to sleep in railroad-train pajamas. He resolves several  times to move to Australia. <P> Judith Viorst flawlessly and humorously captures a child's testy  temperament, rendering Alexander sympathetic rather than whiny.  Our hero's gum-styled hair and peevish countenance are artfully  depicted by Ray Cruz's illustrations. An ALA Notable Book,  <i>Alexander and the Terrible, Horrible, No Good, Very Bad Day</i>  is a great antidote to bad days everywhere, sure to put a smile on  even the crabbiest of faces. <I>(Ages 5 to 9)</I>}"
375,78,13690,1,Cocoa(R) Programming for Mac(R) OS X (3rd Edition),"{<B>  <P style=""MARGIN: 0px"">The best-selling introduction to Cocoa, once again updated to cover the latest Mac programming technologies, and still enthusiastically recommended by experienced Mac OS X developers.</P>  <P style=""MARGIN: 0px""></B>Â </P>  <P style=""MARGIN: 0px"">âAaronâs book is the gold standard for Mac OS X programming booksâbeautifully written, and thoughtfully sculpted. The best book on Leopard development.â</P>  <P style=""MARGIN: 0px"">âScott Stevenson, www.theocacao.com</P>  <P style=""MARGIN: 0px"">Â </P>  <P style=""MARGIN: 0px"">âThis is the first book Iâd recommend for anyone wanting to learn Cocoa from scratch. Aaronâs one of the few (perhaps only) full-time professional Cocoa instructors, and his teaching experience shows in the book.â</P>  <P style=""MARGIN: 0px"">âTim Burks, software developer and creator of the Nu programming language, www.programming.nu</P>  <P style=""MARGIN: 0px"">Â </P>  <P style=""MARGIN: 0px"">âIf youâre a UNIX or Windows developer who picked up a Mac OS X machine recently in hopes of developing new apps or porting your apps to Mac users, this book should be strongly considered as one of your essential reference and training tomes.â </P>  <P style=""MARGIN: 0px"">âKevin H. Spencer, Apple Certified Technical Coordinator</P>  <P style=""MARGIN: 0px"">Â </P>  <P style=""MARGIN: 0px"">If youâre developing applications for Mac OS X, <B><I><B>CocoaÂ® Programming for MacÂ® OS X, Third Edition,</B></I></B> is the book youâve been waiting to get your hands on. If youâre new to the Mac environment, itâs probably the book youâve been told to read first. Covering the bulk of what you need to know to develop full-featured applications for OS X, written in an engaging tutorial style, and thoroughly class-tested to assure clarity and accuracy, it is an invaluable resource for any Mac programmer.</P>  <P style=""MARGIN: 0px"">Â </P>  <P style=""MARGIN: 0px"">Specifically, Aaron Hillegass introduces the three most commonly used Mac developer tools: Xcode, Interface Builder, and Instruments. He also covers the Objective-C language and the major design patterns of Cocoa. Aaron illustrates his explanations with exemplary code, written in the idioms of the Cocoa community, to show you how Mac programs should be written. After reading this book, you will know enough to understand and utilize Appleâs online documentation for your own unique needs. And you will know enough to write your own stylish code.</P>  <P style=""MARGIN: 0px"">Â </P>  <P style=""MARGIN: 0px"">Updated for Mac OS X 10.4 and 10.5, this revised edition includes coverage of Xcode 3, Objective-C 2, Core Data, the garbage collector, and CoreAnimation.</P>  <DIV></DIV>} {There's a reason that a large slice of the open-source movement has defected from running Linux on its laptops to running Mac OS X. The reason is the Unix core that underlies Mac OS X, and the development tools that run on that core. Cocoa makes it easy to create very slick Mac OS X interfaces for software (as well as to create applications in a hurry), and this new edition of <I>Cocoa Programming for Mac OS X</I> does an excellent job of teaching its readers how to put a Cocoa face on top of code (Objective-C code almost exclusively). If you know something about C and/or C++ programming and want to apply your skills to the Mac, this is precisely the book you want.<p>  Author Aaron Hillegass teaches a Cocoa class, and his book reads like a demonstration-driven lecture in a computer lab. That is, the book takes a heavily example-centric approach to its subject, beginning with simple announcement windows and proceeding to cover the more advanced controls and object-oriented features of Cocoa and Objective-C. Throughout, he hops back and forth between descriptions of the goal to be accomplished, listings of the code that does the job, and instructions on how to use the Mac OS X development tools to speed the development process. <I>--David Wall</I><p>  <B>Topics covered</B>: How to write software for Mac OS X in Objective-C and, especially, with Cocoa. The new edition shows how to use NSUndoManager, add AppleScript capability to an application, do graphics work with OpenGL, and use Cocoa under Linux using GNUstep. As well, all the basic controls and design patterns are covered.}"
376,78,13706,1,Trail Food: Drying and Cooking Food for Backpacking and Paddling,"{<p>"" . . . a book that will appeal to everyone who has ever choked down the pre-packaged, bargain-basement camp food (or gone bankrupt buying the good stuff).""  --<i>Canoe & Kayak</i>  <p> . . . if you're on the lookout for a way to bring real meals to the field, [this book] might have the answer.""  --<i>Field & Stream</i>  <p>Life in the outdoors revolves around food--cooking it, eating it, packing it, carrying it. We even fantasize about it, especially after a week of eating store-bought provisions. This book is all about fulfulling those food fantasies and avoiding those expensive disappointments. <i>Trail Food</i> tells you how to remove water from food, to make it lighter and longer-lasting, without removing its taste. Learn to plan menus and prepare meals just like the ones you left behind, using fresh foods from your garden or market, prepared and seasoned the way <i>you</i> like them.  <p>Why fantasize when you can have the real thing?}"
377,78,13714,1,The Age of Miracles: Embracing the New Midlife,"{<div> The need for change as we get olderâan emotional pressure for one phase of our lives to transition into anotherâis a human phenomenon, neither male nor female. There simply comes a time in our livesânot fundamentally different from the way puberty separates childhood from adulthoodâwhen itâs time for one part of ourselves to die and for something new to be born. <br> The purpose of this book by best-selling author and lecturer <b>Marianne Williamson</b> is to psychologically and spiritually reframe this transition so that it leads to a wonderful sense of joy and awakening.<br> In our ability to rethink our lives lies our greatest power to change them. What we have called âmiddle ageâ need not be seen as a turning point toward death. It can be viewed as a magical turning point toward life as weâve never known it, if we allow ourselves the power of an independent imaginationâthought-forms that donât flow in a perfunctory manner from ancient assumptions merely handed down to us, but rather flower into new archetypal images of a humanity just getting started at 45 or 50. <br> What weâve learned by that time, from both our failures as well as our successes, tends to have humbled us into purity. When we were young, we had energy but we were clueless about what to do with it. Today, we have less energy, perhaps, but we have far more understanding of what each breath of life is for. And now at last, we have a destiny to fulfillânot a destiny of a life thatâs simply over, but rather a destiny of a life that is finally truly lived.<br> Midlife is not a crisis; itâs a time of rebirth. Itâs not a time to accept your death; itâs a time to accept your <i>life</i>âand to finally, truly live it, as you and you alone know deep in your heart it was meant to be lived.<br> Â <br> </div>}"
378,78,13722,1,Healing Back Pain: The Mind-Body Connection,"{Healing Back Pain promises permanent elimination of back pain without drugs, surgery, or exercise. It should have been titled Understanding TMS Pain, because it discusses one particular cause of back pain--Tension Myositis Syndrome (TMS)--and isn't really a program for self-treatment, with only five pages of action plan (and many more pages telling why conventional methods don't work). According to John E. Sarno, M.D., TMS is the major cause of pain in the back, neck, shoulders, buttocks, and limbs--and it is caused not by structural abnormalities but by the mind's effort to repress emotions. He's not saying that your pain is all in your head; rather, he's saying that the battle going on in your mind results in a real physical disorder that may affect muscles, nerves, tendons, or ligaments. An injury may have triggered the disorder, but is not the cause of the amount or intensity of the resulting pain. According to Sarno, the mind tricks you into not facing repressed emotion by making you focus on pain in the body. When this realization sinks in (""and it must sink in, for mere intellectual appreciation of the process is not enough""), the trick doesn't work any more, and there's no need for the pain. (Healing Back Pain should not be used for self-diagnosis. Always consult a physician for chronic or acute back pain.) --Joan Price} {<I>Healing Back Pain</I> promises permanent elimination of back pain without drugs, surgery, or exercise. It should have been titled <I>Understanding TMS Pain</I>, because it discusses one particular cause of back pain--Tension Myositis Syndrome (TMS)--and isn't really a program for self-treatment, with only five pages of action plan (and many more pages telling why conventional methods don't work). According to John E. Sarno, M.D., TMS is the major cause of pain in the back, neck, shoulders, buttocks, and limbs--and it is caused not by structural abnormalities but by the mind's effort to repress emotions. He's not saying that your pain is all in your head; rather, he's saying that the battle going on in your mind results in a real physical disorder that may affect muscles, nerves, tendons, or ligaments. An injury may have triggered the disorder, but is not the cause of the amount or intensity of the resulting pain. According to Sarno, the mind tricks you into not facing repressed emotion by making you focus on pain in the body. When this realization sinks in (""and it must sink in, for mere intellectual appreciation of the process is not enough""), the trick doesn't work any more, and there's no need for the pain. (<I>Healing Back Pain</I> should not be used for self-diagnosis. Always consult a physician for chronic or acute back pain.) <I>--Joan Price</I>}"
379,78,13754,1,Spark: The Revolutionary New Science of Exercise and the Brain,"A groundbreaking and fascinating investigation into the transformative effects of exercise on the brain, from the bestselling author and renowned psychiatrist John J. Ratey, MD.  ><BR>  ><BR>Did you know you can beat stress, lift your mood, fight memory loss, sharpen your intellect, and function better than ever simply by elevating your heart rate and breaking a sweat? The evidence is incontrovertible: Aerobic exercise physically remodels our brains for peak performance. <BR>  ><BR> In SPARK, John J. Ratey, M.D., embarks upon a fascinating and entertaining journey through the mind-body connection, presenting startling research to prove that exercise is truly our best defense against everything from depression to ADD to addiction to aggression to menopause to Alzheimer's. Filled with amazing case studies (such as the revolutionary fitness program in Naperville, Illinois, which has put this school district of 19,000 kids first in the world of science test scores), SPARK is the first book to explore comprehensively the connection between exercise and the brain. It will change forever the way you think about your morning run---or, for that matter, simply the way you think"
380,78,13766,1,"Raising Adopted Children, Revised Edition: Practical Reassuring Advice for Every Adoptive Parent","""Some people may describe adoption as _difficult_; others simply describe it as _different_. I am inclined to think of it as complex,"" writes Lois Ruskai Melina in the updated, revised _Raising Adopted Children: Practical, Reassuring Advice for Every Adoptive Parent_.  Adoption practices have evolved considerably since this book's first publication in 1986, and the new version of the ""Dr. Spock for adoptive parents"" reflects the latest theories. Drawing on the findings and practices of pediatricians, social workers, scientists, and adoptive parents, _Raising Adopted Children_ is carefully and thoroughly researched. Chapters on open adoption, international adoption, and transracial adoption are combined with advice on bonding and attachment, breast-feeding an adoptive infant (possible but complicated), dealing with schools, privacy issues, adopting a child with disabilities, adopting as a single parent, and the challenges of adolescence. While Melina's many years of professional and personal experience shape her advice, she remains very evenhanded. For example, she's a strong proponent of the ""early telling"" theory of adoption (being open about the adoption with the child from the beginning), but she also clearly presents other points of view, and, throughout the book, encourages parents to make decisions that feel right for them.  The text includes specific suggestions for explaining a child's birth circumstances, including common misconceptions, and a valuable discussion about whether adoptees are at greater risk for behavior problems or learning disabilities. She also provides suggestions for setting rules for contact with biological parents, easing grief, and acknowledging a child's history. A completely annotated list of selected references and resources rounds out this superior guide. _--Ericka Lutz_"
381,78,13774,1,Healthy Aging: A Lifelong Guide to Your Well-Being,"Dr. Weil has raised dispensing health advice to an art form. Instead of making his audience feel inadequate or guilty about bad habits, he seems to subconsciously convince readers to do better merely by presenting health facts in a non-threatening way. _Healthy Aging_ is his most scientifically technical book yet (you'll learn all about enzymes like telomerase and cell division and the chemistry behind phytonutrients like indole-3-carbinol, and the connection between cancer and other degenerative diseases like diabetes) yet by far his most fascinating. His main mission here is to recommend ""aging gracefully,"" which he considers accepting the process instead of fighting it. As the director of the country's leading integrative-medicine clinic (combining the best of traditional and alternative worlds), of course he disses Botox and the slew of \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\$100-a-jar face creams out there. It's also no surprise that he focuses on proper nutrition, moderate exercise, and meditation and rest among his ""12-point program for healthy aging."" (Triathletes and exercise addicts should take special note of the research linking excessive exercise and ALS, or amyotrophic lateral sclerosis.) He occasionally references his earlier works, including _8 Weeks to Optimum Health_. But the most eye-opening sections are those that discuss the spirituality of aging and its emotional aspects. ""Aging can bring frailty and suffering, but it can also bring depth and richness of experience, complexity of being, serenity, wisdom, and its own kind of power and grace,"" he writes. At 63, Weil is still a bit shy of senior status, but is aging well indeed, with the legacy of his late 93-year-old mother (who's touchingly eulogized by Weil in this book) to guide him.--_Erica Jorgensen_"
382,78,13782,1,"Change Your Brain, Change Your Life: The Breakthrough Program for Conquering Anxiety, Depression, Obsessiveness, Anger, and Impulsiveness","In this age of do-it-yourself health care (heck, if the doctor only sees you for 10 minutes each visit, what other options are there?), _Change Your Brain, Change Your Life_ fits in perfectly. Filled with ""brain prescriptions"" (among them cognitive exercises and nutritional advice) that are geared toward readers who've experienced anxiety, depression, impulsiveness, excessive anger or worry, and obsessive behavior, _Change Your Brain, Change Your Life_ milks the mind-body connection for all it's worth.  Written by a psychiatrist and neuroscientist who has also authored a book on attention deficit disorder, _Change Your Brain_ contains dozens of brain scans of patients with various neurological problems, from caffeine, nicotine, and heroin addiction to manic-depression to epilepsy. These scans, often showing large gaps in neurological activity or areas of extreme overactivity, are downright frightening to look at, and Dr. Amen should know better than to resort to such scare tactics. But he should also be commended for advocating natural remedies, including deep breathing, guided imagery, meditation, self- hypnosis, and biofeedback for treating disorders that are so frequently dealt with by prescription only."
383,78,14042,1,Quick & Easy Tsukemono: Japanese Pickling Recipes,"Among the many authentic flavors of Japan, tsukemono, or pickled vegetables, has been a must for everyday meals and with tea. For most of the Japanese nothing can replace enjoying plain hot rice with tsukemono, and dinner is not complete without it as the final course. Today most dishes are available at Japanese grocery stores or specialty supermarkets, but they often lack the seasonal quality and freshness of true tsukemono. The term tsukemono covers a wide range of dishes from a marinated salad to preserved foods. Traditional tsukemono such as takuan or umeboshi might seem difficult to prepare but Quick & Easy Tsukemono makes these and many more, easy with its simple step-by-step, full-color photo instructions. There are myriads of methods to make them, some as simple as just rubbing fruits and vegetables with salt just before serving, while other require several days to fully marinate. Packed with over 73 mouthwatering recipes for easily preserving fruits and vegetables, Quick & Easy Tsukemono is the perfect book for beginning cooks and seasoned foodies alike."
384,80,1812,1,Multidimensional Scaling,"Suppose dissimilarity data have been collected on a set of n objects or individuals, where there is a value of dissimilarity measured for each pair.The dissimilarity measure used might be a subjective judgement made by a judge, where for example a teacher subjectively scores the strength of friendship between pairs of pupils in her class, or, as an alternative, more objective, measure, she might count the number of contacts made in a day between each pair of pupils. In other situations the dissimilarity measure might be based on a data matrix. The general aim of multidimensional scaling is to find a configuration of points in a space, usually Euclidean, where each point represents one of the objects or individuals, and the distances between pairs of points in the configuration match as well as possible the original dissimilarities between the pairs of objects or individuals. Such configurations can be found using metric and non-metric scaling, which are covered in Sects. 2 and 3. A number of other techniques are covered by the umbrella title of multidimensional scaling (MDS), and here the techniques of Procrustes analysis, unidimensional scaling, individual differences scaling, correspondence analysis and reciprocal averaging are briefly introduced and illustrated with pertinent data sets."
385,80,6672,1,The Wake-Sleep Algorithm for Unsupervised Neural Networks,"An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up &#034;recognition&#034; connections convert the input into representations in successive hidden layers and top-down &#034;generative&#034; connections reconstruct the representation in one layer from the representation in the layer above. In the &#034;wake&#034; phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the &#034;sleep&#034; phase, neurons are driven by generative connections and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above. Supervised learning algorithms for multilayer neural networks face two problems: They require a teacher to specify the desired output of the network and they require some method of communicating error information to all of the connections. The wake-sleep al..."
386,81,8411,1,Experimental quantum teleportation,"Quantum teleportation â the transmission and reconstruction over arbitrary distances of the state of a quantum system â is demonstrated experimentally. During teleportation, an initial photon which carries the polarization that is to be transferred and one of a pair of entangled photons are subjected to a measurement such that the second photon of the entangled pair acquires the polarization of the initial photon. This latter photon can be arbitrarily far away from the initial one. Quantum teleportation will be a critical ingredient for quantum computation networks."
387,82,272,1,The `Independent Components' of Natural Scenes are Edge Filters,"It has previously been suggested that neurons with line and edge selectivities found in primary visual cortex of cats and monkeys form a sparse, distributed representation of natural scenes, and it has been reasoned that such responses should emerge from an unsupervised learning algorithm that attempts to find a factorial code of independent visual features. We show here that a new unsupervised learning algorithm based on information maximization, a nonlinear âinfomaxâ network, when applied to an ensemble of natural scenes produces sets of visual filters that are localized and oriented. Some of these filters are Gabor-like and resemble those produced by the sparseness-maximization network. In addition, the outputs of these filters are as independent as possible, since this infomax network performs Independent Components Analysis or ICA, for sparse (super-gaussian) component distributions. We compare the resulting ICA filters and their associated basis functions, with other decorrelating filters produced by Principal Components Analysis (PCA) and zero-phase whitening filters (ZCA). The ICA filters have more sparsely distributed (kurtotic) outputs on natural scenes. They also resemble the receptive fields of simple cells in visual cortex, which suggests that these neurons form a natural, information-theoretic coordinate system for natural images."
388,82,572,1,Neural computations that underlie decisions about sensory stimuli.,"Decision-making behavior has been studied extensively, but the neurophysiological mechanisms responsible for this remarkable cognitive ability are just beginning to be understood. Here we propose neural computations that can account for the formation of categorical decisions about sensory stimuli by accumulating information over time into a single quantity: the logarithm of the likelihood ratio favoring one alternative over another. We also review electrophysio-logical studies that have identified brain structures that may be involved in computing this sort of decision variable. The ideas presented constitute a framework for understanding how and where perceptual decisions are formed in the brain."
389,82,607,1,Psychology and neurobiology of simple decisions.,"Patterns of neural firing linked to eye movement decisions show that behavioral decisions are predicted by the differential firing rates of cells coding selected and nonselected stimulus alternatives. These results can be interpreted using models developed in mathematical psychology to model behavioral decisions. Current models assume that decisions are made by accumulating noisy stimulus information until sufficient information for a response is obtained. Here, the models, and the techniques used to test them against response-time distribution and accuracy data, are described. Such models provide a quantitative link between the time-course of behavioral decisions and the growth of stimulus information in neural firing data."
390,82,618,1,Reactivation of hippocampal ensemble memories during sleep.,"Simultaneous recordings were made from large ensembles of hippocampal ""place cells"" in three rats during spatial behavioral tasks and in slow-wave sleep preceding and following these behaviors. Cells that fired together when the animal occupied particular locations in the environment exhibited an increased tendency to fire together during subsequent sleep, in comparison to sleep episodes preceding the behavioral tasks. Cells that were inactive during behavior, or that were active but had non-overlapping spatial firing, did not show this increase. This effect, which declined gradually during each post-behavior sleep session, may result from synaptic modification during waking experience. Information acquired during active behavior is thus re-expressed in hippocampal circuits during sleep, as postulated by some theories of memory consolidation."
391,82,628,1,Inference and computation with population codes.,"In the vertebrate nervous system, sensory stimuli are typically encoded through the concerted activity of large populations of neurons. Classically, these patterns of activity have been treated as encoding the value of the stimulus (e.g., the orientation of a contour), and computation has been formalized in terms of function approximation. More recently, there have been several suggestions that neural computation is akin to a Bayesian inference process, with population activity patterns representing uncertainty about stimuli in the form of probability distributions (e.g., the probability density function over the orientation of a contour). This paper reviews both approaches, with a particular emphasis on the latter, which we see as a very promising framework for future modeling and experimental work."
392,82,727,1,Tuning curve sharpening for orientation selectivity: coding efficiency and the impact of correlations,"Several studies have shown that the information conveyed by bell-shaped tuning curves increases as their width decreases, leading to the notion that sharpening of tuning curves improves population codes. This notion, however, is based on assumptions that the noise distribution is independent among neurons and independent of the tuning curve width. Here we reexamine these assumptions in networks of spiking neurons by using orientation selectivity as an example. We compare two principal classes of model: one in which the tuning curves are sharpened through cortical lateral interactions, and one in which they are not. We report that sharpening through lateral interactions does not improve population codes but, on the contrary, leads to a severe loss of information. In addition, the sharpening models generate complicated codes that rely extensively on pairwise correlations. Our study generates several experimental predictions that can be used to distinguish between these two classes of model."
393,82,748,1,Information processing in the primate visual system: an integrated systems perspective,"The primate visual system contains dozens of distinct areas in the cerebral cortex and several major subcortical structures. These subdivisions are extensively interconnected in a distributed hierarchical network that contains several intertwined processing streams. A number of strategies are used for efficient information processing within this hierarchy. These include linear and nonlinear filtering, passage through information bottlenecks, and coordinated use of multiple types of information. In addition, dynamic regulation of information flow within and between visual areas may provide the computational flexibility needed for the visual system to perform a broad spectrum of tasks accurately and at high resolution. 10.1126/science.1734518"
394,82,791,1,"Neural coding of basic reward terms of animal learning theory, game theory, microeconomics and behavioural ecology.","Neurons in a small number of brain structures detect rewards and reward-predicting stimuli and are active during the expectation of predictable food and liquid rewards. These neurons code the reward information according to basic terms of various behavioural theories that seek to explain reward-directed learning, approach behaviour and decision-making. The involved brain structures include groups of dopamine neurons, the striatum including the nucleus accumbens, the orbitofrontal cortex and the amygdala. The reward information is fed to brain structures involved in decision-making and organisation of behaviour, such as the dorsolateral prefrontal cortex and possibly the parietal cortex. The neural coding of basic reward terms derived from formal theories puts the neurophysiological investigation of reward mechanisms on firm conceptual grounds and provides neural correlates for the function of rewards in learning, approach behaviour and decision-making."
395,82,1193,1,A Mathematical Theory of Communication,"THE recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist1 and Hartley2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information. The fundamental problem of communication is that of reproducing at one point either exactly or approximately a message selected at another point. Frequently the messages have meaning; that is they refer to or are correlated according to some system with certain physical or conceptual entities. These semantic aspects of communication are irrelevant to the engineering problem. The significant aspect is that the actual message is one selected from a set of possible messages. The system must be designed to operate for each possible selection, not just the one which will actually be chosen since this is unknown at the time of design. If the number of messages in the set is finite then this number or any monotonic function of this number can be regarded as a measure of the information produced when one message is chosen from the set, all choices being equally likely. As was pointed out by Hartley the most natural choice is the logarithmic function. Although this definition must be generalized considerably when we consider the influence of the statistics of the message and when we have a continuous range of messages, we will in all cases use an essentially logarithmic measure."
396,82,1488,1,Dopamine: the salient issue.,"There is general agreement that midbrain dopamine neurons play key roles in reward processing. What is more controversial is the role they play in processing salient stimuli that are not rewarding. This controversy has arisen for three main reasons. First, salient sensory stimuli such as tones and lights, which are assumed not to be rewarding, increase dopamine neuron activity. Second, aversive stimuli increase firing in a minority of putative dopamine neurons. Third, dopamine release is increased following aversive stimuli. Consequently, it has been suggested that these midbrain dopamine neurons are activated by all salient stimuli, rather than specifically by rewards. However, reconsideration of these issues, in light of new findings, suggests this controversy can be resolved in favour of reward theories."
397,82,2200,1,Parsing reward.,"Advances in neurobiology permit neuroscientists to manipulate specific brain molecules, neurons and systems. This has lead to major advances in the neuroscience of reward. Here, it is argued that further advances will require equal sophistication in parsing reward into its specific psychological components: (1) learning (including explicit and implicit knowledge produced by associative conditioning and cognitive processes); (2) affect or emotion (implicit âlikingâ and conscious pleasure) and (3) motivation (implicit incentive salience âwantingâ and cognitive incentive goals). The challenge is to identify how different brain circuits mediate different psychological components of reward, and how these components interact."
398,82,2823,1,The Pricing of Options and Corporate Liabilities,"If options are correctly priced in the market, it should not be possible to make sure profits by creating portfolios of long and short positions in options and their underlying stocks. Using this principle, a theoretical valuation formula for options is derived. Since almost all corporate liabilities can be viewed as combinations of options, the formula and the analysis that led to it are also applicable to corporate liabilities such as common stock, corporate bonds, and warrants. In particular, the formula can be used to derive the discount that should be applied to a corporate bond because of the possibility of default."
399,82,3060,1,A Model of Neuronal Responses in Visual Area MT,"Electrophysiological studies indicate that neurons in the middle temporal (MT) area of the primate brain are selective for the velocity of visual stimuli. This paper describes a computational model of MT physiology, in which local image velocities are represented via the distribution of MT neuronal responses. The computation is performed in two stages, corresponding to neurons in cortical areas V1 and MT. Each stage computes a weighted linear sum of inputs, followed by rectification and divisive normalization. V1 receptive field weights are designed for orientation and direction selectivity. MT receptive field weights are designed for velocity (both speed and direction) selectivity. The paper includes computational simulations accounting for a wide range of physiological data, and describes experiments that could be used to further test and refine the model."
400,82,4236,1,Visual receptive field organization,"Increasingly systematic approaches to quantifying receptive fields in primary visual cortex, combined with inspired ideas about functional circuitry, non-linearities, and visual stimuli, are bringing new interest to classical problems. This includes the distinction and hierarchy between simple and complex cells, the mechanisms underlying the receptive field surround, and debates about optimal stimuli for mapping receptive fields. An important new problem arises from recent observations of stimulus-dependent spatial and temporal summation in primary visual cortex. It appears that the receptive field can no longer be considered unique, and we might have to relinquish this cherished notion as the embodiment of neuronal function in primary visual cortex."
401,82,4696,1,Neuronal oscillations in cortical networks.,"Clocks tick, bridges and skyscrapers vibrate, neuronal networks oscillate. Are neuronal oscillations an inevitable by-product, similar to bridge vibrations, or an essential part of the brain's design? Mammalian cortical neurons form behavior-dependent oscillating networks of various sizes, which span five orders of magnitude in frequency. These oscillations are phylogenetically preserved, suggesting that they are functionally relevant. Recent findings indicate that network oscillations bias input selection, temporally link neurons into assemblies, and facilitate synaptic plasticity, mechanisms that cooperatively support temporal representation and long-term consolidation of information. 10.1126/science.1099745"
402,82,5205,1,Putting a spin on the dorsal-ventral divide of the striatum,"Since its conception three decades ago, the idea that the striatum consists of a dorsal sensorimotor part and a ventral portion processing limbic information has sparked a quest for functional correlates and anatomical characteristics of the striatal divisions. But this classic dorsal-ventral distinction might not offer the best view of striatal function. Anatomy and neurophysiology show that the two striatal areas have the same basic structure and that sharp boundaries are absent. Behaviorally, a distinction between dorsolateral and ventromedial seems most valid, in accordance with a mediolateral functional zonation imposed on the striatum by its excitatory cortical, thalamic and amygdaloid inputs. Therefore, this review presents a synthesis between the dorsal-ventral distinction and the more mediolateral-oriented functional striatal gradient."
403,82,5859,1,"Characterizing the hemodynamic response: Effects of presentation rate, sampling procedure, and the possibility of ordering brain activity based on relative timing","Rapid-presentation event related functional MIR (ER-MRI) allows neuroimaging methods based on hemodynamics to employ behavioral task paradigms typical of cognitive settings. However, the sluggishness of tyhe hemodynamic res0ponse and its variance provide constraints on how ER-MRI can be applied In a series of two studies, estimates of the hemodynamic response in or near the primary visual and motor cortices were compared across various paradigms and sampling procedures to determine the limits of ER-MRI procedures and , more generally, to describe the behavior of the hemodynamic response. The temporal profile of the hemodynamic response was estimated across overlapping events by  solving a set of linear equations withing the general linear model. No assumptions about the shape were made in solving the equations. Following estimation of the temporal  profile, the amplitude and timing were modeled using a gamma function Results indicated that  (1) within a brain region, for a given subject, estimation of the hemodynamic response is extremely stable for both amplitude (r2 = 0.98) and time to peak (r2 = 0.95), from one series of measurements to the next, and slightly less reliable for estimation of time to onset (r2 = 0.60).  (2) As the trial presentation ratechanged (from those spaced 20 seconds apart to temporally overlapping trials), the hemodynamic response amplitude showed a small, but significant, decrease. Trial onsets spaced (on average) 5 seconds apart showed a 17-25% reduction in amplitude compared to those spaced 20 seconds apart. Power analysis indicated that the increased number of trials at fast rates outweighs this decrease in amplitude if statistically reliable response detection is the goal.  (3) Knowledge of the amplitude and timing of the hemodynamic response in one region failed to predict those properties in another region, even for within-subject comparisons.  (4) Across subjects, the amplitude of the response showed no significant correlation with timing of the response, for either time-to-onset or time-to-peak estimates.  (5) The within-region stability of the response was sufficient to allow offsets in the timing of the response to be detected that were under a second, placing event-related fMRI methods in a position to answer questions about the change in relative timing between regions."
404,82,6512,1,Activity in prefrontal cortex during dynamic selection of action sequences.,"Completing everyday tasks often requires the execution of action sequences matched to a particular problem. To study the neural processes underlying these behaviors, we trained monkeys to produce a series of eye movements according to a sequence that changed unpredictably from one block of trials to the next. We then applied a decoding algorithm to estimate which sequence was being represented by the ensemble activity in prefrontal cortex. We found that the sequence predicted by this analysis changed gradually from the sequence that had been correct in the previous block to the sequence that was correct in the current block, closely following the fraction of executed movements that were consistent with the corresponding sequence. Thus, the neural activity dynamically tracked the monkeys' uncertainty about the correct sequence of actions. These results are consistent with prefrontal involvement in representing subjective knowledge of the correct action sequence."
405,82,6695,1,Neural population codes.,"In many regions of the brain, information is represented by patterns of activity occurring over populations of neurons. Understanding the encoding of information in neural population activity is important both for grasping the fundamental computations underlying brain function, and for interpreting signals that may be useful for the control of prosthetic devices. We concentrate on the representation of information in neurons with Poisson spike statistics, in which information is contained in the average spike firing rate. We analyze the properties of population codes in terms of the tuning functions that describe individual neuron behavior. The discussion centers on three computational questions: first, what information is encoded in a population; second, how does the brain compute using populations; and third, when is a population optimal? To answer these questions, we discuss several methods for decoding population activity in an experimental setting. We also discuss how computation can be performed within the brain in networks of interconnected populations. Finally, we examine questions of optimal design of population codes that may help to explain their particular form and the set of variables that are best represented. We show that for population codes based on neurons that have a Poisson distribution of spike probabilities, the behavior and computational properties of the code can be understood in terms of the tuning properties of individual cells."
406,82,7292,1,Reward Timing in the Primary Visual Cortex,"We discovered that when adult rats experience an association between visual stimuli and subsequent rewards, the responses of a substantial fraction of neurons in the primary visual cortex evolve from those that relate solely to the physical attributes of the stimuli to those that accurately predict the timing of reward. In addition to revealing a remarkable type of response plasticity in adult V1, these data demonstrate that reward-timing activity--a ""higher"" brain function--can occur very early in sensory-processing paths. These findings challenge the traditional interpretation of activity in the primary visual cortex. 10.1126/science.1123513"
407,82,8099,1,"Imitation, mirror neurons and autism.","Various deficits in the cognitive functioning of people with autism have been documented in recent years but these provide only partial explanations for the condition. We focus instead on an imitative disturbance involving difficulties both in copying actions and in inhibiting more stereotyped mimicking, such as echolalia. A candidate for the neural basis of this disturbance may be found in a recently discovered class of neurons in frontal cortex, 'mirror neurons' (MNs). These neurons show activity in relation both to specific actions performed by self and matching actions performed by others, providing a potential bridge between minds. MN systems exist in primates without imitative and 'theory of mind' abilities and we suggest that in order for them to have become utilized to perform social cognitive functions, sophisticated cortical neuronal systems have evolved in which MNs function as key elements. Early developmental failures of MN systems are likely to result in a consequent cascade of developmental impairments characterised by the clinical syndrome of autism."
408,82,9166,1,High-resolution imaging reveals highly selective nonface clusters in the fusiform face area,"A region in ventral human cortex (fusiform face area, FFA) thought to be important for face perception responds strongly to faces and less strongly to nonface objects. This pattern of response may reflect a uniform face-selective neural population or activity averaged across populations with heterogeneous selectivity. Using high-resolution functional magnetic resonance imaging (MRI), we found that the FFA has a reliable heterogeneous structure: localized subregions within the FFA highly selective to faces are spatially interdigitated with localized subregions highly selective to different object categories. We found a preponderance of face-selective responses in the FFA, but no difference in selectivity to faces compared to nonfaces. Thus, standard fMRI of the FFA reflects averaging of heterogeneous highly selective neural populations of differing sizes, rather than higher selectivity to faces. These results suggest that visual processing in this region is not exclusive to faces. Overall, our approach provides a framework for understanding the fine-scale structure of neural representations in the human brain."
409,82,10101,1,Empathy for pain involves the affective but not sensory components of pain.,"Our ability to have an experience of another's pain is characteristic of empathy. Using functional imaging, we assessed brain activity while volunteers experienced a painful stimulus and compared it to that elicited when they observed a signal indicating that their loved one--present in the same room--was receiving a similar pain stimulus. Bilateral anterior insula (AI), rostral anterior cingulate cortex (ACC), brainstem, and cerebellum were activated when subjects received pain and also by a signal that a loved one experienced pain. AI and ACC activation correlated with individual empathy scores. Activity in the posterior insula/secondary somatosensory cortex, the sensorimotor cortex (SI/MI), and the caudal ACC was specific to receiving pain. Thus, a neural response in AI and rostral ACC, activated in common for ""self"" and ""other"" conditions, suggests that the neural substrate for empathic experience does not involve the entire ""pain matrix."" We conclude that only that part of the pain network associated with its affective qualities, but not its sensory qualities, mediates empathy."
410,82,10863,1,Damage to the prefrontal cortex increases utilitarian moral judgements.,"The psychological and neurobiological processes underlying moral judgement have been the focus of many recent empirical studies. Of central interest is whether emotions play a causal role in moral judgement, and, in parallel, how emotion-related areas of the brain contribute to moral judgement. Here we show that six patients with focal bilateral damage to the ventromedial prefrontal cortex (VMPC), a brain region necessary for the normal generation of emotions and, in particular, social emotions, produce an abnormally 'utilitarian' pattern of judgements on moral dilemmas that pit compelling considerations of aggregate welfare against highly emotionally aversive behaviours (for example, having to sacrifice one person's life to save a number of other lives). In contrast, the VMPC patients' judgements were normal in other classes of moral dilemmas. These findings indicate that, for a selective set of moral dilemmas, the VMPC is critical for normal judgements of right and wrong. The findings support a necessary role for emotion in the generation of those judgements."
411,82,11486,1,Bootstrap confidence intervals,"{This article surveys bootstrap methods for producing good approximate confidence intervals. The goal is to improve by an order of magnitude upon the accuracy of the standard intervals <(theta)over cap> +/- z((alpha))<(sigma)over cap>, in a way that allows routine application even to very complicated problems. Both theory and examples are used to show how this is done. The first seven sections provide a heuristic overview of four bootstrap confidence interval procedures: BCa, bootstrap-t, ABC and calibration. Sections 8 and 9 describe the theory behind these methods, and their close connection with the Likelihood-based confidence interval theory developed by Barndorff-Nielsen, Cox and Reid and others.}"
412,83,1204,1,{Maximum likelihood from incomplete data via the EM algorithm},"{A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.}"
413,83,12716,1,The Power of Feedback,"Feedback is one of the most powerful influences on learning and achievement, but this impact can be either positive or negative. Its power is frequently mentioned in articles about learning and teaching, but surprisingly few recent studies have systematically investigated its meaning. This article provides a conceptual analysis of feedback and reviews the evidence related to its impact on learning and achievement. This evidence shows that although feedback is among the major influences, the type of feedback and the way it is given can be differentially effective. A model of feedback is then proposed that identifies the particular properties and circumstances that make it effective, and some typically thorny issues are discussed, including the timing of feedback and the effects of positive and negative feedback. Finally, this analysis is used to suggest ways in which feedback can be used to enhance its effectiveness in classrooms. 10.3102/003465430298487"
414,84,2732,1,Remediation: Understanding New Media,"{""The authors do a splendid job of showing precisely how technologies like computer games, digital photography, film television, the Web, and virtual reality all turn on the mutually constructive strategies of generating immediacy and making users hyperaware of the media themselves. . . . The authors lay out a provocative theory of contemporary selfhood, one that draws on and modifies current notions of the `virtual' and `networked' human subject. Clearly written and not overly technical, this book will interest general readers, students, and scholars engaged with current trends in technology."" -- M. Uebel, <i>Choice</i> <P>Media critics remain captivated by the modernist myth of the new: they assume that digital technologies such as the World Wide Web, virtual reality, and computer graphics must divorce themselves from earlier media for a new set of aesthetic and cultural principles. In this richly illustrated study, Jay David Bolter and Richard Grusin offer a theory of mediation for our digital age that challenges this assumption. They argue that new visual media achieve their cultural significance precisely by paying homage to, rivaling, and refashioning such earlier media as perspective painting, photography, film, and television. They call this process of refashioning ""remediation,"" and they note that earlier media have also refashioned one another: photography remediated painting, film remediated stage production and photography, and television remediated film, vaudeville, and radio. <P>More about this book}"
415,84,3655,1,Designing the User Interface: Strategies for Effective Human-Computer Interaction (4th Edition),"{The much-anticipated fourth edition of Designing the User Interface provides a comprehensive, authoritative introduction to the dynamic field of human-computer interaction (HCI). Students and professionals learn practical principles and guidelines needed to develop high quality interface designs\&#151;ones that users can understand, predict, and control. It covers theoretical foundations, and design processes such as expert reviews and usability testing. Numerous examples of direct manipulation, menu selection, and form fill-in give readers an understanding of excellence in design. Recent innovations in collaborative interfaces, online help, and information visualization receive special attention. A major change in this edition is the integration of the World Wide Web and mobile devices throughout the book. Chapters have examples from cell phones, consumer electronics, desktop displays, and Web interfaces.}"
416,84,8823,1,Ambient Displays: Turning Architectural Space into an Interface between People and Digital Information,"We envision that the architectural space we inhabit will be a new  form of interface between humans and online digital information. This paper  discusses Ambient Displays: a new approach to interfacing people with online  digital information. Ambient Displays present information within a space  through subtle changes in light, sound, and movement, which can be  processed in the background of awareness. We describe the design and  implementation of two example Ambient Displays, the ambientROOM and  Ambient Fixtures. Additionally, we discuss applications of Ambient Displays  and propose theories of design of such interfaces based on our initial  experiences.  1 INTRODUCTION  Ambient \Am""bi*ent\, a. Surrounding, encircling, encompassing, and environing.  -Oxford English Dictionary Display \Dis*play""\, n. An opening or unfolding; exhibition; manifestation.  -Webster's Revised Unabridged Dictionary (1913) Nature is filled with subtle, beautiful and expressive ambient displays that engage ..."
417,85,329,1,A tutorial on support vector regression,"In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for regression and function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization and capacity control from a SV point of view."
418,85,7043,1,Greedy Function Approximation: A gradient boosting machine,"Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent âboostingâ paradigm is developed for additive expansions based on any fitting criterion.Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such âTreeBoostâ models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed."
419,86,336,1,As We May Think,"As Director of the Office of Scientific Research and Development, Dr. Vannevar Bush has coordinated the activities of some six thousand leading American scientists in the application of science to warfare. In this significant article he holds up an incentive for scientists when the fighting has ceased. He urges that men of science should then turn to the massive task of making more accessible our bewildering store of knowledge. For many years inventions have extended manâs physical powers rather than the powers of his mind. Trip hammers that multiply the fists, microscopes that sharpen the eye, and engines of destruction and detection are new results, but not the end results, of modern science. Now, says Dr. Bush, instruments are at hand which, if properly developed, will give man access to and command over the inherited knowledge of the ages. The perfection of these pacific instruments should be the first objective of our scientists as they emerge from their war work. Like Emersonâs famous address of 1837 on âThe American Scholar,ââ this paper by Dr. Bush calls for a new relationship between thinking man and the sum of our knowledge. âThe [Atlantic Monthly] Editor, July 1945"
420,86,2208,1,How Popular is Your Paper? An Empirical Study of the Citation Distribution,"Numerical data for the distribution of citations are examined for: (i) papers published in 1981 in journals which are catalogued by the Institute for Scientific Information (783,339 papers) and (ii) 20 years of publications in Physical Review D, vols. 11-50 (24,296 papers). A Zipf plot of the number of citations to a given paper versus its citation rank appears to be consistent with a power-law dependence for leading rank papers, with exponent close to -1/2. This, in turn, suggests that the number of papers with x citations, N(x), has a large-x power law decay N(x)~x^{-alpha}, with alpha approximately equal to 3."
421,86,4037,1,A general language model for information retrieval,"Statistical language modeling has been successfully used for speech recognition, part-of-speech tagging, and syntactic parsing. Recently, it has also been applied to information retrieval. According to this new paradigm, each document is viewed as a language sample, and a query as a generation process. The retrieved documents are ranked based on the probabilities of producing a query from the corresponding language models of these documents. In this paper, we will present a new language model for information retrieval, which is based on a range of data smoothing techniques, including the Good-Turning estimate, curve-fitting functions, and model combinations. Our model is conceptually simple and intuitive, and can be easily extended to incorporate probabilities of phrases such as word pairs and word triples. The experiments with the Wall Street Journal and TREC4 data sets showed that the performance of our model is comparable to that of INQUERY and better than that of another language model for information retrieval. In particular, word pairs are shown to be useful in improving the retrieval performance."
422,86,7175,1,Classification of user image descriptions,"In order to resolve the mismatch between user needs and current image retrieval techniques, we conducted a study to get more information about what users look for in images. First, we developed a framework for the classification of image descriptions by users, based on various classification methods from the literature. The classification framework distinguishes three related viewpoints on images, namely nonvisual metadata, perceptual descriptions and conceptual descriptions. For every viewpoint a set of descriptive classes and relations is specified. We used the framework in an empirical study, in which image descriptions were formulated by 30 participants. The resulting descriptions were split into fragments and categorized in the framework. The results suggest that users prefer general descriptions as opposed to specific or abstract descriptions. Frequently used categories were objects, events and relations between objects in the image. r 2004 Elsevier Ltd. All rights reserved."
423,86,10215,1,"A New Era in Citation and Bibliometric Analyses: Web of Science, Scopus, and Google Scholar","Academic institutions, federal agencies, publishers, editors, authors, and librarians increasingly rely on citation analysis for making hiring, promotion, tenure, funding, and/or reviewer and journal evaluation and selection decisions. The Institute for Scientific Information's (ISI) citation databases have been used for decades as a starting point and often as the only tools for locating citations and/or conducting citation analyses. ISI databases (or Web of Science), however, may no longer be adequate as the only or even the main sources of citations because new databases and tools that allow citation searching are now available. Whether these new databases and tools complement or represent alternatives to Web of Science (WoS) is important to explore. Using a group of 15 library and information science faculty members as a case study, this paper examines the effects of using Scopus and Google Scholar (GS) on the citation counts and rankings of scholars as measured by WoS. The paper discusses the strengths and weaknesses of WoS, Scopus, and GS, their overlap and uniqueness, quality and language of the citations, and the implications of the findings for citation analysis. The project involved citation searching for approximately 1,100 scholarly works published by the study group and over 200 works by a test group (an additional 10 faculty members). Overall, more than 10,000 citing and purportedly citing documents were examined. WoS data took about 100 hours of collecting and processing time, Scopus consumed 200 hours, and GS a grueling 3,000 hours."
424,86,16230,1,The foundation of the concept of relevance,"In 1975 Tefko Saracevic declared ldquothe subject knowledge viewrdquo to be the most fundamental perspective of relevance. This paper examines the assumptions in different views of relevance, including ldquothe system's viewrdquo and ldquothe user's viewrdquo and offers a reinterpretation of these views. The paper finds that what was regarded as the most fundamental view by Saracevic in 1975 has not since been considered (with very few exceptions). Other views, which are based on less fruitful assumptions, have dominated the discourse on relevance in information retrieval and information science. Many authors have reexamined the concept of relevance in information science, but have neglected the subject knowledge view, hence basic theoretical assumptions seem not to have been properly addressed. It is as urgent now as it was in 1975 seriously to consider ldquothe subject knowledge viewrdquo of relevance (which may also be termed ldquothe epistemological viewrdquo). The concept of relevance, like other basic concepts, is influenced by overall approaches to information science, such as the cognitive view and the domain-analytic view. There is today a trend toward a social paradigm for information science. This paper offers an understanding of relevance from such a social point of view."
425,87,1822,1,Intelligence Without Reason,Computers and Thought are the two categories that together de ne Arti cial Intelligence as a discipline. It is generally accepted that work in Arti cial Intelligence over the last thirty years has had a strong in uence on aspects of computer architectures. In this paper we also make the converse claim; that the state of computer architecture has been a strong in uence on our models of thought. The Von Neumann model of computation has lead Arti cial Intelligence in particular directions. Intelligence in biological systems is completely di erent. Recent work in behavior-based Arti cial Intelligence has produced new models of intelligence that are much closer in spirit to biological systems. The non-Von Neumann computational models they use share many characteristics with biological computation.
426,87,1830,1,Extending UML for Agents,"Gaining wide acOI262U&#034; for the use of agents in industry requires both relating it to the nearestantecU3O1 teccU3O1 (objec t-oriented software development) and usingartifac2 to support the development environment throughout the full systemlifec34I1 We address both of these requirements in this paper by desc51U&#034;3 some of the mostcstU2 requirements for modeling agents and agent-based systems---using a set of UML idioms and extensions. This paper illustrates theapproac by presenting a three-layer AUML representation for agent interacOU&#034; protocO andcdUO6521 byinc14O4U other useful agent-based extensions to UML.  Keywords  Ag nts, UML, int racUML, intUOO6O3U&#034;3I643U sign artifacOO6O3U&#034;3I6  ngin ring. 1. Iy&gt;quwm&lt;yzz  Suc ssful industrial d ploym nt of ag nt t c1O157U&#034; r quir s tc1762 s that rduc  th risk inh r nt in any n w tc6664U&#034;24 Two ways that rduc  risk in th y s of pot ntial adopt rs ar :  .  to present the new tecIUIUIUIUIUcOOUc to provide explicmethods, andU2N61NU&#034;21771Uc We apply bo..."
427,87,1838,1,Component Software: Beyond Object-Oriented Programming,"The author describes his book as a ""unique blend of market and technology coverage, broad and fair coverage of current technologies and a deep discussion of real problems with their solutions where known"". The first edition won the ""Jolt Award"" became the leading book on the market to combine explanations of what the key technologies are, how to use them and why they are important in the software market-place, and look at these in terms of both the technical and business issues. The book was also the first to define components and clarify the key questions surrounding them, show how they are key to software design and offer a historical overview of their development. Contents same as US/UK editions."
428,87,3944,1,Statistical mechanics,1 Foundation of statistical mechanics. 1 1.1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Program of statistical mechanics. . . . . . . . . . . . . . . . . . . 4 1.3 States of a system. . . . . . . . . . . . . . . . . . . . . . . . . . . 5 1.4 Averages. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 1.5 Thermal equilibrium. . . . . . . . . . . . . . . . . . . . . . . . . . 14 1.6 Entropy and temperature. . . . . . . . . . . . . . . . . . . . . . . 16 1.7 Laws of thermodynamics. . . . . . . . . . . . . . . . . . . . . . . 19 1.8 Problems for chapter 1 . . . . . . . . . . . . . . . . . . . . . . . . 20 2 The canonical ensemble 23 2.1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.2 Energy and entropy and temperature. . . . . . . . . . . . . . . . 26 2.3 Work and pressure. . . . . . . . . . . . . . . . . . . . . . . . . . . 28 2.4 Helmholtz free energy. . . . . . . . . . . . . . . . . . . . . . . . . 31 2.5 Changes in variables. . . . . . . . . . . . . . . . . . . . . . . . . . 32 2.6 Properties of the Helmholtz free energy. . . . . . . . . . . . . . . 33 2.7 Energy Â°uctuations. . . . . . . . . . . . . . . . . . . . . . . . . . 35 2.8 A simple example. . . . . . . . . . . . . . . . . . . . . . . . . . . 37 2.9 Problems for chapter 2 . . . . . . . . . . . . . . . . . . . . . . . . 39 3 Variable number of particles 43 3.1 Chemical potential. . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.2 Examples of the use of the chemical potential. . . . . . . . . . . . 46 3.3 DiÂ®erential relations and grand potential. . . . . . . . . . . . . . 48 3.4 Grand partition function. . . . . . . . . . . . . . . . . . . . . . . 50 3.5 Overview of calculation methods. . . . . . . . . . . . . . . . . . . 55 3.6 A simple example. . . . . . . . . . . . . . . . . . . . . . . . . . . 57 3.7 Ideal gas in Â¯rst approximation. . . . . . . . . . . . . . . . . . . . 58 3.8 Problems for chapter 3 . . . . . . . . . . . . . . . . . . . . . . . . 64 4 Statistics of independent particles. 67 4.1 Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 4.2 Boltzmann gas again. . . . . . . . . . . . . . . . . . . . . . . . . 74 III IV CONTENTS 4.3 Gas of poly-atomic molecules. . . . . . . . . . . . . . . . . . . . . 77 4.4 Degenerate gas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 4.5 Fermi gas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80 4.6 Boson gas. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 4.7 Problems for chapter 4 . . . . . . . . . . . . . . . . . . . . . . . . 87 5 Fermions and Bosons 89 5.1 Fermions in a box. . . . . . . . . . . . . . . . . . . . . . . . . . . 89 5.2 Bosons in a box. . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 5.3 Bose-Einstein condensation. . . . . . . . . . . . . . . . . . . . . . 113 5.4 Problems for chapter 5 . . . . . . . . . . . . . . . . . . . . . . . . 115 6 Density matrix formalism. 119 6.1 Density operators. . . . . . . . . . . . . . . . . . . . . . . . . . . 119 6.2 General ensembles. . . . . . . . . . . . . . . . . . . . . . . . . . . 123 6.3 Maximum entropy principle. . . . . . . . . . . . . . . . . . . . . . 125 6.4 Equivalence of entropy deÂ¯nitions for canonical ensemble. . . . . 134 6.5 Problems for chapter 6 . . . . . . . . . . . . . . . . . . . . . . . . 136 A Solutions to selected problems. 139 A.1 Solutions for chapter 1. . . . . . . . . . . . . . . . . . . . . . . 139 A.2 Solutions for chapter 2. . . . . . . . . . . . . . . . . . . . . . . 147 A.3 Solutions for chapter 3 . . . . . . . . . . . . . . . . . . . . . . . 152 A.4 Solutions for chapter 4. . . . . . . . . . . . . . . . . . . . . . . 157 A.5 Solutions for chapter 5. . . . . . . . . . . . . . . . . . . . . . . 161 A.6 Solutions for chapter 6. . . . . . . . . . . . . . . . . . . . . . . 167
429,88,2986,1,Democratizing Innovation,"{Innovation is rapidly becoming democratized. Users, aided by improvements in computer and communications technology, increasingly can develop their own new products and services. These innovating users -- both individuals and firms -- often freely share their innovations with others, creating user-innovation communities and a rich intellectual commons. In <i>Democratizing Innovation</i>, Eric von Hippel looks closely at this emerging system of user-centered innovation. He explains why and when users find it profitable to develop new products and services for themselves, and why it often pays users to reveal their innovations freely for the use of all.<br /> <br /> The trend toward democratized innovation can be seen in software and information products -- most notably in the free and open-source software movement -- but also in physical products. Von Hippel's many examples of user innovation in action range from surgical equipment to surfboards to software security features. He shows that product and service development is concentrated among ""lead users,"" who are ahead on marketplace trends and whose innovations are often commercially attractive.<br /> <br /> Von Hippel argues that manufacturers should redesign their innovation processes and that they should systematically seek out innovations developed by users. He points to businesses -- the custom semiconductor industry is one example -- that have learned to assist user-innovators by providing them with toolkits for developing new products. User innovation has a positive impact on social welfare, and von Hippel proposes that government policies, including R&D subsidies and tax credits, should be realigned to eliminate biases against it. The goal of a democratized user-centered innovation system, says von Hippel, is well worth striving for. An electronic version of this book is available under a Creative Commons license.}"
430,89,568,1,Complex queries in dht-based peer-to-peer networks,"Recently a new generation of P2P systems, offering distributed hash table (DHT) functionality, have been proposed. These systems greatly improve the scalability and exact-match accuracy of P2P systems, but offer only the exact-match query facility. This paper outlines a research agenda for building complex query facilities on top of these DHT-based P2P systems. We describe the issues involved and outline our research plan and current status."
431,89,3600,1,What Can Databases do for Peer-to-Peer?,"The Internet community has recently been focused on peer-to-peer systems like Napster, Gnutella, and Freenet. The grand vision â a decentralized community of machines pooling their resources to benefit everyone â is compelling for many reasons: scalability, robustness, lack of need for administration, and even anonymity and resistance to censorship. Existing peer-to-peer (P2P) systems have focused on specific application domains (e.g. music files) or on providing filesystem-like capabilities; these systems ignore the semantics of data. An important question for the database community is how data management can be applied to P2P, and what we can learn from and contribute to the P2P area. We address these questions, identify a number of potential research ideas in the overlap between data management and P2P systems, present some preliminary fundamental results, and describe our initial work in constructing a P2P data management system. 1"
432,90,19,1,Spectra and eigenvectors of scale-free networks.,"We study the spectra and eigenvectors of the adjacency matrices of scale-free networks when bidirectional interaction is allowed, so that the adjacency matrix is real and symmetric. The spectral density shows an exponential decay around the center, followed by power-law long tails at both spectrum edges. The largest eigenvalue lambda1 depends on system size N as lambda1 approximately N1/4 for large N, and the corresponding eigenfunction is strongly localized at the hub, the vertex with largest degree. The component of the normalized eigenfunction at the hub is of order unity. We also find that the mass gap scales as N(-0.68)."
433,90,478,1,Emergence of scaling in random networks,"Recently retired as head of the Global Alliance for Vaccines and Immunization (GAVI) secretariat and as a health advisor to leading global entities, Tore Godal is now a Special Advisor to the Norwegian Prime Minister. He is nevertheless continuing to fight for better global health, cogently articulating the needs of the world's poor and disadvantaged. He is a leading leprosy expert, ex-director of the world's premier agency for research and training in tropical diseases, instigator and prime mover of some global innovative public-private health sector partnerships, adept fund mobilizer, and advocate of the `let's get it done' school of leadership. Few individuals are, therefore, more experienced or better suited for such a crucial and much-needed role"
434,90,918,1,Linked: How Everything Is Connected to Everything Else and What It Means,"How is the human brain like the AIDS epidemic? Ask physicist Albert-LÃ¡szlÃ³ BarabÃ¡si and he'll explain them both in terms of networks of individual nodes connected via complex but understandable relationships. _Linked: The New Science of Networks_ is his bright, accessible guide to the fundamentals underlying neurology, epidemiology, Internet traffic, and many other fields united by complexity.  BarabÃ¡si's gift for concrete, nonmathematical explanations and penchant for eccentric humor would make the book thoroughly enjoyable even if the content weren't engaging. But the results of BarabÃ¡si's research into the behavior of networks are deeply compelling. Not all networks are created equal, he says, and he shows how even fairly robust systems like the Internet could be crippled by taking out a few super-connected nodes, or hubs. His mathematical descriptions of this behavior are helping doctors, programmers, and security professionals design systems better suited to their needs. _Linked_ presents the next step in complexity theory--from understanding chaos to practical applications. _--Rob Lightner_"
435,90,1260,1,"Emergence: The Connected Lives of Ants, Brains, Cities, and Software","An individual ant, like an individual neuron, is just about as dumb as can be. Connect enough of them together properly, though, and you get spontaneous intelligence. Web pundit Steven Johnson explains what we know about this phenomenon with a rare lucidity in Emergence: The Connected Lives of Ants, Brains, Cities, and Software. Starting with the weird behavior of the semi-colonial organisms we call slime molds, Johnson details the development of increasingly complex and familiar behavior among simple components: cells, insects, and software developers all find their place in greater schemes.  Most game players, alas, live on something close to day-trader time, at least when they're in the middle of a game--thinking more about their next move than their next meal, and usually blissfully oblivious to the ten- or twenty-year trajectory of software development. No one wants to play with a toy that's going to be fun after a few decades of tinkering--the toys have to be engaging now, or kids will find other toys.  Johnson has a knack for explaining complicated and counterintuitive ideas cleverly without stealing the scene. Though we're far from fully understanding how complex behavior manifests from simple units and rules, our awareness that such emergence is possible is guiding research across disciplines. Readers unfamiliar with the sciences of complexity will find Emergence an excellent starting point, while those who were chaotic before it was cool will appreciate its updates and wider scope. --Rob Lightner --This text refers to the Hardcover edition. From Publishers Weekly  To have the highly touted editor of a highly touted Web culture organ writing about the innate smartness of interconnectivity seems like a hip, winning combination unless that journal becomes the latest dot-com casualty. Feed, of which Johnson was cofounder and editor-in-chief, recently announced it was shuttering its windows, which should make for a less exuberant launch for his second bricks-and-mortar title, following 1997's Interface Culture. Yet the book's premise and execution make it compelling, even without the backstory. In a paradigmatic example here, ants, without leaders or explicit laws, organize themselves into highly complex colonies that adapt to the environment as a single entity, altering size and behavior to suit conditions exhibiting a weird collective intelligence, or what has come to be called emergence. In the first two parts of the book, Johnson ranges over historical examples of such smart interconnectivity, from the silk trade in medieval Florence to the birth of the software industry and to computer programs that produce their own software offspring, or passively map the Web by ""watching"" a user pool. Johnson's tone is light and friendly, and he has a journalistic gift for wrapping up complex ideas with a deft line: ""you don't want one of the neurons in your brain to suddenly become sentient."" In the third section, which bears whiffs of '90s exuberance, Johnson weighs the impact of Web sites like Napster, eBay and Slashdot, predicting the creation of a brave, new media world in which self-organizing clusters of shared interests structure the entertainment industry. The wide scope of the book may leave some readers wanting greater detail, but it does an excellent job of putting the Web into historical and biological context, with no dot.com diminishment. (Sept. 19) Forecast: All press is good press, so the failure of Feed at least makes a compelling hook for reviews, which should be extensive. A memoir of the author's Feed years can't be far behind, but in the meantime this should sell solidly, with a possible breakout if Johnson's media friends get behind it fully.  Copyright 2001 Cahners Business Information, Inc."
436,90,2128,1,Sync: The Emerging Science of Spontaneous Order,"Strogatz is a Cornell mathematician and pioneer of the science of synchrony, which brings mathematics, physics and biology to bear on the mystery of how spontaneous order occurs at every level of the cosmos, from the nucleus on up. In this eminently accessible and entertaining book, Strogatz explores the mysterious synchrony achieved by fireflies that flash in unison by the thousands, and the question of what makes our own body clocks synchronize with night and day and even with one another. He explores the sync of inanimate objects, inadvertently discovered by Christiaan Huygens in 1665 when he observed that his two pendulum clocks would swing in unison when they were within a certain distance of each other. A case of spontaneous synchrony occurred on the 2000 opening of the Millennium footbridge in London when hundreds of pedestrians caused the bridge to undulate erratically as they unconsciously adjusted their pace to the bridge's swaying-it was closed two days later. Strogatz explores synchrony in chaos systems, at the quantum level, in small-world networks as exemplified by the parlor game ""six degrees of Kevin Bacon"" and in human behavior involving fads, mobs and the herd mentality of stock traders. The author traces how the isolated and often accidental discoveries of researchers are beginning to gel into the science of synchrony, and he amply illustrates how the laws of mathematics underlie the universe's uncanny capacity for spontaneous order."
437,90,3960,1,Bursty and hierarchical structure in streams,"A fundamental problem in text data mining is to extract meaningful structure from document streams that arrive continuously over time. E-mail and news articles are two natural examples of such streams, each characterized by topics that appear, grow in intensity for a period of time, and then fade away. The published literature in a particular research field can be seen to exhibit similar phenomena over a much longer time scale. Underlying much of the text mining work in this area is the following intuitive premise --- that the appearance of a topic in a document stream is signaled by a ``burst of activity,'' with certain features rising sharply in frequency as the topic emerges. The goal of the present work is to develop a formal approach for modeling such ``bursts,'' in such a way that they can be robustly and efficiently identified, and can provide an organizational framework for analyzing the underlying content. The approach is based on modeling the stream using an infinite-state automaton, in which bursts appear naturally as state transitions; in some ways, it can be viewed as drawing an analogy with models from queueing theory for bursty network traffic. The resulting algorithms are highly efficient, and yield a nested representation of the set of bursts that imposes a hierarchical structure on the overall stream. Experiments with e-mail and research paper archives suggest that the resulting structures have a natural meaning in terms of the content that gave rise to them."
438,90,5751,1,Bioinformatics: an introduction for computer scientists,"The article aims to introduce computer scientists to the new field of bioinformatics. This area has arisen from the needs of biologists to utilize and help interpret the vast amounts of data that are constantly being gathered in genomic researchâand its more recent counterparts, proteomics and functional genomics. The ultimate goal of bioinformatics is to develop in silico models that will complement in vitro and in vivo biological experiments. The article provides a birdâs eye view of the basic concepts in molecular cell biology, outlines the nature of the existing data, and describes the kind of computer algorithms and techniques that are necessary to understand cell behavior. The underlying motivation for many of the bioinformatics approaches is the evolution of organisms and the complexity of working with incomplete and noisy data. The topics covered include: descriptions of the current software especially developed for biologists, computer and mathematical cell models, and areas of computer science that play an important role in bioinformatics."
439,90,7610,1,Formal Concept Analysis Mathematical Foundations,"This is the first textbook on formal concept analysis. It gives a systematic presentation of the mathematical foundations and their relations to applications in computer science, especially in data analysis and knowledge processing. Above all, it presents graphical methods for representing conceptual systems that have proved themselves in communicating knowledge. Theory and graphical representation are thus closely coupled together. The mathematical foundations are treated thoroughly and illuminated by means of numerous examples. Since computers are being used ever more widely for knowledge processing, formal methods for conceptual analysis are gaining in importance. This book makes the basic theory for such methods accessible in a compact form."
440,90,9412,1,Using graph concepts to understand the organization of complex systems,"Complex networks are universal, arising in fields as disparate as sociology, physics, and biology. In the past decade, extensive research into the properties and behaviors of complex systems has uncovered surprising commonalities among the topologies of different systems. Attempts to explain these similarities have led to the ongoing development and refinement of network models and graph-theoretical analysis techniques with which to characterize and understand complexity. In this tutorial, we demonstrate through illustrative examples, how network measures and models have contributed to the elucidation of the organization of complex systems."
441,90,10592,1,K-means clustering via principal component analysis,"Principal component analysis (PCA) is a widely used statistical technique for unsupervised dimension reduction. K-means clustering is a commonly used data clustering for performing unsupervised learning tasks. Here we prove that principal components are the continuous solutions to the discrete cluster membership indicators for K-means clustering. New lower bounds for K-means objective function are derived, which is the total variance minus the eigenvalues of the data covariance matrix. These results indicate that unsupervised dimension reduction is closely related to unsupervised learning. Several implications are discussed. On dimension reduction, the result provides new insights to the observed effectiveness of PCA-based data reductions, beyond the conventional noisereduction explanation that PCA, via singular value decomposition, provides the best low-dimensional linear approximation of the data. On learning, the result suggests effective techniques for K-means data clustering. DNA gene expression and Internet newsgroups are analyzed to illustrate our results. Experiments indicate that the new bounds are within 0.5-1.5 % of the optimal values. 1."
442,90,11568,1,Power-law distributions in empirical data,"Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution -- the part of the distribution representing large but rare events -- and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out.. Comment: 43 pages, 11 figures, 7 tables, 4 appendices; code available at http://www.santafe.edu/~aaronc/powerlaws/"
443,90,13145,1,Informetrics at the beginning of the 21st centuryâA review,"This paper reviews developments in informetrics between 2000 and 2006. At the beginning of the 21st century we witness considerable growth in webometrics, mapping and visualization and open access. A new topic is comparison between citation databases, as a result of the introduction of two new citation databases Scopus and Google Scholar. There is renewed interest in indicators as a result of the introduction of the h -index. Traditional topics like citation analysis and informetric theory also continue to develop. The impact factor debate, especially outside the informetric literature continues to thrive. Ranked lists (of journal, highly cited papers or of educational institutions) are of great public interest."
444,91,138,1,Automatic organization for digital photographs with geographic coordinates,"Uses time and location information in the EXIF header to automatically organise a personal photograph collection. Foruses on auto-genereating a structure that can be used WITHOUT A MAP. Makes a nice point about the inneficient use of screen real estate when using maps, and that zooming an paning could be cumbersome. The paper presents the algorithm for grouping by time and location for event detection. The tool generates a text caption to describe (by location) each grouping--this is crucial since no map. Description/evaluation of the user interface is left for future work. Evaluation: the algorithms are tested on three image collections (2580, 1192, 1823 images) . This could be interesting to look at again when writing the machine learning stuff."
445,91,2353,1,"Indexing and access for digital libraries and the internet: Human, database, and domain factors","Discussion in the research community and among the general public regarding content indexing (especially subject indexing) and access to digital resources, especially on the Internet, has underutilized research on a variety of factors that are important in the design of such access mechanisms. Some of these factors and issues are reviewed and implications drawn for information system design in the era of electronic access. Specifically the following are discussed: <I >Human factors:</I > Subject searching vs. indexing, multiple terms of access, folk classification, basic-level terms, and folk access; <I >Database factors:</I > Bradford's Law, vocabulary scalability, the Resnikoff-Dolby 30:1 Rule; <I >Domain factors:</I > Role of domain in indexing."
446,92,6575,1,"Embedded-atom method: Derivation and application to impurities, surfaces, and other defects in metals","We develop the embedded-atom method [Phys. Rev. Lett.  50 , 1285 (1983)], based on density-functional theory, as a new means of calculating ground-state properties of realistic metal systems. We derive an expression for the total energy of a metal using the embedding energy from which we obtain several ground-state properties, such as the lattice constant, elastic constants, sublimation energy, and vacancy-formation energy. We obtain the embedding energy and accompanying pair potentials semiempirically for Ni and Pd, and use these to treat several problems: surface energy and relaxation of the (100), (110), and (111) faces; properties of H in bulk metal (H migration, binding of H to vacancies, and lattice expansion in the hydride phase); binding site and adsorption energy of hydrogen on (100), (110), and (111) surfaces; and lastly, fracture of Ni and the effects of hydrogen on the fracture. We emphasize problems with hydrogen and with surfaces because none of these can be treated with pair potentials. The agreement with experiment, the applicability to practical problems, and the simplicity of the technique make it an effective tool for atomistic studies of defects in metals."
447,93,159,1,CiteSeer-API: towards seamless resource location and interlinking for digital libraries,"We introduce CiteSeer-API, a public API to CiteSeer-like services. CiteSeer-API is SOAP/WSDL based and allows for easy programmatical access to all the specific functionalities offered by CiteSeer services, including full text search of documents and citations and citation-based document discovery. In order to enable operability and interlinking with arbitrary software agents and digital library systems, CiteSeer-API uses digital content signatures to create system-independent handles for the..."
448,93,3908,1,Spinning the Semantic Web: Bringing the World Wide Web to Its Full Potential,"{As the World Wide Web continues to expand, it becomes increasingly difficult for users to obtain information efficiently. Because most search engines read format languages such as HTML or SGML, search results reflect formatting tags more than actual page content, which is expressed in natural language. <i>Spinning the Semantic Web</i> describes an exciting new type of hierarchy and standardization that will replace the current ""web of links"" with a ""web of meaning."" Using a flexible set of languages and tools, the Semantic Web will make all available information -- display elements, metadata, services, images, and especially content -- accessible. The result will be an immense repository of information accessible for a wide range of new applications.<br /> <br /> This first handbook for the Semantic Web covers, among other topics, software agents that can negotiate and collect information, markup languages that can tag many more types of information in a document, and knowledge systems that enable machines to read Web pages and determine their reliability. The truly interdisciplinary Semantic Web combines aspects of artificial intelligence, markup languages, natural language processing, information retrieval, knowledge representation, intelligent agents, and databases.}"
449,93,5720,1,Cognitive Factors in Programming with Diagrams,"Visual programming languages aim to broaden the use of diagrams within the software industry, to the  extent that they are integrated into the programming language itself. As a result, they provide an ideal  opportunity to study the benefits of diagrams as an external representation during problem solving: not  only is programming a challenging problem-solving activity, but the effect of diagram usage can be  directly assessed by comparing performance while using a visual programming language..."
450,93,7970,1,Social Networks and Collective Action: A Theory of the Critical Mass. III,"Most analyses of collective action agree that overcoming the freerider problem requires organizing potential contributors, thus making their decisions interdependent. The potential for organizing depends on the social ties in the group, particularly on the overall density or frequency of ties, on the extent to which they are centralized in a few individuals, and on the costs of communicating and coordinating actions through these ties. Mathematical analysis and computer simulations extend a formal microsocial theory of interdependent collective action to treat social networks and organization costs. As expected, the overall density of social ties in a group improves its prospects for collective action. More significant, because less expected, are the findings that show that the centralization of network ties always has a positive effect on collective action and that the negative effect of costs on collective action declines as the group's resource or interest heterogeneity increases. These nonobvious results are due to the powerful effects of selectivity, the organizer's ability to concentrate organizing efforts on those individuals whose potential contributions are the largest."
451,93,13408,1,Robust dynamic classes revealed by measuring the response function of a social system,"We study the relaxation response of a social system after endogenous and exogenous bursts of activity using the time series of daily views for nearly 5 million videos on YouTube. We find that most activity can be described accurately as a Poisson process. However, we also find hundreds of thousands of examples in which a burst of activity is followed by an ubiquitous power-law relaxation governing the timing of views. We find that these relaxation exponents cluster into three distinct classes and allow for the classification of collective human dynamics. This is consistent with an epidemic model on a social network containing two ingredients: a power-law distribution of waiting times between cause and action and an epidemic cascade of actions becoming the cause of future actions. This model is a conceptual extension of the fluctuation-dissipation theorem to social systems [Ruelle, D (2004) Phys Today 57:48Ã¢ÂÂ53] and [Roehner BM, et al., (2004) Int J Mod Phys C 15:809Ã¢ÂÂ834], and provides a unique framework for the investigation of timing in complex systems."
452,94,275,1,Languages of the Future,"This paper explores a new point in the design space of formal reasoning systems - part programming language, part logical framework. The system is built on a programming language where the user expresses equality constraints between types and the type checker then enforces these constraints. This simple extension to the type system allows the programmer to describe properties of his program in the types of  witness  objects which can be thought of as concrete evidence that the program has the property desired. These techniques and two other rich typing mechanisms, rank-N polymorphism and extensible kinds, create a powerful new programming idiom for writing programs whose types enforce semantic properties.A language with these features is  both  a practical programming language  and  a logic. This marriage between two previously separate entities increases the probability that users will apply formal methods to their programming designs. This kind of synthesis creates the foundations for the languages of the future."
453,94,751,1,Generalising monads to arrows,"this paper. Pleasingly, the arrow interface turned out to be applicable to other kinds of non-monadic library also, for example the fudgets library for graphical user interfaces [CH93], and a new library for programming active web pages. These applications will be described in sections 6 and 9. While arrows are a little less convenient to use than monads, they have significantly wider applicability. They can therefore be used to bring the benefits of monad-like programming to a much wider class of applications. 2 Background: Library Design Using Monads"
454,94,9702,1,"Quantum theory, the {Church-Turing} principle and the universal quantum computer","It is argued that underlying the Church-Turing hypothesis there is an implicit physical assertion. Here, this assertion is presented explicitly as a physical principle: `every finitely realizible physical system can be perfectly simulated by a universal model computing machine operating by finite means'. Classical physics and the universal Turing machine, because the former is continuous and the latter discrete, do not obey the principle, at least in the strong form above. A class of model computing machines that is the quantum generalization of the class of Turing machines is described, and it is shown that quantum theory and the `universal quantum computer' are compatible with the principle. Computing machines resembling the universal quantum computer could, in principle, be built and would have many remarkable properties not reproducible by any Turing machine. These do not include the computation of non-recursive functions, but they do include `quantum parallelism', a method by which certain probabilistic tasks can be performed faster by a universal quantum computer than by any classical restriction of it. The intuitive explanation of these properties places an intolerable strain on all interpretations of quantum theory other than Everett's. Some of the numerous connections between the quantum theory of computation and the rest of physics are explored. Quantum complexity theory allows a physically more reasonable definition of the `complexity' or `knowledge' in a physical system than does classical complexity theory."
455,95,1264,1,The nonsense of 'knowledge management',"Abstract  Examines critically the origins and basis of 'knowledge management', its components and its development as a field of consultancy practice. Problems in the distinction between 'knowledge' and 'information' are explored, as well as Polanyi's concept of 'tacit knowing'. The concept is examined in the journal literature, the Web sites of consultancy companies, and in the presentation of business schools. The conclusion is reached that 'knowledge management' is an umbrella term for a variety of organizational activities, none of which are concerned with the management of knowledge. Those activities that are not concerned with the management of information are concerned with the management of work practices, in the expectation that changes in such areas as communication practice will enable information sharing."
456,95,8464,1,Motivation and barriers to participation in virtual knowledge-sharing communities of practice,"This paper reports the results of a qualitative study of motivation and barriers to employee participation in virtual knowledge-sharing communities of practice at Caterpillar Inc., a Fortune 100, multinational corporation. The study indicates that, when employees view knowledge as a public good belonging to the whole organization, knowledge flows easily. However, even when individuals give the highest priority to the interests of the organization and of their community, they tend to shy away from contributing knowledge for a variety of reasons. Specifically, employees hesitate to contribute out of fear of criticism, or of misleading the community members (not being sure that their contributions are important, or completely accurate, or relevant to a specific discussion). To remove the identified barriers, there is a need for developing various types of trust, ranging from the knowledge-based to the institution-based trust. Future research directions and implications for KM practitioners are formulated."
457,96,1455,1,Aspect-Oriented Programming,"<E6> </E6>We have found many <SPAN style='background:silver;'>programming</SPAN> problems for which neither procedural nor object-<SPAN style='background:silver;'>oriented</SPAN> <SPAN style='background:silver;'>programming</SPAN> techniques are sufficient to clearly capture some of the important design decisions the program must implement. This forces the implementation of those design decisions to be scattered throughout the code, resulting in &ldquo;tangled&rdquo; code that is excessively difficult to develop and maintain. We present an analysis of why certain design decisions have been so difficult to clearly capture in actual code. We call the properties these decisions address <SMALL> <SPAN style='background:silver;'>aspect</SPAN>s</SMALL>, and show that the reason they have been hard to capture is that they <SMALL> cross-cut</SMALL> the system's basic functionality. We present the basis for a new <SPAN style='background:silver;'>programming</SPAN> technique, called <SPAN style='background:silver;'>aspect</SPAN>-<SPAN style='background:silver;'>oriented</SPAN> <SPAN style='background:silver;'>programming</SPAN>, that makes it possible to clearly express programs involving such <SPAN style='background:silver;'>aspect</SPAN>s, including appropriate isolation, composition and reuse of the <SPAN style='background:silver;'>aspect</SPAN> code. The discussion is rooted in systems we have built using <SPAN style='background:silver;'>aspect</SPAN>-<SPAN style='background:silver;'>oriented</SPAN> <SPAN style='background:silver;'>programming</SPAN>."
458,96,2487,1,Software Architecture,"Building Object-Oriented Frameworks AIXpert February 95 Part 1 By Deborah Adair By using frameworks1, Taligent(R), Inc.--an independent joint venture of Apple(R) Computer, IBM, andHewlett-Packard--is realizing the full promise of object technology. A framework defines the behavior of a collection of objects, providing an innovative way to reuse software designs and code. Part 1 of thistwo-part series describes different types of frameworks and how they are used and addresses organizational concerns that impact framework development. Part 2 will outline a process foridentifying and designing frameworks.  This article presents a process for developing frameworks and highlights four important guidelines:  Derive frameworks from existing problems and solutions Develop small, focused frameworks Build frameworks using an iterative process driven by client2 participation and prototyping Treat frameworks as products by providing documentation and support, and by planning fordistribution and maintenance  While the process and techniques discussed are ideal for developing in the Taligent CommonPoint(TM)application system, they can also be applied to other object-oriented programming projects. This article is intended primarily for software developers and designers in commercial, corporate, andhigher education software development organizations. However, hardware designers, strategic technology planners, and technical managers might also find it useful.  Understanding Frameworks  A framework captures the programming expertise necessary to solve a particular class of problems.Programmers purchase or reuse frameworks to obtain such problem-solving expertise without having to develop it independently.  Framework Development Developing a framework differs from developing a stand-alone application. A successful frameworksolves problems that appear different from the problem that justified its creation. The problem-solving expertise must be captured so that it is independent of both the original problem and the future solutionsin which it is used; however, each program that uses the framework should appear to be the one for which it was designed. Developers must clearly identify the class of problem that a framework addresses. For your clients toadapt the framework to new problems, they must understand the solution the framework provides and how to incorporate it into their programs. Because others have to understand how to use yourframeworks, it is critical that you follow good software design practices.  Describing Frameworks While clear differences exist between class libraries and frameworks, some libraries exhibitframework-like behavior, and some frameworks can be used like class libraries. As shown in Figure 1, this can be viewed as a continuum, with traditional class libraries at one end and sophisticatedframeworks at the other.  Figure 1. Library framework continuum Frameworks can be characterized by the problem domains that they address, as well as by their internalstructure. Framework Domains The problem domain that a framework addresses can encompass application, domain, or supportfunctions.  Application frameworks encapsulate expertise applicable to many programs. These frameworksencompass a horizontal slice of functionality that can be applied across client domains. Current commercial Graphical User Interface (GUI) application frameworks, which support the standardfunctions required by all GUI applications, are one type of application framework. (The Apple MacApp(TM) system and Borland's OWL(TM) system are two such frameworks.) Domain frameworks encapsulate expertise in a particular problem domain. These frameworksencompass a vertical slice of functionality for a specific client domain. Examples of domain frameworks include a control systems framework for developing manufacturing controlapplications, a securities trading framework, a multimedia framework, or data access framework.  Support frameworks provide system-level services, such as file access, distributed computing  support, and device drivers. Application developers typically use support frameworks directly orafter modification by systems providers. Support frameworks can be customized, such as developing a new file system or device driver. The Taligent CommonPoint application frameworks extend across all three categories. Similarly, youcan create frameworks that capture your own problem-domain expertise. Whether you are developing custom applications in a corporate setting or developing a suite of commercial applications as a softwarevendor, building and using frameworks can increase productivity. The frameworks you build will usually be domain or application frameworks. Framework Structures Identifying the high-level structure of a framework makes it easier to describe the behavior of theframework and provides a starting point for designing framework interactions. For example, some frameworks are manager-driven--a single controlling function triggers most framework actions. You callthe controlling function to start the framework, and the framework creates the necessary objects and calls the appropriate functions to perform a specific task. An application framework generally uses a manager object to take input events from the user anddistribute them to the other objects in the framework.  Another categorization, based on how a framework is used, is whether you derive new classes orinstantiate and combine existing classes.  Architecture-driven frameworks rely on inheritance for customization. Clients customize the behavior ofthe framework by deriving new classes from the framework and overriding member functions. Data-driven frameworks rely primarily on object composition for customization. Clients customize thebehavior of the framework by using different combinations of objects. The objects that clients pass into the framework affect what the framework does, but the framework defines how the objects can becombined.  Frameworks that are heavily architecture-driven can be difficult to use because they require asubstantial amount of code to be written. Purely data-driven frameworks are generally easy to use, but they can be limiting. One approach for building frameworks that are both easy to use and extensible is to provide anarchitecture-driven base with a data-driven layer. Most frameworks provide ways for clients to use the built-in functionality and also modify that functionality. Typically, clients use a framework's built-infunctionality by instantiating classes and calling their member functions. Clients extend and modify a framework's functionality by deriving new classes and overriding member functions. Figure 2 (text) shows a simple example of using and extending a framework. Suppose you have a simpleframework that supports, among other things, drawing shapes. Clients might use it to draw a check box or extend it to draw other types of boxes.  Maximizing Framework Benefits  The key to maximizing the benefits of writing frameworks is to get as many developers as possibleusing them. Frameworks are a long-term investment; the benefits gained from developing frameworks are not necessarily immediate because framework designers need more time to create a framework thana procedural library, and clients need more time to learn a framework than a procedural library.  Managing Dependencies Projects can often be divided into several separate frameworks and assigned to small teams. If it takesmore than three or four programmers to produce a framework, it can probably be split into a set of smaller frameworks. Teams of two to four are usually more effective than teams of one, unless thesingle programmer is both an experienced framework developer and a domain expert.  Working with several small teams has its challenges:  Programmers are focused on one aspect of a large project and might not understand all theinterrelationships and client implications.  Architectural consistency must be maintained across teams. Dependencies between frameworks can create bottlenecks. There are several ways to alleviate these problems:  Appoint a project architect who maintains the ""big picture"" and ensures that the frameworksultimately work together.  Follow standard design and coding guidelines. Decouple the frameworks by isolating the dependencies in intermediary classes. Often when one framework requires the services of another, the connection can be implemented throughan interface or server object. Then, only one object is dependent on the other framework. Until the other framework can support the necessary operations, the intermediary class provides stub code that allowsthe rest of the framework to be tested. Loosely coupled frameworks are generally more flexible from the client's perspective. This approach can also be used to isolate platform-dependent code or code that accesses a specificapplication's framework. To use different platforms or application frameworks, only one piece of the framework needs to be changed. Intermediaries can also be used to access legacy data or non-frameworkservices.  Delivering Your Framework as a Product Even if your framework will only be used internally, it must be treated like a product. Documentingyour framework is an important step in the development cycle, but you must also plan how the finished product will be distributed and supported. To use your frameworks, other developers need to know that they exist and how to access them. Unless  a process is established for providing and distributing frameworks, it is difficult for other developers touse them. Reuse does not just happen; your organization must actively support and encourage it. One way to do this is to reward programmers for writing and distributing frameworks that others can use.Even more important, reward the programmers who use them.  Ideally, all frameworks are kept in a central repository, and a repository manager is responsible fornotifying clients about new frameworks and updates to old ones. With a large enough repository, selecting the appropriate frameworks becomes an integral part of developing new program solutions. To realize the benefits they can provide, your organization must commit resources to supportframeworks. You must be able to assist your clients and respond to their problems and requests.  Over the lifetime of a framework, the cost of supporting it actually becomes a benefit--the support costof one framework with three dependent applications will be less than the cost of supporting three independent applications with duplicate code. The more applications that use a framework, the larger thesavings. Over the long term, using frameworks can reduce support and maintenance costs.  Early in its life, a framework will probably require routine maintenance to fix bugs and respond to clientrequests. Over time, even the best framework will probably need to be updated to support changing requirements. Part 2, which will appear in the May 1995 issue, will focus on developing frameworks. For the latestinformation on Taligent, explore the World Wide Web page http://www.taligent.com.  Recommended Reading The following references include standard object-oriented design references, new publications, andarticles about frameworks from a variety of periodicals.  Beck, Kent. ""Patterns and Software Development."" Dr. Dobb's Journal 19, no. 2 (February 1994):18. Beck, Kent and Johnson, Ralph. ""Patterns Generate Architectures,"" European Conference onObject-Oriented Programming (1994). Birrer, Andreas and Eggenschwiler, Thomas. ""Frameworks in the Financial Engineering Domain:An Experience Report,"" European Conference on Object-Oriented Programming (1993): 21-35. Booch, Grady. ""Designing an Application Framework,"" Dr. Dobb's Journal 19, no. 2 (February1994): 24. Booch, Grady. Object-Oriented Analysis and Design With Applications. Redwood City, CA:Benjamin/Cummings, 1994. Campbell, Roy; Islam, Nayeem; Raila, David; and Madany, Peter. ""Designing and Implementing""CHOICES"": an Object-Oriented System in C++,"" Communications of the ACM 36, no. 9 (September 1993): 117.  Coad, Peter. ""Object-Oriented Patterns,"" Communications of the ACM 35, no. 9 (1992): 152. Eggenschwiler, Thomas and Gamma, Erich. ""ET++ SwapsManager: Using Object Technology inthe Financial Engineering Domain,"" OOPSLA '92 Conference Proceedings, ACM SIG Notices 27, no. 10 (1992): 166. Frameworks: The Journal of Software Development Using Object Technology. SoftwareFrameworks Association.  Gamma, Erich; Helm, Richard; Johnson, Ralph; and Vlissades, John. ""Design Patterns:Abstraction and Reuse of Object-Oriented Design,"" European Conference on Object-Oriented Programming (1993): 406-431. Gamma, Erich; Helm, Richard; Johnson, Ralph; and Vlissades, John. Design Patterns: Elements ofReusable Object-Oriented Software. Addison-Wesley, Forthcoming.  Goldstein, Neal, and Jeff Alger. Developing Object-Oriented Software for the Macintosh.Reading, MA: Addison-Wesley, 1992. Johnson, Ralph. ""How to Design Frameworks,"" OOPSLA '93 Tutorial Notes, 1993. Mallory, Jim. ""TI Software Speeds Semiconductor Production,"" Newsbytes NEW07200011 (July1993).  Nelson, Carl. ""A Forum for Fitting the Task,"" IEEE Computer 27, no. 3 (March 1994): 104. ""Semiconductor Industry: New Advanced C.M. software from Texas Instruments Expected toRevolutionize Semiconductor Manufacturing,"" EDGE: Work-Group Computing Report 4, no.166 (July 1993): 22. Shelton, Robert. ""The Distributed Enterprise,"" The Distributed Computing Monitor 8, no. 10(October 1993): 3.  Stroustrup, Bjarne. The C++ Programming Language. 2d ed. Reading, MA: Addison-Wesley,1991. Taligent's Guide to Designing Programs: Well-Mannered Object-Oriented Design in C++.Addison-Wesley, 1994. Wilson, Dave. ""Designing Object-Oriented Frameworks."" Personal Concepts, Palo Alto, CA 1994. Wong, William. Plug & Play Programming, An Object-Oriented Construction Kit. M&T Books,1993.  Taligent White Papers A Study of America's Top Corporate Innovators, Taligent, Inc., 1992. Lessons Learned from Early Adopters of Object Technology, Taligent, Inc., 1993.Driving Innovation with Technology: The Intelligent Use of Objects, Taligent, Inc., 1993.  Leveraging Object-Oriented Frameworks, Taligent, Inc., 1993. Designing Sucessful Frameworks Taligent Standards  Deborah Adair, Taligent, Inc., 10201 North De Anza Boulevard, Cupertino, CA95014-2233. 1-800-288-5545. Ms. Adair is a staff technical writer in the Taligent Technical Communications Department. She received her BS in Scientific and Technical Communications from theUniversity of Washington.  1 A framework is a set of prefabricated software building blocks that programmers can use, extend, orcustomize for specific computing solutions. Taligent has designed frameworks for system software functions, such as networking, multimedia, and database access. With frameworks, software developersdo not have to start from scratch each time they write an application. Frameworks are built from a collection of objects, so both the design and code of a framework can be reused. 2 This article is written from the viewpoint of framework developers. We use the term ""client"" to refer todevelopers who might use a framework that you developed."
459,96,4787,1,An Analysis of the Requirements Traceability Problem,"Investigates and discusses the underlying nature of the requirements traceability problem. Our work is based on empirical studies, involving over 100 practitioners, and an evaluation of current support. We introduce the distinction between pre-requirements specification (pre-RS) traceability and post-requirements specification (post-RS) traceability to demonstrate why an all-encompassing solution to the problem is unlikely, and to provide a framework through which to understand its multifaceted nature. We report how the majority of the problems attributed to poor requirements traceability are due to inadequate pre-RS traceability and show the fundamental need for improvements. We present an analysis of the main barriers confronting such improvements in practice, identify relevant areas in which advances have been (or can be) made, and make recommendations for research"
460,97,18,1,The anatomy of a large-scale hypertextual Web search engine,"Abstract In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http:// google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical largescale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want."
461,97,1152,1,"{Agent Theories, Architectures, and Languages: A Survey}","The concept of an agent has recently become important in Artificial Intelligence (AI), and its relatively youthful subfield, Distributed AI (DAI). Our aim in this paper is to point the reader at what we perceive to be the most important theoretical and practical issues associated with the design and construction of intelligent agents. For convenience, we divide the area into three themes (though as the reader will see, these divisions are at times somewhat arbitrary). Agent theory is concerned with the question of what an agent is, and the use of mathematical formalisms for representing and reasoning about the properties of agents. Agent architectures can be thought of as software engineering models of agents; researchers in this area are primarily concerned with the problem of constructing software or hardware systems that will satisfy the properties specified by agent theorists. Finally, agent languages are software systems for programming and experimenting with agents; these languages typically embody principles proposed by theorists. The paper is not intended to serve as a tutorial introduction to all the issues mentioned; we hope instead simply to identify the key issues, and point to work that elaborates on them. The paper closes with a detailed bibliography, and some bibliographical remarks. 1"
462,97,3974,1,Agent-based software engineering,"The technology of intelligent agents and multi-agent systems seems set to radically alter the way in which complex, distributed, open systems are conceptualized and implemented. The purpose of this paper is to consider the problem of building a multi-agent system as a software engineering enterprise. The article focuses on three issues: (i) how agents might be specified; (ii) how these specifications might be refined or otherwise transformed into efficient implementations; and (iii) how implemented agents and multi-agent systems might subsequently be verified, in order to show that they are correct with respect to their specifications. These issues are discussed with reference to a number of casestudies. The article concludes by setting out some issues and open problems for future research. 1 Introduction  Intelligent agents are ninety-nine percent computer science and one percent AI.  Oren Etzioni [12] Over its 40-year history, Artificial Intelligence (AI) has been subject to many and..."
463,97,6351,1,"Automated Negotiation: Prospects, Methods and Challenges","The remainder of this paper is structured as follows. Section 2 presents a generic framework for automated negotiation. This framework is then used to structure the subsequent discussion and analysis of the various negotiation techniques; section 3 deals with game theoretic techniques, section 4 with heuristic techniques, and section 5 with argumentation-based techniques. Finally, section 6 outlines some of the major challenges that need to be addressed before automated negotiation becomes pervasive."
464,97,14944,1,Cloud Computing and Grid Computing 360-Degree Compared,"Cloud Computing has become another buzzword after Web 2.0. However, there are dozens of different definitions for Cloud Computing and there seems to be no consensus on what a Cloud is. On the other hand, Cloud Computing is not a completely new concept; it has intricate connection to the relatively new but thirteen-year established Grid Computing paradigm, and other relevant technologies such as utility computing, cluster computing, and distributed systems in general. This paper strives to compare and contrast Cloud Computing with Grid Computing from various angles and give insights into the essential characteristics of both."
465,98,3617,1,Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data,"Machine learning was applied for the automated derivation of causal influences in cellular signaling networks. This derivation relied on the simultaneous measurement of multiple phosphorylated protein and phospholipid components in thousands of individual primary human immune system cells. Perturbing these cells with molecular interventions drove the ordering of connections between pathway components, wherein Bayesian network computational methods automatically elucidated most of the traditionally reported signaling relationships and predicted novel interpathway network causalities, which we verified experimentally. Reconstruction of network models from physiologically relevant primary single cells might be applied to understanding native-state tissue signaling biology, complex drug actions, and dysfunctional signaling in diseased cells."
466,98,10036,1,Thinking Is Social: Experiments with the Adaptive Culture Model,"Robert Axelrod's model of the spread of culture is extended to demonstrate that social interaction can function as an algorithm for optimizing cognition. Mental structures can be represented as strings of symbols that can be evaluated according to some criterion of goodness. Individuals interact with their neighbors, often resulting in patterns of similarities within and differences between regions of the population. Group-level phenomena, in turn, result in the optimization of individual structures. 10.1177/0022002798042001003"
467,99,2546,1,What a to-do: studies of task management towards the design of a personal task list manager,"This paper reports on the results of studies of task management to support the design of a task list manager. We examined the media used to record and organize to-dos and tracked how tasks are completed over time. Our work shows that, contrary to popular wisdom, people are not poor at prioritizing. Rather, they have well-honed strategies for tackling particular task management challenges. By illustrating what factors influence task completion and how representations function to support task management, we hope to provide a strong foundation for the design of a personal to-do list manager. We also present some preliminary efforts in this direction."
468,99,4447,1,Collaboration with lean media: How open-source software succeeds,"<p>Open-source software, usually created by volunteer programmers dispersed worldwide, now competes with that developed by software firms. This achievement is particularly impressive as open-source programmers rarely meet. They rely heavily on electronic media, which preclude the benefits of face-to-face contact that programmers enjoy within firms. In this paper, we describe findings that address this paradox based on observation, interviews and quantitative analyses of two open-source projects. The findings suggest that spontaneous work coordinated afterward is effective, rational organizational culture helps achieve agreement among members and communications media moderately support spontaneous work. These findings can imply a new model of dispersed collaboration.</p>"
469,100,248,1,Data clustering: a review,"Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify  cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval."
470,100,3389,1,"Wide Open Spaces: Wikis, Ready or Not","Remember when the Internet was about opening up access to information and breaking down the barriers between content creators and content consumers? Think back to when spam was just a meatlike substance. To those heady days when Timothy Leary was predicting that the PC would be the LSD of the nineties. Before the DMCA. Before eBay. Back when the Web was supposed to be a boundless Borgesian ""Library of Babel"" and not a global supermarket. The term wiki (derived from the Hawaiian word for ""quick"") is applied to a diverse set of systems, features, approaches, and projects. Even dedicated wikiheads engage in perpetual arguments about what constitutes true wikiness. But some fundamental principles (usually) apply."
471,100,5856,1,Semantic Wikipedia,"Wikipedia is the worldâs largest collaboratively edited source of encyclopaedic knowledge. But in spite of its utility, its contents are barely machine-interpretable. Structural knowledge, e. g. about how concepts are interrelated, can neither be formally stated nor automatically processed. Also the wealth of numerical data is only available as plain text and thus can not be processed by its actual meaning. We provide an extension to be integrated in Wikipedia, that allows the typing of links between articles and the specification of typed data inside the articles in an easy-to-use manner. Enabling even casual users to participate in the creation of an open semantic knowledge base, Wikipedia has the chance to become a resource of semantic statements, hitherto unknown regarding size, scope, openness, and internationalisation. These semantic enhancements bring toWikipedia benefits of todayâs semantic technologies: more specific ways of searching and browsing. Also, the RDF export, that gives direct access to the formalised knowledge, opens Wikipedia up to a wide range of external applications, that will be able to use it as a background knowledge base. In this paper, we present the design, implementation, and possible uses of this extension."
472,101,3446,1,"On clusterings - good, bad and spectral",We motivate and develop a new bicriteria measure for assessing the quality of a clustering which avoids the drawbacks of existing measures. A simple recursive heuristic has poly-logarithmic worst-case guarantees under the new measure. The main result of the paper is the analysis of a popular spectral algorithm. One variant of spectral clustering turns out to have eective worst-case guarantees; another nds a \good clustering if it exists.   Supported in part by NSF grant CCR-9820850.
473,101,5829,1,Principles of Artificial Intelligence,"{<p>A classic introduction to artificial intelligence intended to bridge the gap between theory and practice, <I>Principles of Artificial Intelligence</I> describes fundamental AI ideas that underlie applications such as natural language processing, automatic programming, robotics, machine vision, automatic theorem proving, and intelligent data retrieval.  Rather than focusing on the subject matter of the applications, the book is organized around general computational concepts involving the kinds of data structures used, the types of operations performed on the data structures, and the properties of the control strategies used.</p><br><br><p><I>Principles of Artificial Intelligence</I>evolved from the author's courses and seminars at Stanford University and University of Massachusetts, Amherst, and is suitable for text use in a senior or graduate AI course, or for individual study.}"
474,101,9572,1,Adaptive Task Allocation Inspired by a Model of Division of Labor in Social Insects,"Social insects provide us with a powerful metaphor to create decentralized systems of simple interacting, and often mobile, agents. The emergent collective intelligence of social insects---swarm intelligence---resides not in complex individual abilities but rather in networks of interactions that exist among individuals and between individuals and their environment. In particular, a recently proposed model of division of labor in a colony of primitively eusocial wasps, based on a simple reinforcement of response thresholds, can be transformed into a decentralized adaptive algorithm of task allocation. An application of such an algorithm is proposed in the context of a mail company, but virtually any type of flexible task allocation can be described within the same framework."
475,102,1404,1,The vision of autonomic computing,"A 2001 IBM manifesto observed that a looming software complexity crisis -caused by applications and environments that number into the tens of millions of lines of code - threatened to halt progress in computing. The manifesto noted the almost impossible difficulty of managing current and planned computing systems, which require integrating several heterogeneous environments into corporate-wide computing systems that extend into the Internet. Autonomic computing, perhaps the most attractive approach to solving this problem, creates systems that can manage themselves when given high-level objectives from administrators. Systems manage themselves according to an administrator's goals. New components integrate as effortlessly as a new cell establishes itself in the human body. These ideas are not science fiction, but elements of the grand challenge to create self-managing computing systems."
476,102,4446,1,Understanding the requirements for developing Open Source Software systems,"Presents an initial set of findings from an empirical study of social processes, technical system configurations, organisational contexts and interrelationships that give rise to open software. The focus is directed at understanding the requirements for open software development efforts, and how the development of these requirements differs from those traditional to software engineering and requirements engineering. Four open software development communities are described, examined and compared to help discover what these differences may be. Eight kinds of software informalisms are found to play a critical role in the elicitation, analysis, specification, validation and management of requirements for developing open software systems. Subsequently, understanding the roles these software informalisms take in a new formulation of the requirements development process for open source software is the focus of the study. This focus enables the consideration of a reformulation of the requirements engineering process and its associated artefacts, or (in)formalisms, to better account for the requirements for developing open source software systems"
477,102,9234,1,Describing software architecture with UML,"The presence of a solid architectural vision is a key discriminator in the success or failure of a software project. This tutorial examines what software architecture is and what it is not. It discusses and illustrates how to describe architecture through a set of design viewpoints and views and how to express these views in the UML [1], in the spirit of the new IEEE Standard: Recommended practice for architectural description [2]. The tutorial shows of how architectures drive the development process and how to capture architectural design patterns using the UML. It is illustrated by several widely applicable architectural patterns in different domain."
478,102,12895,1,The Art and Science of Software Architecture,"The past 20 years has seen significant investments in the theory and practice of software architecture. However, architectural deficiencies are frequently cited as a key factor in the shortcomings and failures that lead to unpredictable delivery of complex operational systems. Here, we consider the art and science of software architecture: we explore the current state of software architecture, identify key architectural trends and directions in academia and industry, and highlight some of the architectural research challenges which need to be addressed. The paper proposes an agenda of research activities to be carried out by a partnership between academia and industry. While challenges exist in many domains, for this paper we draw examples from one area of particular concern: safety-critical systems."
479,103,13437,1,Preferential behavior in online groups,"Online communities in the form of message boards, listservs, and newsgroups continue to represent a considerable amount of the social activity on the Internet. Every year thousands of groups flourish while others decline into relative obscurity; likewise, millions of members join a new community every year, some of whom will come to manage or moderate the conversation while others simply sit by the sidelines and observe. These processes of group formation, growth, and dissolution are central in social science, and in an online venue they have ramifications for the design and development of community software"
480,104,8066,1,"Combining evidence, biomedical literature and statistical dependence: new insights for functional annotation of gene sets.","BACKGROUND: Large-scale genomic studies based on transcriptome technologies provide clusters of genes that need to be functionally annotated. The Gene Ontology (GO) implements a controlled vocabulary organised into three hierarchies: cellular components, molecular functions and biological processes. This terminology allows a coherent and consistent description of the knowledge about gene functions. The GO terms related to genes come primarily from semi-automatic annotations made by trained biologists (annotation based on evidence) or text-mining of the published scientific literature (literature profiling). RESULTS: We report an original functional annotation method based on a combination of evidence and literature that overcomes the weaknesses and the limitations of each approach. It relies on the Gene Ontology Annotation database (GOA Human) and the PubGene biomedical literature index. We support these annotations with statistically associated GO terms and retrieve associative relations across the three GO hierarchies to emphasise the major pathways involved by a gene cluster. Both annotation methods and associative relations were quantitatively evaluated with a reference set of 7397 genes and a multi-cluster study of 14 clusters. We also validated the biological appropriateness of our hybrid method with the annotation of a single gene (cdc2) and that of a down-regulated cluster of 37 genes identified by a transcriptome study of an in vitro enterocyte differentiation model (CaCo-2 cells). CONCLUSION: The combination of both approaches is more informative than either separate approach: literature mining can enrich an annotation based only on evidence. Text-mining of the literature can also find valuable associated MEDLINE references that confirm the relevance of the annotation. Eventually, GO terms networks can be built with associative relations in order to highlight cooperative and competitive pathways and their connected molecular functions."
481,104,11093,1,Improving clinical practice using clinical decision support systems: a systematic review of trials to identify features critical to success,"Objective To identify features of clinical decision support systems critical for improving clinical practice. Design Systematic review of randomised controlled trials. Data sources Literature searches via Medline, CINAHL, and the Cochrane Controlled Trials Register up to 2003; and searches of reference lists of included studies and relevant reviews. Study selection Studies had to evaluate the ability of decision support systems to improve clinical practice. Data extraction Studies were assessed for statistically and clinically significant improvement in clinical practice and for the presence of 15 decision support system features whose importance had been repeatedly suggested in the literature. Results Seventy studies were included. Decision support systems significantly improved clinical practice in 68% of trials. Univariate analyses revealed that, for five of the system features, interventions possessing the feature were significantly more likely to improve clinical practice than interventions lacking the feature. Multiple logistic regression analysis identified four features as independent predictors of improved clinical practice: automatic provision of decision support as part of clinician workflow (P < 0.00001), provision of recommendations rather than just assessments (P = 0.0187), provision of decision support at the time and location of decision making (P = 0.0263), and computer based decision support (P = 0.0294). Of 32 systems possessing all four features, 30 (94%) significantly improved clinical practice. Furthermore, direct experimental justification was found for providing periodic performance feedback, sharing recommendations with patients, and requesting documentation of reasons for not following recommendations. Conclusions Several features were closely correlated with decision support systems' ability to improve patient care significantly. Clinicians and other stakeholders should implement clinical decision support systems that incorporate these features whenever feasible and appropriate."
482,105,791,1,"Neural coding of basic reward terms of animal learning theory, game theory, microeconomics and behavioural ecology.","Neurons in a small number of brain structures detect rewards and reward-predicting stimuli and are active during the expectation of predictable food and liquid rewards. These neurons code the reward information according to basic terms of various behavioural theories that seek to explain reward-directed learning, approach behaviour and decision-making. The involved brain structures include groups of dopamine neurons, the striatum including the nucleus accumbens, the orbitofrontal cortex and the amygdala. The reward information is fed to brain structures involved in decision-making and organisation of behaviour, such as the dorsolateral prefrontal cortex and possibly the parietal cortex. The neural coding of basic reward terms derived from formal theories puts the neurophysiological investigation of reward mechanisms on firm conceptual grounds and provides neural correlates for the function of rewards in learning, approach behaviour and decision-making."
483,105,2122,1,Internal models for motor control and trajectory planning,"A number of internal model concepts are now widespread in neuroscience and cognitive science. These concepts are supported by behavioral, neurophysiological, and imaging data; furthermore, these models have had their structures and functions revealed by such data. In particular, a specific theory on inverse dynamics model learning is directly supported by unit recordings from cerebellar Purkinje cells. Multiple paired forward inverse models describing how diverse objects and environments can be controlled and learned separately have recently been proposed. The âminimum variance modelâ is another major recent advance in the computational theory of motor control. This model integrates two furiously disputed approaches on trajectory planning, strongly suggesting that both kinematic and dynamic internal models are utilized in movement planning and control."
484,106,15317,1,New class of microRNA targets containing simultaneous 5â²-UTR and 3â²-UTR interaction sites,"MicroRNAs (miRNAs) are known to post-transcriptionally regulate target mRNAs through the 3'-UTR, which interacts mainly with the 5'-end of miRNA in animals. Limited knowledge of the mechanism of translation repression and the challenge of identifying real miRNA targets among the many predicted have hindered the use of miRNA in biomedical applications. Here we identify many endogenous motifs within human 5'-UTRs specific to the 3'-ends of miRNAs. The 3'-end of conserved miRNAs in particular have significant interaction sites in the human-enriched, less-conserved 5'-UTR miRNA motifs, while human-specific miRNAs have significant interaction sites only in the conserved 5'-UTR motifs, implying both miRNA and 5'-UTR are actively evolving in response to each other. Additionally, many miRNAs with their 3'-end interaction sites in the 5'-UTRs turn out to simultaneously contain 5'-end interaction sites in the 3'-UTRs. Based on these findings we demonstrate combinatory interactions between a single miRNA and both end-regions of an mRNA using model systems by experimentally validating miRNA functional dependency on both UTRs. We further show that genes exhibiting large-scale protein changes due to miRNA overexpression or deletion contain both UTR interaction sites predicted. We provide the predicted targets of this new miRNA target class, miBridge, as an efficient way to screen potential targets, especially for non-conserved miRNAs, since the target search space is reduced by an order of magnitude compared with considering the 3'-UTR alone. Efficacy is confirmed by showing SEC24D regulation with hsa-miR-605, a miRNA identified only in primate, opening the door to the study of non-conserved miRNAs. Finally, miRNAs (and associated proteins) involved in this new targeting class may prevent 40S ribosome scanning through the 5'-UTR and keep it from reaching the start-codon, preventing 60S association."
485,106,16777,1,TabSQL: a MySQL tool to facilitate mapping user data to public databases.,"BACKGROUND: With advances in high-throughput genomics and proteomics, it is challenging for biologists to deal with large data files and to map their data to annotations in public databases. RESULTS: We developed TabSQL, a MySQL-based application tool, for viewing, filtering and querying data files with large numbers of rows. TabSQL provides functions for downloading and installing table files from public databases including the Gene Ontology database (GO), the Ensembl databases, and genome databases from the UCSC genome bioinformatics site. Any other database that provides tab-delimited flat files can also be imported. The downloaded gene annotation tables can be queried together with users' data in TabSQL using either a graphic interface or command line. CONCLUSIONS: TabSQL allows queries across the user's data and public databases without programming. It is a convenient tool for biologists to annotate and enrich their data."
486,107,1043,1,"Named Graphs, Provenance and Trust","The Semantic Web consists of many {RDF} graphs nameable by {URIs.} This paper extends the syntax and semantics of {RDF} to cover such Named Graphs. This enables {RDF} statements that describe graphs, which is beneficial in many Semantic Web application areas. As a case study, we explore the application area of Semantic Web publishing: Named Graphs allow publishers to communicate assertional intent, and to sign their graphs; information consumers can evaluate specific graphs using task-specific trust policies, and act on information from those Named Graphs that they accept. Graphs are trusted depending on: their content; information about the graph; and the task the user is performing. The extension of {RDF} to Named Graphs provides a formally defined framework to be a foundation for the Semantic Web trust layer."
487,107,4739,1,Keyword Searching and Browsing in databases using BANKS,"With the growth of the Web, there has been a rapid increase in the number of users who need to access on- line databases without having a detailed knowledge of the schema or of query languages; even relatively simple query languages designed for non-experts are too complicated for them. We describe BANKS, a system which enables keyword-based search on relational databases, together with data and schema browsing. BANKS enables users to extract information in a simple manner without any knowl- edge of the schema or any need for writing complex queries. A user can get information by typing a few keywords, fol- lowing hyperlinks, and interacting with controls on the dis- played results. BANKS models tuples as nodes in a graph, connected by links induced by foreign key and other relationships. An- swers to a query are modeled as rooted trees connecting tu- ples that match individual keywords in the query. Answers are ranked using a notion of proximity coupled with a notion of prestige of nodes based on inlinks, similar to techniques developed for Web search. We present an efficient heuristic algorithm for finding and ranking query results."
488,107,11795,1,"Distributed Knowledge Representation on the Social Semantic Desktop: Named Graphs, Views and Roles in NRL","The vision of the Social Semantic Desktop defines a userâs personal information environment as a source and end-point of the Semantic Web: Knowledge workers comprehensively express their information and data with respect to their own conceptualizations. Semantic Web languages and protocols are used to formalize these conceptualizations and for coordinating local and global information access. From the way this vision is being pursued in the NEPOMUK project, we identified several requirements and research questions with respect to knowledge representation. In addition to the general question of the expressivity needed in such a scenario, two main challenges come into focus: i) How can we cope with the heterogeneity of knowledge models and ontologies, esp. multiple knowledge modules with potentially different interpretations? ii) How can we support the tailoring of ontologies towards different needs in various exploiting applications? In this paper, we present NRL, an approach to these two question that is based on named graphs for the modularization aspect and a view concept for the tailoring of ontologies. This view concept turned out to be of additional value, as it also provides a mechanism to impose different semantics on the same syntactical structure. We think that the elements of our approach are not only adequate for the semantic desktop scenario, but are also of importance as building blocks for the general Semantic Web."
489,108,486,1,Community structure in social and biological networks,"10.1073/pnas.122653799 A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well knownâa collaboration network and a food webâand find that it detects significant and informative community divisions in both cases."
490,108,1384,1,Graphical {F}isheye {V}iews,A fisheye camera lens is a very wide angle lens that magnifies nearby objects while shrinking distant objects. It is a valuable tool for seeing both local detail and global context simultaneously. This paper describes a system for viewing and browsing graphs using a software analog of a fisheye lens. We first show how to implement such a view using solely geometric transformations. We then describe a more general transformation that allows global information about the graph to affect the...
491,108,3079,1,Bunch: A Clustering Tool for the Recovery and Maintenance of Software System Structures,"Software systems are typically modified in order to extend or change their functionality, improve their performance, port them to different platforms, and so on. For developers, it is crucial to understand the structure of a system before attempting to modify it. The structure of a system, however, may not be apparent to new developers, because the design documentation is non-existent or, worse, inconsistent with the implementation. This problem could be alleviated if developers were somehow able to produce high-level system decomposition descriptions from the low-level structures present in the source code. We have developed a clustering tool called Bunch that creates a system decomposition automatically by treating clustering as an optimization problem. The paper describes the extensions made to Bunch in response to feedback we received from users. The most important extension, in terms of the quality of results and execution efficiency, is a feature that enables the integration of designer knowledge about the system structure into an otherwise fully automatic clustering process. We use a case study to show how our new features simplified the task of extracting the subsystem structure of a medium size program, while exposing an interesting design flaw in the process"
492,108,3473,1,"Understanding, building and using ontologies","I defend here the thesis of the independence between domain knowledge and problem-solving knowledge, arguing against the dominance of the so-called ââinteraction problemââ mentioned in a recent paper by Van Heijst, Schreiber and Wielinga to dispute the feasibility of a single domain ontology shared by a number of different applications. The main point is that reusability across multiple tasks or methods can and should be systematically pursued even when modelling knowledge related to a single task or method. Under this view, I discuss how the principles of formal ontology and ontological engineering can be used in the practice of knowledge engineering, focusing in particular on the interplay between general ontologies, method ontologies and application ontologies, and on the role of ontologies in the knowledge engineering process. I will then stress the role of domain analysis, often absent in current methodologies for the development of knowledge-based systems."
493,109,1029,1,Facts from TextâIs Text Mining Ready to Deliver?,"Biological databases offer access to formalized facts about many aspects of biology?genes and gene products, protein structure, metabolic pathways, diseases, organisms, and so on. These databases are becoming increasingly important to researchers. The information that populates databases is generated by research teams and is usually published in peer-reviewed journals. As part of the publication process, some authors deposit data into a database but, more often, it is extracted from the published literature and deposited into the databases by human curators, a painstaking process."
494,110,1116,1,The Tragedy of the Commons,"Hardin, professor of biology, University of California, Santa Barbara presented this as a presidential address to the Pacific Division of the American Association for the Advancement of Science at Utah State University, Logan, 25 June 1968. Garrett Hardin described the pathogenic effects of conscience when asking someone to desist exploiting the commons in the name of conscience. One is faced with a Batesonâs âdouble bindâ of being accused of being an irresponsible citizen for exploiting the commons or a simpleton who stands aside while everyone else exploits the commons."
495,110,7824,1,The Measurement of Selection on Correlated Characters,"Multivariate statistical methods are derived for measuring selection solely from observed changes in the distribution of phenotypic characters in a population within a generation. Selective effects are readily detectable in characters that do not change with age, such as meristic traits or adult characters in species with determinate growth. Ontogenetic characters, including allometric growth rates, can be analyzed in longitudinal studies where individuals are followed through time. Following an approach pioneered by Pearson (1903), this analysis helps to reveal the target(s) of selection, and to quantify its intensity, without identifying the selective agent(s). By accounting for indirect selection through correlated characters, separate forces of directional and stabilizing (or disruptive) selection acting directly on each character can be measured. These directional and stabilizing selection coefficients are respectively the parameters that describe the best linear and quadratic approximations to the selective surface of individual fitness as a function of the phenotypic characters. The theory is illustrated by estimating selective forces on morphological characters influencing survival in pentatomid bugs [Euschistus variolarius] and in house sparrows [Passer domesticus] during severe weather conditions."
496,111,10945,1,Quantifying social group evolution,"The rich set of interactions between individuals in society1, 2, 3, 4, 5, 6, 7 results in complex community structure, capturing highly connected circles of friends, families or professional cliques in a social network3, 7, 8, 9, 10. Thanks to frequent changes in the activity and communication patterns of individuals, the associated social and communication network is subject to constant evolution7, 11, 12, 13, 14, 15, 16. Our knowledge of the mechanisms governing the underlying community dynamics is limited, but is essential for a deeper understanding of the development and self-optimization of society as a whole17, 18, 19, 20, 21, 22. We have developed an algorithm based on clique percolation23, 24 that allows us to investigate the time dependence of overlapping communities on a large scale, and thus uncover basic relationships characterizing community evolution. Our focus is on networks capturing the collaboration between scientists and the calls between mobile phone users. We find that large groups persist for longer if they are capable of dynamically altering their membership, suggesting that an ability to change the group composition results in better adaptability. The behaviour of small groups displays the opposite tendencyâthe condition for stability is that their composition remains unchanged. We also show that knowledge of the time commitment of members to a given community can be used for estimating the communityâs lifetime. These findings offer insight into the fundamental differences between the dynamics of small groups and large institutions."
497,112,77,1,Information diffusion through blogspace,"We study the dynamics of information propagation in environments of low-overhead personal publishing, using a large collection of weblogs over time as our example domain. We characterize and model this collection at two levels. First, we present a macroscopic characterization of topic propagation through our corpus, formalizing the notion of long-running âchatterâ topics consisting recursively of âspikeâ topics generated by outside world events, or more rarely, by resonances within the community. Second, we present a microscopic characterization of propagation from individual to individual, drawing on the theory of infectious diseases to model the flow. We propose, validate, and employ an algorithm to induce the underlying propagation network from a sequence of posts, and report on the results."
498,112,218,1,"Real life, real users, and real needs: a study and analysis of user queries on the web",". We analyzed transaction logs containing 51,473 queries posed by 18,113 users of Excite, a major Internet search service. We provide data on: (i) sessions - changes in queries during a session, number of pages viewed, and use of relevance feedback, (ii) queries - the number of search terms, and the use of logic and modifiers, and (iii) terms - their rank/frequency distribution and the most highly used search terms. We then shift the focus of analysis from the query to the user to gain insight to the characteristic of the Web user. With these characteristics as a basis, we then conducted a failure analysis, identifying trends among user mistakes. We conclude with a summary of findings and a discussion of the implications of these finding.  INTRODUCTION  A panel session at the 1997 ACM Special Interest Group on Research Issues In Information Retrieval conference entitled &#034;Real Life Information Retrieval: Commercial Search Engines&#034; included representatives from several Internet search se..."
499,112,424,1,Constraining Theories of Embodied Cognition,Influences of perceptual and motor activity on evaluation have led to theories of embodied cognition suggesting that putatively complex judgments can be carried out using only perceptual and motor representations. We present an experiment that revisited a movement-compatibility effect in which people are faster to respond to positive words by pulling a lever than by pushing a lever and are faster to respond to negative words by pushing than by pulling. We demonstrate that the compatibility effect depends on people's representation of their selves in space rather than on their physical location. These data suggest that accounting for embodied phenomena requires understanding the complex interplay between perceptual and motor representations and people's representations of their selves in space.
500,112,882,1,"Conversations in the Blogosphere: An Analysis ""From the Bottom Up""","The ""blogosphere"" has been claimed to be a densely in- terconnected conversation, with bloggers linking to other bloggers, referring to them in their entries, and posting comments on each other's blogs. Most such characteriza- tions have privileged a subset of popular blogs, known as the 'A-list.' This study empirically investigates the extent to which, and in what patterns, blogs are interconnected, taking as its point of departure randomly-selected blogs. Quantitative social network analysis, visualization of link patterns, and qualitative analysis of references and comments in pairs of reciprocally-linked blogs show that A-list blogs are overrepresented and central in the network, although other groupings of blogs are more densely interconnected. At the same time, a majority of blogs link sparsely or not at all to other blogs in the sam- ple, suggesting that the blogosphere is partially intercon- nected and sporadically conversational."
501,112,1287,1,In pursuit of desktop evolution: User problems and practices with modern desktop systems,"This study deals with the problems users encounter in their daily work with computers and the typical practices that they employ. Sixteen daily computer users were interviewed about their habits and problems that they encountered during document classification and retrieval. For both these areas, we provide an overview of identified user practices and a citation-based analysis of the problems users encountered, including those related to the use of the screen real estate (the actual desktop). Two types of problems were identified: (1) Problems that concern the actual use of the system installed on the computer. (2) Problems that arise when people realise that they are using a system that does not allow for the desired work or organizational functions sought. We were able to show that skill continues to be an important factor with respect to the ease of using today's systems. We suggest the following necessary improvements for the evolution of personal information systems: A storage facility that represents the user's view of information; replacing pure technical file metadata with more user-friendly attributes; and introduction of annotations as a new information type."
502,112,1429,1,Supporting Trust in Virtual Communities,"At any given time, the stability of a community depends on the right balance of trust and distrust. Furthermore, we face information overload, increased uncertainty and risk taking as a prominent feature of modern living. As members of society, we cope with these complexities and uncertainties by relying trust, which is the basis of all social interactions. Although a small number of trust models have been proposed for the virtual medium, we find that they are largely impractical and artificial. In this paper we provide and discuss a trust model that is grounded in real-world social trust characteristics, and based on a reputation mechanism, or word-of-mouth. Our proposed model allows agents to decide which other agents&#039; opinions they trust more and allows agents to progressively tune their understanding of another agent&#039;s subjective recommendations. 1. Introduction  &#034;Trustworthiness, the capacity to commit oneself to fulfilling the legitimate expectations of others, is both the consti..."
503,112,2099,1,Use of innovative technologies on an e-learning course,"This paper examines how four innovative Internet technologies were incorporated into one course at The UK Open University. The technologies were: blogging, audio conferencing, instant messaging and Harvard's Rotisserie system. Each of the technologies is addressed, and details from the student evaluation are provided. The student feedback on all the technologies was positive. The role of the learning object based course design is examined and it is suggested that this approach facilitates the incorporation of innovative technologies into a course. The authors suggest that as students become increasingly accustomed to standard communication tools such as asynchronous bulletin boards, there will be a shift towards implementing a range of technologies, each offering particular affordances for different forms of communication."
504,112,3119,1,Fifteen Minutes of Fame: The Dynamics of Information Access on the Web,"While current studies on complex networks focus on systems that change relatively slowly in time, the structure of the most visited regions of the Web is altered at the timescale from hours to days. Here we investigate the dynamics of visitation of a major news portal, representing the prototype for such a rapidly evolving network. The nodes of the network can be classified into stable nodes, that form the time independent skeleton of the portal, and news documents. The visitation of the two node classes are markedly different, the skeleton acquiring visits at a constant rate, while a news document's visitation peaking after a few hours. We find that the visitation pattern of a news document decays as a power law, in contrast with the exponential prediction provided by simple models of site visitation. This is rooted in the inhomogeneous nature of the browsing pattern characterizing individual users: the time interval between consecutive visits by the same user to the site follows a power law distribution, in contrast with the exponential expected for Poisson processes. We show that the exponent characterizing the individual user's browsing patterns determines the power-law decay in a document's visitation. Finally, our results document the fleeting quality of news and events: while fifteen minutes of fame is still an exaggeration in the online media, we find that access to most news items significantly decays after 36 hours of posting."
505,112,3982,1,The Structure of Collaborative Tagging Systems,"Collaborative tagging describes the process by which many users add metadata in the form of keywords to shared content.  Recently, collaborative tagging has grown in popularity on the web, on sites that allow users to tag bookmarks, photographs and other content.  In this paper we analyze the structure of collaborative tagging systems as well as their dynamical aspects. Specifically, we discovered regularities in user activity, tag frequencies, kinds of tags used, bursts of popularity in bookmarking and a remarkable stability in the relative proportions of tags within a given url. We also present a dynamical model of collaborative tagging that predicts these stable patterns and relates them to imitation and shared knowledge."
506,112,4727,1,Personal knowledge publishing: fostering interdisciplinary communication,"We address this article in relation to blogging and personal knowledge publishing (PKP). Derived from blogging, personal knowledge publishing is a form of Web-based communication that lowers social and linguistic barriers, facilitating knowledge migration across disciplinary boundaries. The aggregate output of this practice could provide a promising basis for intelligent systems development. These individual-centered, community-discussion-support systems have emerged as an interesting complement to journals, mailing lists, and other media for interdisciplinary-minded individuals. PKP offers individual researchers a tool for building strong interdisciplinary research networks. These networks enable individuals to find relevant literature and experts outside their core research community."
507,112,5685,1,Current practice in measuring usability: Challenges to usability studies and research,"How to measure usability is an important question in HCI research and user interface evaluation. We review current practice in measuring usability by categorizing and discussing usability measures from 180 studies published in core HCI journals and proceedings. The discussion distinguish several problems with the measures, including whether they actually measure usability, if they cover usability broadly, how they are reasoned about, and if they meet recommendations on how to measure usability. In many studies, the choice of and reasoning about usability measures fall short of a valid and reliable account of usability as quality-in-use of the user interface being studied. Based on the review, we discuss challenges for studies of usability and for research into how to measure usability. The challenges are to distinguish and empirically compare subjective and objective measures of usability; to focus on developing and employing measures of learning and retention; to study long-term use and usability; to extend measures of satisfaction beyond post-use questionnaires; to validate and standardize the host of subjective satisfaction questionnaires used; to study correlations between usability measures as a means for validation; and to use both micro and macro tasks and corresponding measures of usability. In conclusion, we argue that increased attention to the problems identified and challenges discussed may strengthen studies of usability and usability research."
508,112,6826,1,Changing Places: Contexts of Awareness in Computing,"By allowing any social institution to structure activity in any place, wireless information services break down the traditional mapping between institutions and places. This phenomenon greatly complicates the analysis of context for purposes of designing context-aware computing systems. Context has a physical aspect, but most aspects of context will also be defined in institutional terms. This essay develops two conceptual frameworks for the analysis of context in mobile and ubiquitous computing. The first framework concerns the relation between architecture, practices, and institutions; it directs attention to the complex middle ground in which information services make use of whatever computational resources happen to be in the userâs physical surroundings. The second framework is called the capture model; it rationally reconstructs the traditional systems analysis methods, which reorganize work activities to enable a computer to capture the information it needs. Context-aware computing devices that depart from the capture model face a difficult set of design tradeoffs."
509,112,8015,1,Evaluating interfaces for privacy policy rule authoring,"Privacy policy rules are often written in organizations by a team of people in different roles. Currently, people in these roles have no technological tools to guide the creation of clear and implementable high-quality privacy policy rules. High-quality privacy rules can be the basis for verifiable automated privacy access decisions. An empirical study was conducted with 36 users who were novices in privacy policy authoring to evaluate the quality of rules created and user satisfaction with two experimental privacy authoring tools and a control condition. Results show that users presented with scenarios were able to author significantly higher quality rules using either the natural language with a privacy rule guide tool or a structured list tool as compared to an unguided natural language control condition. The significant differences in quality were found in both user self-ratings of rule quality and objective quality scores. Users ranked the two experimental tools significantly higher than the control condition. Implications of the research and future research directions are discussed."
510,112,11323,1,New learning design in distance education: The impact on student perception and motivation,"Many forms of e-learning (such as online courses with authentic tasks and computer-supported collaborative learning) have become important in distance education. Very often, such e-learning courses or tasks are set up following constructivist design principles. Often, this leads to learning environments with authentic problems in ill-structured tasks that are supposed to motivate students. However, constructivist design principles are difficult to implement because developers must be able to predict how students perceive the tasks and whether the tasks motivate the students. The research in this article queries some of the assumed effects. It presents a study that provides increased insight into the actual perception of electronic authentic learning tasks. The main questions are how students learn in such e-learning environments with &ldquo;virtual&rdquo; reality and authentic problems and how they perceive them. To answer these questions, in two e-learning programs developed at the Open University of the Netherlands (OUNL) designers&rsquo; expectations were contrasted with student perceptions. The results show a gap between the two, for students experience much less authenticity than developers assume."
511,113,5692,1,Adaptive evolution of bacterial metabolic networks by horizontal gene transfer,"Numerous studies have considered the emergence of metabolic pathways1, but the modes of recent evolution of metabolic networks are poorly understood. Here, we integrate comparative genomics with flux balance analysis to examine (i) the contribution of different genetic mechanisms to network growth in bacteria, (ii) the selective forces driving network evolution and (iii) the integration of new nodes into the network. Most changes to the metabolic network of Escherichia coli in the past 100 million years are due to horizontal gene transfer, with little contribution from gene duplicates. Networks grow by acquiring genes involved in the transport and catalysis of external nutrients, driven by adaptations to changing environments. Accordingly, horizontally transferred genes are integrated at the periphery of the network, whereas central parts remain evolutionarily stable. Genes encoding physiologically coupled reactions are often transferred together, frequently in operons. Thus, bacterial metabolic networks evolve by direct uptake of peripheral reactions in response to changed environments."
512,113,15578,1,Fast statistical alignment.,"We describe a new program for the alignment of multiple biological sequences that is both statistically motivated and fast enough for problem sizes that arise in practice. Our Fast Statistical Alignment program is based on pair hidden Markov models which approximate an insertion/deletion process on a tree and uses a sequence annealing algorithm to combine the posterior probabilities estimated from these models into a multiple alignment. FSA uses its explicit statistical model to produce multiple alignments which are accompanied by estimates of the alignment accuracy and uncertainty for every column and character of the alignment--previously available only with alignment programs which use computationally-expensive Markov Chain Monte Carlo approaches--yet can align thousands of long sequences. Moreover, FSA utilizes an unsupervised query-specific learning procedure for parameter estimation which leads to improved accuracy on benchmark reference alignments in comparison to existing programs. The centroid alignment approach taken by FSA, in combination with its learning procedure, drastically reduces the amount of false-positive alignment on biological data in comparison to that given by other methods. The FSA program and a companion visualization tool for exploring uncertainty in alignments can be used via a web interface at http://orangutan.math.berkeley.edu/fsa/, and the source code is available at http://fsa.sourceforge.net/."
513,114,1199,1,Inducing features of random fields,"We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classification in natural language processing."
514,114,2238,1,{SIMPLI}city: {S}emantics-sensitive Integrated Matching for Picture Libraries,"AbstractÃThe need for efficient content-based image retrieval has increased tremendously in many application areas such as biomedicine, military, commerce, education, and Web image classification and searching. We present here SIMPLIcity (Semanticssensitive Integrated Matching for Picture LIbraries), an image retrieval system, which uses semantics classification methods, a wavelet-based approach for feature extraction, and integrated region matching based upon image segmentation. As in other regionbased retrieval systems, an image is represented by a set of regions, roughly corresponding to objects, which are characterized by color, texture, shape, and location. The system classifies images into semantic categories, such as textured-nontextured, graphphotograph. Potentially, the categorization enhances retrieval by permitting semantically-adaptive searching methods and narrowing down the searching range in a database. A measure for the overall similarity between images is developed using a region-matching scheme that integrates properties of all the regions in the images. Compared with retrieval based on individual regions, the overall similarity approach 1) reduces the adverse effect of inaccurate segmentation, 2) helps to clarify the semantics of a particular region, and 3) enables a simple querying interface for region-based image retrieval systems. The application of SIMPLIcity to several databases, including a database of about 200,000 general-purpose images, has demonstrated that our system performs significantly better and faster than existing ones. The system is fairly robust to image alterations. Index TermsÃContent-based image retrieval, image classification, image segmentation, integrated region matching, clustering, robustness. 1"
515,114,6428,1,"The Bayesian Image Retrieval System, {P}ic{H}unter: Theory, Implementation and Psychophysical Experiments","This paper presents the theory, design principles, implementation, and performance results of PicHunter, a prototype content-based image retrieval (CBII) system that has been developed over the past three years. In addition, this document presents the rationale, design, and results of psychophysical experiments that were conducted to address some key issues that arose during PicHunter&#039;s development. The PicHunter project makes four primary contributions to research on content-based image retrieval. First, PicHunter represents a simple instance of a general Bayesian framework we describe for using relevance feedback to direct a search. With an explicit model of what users would do, given what target image they want, PicHunter uses Bayes&#039;s rule to predict what is the target they want, given their actions. This is done via a probability distribution over possible image targets, rather than by refining a query. Second, an entropy-minimizing display algorithm is described that attempts to maximize the information obtained from a user at each iteration of the search. Third, PicHunter makes use of hidden annotation rather than a possibly inac- curate/inconsistent annotation structure that the user must learn and make queries in. Finally, PicHunter introduces two experimental paradigms to quantitatively evaluate the performance of the system, and psychophysical experiments are presented that support the theoretical claims."
516,114,8912,1,Interactive graph cuts for optimal boundary &amp; region segmentation of objects in N-D images,In this paper we describe a new technique for general purpose interactive segmentation of N-dimensional images. The user marks certain pixels as &ldquo;object&rdquo; or &ldquo;background&rdquo; to provide hard constraints for segmentation. Additional soft constraints incorporate both boundary and region information. Graph cuts are used to find the globally optimal segmentation of the N-dimensional image. The obtained solution gives the best balance of boundary and region properties among all segmentations satisfying the constraints. The topology of our segmentation is unrestricted and both &ldquo;object&rdquo; and &ldquo;background&rdquo; segments may consist of several isolated parts. Some experimental results are presented in the context of photo/video editing and medical image segmentation. We also demonstrate an interesting Gestalt example. A fast implementation of our segmentation method is possible via a new max-flow algorithm
517,114,10217,1,Real-time computerized annotation of pictures,"Developing effective methods for automated annotation of digital pictures continues to challenge computer scientists. The capability of annotating pictures by computers can lead to breakthroughs in a wide range of applications, including Web image search, online picture-sharing communities, and scientific experiments. In this work, the authors developed new optimization and estimation techniques to address two fundamental problems in machine learning. These new techniques serve as the basis for the Automatic Linguistic Indexing of Pictures - Real Time (ALIPR) system of fully automatic and high speed annotation for online pictures. In particular, the D2-clustering method, in the same spirit as k-means for vectors, is developed to group objects represented by bags of weighted vectors. Moreover, a generalized mixture modeling technique (kernel smoothing as a special case) for non-vector data is developed using the novel concept of Hypothetical Local Mapping (HLM). ALIPR has been tested by thousands of pictures from an Internet photo-sharing site, unrelated to the source of those pictures used in the training process. Its performance has also been studied at an online demo site where arbitrary users provide pictures of their choices and indicate the correctness of each annotation word. The experimental results show that a single computer processor can suggest annotation terms in real-time and with good accuracy."
518,115,1398,1,Usability analysis of visual programming environments: a `cognitive dimensions' framework,"The cognitive dimensions framework is a broad-brush evaluation technique for interactive devices and for non-interactive notations. It sets out a small vocabulary of terms designed to capture the cognitively-relevant aspects of structure, and shows how they can be traded off against each other. The purpose of this paper is to propose the framework as an evaluation technique for visual programming environments. We apply it to two commercially-available dataflow languages (with further examples from other systems) and conclude that it is effective and insightful; other HCI-based evaluation techniques focus on different aspects and would make good complements. Insofar as the examples we used are representative, current VPLs are successful in achieving a good &#x2018;closeness of match&#x2019;, but designers need to consider the &#x2018;viscosity &#x2019; (resistance to local change) and the &#x2018;secondary notation&#x2019; (possibility of conveying extra meaning by choice of layout, colour, etc.)."
519,115,6550,1,Video games in education,"Computer and video games are a maturing medium and industry and have caught the attention of scholars across a variety of disciplines. By and large, computer and video games have been ignored by educators. When educators have discussed games, they have focused on the social consequences of game play, ignoring important educational potentials of gaming. This paper examines the history of games in educational research, and argues that the cognitive potential of games have been largely ignored by educators. Contemporary developments in gaming, particularly interactive stories, digital authoring tools, and collaborative worlds, suggest powerful new opportunities for educational media."
520,116,295,1,Reinforcement Learning: A Survey,"This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ``reinforcement.'' The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning."
521,116,922,1,Foundations of Statistical Natural Language Processing,"{""Statistical natural-language processing is, in my estimation, one of the most fast-moving and exciting areas of computer science these days. Anyone who wants to learn this field would be well advised to get this book. For that matter, the same goes for anyone who is already in the field. I know that it is going to be one of the most well-thumbed books on my bookshelf."" -- Eugene Charniak, Department of Computer Science, Brown University  <P>Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.  <P>More on this book}"
522,116,1547,1,{Structure of growing networks with preferential linking},"The model of growing networks with the preferential attachment of new links is generalized to include initial attractiveness of sites. We find the exact form of the stationary distribution of the number of incoming links of sites in the limit of long times,  P ( q ) , and the long-time limit of the average connectivity  q Ì        ( s , t )  of a site  s  at time  t  (one site is added per unit of time). At long times,  P ( q ) â¼ q - Î³  at  q â â  and  q Ì        ( s , t ) â¼ ( s / t ) - Î²  at  s / t â0 , where the exponent  Î³  varies from  2  to  â  depending on the initial attractiveness of sites. We show that the relation  Î² ( Î³ -1)  Â =  Â 1  between the exponents is universal."
523,116,2122,1,Internal models for motor control and trajectory planning,"A number of internal model concepts are now widespread in neuroscience and cognitive science. These concepts are supported by behavioral, neurophysiological, and imaging data; furthermore, these models have had their structures and functions revealed by such data. In particular, a specific theory on inverse dynamics model learning is directly supported by unit recordings from cerebellar Purkinje cells. Multiple paired forward inverse models describing how diverse objects and environments can be controlled and learned separately have recently been proposed. The âminimum variance modelâ is another major recent advance in the computational theory of motor control. This model integrates two furiously disputed approaches on trajectory planning, strongly suggesting that both kinematic and dynamic internal models are utilized in movement planning and control."
524,116,3322,1,Evolving Neural Networks through Augmenting Topologies,"An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT) that outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution."
525,116,4137,1,Computational models of collective behavior,"Computational models of human collective behavior offer promise in providing quantitative and empirically verifiable accounts of how individual decisions lead to the emergence of group-level organizations. Agent-based models (ABMs) describe interactions among individual agents and their environment, and provide a process-oriented alternative to descriptive mathematical models. Recent ABMs provide compelling accounts of group pattern formation, contagion and cooperation, and can be used to predict, manipulate and improve upon collective behavior. ABMs overcome an assumption that underlies much of cognitive science - that the individual is the crucial unit of cognition. The alternative advocated here is that individuals participate in collective organizations that they might not understand or even perceive, and that these organizations affect and are affected by individual behavior."
526,116,6055,1,"Learning, Bottlenecks and the Evolution of Recursive Syntax","{Human language is a unique natural communication system for two reasons.1 Firstly, the mapping from meanings to signals in language has structural properties that are not found in any other animal's communication systems. In particular, syntax gives us the ability to produce an infinite range of expressions through the dual tools of compositionality and recursion. Compositionality is defined here as the property whereby â the meaning of an expression is a monotonic function of the meaning of its parts and the way they are put together.â (Cann 1993:4) Recursion is â the phenomenon by which a constituent of a sen- tence dominates another instance of the same syntactic category . . . recursion is the principle reason that the number of sentences in a natural language is normally taken to be infiniteâ (Trask 1993:229-230). Together with recursion' compositionality is the reason that this infi- nite set of sentences can be used to express different meanings. Secondly, at least some of the content of this mapping is learned by children through observation of others' use of language. This seems not to be true of most, maybe all, of animal communication (see review in Oliphant, this volume). In this chapter I formally investigate the interaction of these two unique properties of human language: the way it is learned and its syntactic structure.}"
527,116,7201,1,Finding Structure in Time,"Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by [Jordan, 1986] which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
528,116,9301,1,Symmetry and Self-Organization in Complex Systems,"We show that, in contrast to classical random graph models, many real-world complex systems -- including a variety of biological regulatory networks and technological networks such as the internet -- spontaneously self-organize to a richly symmetric state. We consider the organizational origins of symmetry and find that growth with preferential attachment confers symmetry in highly branched networks. We deconstruct the automorphism group of some real-world networks and find that some, but not all, real-world symmetry can be accounted for by branching. We also uncover an intriguing correspondence between the size of the automorphism group of growing random trees and the random Fibonacci sequences."
529,116,10399,1,{Language as Shaped by the Brain},"{It is widely assumed that human learning and the structure of human languages are intimately related. This relationship is frequently suggested to be rooted in a language-specific biological endowment, which encodes universal, but arbitrary, principles of language structure (a universal grammar or UG). How might such a UG have evolved? We argue that UG could not have arisen either by biological adaptation or non- adaptationist genetic processes. The resulting puzzle concerning the origin of UG we call the logical problem of language evolution. Because the processes of language change are much more rapid than processes of genetic change, language constitutes a â moving targetâ both over time and across different human populations, and hence cannot provide a stable environment to which UG genes could have adapted. We conclude that a biologically determined UG is not evolutionarily viable. Instead, the original motivation for UGâthe mesh between learners and languagesâarises because language has been shaped to fit the human brain, rather than vice versa. Following Darwin, we view language itself as a complex and interdependent â organism,â which evolves under selectional pressures from human learning and processing mechanisms. That is, languages are themselves undergoing severe selectional pressure from each generation of language users and learners. This suggests that apparently arbitrary aspects of linguistic structure may result from general learning and processing biases, independent of language. We illustrate how this framework can integrate evidence from different literatures and methodologies to explain core linguistic phenomena, including binding constraints, word order universals, and diachronic language change.}"
530,116,11120,1,Efficient Algorithms for Online Decision Problems,"In an online decision problem, one makes a sequence of decisions without knowledge of the future. Tools from learning such as Weighted Majority and its many variants [4, 13, 18] demonstrate that online algorithms can perform nearly as well as the best single decision chosen in hindsight, even when there are exponentially many possible decisions. However, the naive application of these algorithms is inefficient for such large problems. For some problems with nice structure, specialized efficient solutions have been developed [3, 6, 10, 16, 17]. We show that a very simple idea, used in Hannan&#8217;s seminal 1957 paper [9], gives efficient solutions to all of these problems. Essentially, in each period, one chooses the decision that worked best in the past. To guarantee low regret, it is necessary to add randomness. Surprisingly, this simple approach gives additive e regret per period, efficiently. We present a simple general analysis and several extensions, including a (1+ e)-competitive algorithm as well as a lazy one that rarely switches between decisions."
531,117,7441,1,Functional classification of drugs by properties of their pairwise interactions,"Multidrug treatments are increasingly important in medicine and for probing biological systems1, 2, 3, 4, 5, 6. Although many studies have focused on interactions between specific drugs, little is known about the system properties of a full drug interaction network6. Like their genetic counterparts, two drugs may have no interaction, or they may interact synergistically or antagonistically to increase or suppress their individual effects. Here we use a sensitive bioluminescence technique7, 8 to provide quantitative measurements of pairwise interactions among 21 antibiotics that affect growth rate in Escherichia coli. We find that the drug interaction network possesses a special property: it can be separated into classes of drugs such that any two classes interact either purely synergistically or purely antagonistically. These classes correspond directly to the cellular functions affected by the drugs. This network approach provides a new conceptual framework for understanding the functional mechanisms of drugs and their cellular targets and can be applied in systems intractable to mutant screening, biochemistry or microscopy."
532,117,15159,1,Communities in Networks,"We survey some of the concepts, methods, and applications of community detection, which has become an increasingly important area of network science. To help ease newcomers into the field, we provide a guide to available methodology and open problems, and discuss why scientists from diverse backgrounds are interested in these problems. As a running theme, we emphasize the connections of community detection to problems in statistical physics and computational optimization."
533,117,15712,1,Diffusion of scientific credits and the ranking of scientists,"Recently, the abundance of digital data enabled the implementation of graph based ranking algorithms that provide system level analysis for ranking publications and authors. Here we take advantage of the entire Physical Review publication archive (1893-2006) to construct authors' networks where weighted edges, as measured from opportunely normalized citation counts, define a proxy for the mechanism of scientific credit transfer. On this network we define a ranking method based on a diffusion algorithm that mimics the spreading of scientific credits on the network. We compare the results obtained with our algorithm with those obtained by local measures such as the citation count and provide a statistical analysis of the assignment of major career awards in the area of Physics. A web site where the algorithm is made available to perform customized rank analysis can be found at the address <a href=""http://www.physauthorsrank.org"">this http URL</a>"
534,117,15791,1,Predicting the Behavior of Techno-Social Systems,"We live in an increasingly interconnected world of techno-social systems, in which infrastructures composed of different technological layers are interoperating within the social component that drives their use and development. Examples are provided by the Internet, the World Wide Web, WiFi communication technologies, and transportation and mobility infrastructures. The multiscale nature and complexity of these networks are crucial features in understanding and managing the networks. The accessibility of new data and the advances in the theory and modeling of complex networks are providing an integrated framework that brings us closer to achieving true predictive power of the behavior of techno-social systems. 10.1126/science.1171990"
535,117,15952,1,Edge Direction and the Structure of Networks,"10.1073/pnas.0912671107 Directed networks are ubiquitous and are necessary to represent complex systems with asymmetric interactionsâfrom food webs to the World Wide Web. Despite the importance of edge direction for detecting local and community structure, it has been disregarded in studying a basic type of global diversity in networks: the tendency of nodes with similar numbers of edges to connect. This tendency, called assortativity, affects crucial structural and dynamic properties of real-world networks, such as error tolerance or epidemic spreading. Here we demonstrate that edge direction has profound effects on assortativity. We define a set of four directed assortativity measures and assign statistical significance by comparison to randomized networks. We apply these measures to three network classesâonline/social networks, food webs, and word-adjacency networks. Our measures (i) reveal patterns common to each class, (ii) separate networks that have been previously classified together, and (iii) expose limitations of several existing theoretical models. We reject the standard classification of directed networks as purely assortative or disassortative. Many display a class-specific mixture, likely reflecting functional or historical constraints, contingencies, and forces guiding the systemâs evolution."
536,118,243,1,"Image retrieval: current techniques, promising directions and open issues","This paper provides a comprehensive survey of the technical achievements in the research area of image retrieval, especially content-based image retrieval, an area that has been so active and prosperous in the past few years. The survey includes 100+ papers covering the research aspects of image feature representation and extraction, multidimensional indexing, and system design, three of the fundamental bases of content-based image retrieval. Furthermore, based on the state-of-the-art technology available now and the demand from real-world applications, open research issues are identified and future promising research directions are suggested. Copyright 1999 Academic Press."
537,118,1520,1,A guided tour to approximate string matching,"We survey the current techniques to cope with the problem of string matching allowing  errors. This is becoming a more and more relevant issue for many fast growing areas such  as information retrieval and computational biology. We focus on online searching and mostly  on edit distance, explaining the problem and its relevance, its statistical behavior, its history  and current developments, and the central ideas of the algorithms and their complexities. We  present a number of experiments to compare the performance of the different algorithms and  show which are the best choices according to each case. We conclude with some future work  directions and open problems.  1"
538,118,2760,1,Explaining collaborative filtering recommendations,"Automated collaborative filtering (ACF) systems predict a personâs affinity for items or information by connecting that personâs recorded interests with the recorded interests of a community of people and sharing ratings between likeminded persons. However, current recommender systems are black boxes, providing no transparency into the working of the recommendation. Explanations provide that transparency, exposing the reasoning and data behind a recommendation. In this paper, we address explanation interfaces for ACF systems â how they should be implemented and why they should be implemented. To explore how, we present a model for explanations based on the userâs conceptual model of the recommendation process. We then present experimental results demonstrating what components of an explanation are the most compelling. To address why, we present experimental evidence that shows that providing explanations can improve the acceptance of ACF systems. We also describe some initial explorations into measuring how explanations can improve the filtering performance of users."
539,118,3521,1,Mining frequent patterns without candidate generation,"SIGMOD&#039;2000 Paper ID: 196 Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist long patterns. In this study, we propose a novel frequent pattern tree (FP-tree) structure, which is an extended pre xtree structure for storing compressed, crucial information about frequent patterns, and develop an e cient FP-tree-based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth. E ciency of mining is achieved with three techniques: (1) a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans, (2) our FP-treebased mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a partitioning-based divide-and-conquer method is used to dramatically reduce the search space. Our performance study shows that the FP-growth method is e cient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent pattern mining methods."
540,118,6394,1,{A Survey on Context-aware systems},"Context&#45;aware systems offer entirely new opportunities for application developers and for end users by gathering context data and adapting systems behaviour accordingly. Especially in combination with mobile devices, these mechanisms are of high value and are used to increase usability tremendously. In this paper, we present common architecture principles of context&#45;aware systems and derive a layered conceptual design framework to explain the different elements common to most context&#45;aware architectures. Based on these design principles, we introduce various existing context&#45;aware systems focusing on context&#45;aware middleware and frameworks, which ease the development of context&#45;aware applications. We discuss various approaches and analyse important aspects in context&#45;aware computing on the basis of the presented systems."
541,118,9221,1,Generic User Modeling Systems,"The paper reviews the development of generic user modeling systems over the past twenty years. It describes their purposes, their services within user-adaptive systems, and the different design requirements for research prototypes and commercially deployed servers. It discusses the architectures that have been explored so far, namely shell systems that form part of the application, central server systems that communicate with several applications, and possible future user modeling agents that physically follow the user. Several implemented research prototypes and commercial systems are briefly described."
542,118,9970,1,A Framework for Web Science,"This text sets out a series of approaches to the analysis and synthesis of the World Wide Web, and other web-like information structures. A comprehensive set of research questions is outlined, together with a sub-disciplinary breakdown, emphasising the multi-faceted nature of the Web, and the multi-disciplinary nature of its study and development. These questions and approaches together set out an agenda for Web Science, the science of decentralised information systems. Web Science is required both as a way to understand the Web, and as a way to focus its development on key communicational and representational requirements. The text surveys central engineering issues, such as the development of the Semantic Web, Web services and P2P. Analytic approaches to discover the Webâs topology, or its graph-like structures, are examined. Finally, the Web as a technology is essentially socially embedded; therefore various issues and requirements for Web use and governance are also reviewed."
543,118,11754,1,SentiWordNet: A Publicly Available Lexical Resource for Opinion Mining,"Opinion mining (OM) is a recent subdiscipline at the crossroads of information retrieval and computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses. OM has a rich set of applications, ranging from tracking users&#039; opinions about products or about political candidates as expressed in online forums, to customer relationship management. In order to aid the extraction of opinions from text, recent research has tried to automatically determine the &#034;PN-polarity&#034; of subjective terms, i.e. identify whether a term that is a marker of opinionated content has a positive or a negative connotation. Research on determining whether a term is indeed a marker of opinionated content (a subjective term) or not (an objective term) has been, instead, much more scarce. In this work we describe SENTIWORDNET, a lexical resource in which each WORDNET synset s is associated to three numerical scores Obj(s), Pos(s) and Neg(s), describing how objective, positive, and negative the terms contained in the synset are. The method used to develop SENTIWORDNET is based on the quantitative analysis of the glosses associated to synsets, and on the use of the resulting vectorial term representations for semi-supervised synset classification. The three scores are derived by combining the results produced by a committee of eight ternary classifiers, all characterized by similar accuracy levels but different classification behaviour. SENTIWORDNET is freely available for research purposes, and is endowed with a Web-based graphical user interface."
544,118,16480,1,Solving the apparent diversity-accuracy dilemma of recommender systems,"10.1073/pnas.1000488107 Recommender systems use data on past user preferences to predict possible future likes and interests. A key challenge is that while the most useful individual recommendations are to be found among diverse niche objects, the most reliably accurate results are obtained by methods that recommend objects based on user or object similarity. In this paper we introduce a new algorithm specifically to address the challenge of diversity and show how it can be used to resolve this apparent dilemma when combined in an elegant hybrid with an accuracy-focused algorithm. By tuning the hybrid appropriately we are able to obtain, without relying on any semantic or context-specific information, simultaneous gains in both accuracy and diversity of recommendations."
545,119,4738,1,M-tree: An Efficient Access Method for Similarity Search in Metric Spaces,"A new access method, called M-tree, is proposed to organize and search large data sets from a generic ""metric space"", i.e. where object proximity is only defined by a distance function satisfying the positivity, symmetry, and triangle inequality postulates. We detail algorithms for insertion of objects and split management, which keep the M-tree always balanced - several heuristic split alternatives are considered and experimentally evaluated. Algorithms for similarity (range and k-nearest neighbors) queries are also described. Results from extensive experimentation with a prototype system are reported, considering as the performance criteria the number of page I/O's and the number of distance computations. The results demonstrate that the Mtree indeed extends the domain of applicability beyond the traditional vector spaces, performs reasonably well in high-dimensional data spaces, and scales well in case of growing files."
546,120,8361,1,A Descriptive Framework of Workspace Awareness for Real-Time Groupware,"Supporting awareness of others is an idea that holds promise forimproving the usability of real-time distributed groupware.However, there is little principled information available aboutawareness that can be used by groupware designers. In thisarticle, we develop a descriptive theory of awareness for thepurpose of aiding groupware design, focusing on one kind of groupawareness called workspace awareness. We focus on how smallgroups perform generation and execution tasks in medium-sizedshared workspaces â tasks where group members frequently shiftbetween individual and shared activities during the work session.We have built a three-part framework that examines the concept ofworkspace awareness and that helps designers understand theconcept for purposes of designing awareness support in groupware.The framework sets out elements of knowledge that make upworkspace awareness, perceptual mechanisms used to maintainawareness, and the ways that people use workspace awareness incollaboration. The framework also organizes previous research onawareness and extends it to provide designers with a vocabularyand a set of ground rules for analysing work situations, forcomparing awareness devices, and for explaining evaluationresults. The basic structure of the theory can be used todescribe other kinds of awareness that are important to theusability of groupware."
547,121,4962,1,"Structure, function and evolution of multidomain proteins.",Proteins are composed of evolutionary units called domains; the majority of proteins consist of at least two domains. These domains and nature of their interactions determine the function of the protein. The roles that combinations of domains play in the formation of the protein repertoire have been found by analysis of domain assignments to genome sequences. Additional findings on the geometry of domains have been gained from examination of three-dimensional protein structures. Future work will require a domain-centric functional classification scheme and efforts to determine structures of domain combinations.
548,121,13438,1,Darwinian Evolution on a Chip,"Computer control of Darwinian evolution has been demonstrated by propagating a population of {RNA} enzymes in a microfluidic device. The {RNA} population was challenged to catalyze the ligation of an oligonucleotide substrate under conditions of progressively lower substrate concentrations. A microchip-based serial dilution circuit automated an exponential growth phase followed by a 10-fold dilution, which was repeated for 500 log-growth iterations. Evolution was observed in real time as the population adapted and achieved progressively faster growth rates over time. The final evolved enzyme contained a set of 11 mutations that conferred a 90-fold improvement in substrate utilization, coinciding with the applied selective pressure. This system reduces evolution to a microfluidic algorithm, allowing the experimenter to observe and manipulate adaptation."
549,122,8749,1,Global mapping of pharmacological space.,"We present the global mapping of pharmacological space by the integration of several vast sources of medicinal chemistry structure-activity relationships (SAR) data. Our comprehensive mapping of pharmacological space enables us to identify confidently the human targets for which chemical tools and drugs have been discovered to date. The integration of SAR data from diverse sources by unique canonical chemical structure, protein sequence and disease indication enables the construction of a ligand-target matrix to explore the global relationships between chemical structure and biological targets. Using the data matrix, we are able to catalog the links between proteins in chemical space as a polypharmacology interaction network. We demonstrate that probabilistic models can be used to predict pharmacology from a large knowledge base. The relationships between proteins, chemical structures and drug-like properties provide a framework for developing a probabilistic approach to drug discovery that can be exploited to increase research productivity."
550,122,12527,1,The OBO Foundry: coordinated evolution of ontologies to support biomedical data integration,"The value of any kind of data is greatly enhanced when it exists in a form that allows it to be integrated with other data. One approach to integration is through the annotation of multiple bodies of data using common controlled vocabularies or 'ontologies'. Unfortunately, the very success of this approach has led to a proliferation of ontologies, which itself creates obstacles to integration. The Open Biomedical Ontologies {(OBO)} consortium is pursuing a strategy to overcome this problem. Existing {OBO} ontologies, including the Gene Ontology, are undergoing coordinated reform, and new ontologies are being created on the basis of an evolving set of shared principles governing ontology development. The result is an expanding family of ontologies designed to be interoperable and logically well formed and to incorporate accurate representations of biological reality. We describe this {OBO} Foundry initiative and provide guidelines for those who might wish to become involved."
551,122,14677,1,CREDO: A protein-ligand interaction database for drug discovery.,"Harnessing data from the growing number of protein-ligand complexes in the Protein Data Bank is an important task in drug discovery. In order to benefit from the abundance of three-dimensional structures, structural data must be integrated with sequence as well as chemical data and the protein-small molecule interactions characterized structurally at the inter-atomic level. In this study, we present CREDO, a new publicly available database of protein-ligand interactions, which represents contacts as structural interaction fingerprints, implements novel features and is completely scriptable through its application programming interface. Features of CREDO include implementation of molecular shape descriptors with ultrafast shape recognition, fragmentation of ligands in the Protein Data Bank, sequence-to-structure mapping and the identification of approved drugs. Selected analyses of these key features are presented to highlight a range of potential applications of CREDO. The CREDO dataset has been released into the public domain together with the application programming interface under a Creative Commons license at http://www-cryst.bioc.cam.ac.uk/credo. We believe that the free availability and numerous features of CREDO database will be useful not only for commercial but also for academia-driven drug discovery programmes."
552,122,16070,1,The powerful law of the power law and other myths in network biology,"For almost 10 years, topological analysis of different large-scale biological networks (metabolic reactions, protein interactions, transcriptional regulation) has been highlighting some recurrent properties: power law distribution of degree, scale-freeness, small world, which have been proposed to confer functional advantages such as robustness to environmental changes and tolerance to random mutations. Stochastic generative models inspired different scenarios to explain the growth of interaction networks during evolution. The power law and the associated properties appeared so ubiquitous in complex networks that they were qualified as ""universal laws"". However, these properties are no longer observed when the data are subjected to statistical tests: in most cases, the data do not fit the expected theoretical models, and the cases of good fitting merely result from sampling artefacts or improper data representation. The field of network biology seems to be founded on a series of myths, i.e. widely believed but false ideas. The weaknesses of these foundations should however not be considered as a failure for the entire domain. Network analysis provides a powerful frame for understanding the function and evolution of biological processes, provided it is brought to an appropriate level of description, by focussing on smaller functional modules and establishing the link between their topological properties and their dynamical behaviour."
553,123,157,1,Library weblogs,"A total of 55 weblogs maintained by libraries were identified in late 2003 using Internet search engines and directories. The weblogs were studied using content analysis techniques. Library weblogs were found in just three countries, with the majority being in the USA. Public and academic libraries were more likely to have a weblog than other types of libraries. The most common aim or purpose was to provide news, information and links to Internet resources for library users. Few provided interactive facilities, and when provided, there was little evidence that the facilities were used to any extent. Only one-fifth of the weblogs had been updated within the past day and only half within the previous week. Less than half provided an RSS feed. Given the small number of library weblogs in the study, the question of why so few? is discussed. Finally, the article addresses the implications of the findings for library managers."
554,123,9359,1,Library 2.0 Theory: Web 2.0 and Its Implications for Libraries,"This article posits a definition and theory for ""Library 2.0"". It suggests that recent thinking describing the changing Web as ""Web 2.0"" will have substantial implications for libraries, and recognizes that while these implications keep very close to the history and mission of libraries, they still necessitate a new paradigm for librarianship. The paper applies the theory and definition to the practice of librarianship, specifically addressing how Web 2.0 technologies such as synchronous messaging and streaming media, blogs, wikis, social networks, tagging, RSS feeds, and mashups might intimate changes in how libraries provide access to their collections and user support for that access. Copyright Â© 2006, Jack M. Maness."
