,user.id,doc.id,rating,raw.title,raw.abstract
0,0,3298,1,Experimental and computational approaches to estimate solubility and permeability in drug discovery and development settings.,"Experimental and computational approaches to estimate solubility and permeability in discovery and development settings are described. In the discovery setting âthe rule of 5â predicts that poor absorption or permeation is more likely when there are more than 5 H---bond donors, 10 H---bond acceptors, the molecular weight (MWT) is greater than 500 and the calculated Log P (CLogP) is greater than 5 (or MlogP > 4.15). Computational methodology for the rule-based Moriguchi Log P (MLogP) calculation is described. Turbidimetric solubility measurement is described and applied to known drugs. High throughput screening (HTS) leads tend to have higher MWT and Log P and lower turbidimetric solubility than leads in the pre-HTS era. In the development setting, solubility calculations focus on exact value prediction and are difficult because of polymorphism. Recent work on linear free energy relationships and Log P approaches are critically reviewed. Useful predictions are possible in closely related analog series when coupled with experimental thermodynamic solubility measurements."
1,0,3333,1,Binding MOAD (Mother Of All Databases).,"Binding MOAD (Mother of All Databases) is the largest collection of high-quality, protein-ligand complexes available from the Protein Data Bank. At this time, Binding MOAD contains 5331 protein-ligand complexes comprised of 1780 unique protein families and 2630 unique ligands. We have searched the crystallography papers for all 5000+ structures and compiled binding data for 1375 (26%) of the protein-ligand complexes. The binding-affinity data ranges 13 orders of magnitude. This is the largest collection of binding data reported to date in the literature. We have also addressed the issue of redundancy in the data. To create a nonredundant dataset, one protein from each of the 1780 protein families was chosen as a representative. Representatives were chosen by tightest binding, best resolution, etc. For the 1780 ""best"" complexes that comprise the nonredundant version of Binding MOAD, 475 (27%) have binding data. This significant collection of protein-ligand complexes will be very useful in elucidating the biophysical patterns of molecular recognition and enzymatic regulation. The complexes with binding-affinity data will help in the development of improved scoring functions and structure-based drug discovery techniques. The dataset can be accessed at http://www.BindingMOAD.org. Proteins 2005. (c) 2005 Wiley-Liss, Inc."
2,0,4663,1,Coarse-grained normal mode analysis in structural biology.,"The realization that experimentally observed functional motions of proteins can be predicted by coarse-grained normal mode analysis has renewed interest in applications to structural biology. Notable applications include the prediction of biologically relevant motions of proteins and supramolecular structures driven by their structure-encoded collective dynamics; the refinement of low-resolution structures, including those determined by cryo-electron microscopy; and the identification of conserved dynamic patterns and mechanically key regions within protein families. Additionally, hybrid methods that couple atomic simulations with deformations derived from coarse-grained normal mode analysis are able to sample collective motions beyond the range of conventional molecular dynamics simulations. Such applications have provided great insight into the underlying principles linking protein structures to their dynamics and their dynamics to their functions."
3,0,4890,1,Diversity of protein-protein interactions.,"In this review, we discuss the structural and functional diversity of protein-protein interactions (PPIs) based primarily on protein families for which three-dimensional structural data are available. PPIs play diverse roles in biology and differ based on the composition, affinity and whether the association is permanent or transient. In vivo, the protomer's localization, concentration and local environment can affect the interaction between protomers and are vital to control the composition and oligomeric state of protein complexes. Since a change in quaternary state is often coupled with biological function or activity, transient PPIs are important biological regulators. Structural characteristics of different types of PPIs are discussed and related to their physiological function, specificity and evolution."
4,0,6104,1,SCOPPI: a structural classification of protein-protein interfaces,"{SCOPPI,} the structural classification of protein-protein interfaces, is a comprehensive database that classifies and annotates domain interactions derived from all known protein structures. {SCOPPI} applies {SCOP} domain definitions and a distance criterion to determine inter-domain interfaces. Using a novel method based on multiple sequence and structural alignments of {SCOP} families, {SCOPPI} presents a comprehensive geometrical classification of domain interfaces. Various interface characteristics such as number, type and position of interacting amino acids, conservation, interface size, and permanent or transient nature of the interaction are further provided. Proteins in {SCOPPI} are annotated with Gene Ontology terms, and the ontology can be used to quickly browse {SCOPPI.} Screenshots are available for every interface and its participating domains. Here, we describe contents and features of the web-based user interface as well as the underlying methods used to generate {SCOPPI's} data. In addition, we present a number of examples where {SCOPPI} becomes a useful tool to analyze viral mimicry of human interface binding sites, gene fusion events, conservation of interface residues and diversity of interface localizations. {SCOPPI} is available at http://www.scoppi.org."
5,0,7107,1,SCOWLP: a web-based database for detailed characterization and visualization of protein interfaces.,"ABSTRACT: BACKGROUND: Currently there is a strong need for methods that help to obtain an accurate description of protein interfaces in order to be able to understand the principles that govern molecular recognition and protein function. Many of the recent efforts to computationally identify and characterize protein networks extract protein interaction information at atomic resolution from the PDB. However, they pay none or little attention to small protein ligands and solvent. They are key components and mediators of protein interactions and fundamental for a complete description of protein interfaces. Interactome profiling requires the development of computational tools to extract and analyze protein-protein, protein-ligand and detailed solvent interaction information from the PDB in an automatic and comparative fashion. Adding this information to the existing one on protein-protein interactions will allow us to better understand protein interaction networks and protein function. RESULTS: SCOWLP (Structural Characterization Of Water, Ligands and Proteins) is a user-friendly and publicly accessible web-based relational database for detailed characterization and visualization of the PDB protein interfaces. The SCOWLP database includes proteins, peptidic-ligands and interface water molecules as descriptors of protein interfaces. It contains currently 74,907 protein interfaces and 2,093,976 residue-residue interactions formed by 60,664 structural units (protein domains and peptidic-ligands) and their interacting solvent. The SCOWLP web-server allows detailed structural analysis and comparisons of protein interfaces at atomic level by text query of PDB codes and/or by navigating a SCOP-based tree. It includes a visualization tool to interactively display the interfaces and label interacting residues and interface solvent by atomic physicochemical properties. SCOWLP is automatically updated with every SCOP release. CONCLUSIONS: SCOWLP enriches substantially the description of protein interfaces by adding detailed interface information of peptidic-ligands and solvent to the existing protein-protein interaction databases. SCOWLP may be of interest to many structural bioinformaticians. It provides a platform for automatic global mapping of protein interfaces at atomic level, representing a useful tool for classification of protein interfaces, protein binding comparative studies, reconstruction of protein complexes and understanding protein networks. The web-server with the database and its additional summary tables used for our analysis are available at http://www.scowlp.org."
6,0,10009,1,How many drug targets are there?,"For the past decade, the number of molecular targets for approved drugs has been debated. Here, we reconcile apparently contradictory previous reports into a comprehensive survey, and propose a consensus number of current drug targets for all classes of approved therapeutic drugs. One striking feature is the relatively constant historical rate of target innovation (the rate at which drugs against new targets are launched); however, the rate of developing drugs against new families is significantly lower. The recent approval of drugs that target protein kinases highlights two additional trends: an emerging realization of the importance of polypharmacology, and also the power of a gene-family-led approach in generating novel and important therapies."
7,0,10509,1,The molecular architecture of protein-protein binding sites.,"The formation of specific protein interactions plays a crucial role in most, if not all, biological processes, including signal transduction, cell regulation, the immune response and others. Recent advances in our understanding of the molecular architecture of protein-protein binding sites, which facilitates such diversity in binding affinity and specificity, are enabling us to address key questions. What is the amino acid composition of binding sites? What are interface hotspots? How are binding sites organized? What are the differences between tight and weak interacting complexes? How does water contribute to binding? Can the knowledge gained be translated into protein design? And does a universal code for binding exist, or is it the architecture and chemistry of the interface that enable diverse but specific binding solutions?"
8,0,11227,1,Analysing six types of protein-protein interfaces.,"Non-covalent residue side-chain interactions occur in many different types of proteins and facilitate many biological functions. Are these differences manifested in the sequence compositions and/or the residue-residue contact preferences of the interfaces? Previous studies analysed small data sets and gave contradictory answers. Here, we introduced a new data-mining method that yielded the largest high-resolution data set of interactions analysed. We introduced an information theory-based analysis method. On the basis of sequence features, we were able to differentiate six types of protein interfaces, each corresponding to a different functional or structural association between residues. Particularly, we found significant differences in amino acid composition and residue-residue preferences between interactions of residues within the same structural domain and between different domains, between permanent and transient interfaces, and between interactions associating homo-oligomers and hetero-oligomers. The differences between the six types were so substantial that, using amino acid composition alone, we could predict statistically to which of the six types of interfaces a pool of 1000 residues belongs at 63-100% accuracy. All interfaces differed significantly from the background of all residues in SWISS-PROT, from the group of surface residues, and from internal residues that were not involved in non-trivial interactions. Overall, our results suggest that the interface type could be predicted from sequence and that interface-type specific mean-field potentials may be adequate for certain applications."
9,0,12685,1,Determining the architectures of macromolecular assemblies.,"To understand the workings of a living cell, we need to know the architectures of its macromolecular assemblies. Here we show how proteomic data can be used to determine such structures. The process involves the collection of sufficient and diverse high-quality data, translation of these data into spatial restraints, and an optimization that uses the restraints to generate an ensemble of structures consistent with the data. Analysis of the ensemble produces a detailed architectural map of the assembly. We developed our approach on a challenging model system, the nuclear pore complex (NPC). The NPC acts as a dynamic barrier, controlling access to and from the nucleus, and in yeast is a 50 MDa assembly of 456 proteins. The resulting structure, presented in an accompanying paper, reveals the configuration of the proteins in the NPC, providing insights into its evolution and architectural principles. The present approach should be applicable to many other macromolecular assemblies."
10,0,12804,1,The origin of protein interactions and allostery in colocalization.,"Two fundamental principles can account for how regulated networks of interacting proteins originated in cells. These are the law of mass action, which holds that the binding of one molecule to another increases with concentration, and the fact that the colocalization of molecules vastly increases their local concentrations. It follows that colocalization can amplify the effect on one protein of random mutations in another protein and can therefore, through natural selection, lead to interactions between proteins and to a startling variety of complex allosteric controls. It also follows that allostery is common and that homologous proteins can have different allosteric mechanisms. Thus, the regulated protein networks of organisms seem to be the inevitable consequence of natural selection operating under physical laws."
11,0,13188,1,A careful disorderliness in the proteome: Sites for interaction and targets for future therapies,"The community of scientists interested in studying intrinsically unstructured (or disordered) proteins has emerged in recent years. What began as a controversial idea has become an established phenomenon. The new, greater focus on proteins that are in some way normally unstructured promises to provide a greater understanding of protein function, particularly with respect to proteinâprotein interactions. These regions also offer new possibilities into how interactions can be targeted by small molecules."
12,0,13924,1,Biochemistry. How do proteins interact?,"PERSPECTIVE. 1st paragraph: Interactions between proteins are central to biology and are becoming increasingly important targets for drug design. Upon forming complexes, protein conformations usually change substantially compared to the unbound protein. Two main hypotheses have been advanced to explain these changes (see the figure). According to the ""induced fit"" hypothesis, the initial interaction between a protein and a binding partner induces a conformational change in the protein through a stepwise process (1). In the ""conformational selection"" model, it is assumed that, prior to the binding interaction, the unliganded protein exists as an ensemble of conformations in dynamic equilibrium. The binding partner interacts preferentially with a weakly populated, higher-energy conformation-causing the equilibrium to shift in favor of the selected conformation. This conformation then becomes the major conformation in the complex (2). Although biochemistry textbooks have championed the induced fit mechanism for more than 50 years, there is now growing support for the additional binding mechanism, including the seminal work by Lange, Lakomek, and co-workers on page 1471 of this issue. [Lange, Science (2008), 320, 1471]"
13,0,15192,1,"Protein allostery, signal transmission and dynamics: a classification scheme of allosteric mechanisms.","Allostery has come of age; the number, breadth and functional roles of documented protein allostery cases are rising quickly. Since all dynamic proteins are potentially allosteric and allostery plays crucial roles in all cellular pathways, sorting and classifying allosteric mechanisms in proteins should be extremely useful in understanding and predicting how the signals are regulated and transmitted through the dynamic multi-molecular cellular organizations. Classification organizes the complex information thereby unraveling relationships and patterns in molecular activation and repression. In signaling, current classification schemes consider classes of molecules according to their functions; for example, epinephrine and norepinephrine secreted by the central nervous system are classified as neurotransmitters. Other schemes would account for epinephrine when secreted by the adrenal medulla to be hormone-like. Yet, such classifications account for the global function of the molecule; not for the molecular mechanism of how the signal transmission initiates and how it is transmitted. Here we provide a unified view of allostery and the first classification framework. We expect that a classification scheme would assist in comprehension of allosteric mechanisms, in prediction of signaling on the molecular level, in better comprehension of pathways and regulation of the complex signals, in translating them to the cascading events, and in allosteric drug design. We further provide a range of examples illustrating mechanisms in protein allostery and their classification from the cellular functional standpoint."
14,0,15283,1,Linking folding and binding.,"Many cellular proteins are intrinsically disordered and undergo folding, in whole or in part, upon binding to their physiological targets. The past few years have seen an exponential increase in papers describing characterization of intrinsically disordered proteins, both free and bound to targets. Although NMR spectroscopy remains the favored tool, a number of new biophysical techniques are proving exceptionally useful in defining the limits of the conformational ensembles. Advances have been made in prediction of the recognition elements in disordered proteins, in elucidating the kinetics and mechanism of the coupled folding and binding process, and in understanding the role of post-translational modifications in tuning the biological response. Here we review these and other recent advances that are providing new insights into the conformational propensities and interactions of intrinsically disordered proteins and are beginning to reveal general principles underlying their biological functions."
15,1,943,1,MedKit: a helper toolkit for automatic mining of MEDLINE/PubMed citations,"Summary: MEDLINE/PubMed is one of the most important information sources for bioinformatics text mining. However, there remain limitations in working with MEDLINE/PubMed citations. For example, PubMed imposes an upper limit of 10 000 for downloading PMID list or citations; and MEDLINE files are too large for most off-the-shelf XML parsers. We developed a Java package, MedKit, to work-around the limitations, as well as provide other useful functionalities, e.g. random sampling. Its four modules (querier, sampler, fetcher and parser) can work independently, or be pipelined in various combinations. It can be used as a stand-alone GUI application, or integrated into other text-mining systems. Text mining researchers and others may download and use the toolkit free for non-commercial purposes.  Availability: http://metnetdb.gdcb.iastate.edu/medkit  Contact: berleant@iastate.edu 10.1093/bioinformatics/bti087"
16,1,5573,1,404 not found: the stability and persistence of URLs published in MEDLINE,"Motivation: The advent of the World Wide Web has enabled unprecedented supplementation of traditional journal publications, allowing access to resources, such as video, sound, software, databases, datasets too large to publish, and even supplementary information and discussion. However, unlike traditional publications, continued availability of these online resources is not guaranteed. An automated survey was conducted to quantify the growth in Uniform Resource Locators (URLs) published to date in MEDLINE abstracts, their current availability and distribution by journal.  Results: Of 1630 unique URLs identified, formatting and/or spelling errors were detected within 201 (12%) of them as published. After corrections were made, a survey revealed that [~]63% of these URLs were consistently available, and another 19% were available intermittently. The rate of failure was far worse for anonymous login to FTP sites, with only 12 of 33 sites (36%) responding. This survey also shows that journals vary disproportionately in the number of web citations published, suggesting policy implementation among a few could have a profound impact overall. Out of the 306 journals with a URL published in an abstract, Bioinformatics published the most (12% of total).  Availability: URL database and program available by request. 10.1093/bioinformatics/btg465"
17,1,6458,1,"Mining MEDLINE: abstracts, sentences, or phrases?","A growing body of works address automated mining of biochemical knowledge from digital repositories of scientific literature, such as MEDLINE. Some of these works use abstracts as the unit of text from which to extract facts. Others use sentences for this purpose, while still others use phrases. Here we compare abstracts, sentences, and phrases in MEDLINE using the standard information retrieval performance measures of recall, precision, and effectiveness, for the task of mining interactions among biochemical terms based on term co-occurrence. Results show statistically significant differences that can impact the choice of text unit."
18,1,12897,1,Corpus annotation for mining biomedical events from literature.,"ABSTRACT: BACKGROUND: Advanced text-mining (TM) such as semantic enrichment of papers, event or relation extraction, and intelligent question answering have increasingly attracted attention in the bio-medical domain. For such attempts to succeed, text annotation from the biological point of view is indispensable. However, due to the complexity of the task, semantic annotation has never been tried on a large scale, apart from relatively simple term annotation. RESULTS: We have completed a new type of semantic annotation, event annotation, which is an addition to the existing annotations in the GENIA corpus. The corpus has already been annotated with POS (Parts of Speech), syntactic trees, terms, etc. The new annotation was made on half of the GENIA corpus, consisting of 1,000 Medline abstracts. It contains 9,372 sentences in which 36,114 events are identified. The major challenges during event annotation were (1) to design a scheme of annotation which meets specific requirements of text annotation, (2) to achieve biology-oriented annotation which reflect biologists' interpretation of text, and (3) to ensure the homogeneity of annotation quality across annotators. To meet these challenges, we introduced new concepts such as Single-facet Annotation and Semantic Typing, which have collectively contributed to successful completion of a large scale annotation. CONCLUSIONS: The resulting event-annotated corpus is the largest and one of the best in quality among similar annotation efforts. We expect it to become a valuable resource for NLP (Natural Language Processing)-based TM in the bio-medical domain."
19,1,13617,1,Comparative analysis of five protein-protein interaction corpora,"BACKGROUND: Growing interest in the application of natural language processing methods to biomedical text has led to an increasing number of corpora and methods targeting protein-protein interaction (PPI) extraction. However, there is no general consensus regarding PPI annotation and consequently resources are largely incompatible and methods are difficult to evaluate. RESULTS: We present the first comparative evaluation of the diverse PPI corpora, performing quantitative evaluation using two separate information extraction methods as well as detailed statistical and qualitative analyses of their properties. For the evaluation, we unify the corpus PPI annotations to a shared level of information, consisting of undirected, untyped binary interactions of non-static types with no identification of the words specifying the interaction, no negations, and no interaction certainty.We find that the F-score performance of a state-of-the-art PPI extraction method varies on average 19 percentage units and in some cases over 30 percentage units between the different evaluated corpora. The differences stemming from the choice of corpus can thus be substantially larger than differences between the performance of PPI extraction methods, which suggests definite limits on the ability to compare methods evaluated on different resources. We analyse a number of potential sources for these differences and identify factors explaining approximately half of the variance. We further suggest ways in which the difficulty of the PPI extraction tasks codified by different corpora can be determined to advance comparability. Our analysis also identifies points of agreement and disagreement in PPI corpus annotation that are rarely explicitly stated by the authors of the corpora. CONCLUSIONS: Our comparative analysis uncovers key similarities and differences between the diverse PPI corpora, thus taking an important step towards standardization. In the course of this study we have created a major practical contribution in converting the corpora into a shared format. The conversion software is freely available at http://mars.cs.utu.fi/PPICorpora."
20,1,15048,1,Is searching full text more effective than searching abstracts?,"BACKGROUND: With the growing availability of full-text articles online, scientists and other consumers of the life sciences literature now have the ability to go beyond searching bibliographic records (title, abstract, metadata) to directly access full-text content. Motivated by this emerging trend, I posed the following question: is searching full text more effective than searching abstracts? This question is answered by comparing text retrieval algorithms on MEDLINE abstracts, full-text articles, and spans (paragraphs) within full-text articles using data from the TREC 2007 genomics track evaluation. Two retrieval models are examined: bm25 and the ranking algorithm implemented in the open-source Lucene search engine. RESULTS: Experiments show that treating an entire article as an indexing unit does not consistently yield higher effectiveness compared to abstract-only search. However, retrieval based on spans, or paragraphs-sized segments of full-text articles, consistently outperforms abstract-only search. Results suggest that highest overall effectiveness may be achieved by combining evidence from spans and full articles. CONCLUSION: Users searching full text are more likely to find relevant articles than searching only abstracts. This finding affirms the value of full text collections for text retrieval and provides a starting point for future work in exploring algorithms that take advantage of rapidly-growing digital archives. Experimental results also highlight the need to develop distributed text retrieval algorithms, since full-text articles are significantly longer than abstracts and may require the computational resources of multiple machines in a cluster. The MapReduce programming model provides a convenient framework for organizing such computations."
21,1,15459,1,U-Compare: share and compare text mining tools with UIMA,"Summary: Due to the increasing number of text mining resources (tools and corpora) available to biologists, interoperability issues between these resources are becoming significant obstacles to using them effectively. UIMA, the Unstructured Information Management Architecture, is an open framework designed to aid in the construction of more interoperable tools. U-Compare is built on top of the UIMA framework, and provides both a concrete framework for out-of-the-box text mining and a sophisticated evaluation platform allowing users to run specific tools on any target text, generating both detailed statistics and instance-based visualizations of outputs. U-Compare is a joint project, providing the world's largest, and still growing, collection of UIMA-compatible resources. These resources, originally developed by different groups for a variety of domains, include many famous tools and corpora. U-Compare can be launched straight from the web, without needing to be manually installed. All U-Compare components are provided ready-to-use and can be combined easily via a drag-and-drop interface without any programming. External UIMA components can also simply be mixed with U-Compare components, without distinguishing between locally and remotely deployed resources.  Availability: http://u-compare.org/  Contact: kano@is.s.u-tokyo.ac.jp 10.1093/bioinformatics/btp289"
22,1,16023,1,A dictionary to identify small molecules and drugs in free text,"Motivation: From the scientific community, a lot of effort has been spent on the correct identification of gene and protein names in text, while less effort has been spent on the correct identification of chemical names. Dictionary-based term identification has the power to recognize the diverse representation of chemical information in the literature and map the chemicals to their database identifiers.Results: We developed a dictionary for the identification of small molecules and drugs in text, combining information from UMLS, MeSH, ChEBI, DrugBank, KEGG, HMDB and ChemIDplus. Rule-based term filtering, manual check of highly frequent terms and disambiguation rules were applied. We tested the combined dictionary and the dictionaries derived from the individual resources on an annotated corpus, and conclude the following: (i) each of the different processing steps increase precision with a minor loss of recall; (ii) the overall performance of the combined dictionary is acceptable (precision 0.67, recall 0.40 (0.80 for trivial names); (iii) the combined dictionary performed better than the dictionary in the chemical recognizer OSCAR3; (iv) the performance of a dictionary based on ChemIDplus alone is comparable to the performance of the combined dictionary.Availability: The combined dictionary is freely available as an XML file in Simple Knowledge Organization System format on the web site http://www.biosemantics.org/chemlist.Contact: k.hettne@erasmusmc.nlSupplementary information: Supplementary data are available at Bioinformatics online."
23,1,16746,1,Complex event extraction at PubMed scale,"Motivation: There has recently been a notable shift in biomedical information extraction (IE) from relation models toward the more expressive event model, facilitated by the maturation of basic tools for biomedical text analysis and the availability of manually annotated resources. The event model allows detailed representation of complex natural language statements and can support a number of advanced text mining applications ranging from semantic search to pathway extraction. A recent collaborative evaluation demonstrated the potential of event extraction systems, yet there have so far been no studies of the generalization ability of the systems nor the feasibility of large-scale extraction.  Results: This study considers event-based IE at PubMed scale. We introduce a system combining publicly available, state-of-the-art methods for domain parsing, named entity recognition and event extraction, and test the system on a representative 1% sample of all PubMed citations. We present the first evaluation of the generalization performance of event extraction systems to this scale and show that despite its computational complexity, event extraction from the entire PubMed is feasible. We further illustrate the value of the extraction approach through a number of analyses of the extracted information.  Availability: The event detection system and extracted data are open source licensed and available at http://bionlp.utu.fi/.  Contact: jari.bjorne@utu.fi 10.1093/bioinformatics/btq180"
24,2,1330,1,Long-lasting potentiation of synaptic transmission in the dentate area of the anaesthetized rabbit following stimulation of the perforant path,"1. The after-effects of repetitive stimulation of the perforant path fibres to the dentate area of the hippocampal formation have been examined with extracellular micro-electrodes in rabbits anaesthetized with urethane.2. In fifteen out of eighteen rabbits the population response recorded from granule cells in the dentate area to single perforant path volleys was potentiated for periods ranging from 30 min to 10 hr after one or more conditioning trains at 10-20/sec for 10-15 sec, or 100/sec for 3-4 sec.3. The population response was analysed in terms of three parameters: the amplitude of the population excitatory post-synaptic potential (e.p.s.p.), signalling the depolarization of the granule cells, and the amplitude and latency of the population spike, signalling the discharge of the granule cells.4. All three parameters were potentiated in 29% of the experiments; in other experiments in which long term changes occurred, potentiation was confined to one or two of the three parameters. A reduction in the latency of the population spike was the commonest sign of potentiation, occurring in 57% of all experiments. The amplitude of the population e.p.s.p. was increased in 43%, and of the population spike in 40%, of all experiments.5. During conditioning at 10-20/sec there was massive potentiation of the population spike (;frequency potentiation'). The spike was suppressed during stimulation at 100/sec. Both frequencies produced long-term potentiation.6. The results suggest that two independent mechanisms are responsible for long-lasting potentiation: (a) an increase in the efficiency of synaptic transmission at the perforant path synapses; (b) an increase in the excitability of the granule cell population."
25,2,4459,1,Discovering models of software processes from event-based data,"Many software process methods and tools presuppose the existence of a formal model of a process. Unfortunately, developing a formal model for an on-going, complex process can be difficult, costly, and error prone. This presents a practical barrier to the adoption of process technologies, which would be lowered by automated assistance in creating formal models. To this end, we have developed a data analysis technique that we term process discovery. Under this technique, data describing process events are first captured from an on-going process and then used to generate a formal model of the behavior of that process. In this article we describe a Markov method that we developed specifically for process discovery, as well as describe two additional methods that we   adopted from other domains and augmented for our purposes. The three methods range from the purely algorithmic to the purely statistical. We compare the methods and discuss their application in an industrial case study."
26,2,5130,1,The episodic buffer: a new component of working memory?,"In 1974, Baddeley and Hitch proposed a three-component model of working memory. Over the years, this has been successful in giving an integrated account not only of data from normal adults, but also neuropsychological, developmental and neuroimaging data. There are, however, a number of phenomena that are not readily captured by the original model. These are outlined here and a fourth component to the model, the episodic buffer, is proposed. It comprises a limited capacity system that provides temporary storage of information held in a multimodal code, which is capable of binding information from the subsidiary systems, and from long-term memory, into a unitary episodic representation. Conscious awareness is assumed to be the principal mode of retrieval from the buffer. The revised model differs from the old principally in focussing attention on the processes of integrating information, rather than on the isolation of the subsystems. In doing so, it provides a better basis for tackling the more complex aspects of executive control in working memory."
27,2,7873,1,{L}ow-{L}evel {C}omponents of {A}nalytic {A}ctivity in {I}nformation {V}isualization,"Existing system-level taxonomies of visualization tasks are geared more towards the design of particular representations than the facilitation of user analytic activity. We present a set of ten low-level analysis tasks that largely capture peopleâs activities while employing information visualization tools for understanding data. To help develop these tasks, we collected nearly 200 sample questions from students about how they would analyze five particular data sets from different domains. The questions, while not being totally comprehensive, illustrated the sheer variety of analytic questions typically posed by users when employing information visualization systems. We hope that the presented set of tasks is useful for information visualization system designers as a kind of common substrate to discuss the relative analytic capabilities of the systems. Further, the tasks may provide a form of checklist for system designers."
28,2,9091,1,Loss of recent memory after bilateral hippocampal lesions,"Bilateral medial temporal lobe resection in man results in a persistent impairment of recent memory whenever the removal is carried far enough posteriorly to damage portions of the anterior hippocampus and hippocampal gyrus. This conclusion is based on formal psychological testing of nine cases (eight psychotic and one epileptic) carried out from one and one-half to four years after operation. The degree of memory loss appears to depend on the extent of hippocampal removal. In two cases in which bilateral resection was carried to a distance of 8 cm posterior to the temporal tips the loss was particularly severe. Removal of only the uncus and amygdala bilaterally does not appear to cause memory impairment. A case of unilateral inferior temporal lobectomy with radical posterior extension to include the major portion of the hippocampus and hippocampal gyrus showed no lasting memory loss. This is consistent with Milner and Penfield's negative findings in a long series of unilateral removals for temporal lobe epilepsy. The memory loss in these cases of medial temporal lobe excision involved both anterograde and some retrograde amnesia, but left early memories and technical skills intact. There was no deterioration in personality or general intelligence, and no complex perceptual disturbance such as is seen after a more complete bilateral temporal lobectomy. It is concluded that the anterior hippocampus and hippocampal gyrus, either separately or together, are critically concerned in the retention of current experience. It is not known whether the amygdala plays any part in this mechanismi, since the hippocampal complex has not been removed alone, but always together with uncus and amygdala."
29,3,893,1,A cookbook for using the model-view controller user interface paradigm in Smalltalk-80,"This essay describes the Model-View-Controller (MVC) programming paradigm and methodology used in the Smalltalk-80TM programming system. MVC programming is the application of a three-way factoring, whereby objects of different classes take over the operations related to the application domain, the display of the application's state, and the user interaction with the model and the view. We present several extended examples of MVC implementations and of the layout of composite application views. The Appendices provide reference materials for the Smalltalk-80 programmer wishing to understand and use MVC better within the Smalltalk-80 system."
30,3,896,1,Object-oriented application frameworks,"Computing power and network bandwidth have increased dramatically over the past decade, yet the design and implementation of complex software remain expensive and error-prone. Much of the cost and effort stems from the continuous rediscovery and reinvention of core concepts and components across the software industry. In particular, the growing heterogeneity of hardware architectures and diversity of operating system and communication platforms make it difficult to build correct, portable, efficient, and inexpensive applications from scratch."
31,4,4036,1,Crystal structure of the ribosome at 5.5 A resolution.,"We describe the crystal structure of the complete Thermus thermophilus 70S ribosome containing bound messenger RNA and transfer RNAs (tRNAs) at 5.5 angstrom resolution. All of the 16S, 23S, and 5S ribosomal RNA (rRNA) chains, the A-, P-, and E-site tRNAs, and most of the ribosomal proteins can be fitted to the electron density map. The core of the interface between the 30S small subunit and the 50S large subunit, where the tRNA substrates are bound, is dominated by RNA, with proteins located mainly at the periphery, consistent with ribosomal function being based on rRNA. In each of the three tRNA binding sites, the ribosome contacts all of the major elements of tRNA, providing an explanation for the conservation of tRNA structure. The tRNAs are closely juxtaposed with the intersubunit bridges, in a way that suggests coupling of the 20 to 50 angstrom movements associated with tRNA translocation with intersubunit movement."
32,4,13251,1,The MC-Fold and MC-Sym pipeline infers RNA structure from sequence data,"The classical {RNA} secondary structure model considers {A.U} and {G.C} {Watson-Crick} as well as {G.U} wobble base pairs. Here we substitute it for a new one, in which sets of nucleotide cyclic motifs define {RNA} structures. This model allows us to unify all base pairing energetic contributions in an effective scoring function to tackle the problem of {RNA} folding. We show how pipelining two computer algorithms based on nucleotide cyclic motifs, {MC-Fold} and {MC-Sym,} reproduces a series of experimentally determined {RNA} three-dimensional structures from the sequence. This demonstrates how crucial the consideration of all base-pairing interactions is in filling the gap between sequence and structure. We use the pipeline to define rules of precursor {microRNA} folding in double helices, despite the presence of a number of presumed mismatches and bulges, and to propose a new model of the human immunodeficiency virus-1 -1 frame-shifting element."
33,5,649,1,The part-time parliament,"Digital Equipment Corporation Recent archaeological discoveries on the island of Paxos reveal that the parliament functioned despite the peripatetic propensity of its part-time legislators. The legislators maintained consistent copies of the parliamentary record, despite their frequent forays from the chamber and the forgetfulness of their messengers. The Paxon parliamentâs protocol provides a new way of implementing the state-machine approach to the design of distributed systems."
34,5,810,1,Understanding availability,"This paper addresses a simple, yet fundamental question in the design of peer-to-peer systems: What does it mean when we say availability and how does this understanding impact the engineering of practical systems? We argue that existing measurements and models do not capture the complex time-varying nature of availability in todays peer-to-peer environments. Further, we show that unforeseen methodological shortcomings have dramatically biased previous analyses of this phenomenon. As the basis of our study, we empirically characterize the availability of a large peer-to-peer system over a period of 7 days, analyze the dependence of the underlying availability distributions, measure host turnover in the system, and discuss how these results may affect the design of high-availability peer-to-peer services."
35,5,954,1,Highly dynamic Destination-Sequenced Distance-Vector routing (DSDV) for mobile computers,"An  ad-hoc  network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize ad hoc networks. Our modifications address some of the previous  objections  to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks."
36,5,3034,1,Transactional Memory: Architectural Support for Lock-Free Data Structures,"A shared data structure is  lock-free  if its operations do not require mutual exclusion. If one process is interrupted in the middle of an operation, other processes will not be prevented from operating on that object. In highly concurrent systems, lock-free data structures avoid common problems associated with conventional locking techniques, including priority inversion, convoying, and difficulty of avoiding deadlock. This paper introduces  transactional memory , a new multiprocessor architecture intended to make lock-free synchronization as efficient (and easy to use) as conventional techniques based on mutual exclusion. Transactional memory allows programmers to define customized read-modify-write operations that apply to multiple, independently-chosen words of memory. It is implemented by straightforward extensions to any multiprocessor cache-coherence protocol. Simulation results show that transactional memory matches or outperforms the best known locking techniques for simple benchmarks, even in the absence of priority inversion, convoying, and deadlock."
37,5,5452,1,MANET simulation studies: the incredibles,"Simulation is the research tool of choice for a majority of the mobile ad hoc network (MANET) community. However, while the use of simulation has increased, the credibility of the simulation results has decreased. To determine the state of MANET simulation studies, we surveyed the 2000-2005 proceedings of the ACM International Symposium on Mobile Ad Hoc Networking and Computing (MobiHoc). From our survey, we found significant shortfalls. We present the results of our survey in this paper. We then summarize common simulation study pitfalls found in our survey. Finally, we discuss the tools available that aid the development of rigorous simulation studies. We offer these results to the community with the hope of improving the credibility of MANET simulation-based studies."
38,5,6345,1,Computer Networks,"Computer Networks, Fourth Edition is the ideal introduction to computer networks. Renowned author, educator, and researcher Andrew S. Tanenbaum has updated his classic best seller to reflect the newest technologies, including 802.11, broadband wireless, ADSL, Bluetooth, gigabit Ethernet, the Web, the wireless Web, streaming audio, IPsec, AES, quantum cryptography, and more. Using real-world examples, Tanenbaum explains how networks work on the inside, from underlying physical layer hardware up through today's most popular network applications."
39,5,7343,1,DSR The Dynamic Source Routing Protocol for Multihop Wireless Ad Hoc Networks,"The Dynamic Source Routing protocol (DSR) is a simple and efficient routing protocol designed  specifically for use in multi-hop wireless ad hoc networks of mobile nodes. DSR allows the  network to be completely self-organizing and self-configuring, without the need for any existing  network infrastructure or administration. The protocol is composed of the two mechanisms of  Route Discovery and Route Maintenance, which work together to allow nodes to discover and  maintain source routes to arbitrary destinations in the ad hoc network. The use of source routing  allows packet routing to be trivially loop-free, avoids the need for up-to-date routing information  in the intermediate nodes through which packets are forwarded, and allows nodes forwarding  or overhearing packets to cache the routing information in them for their own future use. All  aspects of the protocol operate entirely on-demand, allowing the routing packet overhead of  DSR to scale automatically to only that needed to react to changes in the routes currently in use. We have"
40,6,3293,1,Principles of Neural Science,"{Now in resplendent color, the new edition continues to define the latest in the scientific understanding of the brain, the nervous system, and human behavior. Each chapter is thoroughly revised and includes the impact of molecular biology in the mechanisms underlying developmental processes and in the pathogenesis of disease. Important features to this edition include a new chapter - Genes and Behavior; a complete updating of development of the nervous system; the genetic basis of neurological and psychiatric disease; cognitive neuroscience of perception, planning, action, motivation and memory; ion channel mechanisms; and much more.}"
41,6,7493,1,The basis of anisotropic water diffusion in the nervous system - a technical review,"Anisotropic water diffusion in neural fibres such as nerve, white matter in spinal cord, or white matter in brain forms the basis for the utilization of diffusion tensor imaging (DTI) to track fibre pathways. The fact that water diffusion is sensitive to the underlying tissue microstructure provides a unique method of assessing the orientation and integrity of these neural fibres, which may be useful in assessing a number of neurological disorders. The purpose of this review is to characterize the relationship of nuclear magnetic resonance measurements of water diffusion and its anisotropy (i.e. directional dependence) with the underlying microstructure of neural fibres. The emphasis of the review will be on model neurological systems both in vitro and in vivo. A systematic discussion of the possible sources of anisotropy and their evaluation will be presented followed by an overview of various studies of restricted diffusion and compartmentation as they relate to anisotropy. Pertinent pathological models, developmental studies and theoretical analyses provide further insight into the basis of anisotropic diffusion and its potential utility in the nervous system. Copyright Â© 2002 John Wiley & Sons, Ltd."
42,6,8645,1,The {R}ician distribution of noisy {MRI} data,"The image intensity in magnetic resonance magnitude images in the presence of noise is shown to be governed by a Rician distribution. Low signal intensities (SNR &lt; 2) are therefore biased due to the noise. it is shown how the underlying noise can be estimated from the images and a simple correction scheme is provided to reduce the bias. the noise characteristics in phase images are also studied and shown to be very different from those of the magnitude images. Common to both, however, is that the noise distributions are nearly Gaussian for SNR larger than two."
43,7,5385,1,Inferring DomainâDomain Interactions From ProteinâProtein Interactions,"10.1101/gr.153002 The interaction between proteins is one of the most important features of protein functions. Behind proteinâprotein interactions there are protein domains interacting physically with one another to perform the necessary functions. Therefore, understanding protein interactions at the domain level gives a global view of the protein interaction network, and possibly of protein functions. Two research groups used yeast two-hybrid assays to generate 5719 interactions between proteins of the yeast . This allows us to study the large-scale conserved patterns of interactions between protein domains. Using evolutionarily conserved domains defined in a proteinâdomain database called PFAM (), we apply a Maximum Likelihood Estimation method to infer interacting domains that are consistent with the observed proteinâprotein interactions. We estimate the probabilities of interactions between every pair of domains and measure the accuracies of our predictions at the protein level. Using the inferred domainâdomain interactions, we predict interactions between proteins. Our predicted proteinâprotein interactions have a significant overlap with the proteinâprotein interactions (MIPS: ) obtained by methods other than the two-hybrid assays. The mean correlation coefficient of the gene expression profiles for our predicted interaction pairs is significantly higher than that for random pairs. Our method has shown robustness in analyzing incomplete data sets and dealing with various experimental errors. We found several novel proteinâprotein interactions such as RPS0A interacting with APG17 and TAF40 interacting with SPT3, which are consistent with the functions of the proteins. [Supplementary material is available online at  and.]"
44,7,9423,1,The Many Faces of Protein&#8211;Protein Interactions: A Compendium of Interface Geometry,"A systematic classification of protein&#8211;protein interfaces is a valuable resource for understanding the principles of molecular recognition and for modelling protein complexes. Here, we present a classification of domain interfaces according to their geometry. Our new algorithm uses a hybrid approach of both sequential and structural features. The accuracy is evaluated on a hand-curated dataset of 416 interfaces. Our hybrid procedure achieves 83&#37; precision and 95&#37; recall, which improves the earlier sequence-based method by 5&#37; on both terms. We classify virtually all domain interfaces of known structure, which results in nearly 6,000 distinct types of interfaces. In 40&#37; of the cases, the interacting domain families associate in multiple orientations, suggesting that all the possible binding orientations need to be explored for modelling multidomain proteins and protein complexes. In general, hub proteins are shown to use distinct surface regions (multiple faces) for interactions with different partners. Our classification provides a convenient framework to query genuine gene fusion, which conserves binding orientation in both fused and separate forms. The result suggests that the binding orientations are not conserved in at least one-third of the gene fusion cases detected by a conventional sequence similarity search. We show that any evolutionary analysis on interfaces can be skewed by multiple binding orientations and multiple interaction partners. The taxonomic distribution of interface types suggests that ancient interfaces common to the three major kingdoms of life are enriched by symmetric homodimers. The classification results are online at http://www.scoppi.org."
45,8,2216,1,A Tutorial on Principal Component Analysis,"This tutorial is designed to give the reader an understanding of Principal Components Analysis (PCA). PCA is a useful statistical technique that has found application in fields such as face recognition and image compression, and is a common technique for finding patterns in data of high dimension. Before getting to a description of PCA, this tutorial first introduces mathematical concepts that will be used in PCA. It covers standard deviation, covariance, eigenvec- tors and eigenvalues. This background knowledge is meant to make the PCA section very straightforward, but can be skipped if the concepts are already familiar. There are examples all the way through this tutorial that are meant to illustrate the concepts being discussed. If further information is required, the mathematics textbook âElementary Linear Algebra 5eâ by Howard Anton, Publisher John Wiley & Sons Inc, ISBN 0-471-85223-6 is a good source of information regarding the mathematical back- ground"
46,8,2462,1,Inside PageRank,"Although the interest of a Web page is strictly related to its content and to the subjective readers' cultural background, a measure of the page authority can be provided that only depends on the topological structure of the Web. PageRank is a noticeable way to attach a score to Web pages on the basis of the Web connectivity. In this article, we look inside PageRank to disclose its fundamental properties concerning stability, complexity of computational scheme, and critical role of parameters involved in the computation. Moreover, we introduce a circuit analysis that allows us to understand the distribution of the page score, the way different Web communities interact each other, the role of dangling pages (pages with no outlinks), and the secrets for promotion of Web pages."
47,8,4833,1,The Geometry of Graphs and some of its Algorithmic Applications,"In this paper we explore some implications of viewing graphs as geometric objects. This approach offers a new perspective on a number of graph--theoretic and algorithmic problems. There are several ways to model graphs geometrically and our main concern here is with geometric representations that respect the metric of the (possibly weighted) graph. Given a graph G we map its vertices to a normed space in an attempt to (i) keep down the dimension of the host space, and (ii) guarantee a small distortion, i.e., make sure that distances between vertices in G closely match the distances between their geometric images. In this paper we develop efficient algorithms for embedding graphs low--dimensionally with a small distortion. Further algorithmic applications include:  ffl A simple, unified approach to a number of problems on multicommodity flows, including the Leighton--Rao Theorem [37] and some of its extensions. We solve an open question in this area, showing that the max--flow vs. min--..."
48,8,5769,1,Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time,"We introduce the  smoothed analysis of algorithms , which continuously interpolates between the worst-case and average-case analyses of algorithms. In smoothed analysis, we measure the maximum over inputs of the expected performance of an algorithm under small random perturbations of that input. We measure this performance in terms of both the input size and the magnitude of the perturbations. We show that the simplex algorithm has  smoothed complexity  polynomial in the input size and the standard deviation of Gaussian perturbations."
49,8,6365,1,The Price of Stability for Network Design with Fair Cost Allocation,"Network design is a fundamental problem for which it is important to understand the effects of strategic behavior. Given a collection of selfinterested agents who want to form a network connecting certain endpoints, the set of stable solutions --- the Nash equilibria --- may look quite different from the centrally enforced optimum. We study the quality of the best Nash equilibrium, and refer to the ratio of its cost to the optimum network cost as the price of stability. The best Nash equilibrium solution has a natural meaning of stability in this context --- it is the optimal solution that can be proposed from which no user will ""defect"". We consider the price of stability for network design with respect to one of the most widely-studied protocols for network cost allocation, in which the cost of each edge is divided equally between users whose connections make use of it; this fair-division scheme can be derived from the Shapley value, and has a number of basic economic motivations. We show that the price of stability for network design with respect to this fair cost allocation is $O(log k)$, where $k$ is the number of users, and that a good Nash equilibrium can be achieved via best-response dynamics in which users iteratively defect from a starting solution. This establishes that the fair cost allocation protocol is in fact a useful mechanism for inducing strategic behavior to form near-optimal equilibria. We discuss connections to the class of potential games defined by Monderer and Shapley, and extend our results to cases in which users are seeking to balance network design costs with latencies in the constructed network, with stronger results when the network has only delays and no construction costs. We also present bounds on the convergence time of best-response dynamics, and discuss extensions to a weighted game."
50,8,8588,1,Algebraic Topology and Distributed Computing: A Primer,". Models and techniques borrowed from classical algebraic topology have recently yielded a variety of new lower bounds and impossibility results for distributed and concurrent computation. This paper explains the basic concepts underlying this approach, and shows how they apply to a simple distributed problem. 1 Introduction  The problem of coordinating concurrent processes remains one of the central problems of distributed computing. Coordination problems arise at all scales in distributed and ..."
51,8,9056,1,Discrete mathematics: methods and challenges,"Combinatorics is a fundamental mathematical discipline as well as an essential component of many mathematical areas, and its study has experienced an impressive growth in recent years. One of the main reasons for this growth is the tight connection between Discrete Mathematics and Theoretical Computer Science, and the rapid development of the latter. While in the past many of the basic combinatorial results were obtained mainly by ingenuity and detailed reasoning, the modern theory has grown out of this early stage, and often relies on deep, well developed tools. This is a survey of two of the main general techniques that played a crucial role in the development of modern combinatorics; algebraic methods and probabilistic methods. Both will be illustrated by examples, focusing on the basic ideas and the connection to other areas."
52,9,747,1,The Strength of Weak Ties,"Analysis of social networks is suggested as a tool for linking micro and macro levels of sociological theory. The procedure is illustrated by elaboration of the macro implications of one aspect of small-scale interaction: the strength of dyadic ties. It is argued that the degree of overlap of two individuals' friendship networks varies directly with the strength of their tie to one another. The impact of this principle on diffusion of influence and information, mobility opportunity, and community organization is explored. Stress is laid on the cohesive power of weak ties. Most network models deal, implicitly, with strong ties, thus confining their applicability to small, well-defined groups. Emphasis on weak ties lends itself to discussion of relations between groups and to analysis of segments of social structure not easily defined in terms of primary groups."
53,9,2096,1,Basics of qualitative research: Grounded theory procedures and techniques,"Since the `discovery' of grounded theory in 1969, it has become one of the major strategies for qualitative research available to the social scientist and professional. But only now, with the publication of **Basics of Qualitative Research **do Strauss, co-creator of grounded theory, and Corbin present the practical procedures and techniques for doing grounded theory studies at a level easily understood by students in applied disciplines.  Written in accessible language and replete with definitions and examples, **Basics of Grounded Theory **provides a step-by-step approach to doing research from formulation of the initial research question, through various systems of coding and analysis, to the process of writing or speaking on the research topic. A final chapter containing standards for evaluating a grounded theory study will establish the benchmark for this kind of work. This volume is an invaluable tool for the novice researcher and a useful text for courses in qualitative research in social science programmes and for professionals engaged in research."
54,9,3888,1,"Power, politics, and MIS implementation","Theories of resistance to management information systems {(MIS)} are important because they guide the implementation strategies and tactics chosen by implementors. Three basic theories of the causes of resistance underlie many prescriptions and rules for {MIS} implementation. Simply stated, people resist {MIS} because of their own internal factors, because of poor system design, and because of the interaction of specific system design features with aspects of the organizational context of system use. These theories differ in their basic assumptions about systems, organizations, and resistance; they also differ in predictions that can be derived from them and in their implications for the implementation process. These differences are described and the task of evaluating the theories on the bases of the differences is begun. Data from a case study are used to illustrate the theories and to demonstrate the superiority, for implementors, of the interaction theory."
55,9,7591,1,A review of outsourcing from the resource-based view of the firm,"The phenomenon of outsourcing is becoming increasingly widespread among organizations and is now one of the strategic decisions that attract the greatest interest from professionals and organizational scholars. The primary purpose of the paper is to contribute with a review of the principal works that address outsourcing from the resource-based view of the firm (RBV). The paper begins by setting out the main premises of outsourcing and then presents the different concepts of outsourcing and proposes a concept that is more in line with the theoretical framework used. This is followed by an analysis of the principal differences and similarities of the treatments of outsourcing from the traditional perspective of the transaction costs economics theory (TCE) and from the more strategic and up-to-date RBV. The next section contains a review of the most significant theoretical and empirical works on outsourcing that address outsourcing from the RBV. The contributions are classified into two categories, depending on the objectives: works that study the propensity to outsource and works that study the relationship between the outsourcing decision and organizational performance. Finally, a framework is proposed that is based on the resource and capability view with the aim of contributing to a better understanding of outsourcing and facilitating future empirical works from the RBV that are complementary and examine issues of greater interest that have been less developed in the literature to date."
56,9,8287,1,"Organizations: Rational, Natural, and Open Systems","{<P><B></B> This broad, balanced introduction to organizational studies enables the reader to compare and contrast different approaches to the study of organizations. This book is a valuable tool for the reader, as we are all intertwined with organizations in one form or another. Numerous other disciplines besides sociology are addressed in this book, including economics, political science, strategy and management theory. <B></B> Topic areas discussed in this book are the importance of organizations; defining organizations; organizations as rational, natural, and open systems; environments, strategies, and structures of organizations; and organizations and society. <B></B> For those employed in fields where knowledge of organizational theory is necessary, including sociology, anthropology, cognitive psychology, industrial engineering, managers in corporations and international business, and business strategists.  </P>}"
57,9,10385,1,The Economics of Organization: The Transaction Cost Approach,"The transaction cost approach to the study of economic organization regards the transaction as the basic unit of analysis and holds that an understanding of transaction cost economizing is central to the study of organizations. Applications of this approach require that transactions be dimensionalized and that alternative governance structures be described. Economizing is accomplished by assigning transactions to governance structures in a discriminating way. The approach applies both to the determination of efficient boundaries, as between firms and markets, and to the organization of internal transactions, including the design of employment relations. The approach is compared and contrasted with selected parts of the organization theory literature."
58,9,14003,1,Building theories from case study research,"This paper describes the process of inducting theory using case studies-from specifying the research questions to reaching closure. Some features of the process, such as problem definition and construct validation, are similar to hypothesis-testing research. Others, such as within-case analysis and replication logic, are unique to the inductive, case-oriented process Overall, the process described here is highly iterative and tightly linked to data. This research approach is especially appropriate in new topic areas. The resultant theory is often novel, testable, and empirically valid finally, framebreaking insights, the tests of good theory (e.g. parsimony, logical coherence), and convincing grounding in the evidence are the key criteria for evaluating this type of research. ABSTRACT FROM AUTHOR Copyright of Academy of Management Review is the property of Academy of Management and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright"
59,10,1127,1,STRIPS: A new approach to the application of theorem proving to problem solving,We describe a new problem solver called STRIPS that attempts to find a sequence of operators in a space of world models to transform a given initial world model in which a given goal formula can be proven to be true. STRIPS represents a world model as an arbitrary collection in first-order predicate calculus formulas and is designed to work with models consisting of large numbers of formula. It employs a resolution theorem prover to answer questions of particular models and uses means-ends analysis to guide it to the desired goal-satisfying model.
60,10,1239,1,OWL-S: Semantic Markup for Web Services,"The Semantic Web should enable greater access not only to content but also to services on the Web. Users and software agents should be able to discover, invoke, compose, and monitor Web resources offering particular services and having particular properties, and should be able to do so with a high degree of automation if desired. Powerful tools should be enabled by service descriptions, across the Web service lifecycle. OWL-S (formerly DAML-S) is an ontology of services that makes these functionalities possible. In this submission we describe the overall structure of the ontology and its three main parts: the service profile for advertising and discovering services; the process model, which gives a detailed description of a service's operation; and the grounding, which provides details on how to interoperate with a service, via messages.  Following the layered approach to markup language development, the current version of OWL-S builds on the Ontology Web Language (OWL) Recommendation produced by theWeb-Ontology Working Group at the World Wide Web Consortium"
61,10,2915,1,The Computer for the 21st Century,"The arcane aura that surrounds personal computers is not just a `user interface' problem. The idea of a `personal' computer itself is misplaced and that the vision of laptop machines, dynabooks and knowledge navigators is only a transitional step toward achieving the real potential of information technology. Such machines cannot truly make computing an integral, invisible part of people's lives. The author and his colleagues are therefore trying to conceive a new way of thinking about computers, one that takes into account the human world and allows the computers themselves to vanish into the background."
62,10,2993,1,Ontology evolution: Not the same as schema evolution,"As ontology development becomes a more ubiquitous and collaborative process, ontology versioning and evolution becomes an important area of ontology research. The many similarities between database-schema evolution and ontology evolution will allow us to build on the extensive research in schema evolution. However, there are also important differences between database schemas and ontologies. The differences stem from different usage paradigms, the presence of explicit semantics and different knowledge models. A lot of problems that existed only in theory in database research come to the forefront as practical problems in ontology evolution. These differences have important implications for the development of ontology-evolution frameworks: The traditional distinction between versioning and evolution is not applicable to ontologies. There are several dimensions along which compatibility between versions must be considered. The set of change operations for ontologies is different. We must develop automatic techniques for finding similarities and differences between versions."
63,10,5041,1,Current Solutions for Web Service Composition,"Web service composition lets developers create applications on top of service-oriented computing's native description, discovery, and communication capabilities. Such applications are rapidly deployable and offer developers reuse possibilities and users seamless access to a variety of complex services.  There are many existing approaches to service composition, ranging from abstract methods to those aiming to be industry standards.  The authors describe four key issues for Web service composition."
64,10,10713,1,The Java Context Awareness Framework (JCAF) â A Service Infrastructure and Programming Framework for Context-Aware Applications,"Context-awareness is a key concept in ubiquitous computing. But to avoid developing dedicated context-awareness sub-systems for specific application areas there is a need for more generic programming frameworks. Such frameworks can help the programmer develop and deploy context-aware applications faster. This paper describes the Java Context-Awareness Framework â JCAF, which is a Java-based context-awareness infrastructure and programming API for creating context-aware computer applications. The paper presents the design goals of JCAF, its runtime architecture, and its programming model. The paper presents some applications of using JCAF in three different applications and discusses lessons learned from using JCAF."
65,10,11028,1,A Survey of Automated Web Service Composition Methods,"In todayâs Web, Web services are created and updated on the fly. Itâs already beyond the human ability to analysis them and generate the composition plan manually. A number of approaches have been proposed to tackle that problem. Most of them are inspired by the researches in cross-enterprise workflow and AI planning. This paper gives an overview of recent research efforts of automatic Web service composition both from the workflow and AI planning research community."
66,11,1042,1,No {S}ilver {B}ullet: {E}ssence and {A}ccidents of {S}oftware {E}ngineering,"Of all the monsters that fill the nightmares of our folklore, none terrify more than werewolves, because they transform unexpectedly from the familiar into horrors. For these, one seeks bullets of silver that can magically lay them to rest. The familiar software project, at least as seen by the nontechnical manager, has something of this character; it is usually innocent and straightforward, but is capable of becoming a monster of missed schedules, blown budgets, and flawed products. So we hear desperate cries for a silver bullet--something to make software costs drop as rapidly as computer hardware costs do. But, as we look to the horizon of a decade hence, we see no silver bullet. There is no single development, in either technology or in management technique, that by itself promises even one order-of-magnitude improvement in productivity, in reliability, in simplicity. In this article, I shall try to show why, by examining both the nature of the software problem and the properties of the bullets proposed. Skepticism is not pessimism, however. Although we see no startling breakthroughs--and indeed, I believe such to be inconsistent with the nature of software--many encouraging"
67,11,1756,1,Intelligence without representation,"Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, with strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outline our approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not into independent information processing units which must interface with each other via representations. Instead, the intelligent system is decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather than interface to each other particularly much. The notions of central and peripheral systems evaporate--everything is both central and peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in standard office environments."
68,11,2452,1,The Active Badge Location System,"A novel system for the location of people in an office environment is described. Members of staff wear badges that transmit signals providing information about their location to a centralized location service, through a network of sensors. The paper also examines alternative location techniques, system design issues and applications, particularly relating to telephone call routing. Location systems raise concerns about the privacy of an individual and these issues are also addressed."
69,11,2769,1,"Charting past, present, and future research in ubiquitous computing","The proliferation of computing into the physical world promises more than the ubiquitous availability of computing infrastructure; it suggest new paradigms of interaction inspired by constant access to information and computational capabilities. For the past decade, application-driven research on abiquitous computing (ubicomp) has pushed three interaction themes: natural interfaces, context-aware applications, and automated capture and access . To chart a course for future research in ubiquitous computing, we review the accomplishments of these efforts and point to remaining research challenges. Research in ubiquitious computing implicitly requires addressing some notion of scale, whether in the number and type of devices, the physical space of    distributed computing, or the number of people using a system. We posit a new area of applications research,  everyday computing,  focussed on scaling interaction with respect to time. Just as pushing the availiability of computing away from the traditional desktop fundamentally changes the relationship between humans and computers, providing  continuous interaction moves computing from a localized tool to a constant companion. Designing for continous interaction requires addressing interruption and reumption of intreaction, representing passages of time and providing associative storage models. Inherent in all of these interaction themes are difficult issues in the  social implications  of ubiquitous computing and the challenges of     evaluating>  ubiquitious computing research. Although cumulative experience points to lessons in privacy, security, visibility, and control, there are no simple guidelines for steering research efforts. Akin to any efforts involving new technologies, evaluation strategies form a spectrum from technology feasibility efforts to long-term use studies&mdash;but a user-centric perspective is always possible and necessary"
70,11,3077,1,A middleware infrastructure for active spaces,"The authors present an experimental middleware infrastructure called Gaia, which they have used to prototype the resource management of?and to provide the user-oriented interfaces for-physical spaces populated with network-enabled computing resources. The authors focus on physical spaces used for teaching; classrooms, offices, and lecture rooms. The system described is derived from a series of experiments starting in 1996. The authors show how, by applying the concepts of a conventional operating system to middleware, they can manage the resources, devices, and distributed objects in a room, building, or physical space, how a distributed extension of the model-view-controller that is use in personal computers simplifies and structures practical applications for these environments, and how, by driving context-sensitivity into its data storage mechanisms, the system can help satisfy the requirements for user-centricity and mobility."
71,11,4013,1,Towards a taxonomy of software connectors,"Software systems of today are frequently composed from prefabricated, heterogeneous components that provide complex functionality and engage in complex interactions. Existing research on component-based development has mostly focused on component structure, interfaces, and functionality. Recently, software architecture has emerged as an area that also places significant importance on component interactions, embodied in the notion of software connectors. However, the current level of understanding and support for connectors has been insufficient. This has resulted in their inconsistent treatment and a notable lack of understanding of what the fundamental building blocks of software interaction are and how they can be composed into more complex interactions. This paper attempts to address this problem. It presents a comprehensive classification framework and taxonomy of software connectors. The taxonomy is obtained through an extensive analysis of existing component interactions. The taxonomy is used both to understand existing software connectors and to suggest new, unprecedented connectors. We demonstrate the use of the taxonomy on the architecture of a large, existing system."
72,11,4552,1,Towards architecture-based self-healing systems,"Our approach to creating self-healing systems is based on software architecture, where repairs are done at the level of a software system's components and connectors. In our approach, event-based software architectures are targeted because they offer significant benefits for run-time adaptation. Before an automated planning agent can decide how to repair a self-healing system, a significant infrastructure must be in place to support making the planned repair. Specifically, the self-healing system must be built using a framework that allows for run-time adaptation, there must be a language in which to express the repair plan, and there must be a reconfiguration agent that can execute the repair plan once it is created. In this paper, we present tools and methods that implement these infrastructure elements in the context of an overall architecture-based vision for building self-healing systems. The paper concludes with a gap analysis of our current infrastructure vs. the overall vision, and our plans for fulfilling that vision."
73,11,4729,1,Making sense of sensing systems: five questions for designers and researchers,"This paper borrows ideas from social science to inform the design of novel ""sensing"" user-interfaces for computing technology. Specifically, we present five design challenges inspired by analysis of human-human communication that are mundanely addressed by traditional graphical user interface designs (GUIs). Although classic GUI conventions allow us to finesse these questions, recent research into innovative interaction techniques such as 'Ubiquitous Computing' and 'Tangible Interfaces' has begun to expose the interaction challenges and problems they pose. By making them explicit we open a discourse on how an approach similar to that used by social scientists in studying human-human interaction might inform the design of novel interaction mechanisms that can be used to handle human-computer communication accomplishments"
74,11,5002,1,"Tangible Bits: Towards Seamless Interfaces between People, Bits and Atoms","{T}his paper presents our vision of {H}uman {C}omputer {I}nteraction ({H}{C}{I}): ""{T}angible {B}its"". {T}angible {B}its allows users to ""grasp & manipulate"" bits in the center of users' attention by coupling the bits with everyday physical objects and architectural surfaces. {T}angilble {B}its also enables users to be aware of background bits at the periphery of human perception using ambient display media such as light, sound, airflow, and water movement in an augmented space. {T}he goal of {T}angible {B}its is to bridge the gaps between both cyberspace and the physical environment, as well as foreground and background of human activities. {T}his paper describes three key concepts of {T}angible {B}its: interactive surfaces; the coupling of bits and graspable physical objects; and ambient media for background awareness. {W}e illustrate these concepts with three protype systems - the meta{D}{E}{S}{K}, trans{B}{O}{A}{R}{D}, and ambient{R}{O}{O}{M} - to identify underlying research issues."
75,11,7319,1,Architecting for Usability; a Survey,"Over the years the software engineering community has increasingly realized the important roles oftware architecture plays in fulfilling the quality requirements of a system. The quality attributes of a software system are, to a large extent determined by the system&#039;s software architecture. In recent years, the software engineering community has developed various tools and techniques that allow for design for quality attributes, such as performance or maintainability, at the software architecture level. We believe this design approach can be applied not only to &#034;traditional&#034; quality attributes such as performance or maintainability but also to usability. This survey explores the feasibility of such a design approach. Current practice is surveyed from the perspective of a software architect. Are there any design methods that allow for design for usability at the architectural level? Are there any evaluation tools that allow assessment of architectures for their support of usability? What is usability? A framework is presented which visualizes these three research questions. Usability should drive design at all stages, but current usability engineering practice fails to fully achieve this goal. Our survey shows that there are no design techniques or assessment tools that allow for design for usability at the architectural level."
76,11,9025,1,Autonomic Live Adaptation of Virtual Computational Environments in a Multi-Domain Infrastructure,"A shared distributed infrastructure is formed by federating computation resources from multiple domains. Such shared infrastructures are increasing in popularity and are providing massive amounts of aggregated computation resources to large numbers of users. Meanwhile, virtualization technologies, at machine and network levels, are maturing and enabling mutually isolated virtual computation environments for executing arbitrary parallel/distributed applications on top of such a shared physical infrastructure. In this paper, we go one step further by supporting autonomic adaptation of virtual computation environments as active, integrated entities. More specifically, driven by both dynamic availability of infrastructure resources and dynamic application resource demand, a virtual computation environment is able to automatically relocate itself across the infrastructure and scale its share of infrastructural resources. Such autonomic adaptation is transparent to both users of virtual environments and administrators of infrastructures, maintaining the look and feel of a stable, dedicated environment for the user. As our proofof-concept, we present the design, implementation, and evaluation of a system called VIOLIN, which is composed of a virtual network of virtual machines capable of live migration across a multi-domain physical infrastructure. 1"
77,11,11176,1,{The Architecture of Complexity},"A number of proposals have been advanced in recent years for the development of âgeneral systems theory â that, abstracting from properties peculiar to physical, biological, or social systems, would be applicable to all of them. 1 We might well feel that, while the goal is laudable, systems of such diverse kinds could hardly be expected to have any nontrivial properties in common. Metaphor and analogy can be helpful, or they can be misleading. All depends on whether the similarities the metaphor captures are significant or superficial. It may not be entirely vain, however, to search for common properties among diverse kinds of complex systems. The ideas that go by the name of cybernetics constitute, if not a theory, at least a point of view that has been proving fruitful over a wide range of applications. 2 It has been useful to look at the behavior of adaptive systems in terms of the concepts of feedback and homeostasis, and to analyze adaptiveness in terms of the theory of selective information. 3 The ideas of feedback and information provide a frame of reference for viewing a wide range of situations, just as do the ideas of evolution, of relativism, of axiomatic method, and of"
78,11,11210,1,Human-Machine Reconfigurations: Plans and Situated Actions,"{This book considers how agencies are currently figured at the human-machine interface, and how they might be imaginatively and materially reconfigured. Contrary to the apparent enlivening of objects promised by the sciences of the artificial, the author proposes that the rhetorics and practices of those sciences work to obscure the performative nature of both persons and things. The question then shifts from debates over the status of human-like machines, to that of how humans and machines are enacted as similar or different in practice, and with what theoretical, practical and political consequences. Drawing on recent scholarship across the social sciences, humanities and computing, the author argues for research aimed at tracing the differences within specific sociomaterial arrangements without resorting to essentialist divides. This requires expanding our unit of analysis, while recognizing the inevitable cuts or boundaries through which technological systems are constituted.}"
79,11,14288,1,"A survey of autonomic computing---degrees, models, and applications","Autonomic Computing is a concept that brings together many fields of computing with the purpose of creating computing systems that self-manage. In its early days it was criticised as being a &ldquo;hype topic&rdquo; or a rebadging of some Multi Agent Systems work. In this survey, we hope to show that this was not indeed &lsquo;hype&rsquo; and that, though it draws on much work already carried out by the Computer Science and Control communities, its innovation is strong and lies in its robust application to the specific self-management of computing systems. To this end, we first provide an introduction to the motivation and concepts of autonomic computing and describe some research that has been seen as seminal in influencing a large proportion of early work. Taking the components of an established reference model in turn, we discuss the works that have provided significant contributions to that area. We then look at larger scaled systems that compose autonomic systems illustrating the hierarchical nature of their architectures. Autonomicity is not a well defined subject and as such different systems adhere to different degrees of Autonomicity, therefore we cross-slice the body of work in terms of these degrees. From this we list the key applications of autonomic computing and discuss the research work that is missing and what we believe the community should be considering."
80,12,1481,1,Balanced inhibition underlies tuning and sharpens spike timing in auditory cortex,"Neurons in the primary auditory cortex are tuned to the intensity and specific frequencies of sounds, but the synaptic mechanisms underlying this tuning remain uncertain. Inhibition seems to have a functional role in the formation of cortical receptive fields, because stimuli often suppress similar or neighbouring responses1, 2, 3, and pharmacological blockade of inhibition broadens tuning curves4, 5. Here we use whole-cell recordings in vivo to disentangle the roles of excitatory and inhibitory activity in the tone-evoked responses of single neurons in the auditory cortex. The excitatory and inhibitory receptive fields cover almost exactly the same areas, in contrast to the predictions of classical lateral inhibition models. Thus, although inhibition is typically as strong as excitation, it is not necessary to establish tuning, even in the receptive field surround. However, inhibition and excitation occurred in a precise and stereotyped temporal sequence: an initial barrage of excitatory input was rapidly quenched by inhibition, truncating the spiking response within a few (1â4) milliseconds. Balanced inhibition might thus serve to increase the temporal precision6 and thereby reduce the randomness of cortical operation, rather than to increase noise as has been proposed previously7."
81,12,1539,1,Gene ontology: Tool for the unification of biology,"Genomic sequencing has made it clear that a large fraction of the genes specifying the core biological functions are shared by all eukaryotes. Knowledge of the biological role of such shared proteins in one organism can often be transferred to other organisms. The goal of the Gene Ontology Consortium is to produce a dynamic, controlled vocabulary that can be applied to all eukaryotes even as knowledge of gene and protein roles in cells is accumulating and changing. To this end, three independent ontologies accessible on the World-Wide Web (http://www.geneontology.org) are being constructed: biological process, molecular function and cellular component."
82,12,3308,1,Duty to disclose in medical genetics: a legal perspective.,"As technical knowledge and public information in medical genetics continue to expand, the geneticist may expect to be held responsible for informing patients and clients about new developments in research and diagnosis. The long legal evolution of the physician's duty to disclose, and more recent findings of a physician's duty to recall former patients to inform them about newly discovered risks of treatment, indicate that medical geneticists may have a duty to disclose both current and future information about conditions that are or could be inherited. Recent case law supports findings of professional liability for both present and future disclosure, even in the absence of an active physician-patient relationship. The requirement of candid and complete disclosure will affect the counseling approach in testing for deleterious genes and in providing medical treatment for minors with hereditary diseases. Finding a duty to recall may impose further professional burdens on the geneticist to reach beyond the immediate counseling arena and to recontact patients, perhaps years after their initial visit to genetics clinic."
83,12,3628,1,CSCW at play: <i>'there'</i> as a collaborative virtual environment,"Video games are of increasing importance, both as a cultural phenomenon and as an application of collaborative technology. In particular, many recent online games feature persistent collaborative virtual environments (CVEs), with complex social organisation and strong social bonds between players. This paper presents a study of &#60;i>'There'&#60;/i>, one such game, focusing on how &#60;i>There&#60;/i> has been appropriated by its players. In particular we describe how its flexibility has allowed players to develop their own forms of play within the game. Three aspects of &#60;i>There&#60;/i> are discussed: first, how the environment supports a range of social activities around objects. Second, how the chat environment is used to produce overlapping chat and how the game itself provides topics for conversation. Lastly, how the 'place' of &#60;i>There&#60;/i> is a fluid interaction space that supports safe interactions between strangers. The paper concludes by drawing design lessons concerning the importance of supporting shared online activity, interaction between strangers, and the difficulties of designing for play."
84,12,4409,1,Role of Delays in Shaping Spatiotemporal Dynamics of Neuronal Activity in Large Networks,We study the effect of delays on the dynamics of large networks of neurons. We show that delays give rise to a wealth of bifurcations and to a rich phase diagram; which includes oscillatory bumps; traveling waves; lurching waves; standing waves arising via a period-doubling bifurcation; aperiodic regimes; and regimes of multistability. We study the existence and the stability of the various dynamical patterns analytically and numerically in a simplified rate model as a function of the interaction parameters. The results derived in that framework allow us to understand the origin of the diversity of dynamical states observed in large networks of spiking neurons.
85,12,4980,1,Integration of Touch and Sound in Auditory Cortex,"SummaryTo form a coherent percept of the environment, our brain combines information from different senses. Such multisensory integration occurs in higher association cortices; but supposedly, it also occurs in early sensory areas. Confirming the latter hypothesis, we unequivocally demonstrate supra-additive integration of touch and sound stimulation at the second stage of the auditory cortex. Using high-resolution fMRI of the macaque monkey, we quantified the integration of auditory broad-band noise and tactile stimulation of hand and foot in anaesthetized animals. Integration was found posterior to and along the lateral side of the primary auditory cortex in the caudal auditory belt. Integration was stronger for temporally coincident stimuli and obeyed the principle of inverse effectiveness: greater enhancement for less effective stimuli. These findings demonstrates that multisensory integration occurs early and close to primary sensory areas and--because it occurs in anaesthetized animals--suggests that this integration is mediated by preattentive bottom-up mechanisms."
86,12,6783,1,Agent-organized networks for dynamic team formation,"Many multi-agent systems consist of a complex network of autonomous yet interdependent agents. Examples of such networked multi-agent systems include supply chains and sensor networks. In these systems, agents have a select set of other agents with whom they interact based on environmental knowledge, cognitive capabilities, resource limitations, and communications constraints. Previous findings have demonstrated that the structure of the artificial social network governing the agent interactions is strongly correlated with organizational performance. As multi-agent systems are typically embedded in dynamic environments, we wish to develop distributed, on-line network adaptation mechanisms for discovering effective network structures. Therefore, within the context of dynamic team formation, we propose several strategies for agent-organized networks (AONs) and evaluate their effectiveness for increasing organizational performance."
87,12,7871,1,Laws of Software Evolution Revisited,"Data obtained during a 1968 study of the software process [8] led to an investigation of the evolution of OS/360 [13] and and, over a period of twenty years, to formulation of eight Laws of Software Evolution. The FEAST project recently initiated (see sections 4â6 below) is expected to throw additional light on the phenomenology underlying these laws, to increase understanding of them, to explore their finer detail, to expose their wider relevance and implications and to develop means for their beneficial exploitation. This paper is intended to trigger wider interest in the laws and in the FEAST study of feedback and feedback control in the context of the software process and its improvement to ensure beneficial exploitation of their potential."
88,12,8154,1,"Massively Multiplayer Online Role-Playing Games: The People, the Addiction and the Playing Experience","{This book is about the fastest growing form of electronic game in the world&#151;the Massively Multiplayer Online Role Playing Game (MMORPG). It introduces these self-contained three-dimensional virtual worlds, often inhabited by thousands of players, and describes their evolution and sometimes become addicted to it. It also delves into the psychology of the people who inhabit the game universe and explores the development of the unique cultures, economies, moral codes, and slang in these virtual communities. It explains how the games are built, the spin-offs that players create to enhance their game lives, and peeks at the future of MMORPGs as they evolve from a form of amusement to an educational, scientific, and business tool.  <P>   Based on hundreds of interviews over a three-year period, the work explores reasons people are attracted to and addicted to these games. It also surveys many existing and upcoming games, identifying their unique features and attractions. Two appendices list online addiction organizations and MMORPG information sites.}"
89,12,9836,1,Structure and tie strengths in mobile communication networks,"10.1073/pnas.0610245104 Electronic databases, from phone to e-mails logs, currently provide detailed records of human communication patterns, offering novel avenues to map and explore the structure of social and communication networks. Here we examine the communication patterns of millions of mobile phone users, allowing us to simultaneously study the local and the global structure of a society-wide communication network. We observe a coupling between interaction strengths and the network's local structure, with the counterintuitive consequence that social networks are robust to the removal of the strong ties but fall apart after a phase transition if the weak ties are removed. We show that this coupling significantly slows the diffusion process, resulting in dynamic trapping of information in communities and find that, when it comes to information diffusion, weak and strong ties are both simultaneously ineffective."
90,12,10266,1,Mixed Methods Research: A Research Paradigm Whose Time Has Come,"The purposes of this article are to position mixed methods research (mixed research is a synonym) as the natural complement to traditional qualitative and quantitative research, to present pragmatism as offering an attractive philosophical partner for mixed methods research, and to provide a framework for designing and conducting mixed methods research. In doing this, we briefly review the paradigm ""wars"" and incompatibility thesis, we show some commonalities between quantitative and qualitative research, we explain the tenets of pragmatism, we explain the fundamental principle of mixed research and how to apply it, we provide specific sets of designs for the two major types of mixed methods research (mixed-model designs and mixed-method designs), and, finally, we explain mixed methods research as following (recursively) an eight-step process. A key feature of mixed methods research is its methodological pluralism or eclecticism, which frequently results in superior research (compared to monomethod research). Mixed methods research will be successful as more investigators study and help advance its concepts and as they regularly practice it. 10.3102/0013189X033007014"
91,12,11492,1,ATM and ATR Substrate Analysis Reveals Extensive Protein Networks Responsive to DNA Damage,"Cellular responses to DNA damage are mediated by a number of protein kinases, including ATM (ataxia telangiectasia mutated) and ATR (ATM and Rad3-related). The outlines of the signal transduction portion of this pathway are known, but little is known about the physiological scope of the DNA damage response (DDR). We performed a large-scale proteomic analysis of proteins phosphorylated in response to DNA damage on consensus sites recognized by ATM and ATR and identified more than 900 regulated phosphorylation sites encompassing over 700 proteins. Functional analysis of a subset of this data set indicated that this list is highly enriched for proteins involved in the DDR. This set of proteins is highly interconnected, and we identified a large number of protein modules and networks not previously linked to the DDR. This database paints a much broader landscape for the DDR than was previously appreciated and opens new avenues of investigation into the responses to DNA damage in mammals. 10.1126/science.1140321"
92,12,11568,1,Power-law distributions in empirical data,"Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the detection and characterization of power laws is complicated by the large fluctuations that occur in the tail of the distribution -- the part of the distribution representing large but rare events -- and by the difficulty of identifying the range over which power-law behavior holds. Commonly used methods for analyzing power-law data, such as least-squares fitting, can produce substantially inaccurate estimates of parameters for power-law distributions, and even in cases where such methods return accurate answers they are still unsatisfactory because they give no indication of whether the data obey a power law at all. Here we present a principled statistical framework for discerning and quantifying power-law behavior in empirical data. Our approach combines maximum-likelihood fitting methods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic and likelihood ratios. We evaluate the effectiveness of the approach with tests on synthetic data and give critical comparisons to previous approaches. We also apply the proposed methods to twenty-four real-world data sets from a range of different disciplines, each of which has been conjectured to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out.. Comment: 43 pages, 11 figures, 7 tables, 4 appendices; code available at http://www.santafe.edu/~aaronc/powerlaws/"
93,12,12991,1,Maps of random walks on complex networks reveal community structure,"10.1073/pnas.0706851105 To comprehend the multipartite organization of large-scale biological and social systems, we introduce an information theoretic approach that reveals community structure in weighted and directed networks. We use the probability flow of random walks on a network as a proxy for information flows in the real system and decompose the network into modules by compressing a description of the probability flow. The result is a map that both simplifies and highlights the regularities in the structure and their relationships. We illustrate the method by making a map of scientific communication as captured in the citation patterns of >6,000 journals. We discover a multicentric organization with fields that vary dramatically in size and degree of integration into the network of science. Along the backbone of the networkâincluding physics, chemistry, molecular biology, and medicineâinformation flows bidirectionally, but the map reveals a directional pattern of citation from the applied fields to the basic sciences."
94,12,13003,1,100% Accuracy in Automatic Face Recognition,"Accurate face recognition is critical for many security applications. Current automatic face-recognition systems are defeated by natural changes in lighting and pose, which often affect face images more profoundly than changes in identity. The only system that can reliably cope with such variability is a human observer who is familiar with the faces concerned. We modeled human familiarity by using image averaging to derive stable face representations from naturally varying photographs. This simple procedure increased the accuracy of an industry standard face-recognition algorithm from 54% to 100%, bringing the robust performance of a familiar human to an automated system. 10.1126/science.1149656"
95,12,15879,1,"Strategic reading, ontologies, and the future of scientific publishing.","The revolution in scientific publishing that has been promised since the 1980s is about to take place. Scientists have always read strategically, working with many articles simultaneously to search, filter, scan, link, annotate, and analyze fragments of content. An observed recent increase in strategic reading in the online environment will soon be further intensified by two current trends: (i) the widespread use of digital indexing, retrieval, and navigation resources and (ii) the emergence within many scientific disciplines of interoperable ontologies. Accelerated and enhanced by reading tools that take advantage of ontologies, reading practices will become even more rapid and indirect, transforming the ways in which scientists engage the literature and shaping the evolution of scientific publishing. 10.1126/science.1157784"
96,12,16722,1,myExperiment: a repository and social network for the sharing of bioinformatics workflows.,"myExperiment (http://www.myexperiment.org) is an online research environment that supports the social sharing of bioinformatics workflows. These workflows are procedures consisting of a series of computational tasks using web services, which may be performed on data from its retrieval, integration and analysis, to the visualization of the results. As a public repository of workflows, myExperiment allows anybody to discover those that are relevant to their research, which can then be reused and repurposed to their specific requirements. Conversely, developers can submit their workflows to myExperiment and enable them to be shared in a secure manner. Since its release in 2007, myExperiment currently has over 3500 registered users and contains more than 1000 workflows. The social aspect to the sharing of these workflows is facilitated by registered users forming virtual communities bound together by a common interest or research project. Contributors of workflows can build their reputation within these communities by receiving feedback and credit from individuals who reuse their work. Further documentation about myExperiment including its REST web service is available from http://wiki.myexperiment.org. Feedback and requests for support can be sent to bugs@myexperiment.org."
97,12,16815,1,Online Professionalism and the Mirror of Social Media,"The rise of social mediaâcontent created by Internet users and hosted by popular sites such as Facebook, Twitter, YouTube, and Wikipedia, and blogsâhas brought several new hazards for medical professionalism. First, many physicians may find applying principles for medical professionalism to the online environment challenging in certain contexts. Second, physicians may not consider the potential impact of their online content on their patients and the public. Third, a momentary lapse in judgment by an individual physician to create unprofessional content online can reflect poorly on the entire profession. To overcome these challenges, we encourage individual physicians to realize that as they  tread  through the World Wide Web, they leave behind a  footprint  that may have unintended negative consequences for them and for the profession at large. We also recommend that institutions take a proactive approach to engage users of social media in setting consensus-based standards for  online professionalism.  Finally, given that professionalism encompasses more than the avoidance of negative behaviors, we conclude with examples of more positive applications for this technology. Much like a mirror, social media can reflect the best and worst aspects of the content placed before it for all to see."
98,13,2590,1,Directed diffusion: A scalable and robust communication paradigm for sensor networks,"Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diusion paradigm for such coordination. Directed diusion is datacentric in that all communication is for named data. All nodes in a directed diusion-based network are applicationaware. This enables diusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diusion for a simple remote-surveillance sensor network.  1 Introduction  In the near future, advances in processor, memory and radio technology will enable small and cheap nodes capable of wireless communication and signicant computation. The addition of sensing capability to such devices will make distributed microsensing an activity in which a collection of ..."
99,13,2879,1,Analysis of TCP Performance Over Mobile Ad Hoc Networks,"Mobile ad hoc networks have gained a lot of attention lately as a means of providing continuous network connectivity to mobile computing devices regardless of physical location. Recently, a large amount of research has focused on the routing protocols needed in such an environment. In this paper, we investigate the effects that link breakage due to mobility has on TCP performance. Through simulation, we show that TCP throughput drops significantly when nodes move, due to TCP's inability to..."
100,13,5041,1,Current Solutions for Web Service Composition,"Web service composition lets developers create applications on top of service-oriented computing's native description, discovery, and communication capabilities. Such applications are rapidly deployable and offer developers reuse possibilities and users seamless access to a variety of complex services.  There are many existing approaches to service composition, ranging from abstract methods to those aiming to be industry standards.  The authors describe four key issues for Web service composition."
101,13,7001,1,Ad hoc On-Demand Distance Vector (AODV) Routing,"The Ad Hoc On-Demand Distance Vector (AODV) routing protocol is intended for use by mobile nodes in an ad hoc network. It offers quick adaptation to dynamic link conditions, low processing and memory overhead, low network utilization, and determines both unicast Perkins, Royer, Das Expires 22 April 2000 [Page i]  Internet Draft AODV 22 October 1999 and multicast routes between sources and destinations. It uses destination sequence numbers to ensure loop freedom at all times (even in the face of anomalous delivery of routing control messages), solving problems (such as &#034;counting to infinity&#034;) associated with classical distance vector protocols. Contents Status of This Memo i Abstract i 1. Introduction 1 2. Overview 2 3. AODV Terminology 4 4. Route Request (RREQ) Message Format 6 5. Route Reply (RREP) Message Format 8 6. Route Error (RERR) Message Format 9 7. Multicast Activation (MACT) Message Format 10 8. Group Hello (GRPH) Message Format 11 9. Node Operation - Unicast 12 9.1. Maintai..."
102,14,2534,1,STUDYING ONLINE SOCIAL NETWORKS,"Abstract When a computer network connects people or organizations, it is a social network. Yet the study of such computer-supported social networks has not received as much attention as studies of human-computer interaction, online person-to-person interaction, and computer-supported communication within small groups. We argue the usefulness of a social network approach for the study of computer-mediated communication. We review some basic concepts of social network analysis, describe how to collect and analyze social network data, and demonstrate where social network data can be, and have been, used to study computer-mediated communication. Throughout, we show the utility of the social network approach for studying computer-mediated communication, be it in computer-supported cooperative work, in virtual community, or in more diffuse interactions over less bounded systems such as the Internet."
103,14,7454,1,Communities of practice and social learning systems,"10.1177/135050840072002  This essay argues that the success of organizations depends on their ability to design themselves as social learning systems and also to participate in broader learning systems such as an industry, a region, or a consortium. It explores the structure of these social learning systems. It proposes a social definition of learning and distinguishes between three `modes of belonging' by which we participate in social learning systems. Then it uses this framework to look at three constitutive elements of these systems: communities of practice, boundary processes among these communities, and identities as shaped by our participation in these systems."
104,14,12572,1,"Social network sites: Definition, history, and scholarship","Social network sites (SNSs) are increasingly attracting the attention of academic and industry researchers intrigued by their affordances and reach. This special theme section of the Journal of Computer-Mediated Communication brings together scholarship on these emergent phenomena. In this introductory article, we describe features of SNSs and propose a comprehensive definition. We then present one perspective on the history of such sites, discussing key changes and developments. After briefly summarizing existing scholarship concerning SNSs, we discuss the articles in this special section and conclude with considerations for future research. ABSTRACT FROM AUTHOR Copyright of Journal of Computer-Mediated Communication is the property of Blackwell Publishing Limited and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts)"
105,15,6360,1,Literature mining for the biologist: from information retrieval to biological discovery," For the average biologist, hands-on literature mining currently means a keyword search in PubMed. However, methods for extracting biomedical facts from the scientific literature have improved considerably, and the associated tools will probably soon be used in many laboratories to automatically annotate and analyse the growing number of system-wide experimental data sets. Owing to the increasing body of text and the open-access policies of many journals, literature mining is also becoming useful for both hypothesis generation and biological discovery. However, the latter will require the integration of literature and high-throughput data, which should encourage close collaborations between biologists and computational linguists."
106,15,15114,1,A Principal Component Analysis of 39 Scientific Impact Measures,"<sec> <title>Background</title> <p>The impact of scientific publications has traditionally been expressed in terms of citation counts. However, scientific activity has moved online over the past decade. To better capture scientific impact in the digital era, a variety of new impact measures has been proposed on the basis of social network analysis and usage log data. Here we investigate how these new measures relate to each other, and how accurately and completely they express scientific impact.</p> </sec><sec> <title>Methodology</title> <p>We performed a principal component analysis of the rankings produced by 39 existing and proposed measures of scholarly impact that were calculated on the basis of both citation and usage log data.</p> </sec><sec> <title>Conclusions</title> <p>Our results indicate that the notion of scientific impact is a multi-dimensional construct that can not be adequately measured by any single indicator, although some measures are more suitable than others. The commonly used citation Impact Factor is not positioned at the core of this construct, but at its periphery, and should thus be used with caution.</p> </sec>"
107,16,886,1,Using mixture models for collaborative filtering,"A collaborative filtering system at an e-commerce site or similar service uses data about aggregate user behavior to make recommendations tailored to specific user interests. We develop recommendation algorithms with provable performance guarantees in a probabilistic mixture model for collaborative filtering proposed by Hoffman and Puzicha. We identify certain novel parameters of mixture models that are closely connected with the best achievable performance of a recommendation algorithm; we show that for any system in which these parameters are bounded, it is possible to give recommendations whose quality converges to optimal as the amount of data grows. All our bounds depend on a new measure of independence that can be viewed as an $L_1$-analogue of the smallest singular value of a matrix. Using this, we introduce a technique based on generalized pseudoinverse matrices and linear programming for handling sets of high-dimensional vectors. We also show that standard approaches based on $L_2$ spectral methods are not strong enough to yield comparable results, thereby suggesting some inherent limitations of spectral analysis."
108,16,1182,1,Machine Learning,"Machine Learning is the study of methods for programming computers to learn. Computers are applied to a wide range of tasks, and for most of these it is relatively easy for programmers to design and implement the necessary software. However, there are many tasks for which this is difficult or impossible. These can be divided into four general categories. First, there are problems for which there exist no human experts. For example, in modern automated manufacturing facilities, there is a need to predict machine failures before they occur by analyzing sensor readings. Because the machines are new, there are no human experts who can be interviewed by a programmer to provide the knowledge necessary to build a computer system. A machine learning system can study recorded data and subsequent machine failures and learn prediction rules. Second, there are problems where human experts exist, but where they are unable to explain their expertise. This is the case in many perceptual tasks, such as speech recognition, hand-writing recognition, and natural language understanding. Virtually all humans exhibit expert-level abilities on these tasks, but none of them can describe the detailed steps that they follow as they perform them. Fortunately, humans can provide machines with examples of the inputs and correct outputs for these tasks, so machine learning algorithms can learn to map the inputs to the outputs. Third, there are problems where phenomena are changing rapidly. In finance, for example, people would like to predict the future behavior of the stock market, of consumer purchases, or of exchange rates. These behaviors change frequently, so that even if a programmer could construct a good predictive computer program, it would need to be rewritten frequently. A learning program can relieve the programmer of this burden by constantly modifying and tuning a set of learned prediction rules. Fourth, there are applications that need to be customized for each computer user separately. Consider, for example, a program to filter unwanted electronic mail messages. Different users will need different filters. It is unreasonable to expect each user to program his or her own rules, and it is infeasible to provide every user with a software engineer to keep the rules up-to-date. A machine learning system can learn which mail messages the user rejects and maintain the filtering rules automatically. Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis. Statistics focuses on understanding the phenomena that have generated the data, often with the goal of testing different hypotheses about those phenomena. Data mining seeks to find patterns in the data that are understandable by people. Psychological studies of human learning aspire to understand the mechanisms underlying the various learning behaviors exhibited by people (concept learning, skill acquisition, strategy change, etc.). In contrast, machine learning is primarily concerned with the accuracy and effectiveness of the resulting computer system. To illustrate this, consider the different questions that might be asked about speech data. A machine learning approach focuses on building an accurate and efficient speech recognition system. A statistician might collaborate with a psychologist to test hypotheses about the mechanisms underlying speech recognition. A data mining approach might look for patterns in speech data that could be applied to group speakers according to age, sex, or level of education."
109,16,5604,1,Shilling recommender systems for fun and profit,"Recommender systems have emerged in the past several years as an effective way to help people cope with the problem of information overload. One application in which they have become particularly common is in e-commerce, where recommendation of items can often help a customer find what she is interested in and, therefore can help drive sales. Unscrupulous producers in the never-ending quest for market penetration may find it profitable to shill recommender systems by lying to the systems in order to have their products recommended more often than those of their competitors. This paper explores four open questions that may affect the effectiveness of such shilling attacks: which recommender algorithm is being used, whether the application is producing recommendations or predictions, how detectable the attacks are by the operator of the system, and what the properties are of the items being attacked. The questions are explored experimentally on a large data set of movie ratings. Taken together, the results of the paper suggest that new ways must be used to evaluate and detect shilling attacks on recommender systems."
110,16,5850,1,Application of dimensionality reduction in recommender systems--a case study,"We investigate the use of dimensionality reduction to improve performance for a new class of data analysis software called ârecommender systemsâ. Recommender systems apply knowledge discovery techniques to the problem of making product recommendations during a live customer interaction. These systems are achieving widespread success in E-commerce nowadays, especially with the advent of the Internet. The tremendous growth of customers and products poses three key challenges for recommender systems in the E-commerce domain. These are: producing high quality recommendations, performing many recommendations per second for millions of customers and products, and achieving high coverage in the face of data sparsity. One successful recommender system technology is collaborative filtering, which works by matching customer preferences to other customers in making recommendations. Collaborative filtering has been shown to produce high quality recommendations, but the performance degrades with the number of customers and products. New recommender system technologies are needed that can quickly produce high quality recommendations, even for very largescale problems. This paper presents two different experiments where we have explored one technology called Singular Value Decomposition (SVD) to reduce the dimensionality of recommender system databases. Each experiment compares the quality of a recommender system using SVD with the quality of a recommender system using collaborative filtering. The first experiment compares the effectiveness of the two recommender systems at predicting consumer preferences based on a database of explicit ratings of products. The second experiment compares the effectiveness of the two recommender systems at producing Top-N lists based on a real-life customer purchase database from an E-Commerce site. Our experience suggests that SVD has the potential to meet many of the challenges of recommender systems, under certain conditions. 1"
111,17,442,1,Simultaneous determination of protein structure and dynamics,We present a protocol for the experimental determination of ensembles of protein conformations that represent simultaneously the native structure and its associated dynamics. The procedure combines the strengths of nuclear magnetic resonance spectroscopyâfor obtaining experimental information at the atomic level about the structural and dynamical features of proteinsâwith the ability of molecular dynamics simulations to explore a wide range of protein conformations. We illustrate the method for human ubiquitin in solution and find that there is considerable conformational heterogeneity throughout the protein structure. The interior atoms of the protein are tightly packed in each individual conformation that contributes to the ensemble but their overall behaviour can be described as having a significant degree of liquid-like character. The protocol is completely general and should lead to significant advances in our ability to understand and utilize the structures of native proteins.
112,17,3971,1,Free Energy Calculations: Applications to Chemical and Biochemical Phenomena,"I will review the applications of free energy calcuIations employing molecular dynamics or Monte Carlo methods to a variety of chemical and biochemical phenomena. The focus is on the applications of such calculations to molecular solvation, molecular associ- ation, macromolecular stability, and enzyme catalysis. The molecules discussed range from monovalent ions and small molecules to proteins and nucleic acids."
113,17,4140,1,Calculating structures and free energies of complex molecules: Combining molecular mechanics and continuum models,"PMID: 11123888 A historical perspective on the application of molecular dynamics (MD) to biological macromolecules is presented. Recent developments combining state-of-the-art force fields with continuum solvation calculations have allowed us to reach the fourth era of MD applications in which one can often derive both accurate structure and accurate relative free energies from molecular dynamics trajectories. We illustrate such applications on nucleic acid duplexes, RNA hairpins, protein folding trajectories, and proteinligand, proteinprotein, and proteinnucleic acid interactions."
114,18,3065,1,Sparse Coding of Sensory Inputs,"Several theoretical, computational, and experimental studies suggest that neurons encode sensory information using a small number of active neurons at any given point in time. This strategy, referred to as [`]sparse coding', could possibly confer several advantages. First, it allows for increased storage capacity in associative memories; second, it makes the structure in natural signals explicit; third, it represents complex data in a way that is easier to read out at subsequent levels of processing; and fourth, it saves energy. Recent physiological recordings from sensory neurons have indicated that sparse coding could be a ubiquitous strategy employed in several different modalities across different organisms."
115,18,4236,1,Visual receptive field organization,"Increasingly systematic approaches to quantifying receptive fields in primary visual cortex, combined with inspired ideas about functional circuitry, non-linearities, and visual stimuli, are bringing new interest to classical problems. This includes the distinction and hierarchy between simple and complex cells, the mechanisms underlying the receptive field surround, and debates about optimal stimuli for mapping receptive fields. An important new problem arises from recent observations of stimulus-dependent spatial and temporal summation in primary visual cortex. It appears that the receptive field can no longer be considered unique, and we might have to relinquish this cherished notion as the embodiment of neuronal function in primary visual cortex."
116,18,9701,1,Action recognition in the premotor cortex,"We recorded electrical activity from 532 neurons in the rostral part of inferior area 6 (area F5) of two macaque monkeys. Previous data had shown that neurons of this area discharge during goal-directed hand and mouth movements. We describe here the properties of a newly discovered set of F5 neurons ( mirror neurons', n = 92) all of which became active both when the monkey performed a given action and when it observed a similar action performed by the experimenter. Mirror neurons, in order to be visually triggered, required an interaction between the agent of the action and the object of it. The sight of the agent alone or of the object alone (three-dimensional objects, food) were ineffective. Hand and the mouth were by far the most effective agents. The actions most represented among those activating mirror neurons were grasping, manipulating and placing. In most mirror neurons (92%) there was a clear relation between the visual action they responded to and the motor response they coded. In [~]30% of mirror neurons the congruence was very strict and the effective observed and executed actions corresponded both in terms of general action (e.g. grasping) and in terms of the way in which that action was executed (e.g. precision grip). We conclude by proposing that mirror neurons form a system for matching observation and execution of motor actions. We discuss the possible role of this system in action recognition and, given the proposed homology between F5 and human Brocca's region, we posit that a matching system, similar to that of mirror neurons exists in humans and could be involved in recognition of actions as well as phonetic gestures. 10.1093/brain/119.2.593"
117,19,10606,1,Why We Tag: Motivations for Annotation in Mobile and Online Media,"morganya stanford.edu Why do people tag? Users have mostly avoided annotating media such as photos â both in desktop and mobile environments â despite the many potential uses for annotations, including recall and retrieval. We investigate the incentives for annotation in Flickr, a popular web-based photo-sharing system, and ZoneTag, a cameraphone photo capture and annotation tool that uploads images to Flickr. In Flickr, annotation (as textual tags) serves both personal and social purposes, increasing incentives for tagging and resulting in a relatively high number of annotations. ZoneTag, in turn, makes it easier to tag cameraphone photos that are uploaded to Flickr by allowing annotation and suggesting relevant tags immediately after capture. A qualitative study of ZoneTag/Flickr users exposed various tagging patterns and emerging motivations for photo annotation. We offer a taxonomy of motivations for annotation in this system along two dimensions (sociality and function), and explore the various factors that people consider when tagging their photos. Our findings suggest implications for the design of digital photo organization and sharing applications, as well as other applications that incorporate user-based annotation."
118,19,12463,1,Does it matter who contributes: a study on featured articles in the german wikipedia,"The considerable high quality of Wikipedia articles is often accredited to the large number of users who contribute to Wikipediaâs encyclopedia articles, who watch articles and correct errors immediately. In this paper, we are in particu- lar interested in a certain type of Wikipedia articles, namely, the featured articles â articles marked by a communityâs vote as being of outstanding quality. The German Wikipedia has the nice property that it has two types of featured articles: excellent and worth reading. We explore on the German Wikipedia whether only the mere number of contributors makes the difference or whether the high quality of featured articles results from having experienced authors contribut- ing with a reputation for high quality contributions. Our results indicate that it does matter who contributes."
119,19,13444,1,Understanding the Efficiency of Social Tagging Systems using Information Theory,"Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software."
120,19,13943,1,Harvana: harvesting community tags to enrich collection metadata,"Collaborative, social tagging and annotation systems have exploded on the Internet as part of the Web 2.0 phenomenon. Systems such as Flickr, Del.icio.us, Technorati, Connotea and LibraryThing, provide a community-driven approach to classifying information and resources on the Web, so that they can be browsed, discovered and re-used. Although social tagging sites provide simple, user-relevant tags, there are issues associated with the quality of the metadata and the scalability compared with conventional indexing systems. In this paper we propose a hybrid approach that enables authoritative metadata generated by traditional cataloguing methods to be merged with community annotations and tags. The HarvANA (Harvesting and Aggregating Networked Annotations) system uses a standardized but extensible RDF model for representing the annotations/tags and OAI-PMH to harvest the annotations/tags from distributed community servers. The harvested annotations are aggregated with the authoritative metadata in a centralized metadata store. This streamlined, interoperable, scalable approach enables libraries, archives and repositories to leverage community enthusiasm for tagging and annotation, augment their metadata and enhance their discovery services. This paper describes the HarvANA system and its evaluation through a collaborative testbed with the National Library of Australia using architectural images from PictureAustralia."
121,19,14178,1,Social tag prediction,"In this paper, we look at the âsocial tag predictionâ prob- lem. Given a set of objects, and a set of tags applied to those objects by users, can we predict whether a given tag could/should be applied to a particular object? We inves- tigated this question using one of the largest crawls of the social bookmarking system del.icio.us gathered to date. For URLs in del.icio.us, we predicted tags based on page text, anchor text, surrounding hosts, and other tags applied to the URL. We found an entropy-based metric which captures the generality of a particular tag and informs an analysis of how well that tag can be predicted. We also found that tag-based association rules can produce very high-precision predictions as well as giving deeper understanding into the relationships between tags. Our results have implications for both the study of tagging systems as potential information retrieval tools, and for the design of such systems."
122,20,21,1,Functional discovery via a compendium of expression profiles.,"Ascertaining the impact of uncharacterized perturbations on the cell is a fundamental problem in biology. Here, we describe how a single assay can be used to monitor hundreds of different cellular functions simultaneously. We constructed a reference database or âcompendiumâ of expression profiles corresponding to 300 diverse mutations and chemical treatments in S. cerevisiae , and we show that the cellular pathways affected can be determined by pattern matching, even among very subtle profiles. The utility of this approach is validated by examining profiles caused by deletions of uncharacterized genes: we identify and experimentally confirm that eight uncharacterized open reading frames encode proteins required for sterol metabolism, cell wall function, mitochondrial respiration, or protein synthesis. We also show that the compendium can be used to characterize pharmacological perturbations by identifying a novel target of the commonly used drug dyclonine."
123,20,922,1,Foundations of Statistical Natural Language Processing,"{""Statistical natural-language processing is, in my estimation, one of the most fast-moving and exciting areas of computer science these days. Anyone who wants to learn this field would be well advised to get this book. For that matter, the same goes for anyone who is already in the field. I know that it is going to be one of the most well-thumbed books on my bookshelf."" -- Eugene Charniak, Department of Computer Science, Brown University  <P>Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.  <P>More on this book}"
124,20,995,1,Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon tissues probed by oligonucleotide arrays,"Oligonucleotide arrays can provide a broad picture of the state of the cell, by monitoring the expression level of thousands of genes at the same time. It is of interest to develop techniques for extracting useful information from the resulting data sets. Here we report the application of a two-way clustering method for analyzing a data set consisting of the expression patterns of different cell types. Gene expression in 40 tumor and 22 normal colon tissue samples was analyzed with an Affymetrix oligonucleotide array complementary to more than 6,500 human genes. An efficient two-way clustering algorithm was applied to both the genes and the tissues, revealing broad coherent patterns that suggest a high degree of organization underlying gene expression in these tissues. Coregulated families of genes clustered together, as demonstrated for the ribosomal proteins. Clustering also separated cancerous from noncancerous tissue and cell lines from in vivo tissues on the basis of subtle distributed patterns of genes even when expression of individual genes varied only slightly between the tissues. Two-way clustering thus may be of use both in classifying genes into functional groups and in classifying tissues based on gene expression."
125,20,1182,1,Machine Learning,"Machine Learning is the study of methods for programming computers to learn. Computers are applied to a wide range of tasks, and for most of these it is relatively easy for programmers to design and implement the necessary software. However, there are many tasks for which this is difficult or impossible. These can be divided into four general categories. First, there are problems for which there exist no human experts. For example, in modern automated manufacturing facilities, there is a need to predict machine failures before they occur by analyzing sensor readings. Because the machines are new, there are no human experts who can be interviewed by a programmer to provide the knowledge necessary to build a computer system. A machine learning system can study recorded data and subsequent machine failures and learn prediction rules. Second, there are problems where human experts exist, but where they are unable to explain their expertise. This is the case in many perceptual tasks, such as speech recognition, hand-writing recognition, and natural language understanding. Virtually all humans exhibit expert-level abilities on these tasks, but none of them can describe the detailed steps that they follow as they perform them. Fortunately, humans can provide machines with examples of the inputs and correct outputs for these tasks, so machine learning algorithms can learn to map the inputs to the outputs. Third, there are problems where phenomena are changing rapidly. In finance, for example, people would like to predict the future behavior of the stock market, of consumer purchases, or of exchange rates. These behaviors change frequently, so that even if a programmer could construct a good predictive computer program, it would need to be rewritten frequently. A learning program can relieve the programmer of this burden by constantly modifying and tuning a set of learned prediction rules. Fourth, there are applications that need to be customized for each computer user separately. Consider, for example, a program to filter unwanted electronic mail messages. Different users will need different filters. It is unreasonable to expect each user to program his or her own rules, and it is infeasible to provide every user with a software engineer to keep the rules up-to-date. A machine learning system can learn which mail messages the user rejects and maintain the filtering rules automatically. Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis. Statistics focuses on understanding the phenomena that have generated the data, often with the goal of testing different hypotheses about those phenomena. Data mining seeks to find patterns in the data that are understandable by people. Psychological studies of human learning aspire to understand the mechanisms underlying the various learning behaviors exhibited by people (concept learning, skill acquisition, strategy change, etc.). In contrast, machine learning is primarily concerned with the accuracy and effectiveness of the resulting computer system. To illustrate this, consider the different questions that might be asked about speech data. A machine learning approach focuses on building an accurate and efficient speech recognition system. A statistician might collaborate with a psychologist to test hypotheses about the mechanisms underlying speech recognition. A data mining approach might look for patterns in speech data that could be applied to group speakers according to age, sex, or level of education."
126,20,1195,1,The Nature of Statistical Learning Theory,"The aim of this book is to discuss the fundamental ideas which lie behind the statistical theory of learning and generalization. It considers learning as a general problem of function estimation based on empirical data. Omitting proofs and technical details, the author concentrates on discussing the main results of learning theory and their connections to fundamental problems in statistics. These include: * the setting of learning problems based on the model of minimizing the risk functional from empirical data * a comprehensive analysis of the empirical risk minimization principle including necessary and sufficient conditions for its consistency * non-asymptotic bounds for the risk achieved using the empirical risk minimization principle * principles for controlling the generalization ability of learning machines using small sample sizes based on these bounds * the Support Vector methods that control the generalization ability when estimating function using small sample size. The second edition of the book contains three new chapters devoted to further development of the learning theory and SVM techniques. These include: * the theory of direct method of learning based on solving multidimensional integral equations for density, conditional probability, and conditional density estimation * a new inductive principle of learning. Written in a readable and concise style, the book is intended for statisticians, mathematicians, physicists, and computer scientists. Vladimir N. Vapnik is Technology Leader AT&T Labs-Research and Professor of London University. He is one of the founders of statistical learning theory, and the author of seven books published in English, Russian, German, and Chinese."
127,20,1626,1,Parallel human genome analysis: microarray-based expression monitoring of 1000 genes.,"Microarrays containing 1046 human cDNAs of unknown sequence were printed on glass with high-speed robotics. These 1.0-cm2 DNA ""chips"" were used to quantitatively monitor differential expression of the cognate human genes using a highly sensitive two-color hybridization assay. Array elements that displayed differential expression patterns under given experimental conditions were characterized by sequencing. The identification of known and novel heat shock and phorbol ester-regulated genes in human T cells demonstrates the sensitivity of the assay. Parallel gene analysis with microarrays provides a rapid and efficient method for large-scale human gene discovery."
128,20,2167,1,Statistical Pattern Recognition: A Review,"The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have been receiving increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples, and performance evaluation. In spite of almost 50 years of research and development in this field, the general problem of recognizing complex patterns with arbitrary orientation, location, and scale remains unsolved. New and emerging applications, such as data mining, web searching, retrieval of multimedia data, face recognition, and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarize and compare some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field."
129,20,2220,1,Nonlinear Programming,"{This extensive rigorous texbook, developed through instruction at MIT, focuses on nonlinear and other types of optimization: iterative algorithms for constrained and unconstrained optimization, Lagrange multipliers and duality, large scale problems, and the interface between continuous and discrete optimization.   Among its special features, the book:  1) provides extensive coverage of iterative optimization methods within a unifying framework  2) provides a detailed treatment of interior point methods for linear programming  3) covers in depth duality theory from both a variational and a geometrical/convex analysis point of view  4) includes much new material on a number of topics, such as neural network training, discrete-time optimal control, and large-scale optimization  5) includes a large number of examples and exercises detailed solutions of many of which are posted on the internet  Much supplementary/support material can be found at the book's web page  http://www.athenasc.com/nonlinbook.html}"
130,20,3247,1,From Ukkonen to McCreight and Weiner: A Unifying View of Linear-Time Suffix Tree Construction,". We review the linear time suffix tree constructions by Weiner, McCreight, and Ukkonen. We use the terminology of the most recent algorithm, Ukkonen's online construction, to explain its historic predecessors. This reveals relationships much closer than one would expect, since the three algorithms are based on rather different intuitive ideas. Moreover, it completely explains the differences between these algorithms in terms of simplicity, efficiency, and implementation complexity.  Key Words. ..."
131,20,3322,1,Evolving Neural Networks through Augmenting Topologies,"An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT) that outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution."
132,20,5045,1,GTM: The Generative Topographic Mapping,"Accepted for publication in Neural Computation. Latent variable models represent the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. A familiar example is factor analysis which is based on a linear transformations between the latent space and the data space. In this paper we introduce a form of non-linear latent variable model called the Generative Topographic Mapping for which the parameters of the model can be determined using the EM algorithm. GTM provides a principled alternative to the widely used Self-Organizing Map (SOM) of Kohonen (1982), and overcomes most of the significant limitations of the SOM. We demonstrate the performance of the GTM algorithm on a toy problem and on simulated data from flow diagnostics for a multi-phase oil pipeline. GTM: The Generative Topographic Mapping 2"
133,20,5109,1,A haplotype map of the human genome,"Inherited genetic variation has a critical but as yet largely uncharacterized role in human disease. Here we report a public database of common variation in the human genome: more than one million single nucleotide polymorphisms (SNPs) for which accurate and complete genotypes have been obtained in 269 DNA samples from four populations, including ten 500-kilobase regions in which essentially all information about common DNA variation has been extracted. These data document the generality of recombination hotspots, a block-like structure of linkage disequilibrium and low haplotype diversity, leading to substantial correlations of SNPs with many of their neighbours. We show how the HapMap resource can guide the design and analysis of genetic association studies, shed light on structural variation and recombination, and identify loci that may have been subject to natural selection during human evolution."
134,20,5620,1,Artificial Ant Colonies in Digital Image Habitats - A Mass Behaviour Effect Study on Pattern Recognition,"Some recent studies have pointed that, the self-organization of neurons into brain-like structures, and the self-organization of ants into a swarm are similar in many respects. If possible to implement, these features could lead to important developments in pattern recognition systems, where perceptive capabilities can emerge and evolve from the interaction of many simple local rules. The principle of the method is inspired by the work of Chialvo and Millonas who developed the first numerical simulation in which swarm cognitive map formation could be explained. From this point, an extended model is presented in order to deal with digital image habitats, in which artificial ants could be able to react to the environment and perceive it. Evolution of pheromone fields point that artificial ant colonies could react and adapt appropriately to any type of digital habitat. KEYWORDS: Swarm Intelligence, Self-Organization, Stigmergy, Artificial Ant Systems, Pattern Recognition and Perception, Image Segmentation, Gestalt Perception Theory, Distributed Computation."
135,20,5624,1,"On Image Filtering, Noise and Morphological Size Intensity Diagrams","In the absence of a pure noise-free image it is hard to define what noise is, in any original noisy image, and as a consequence also where it is, and in what amount. In fact, the definition of noise depends largely on our own aim in the whole image analysis process, and (perhaps more important) in our self-perception of noise. For instance, when we perceive noise as disconnected and small it is normal to use MMAF filters to treat it. There is two evidences of this. First, in many instances..."
136,20,5628,1,ANTIDS: Self-Organized Ant-based Clustering Model for Intrusion Detection System,"Security of computers and the networks that connect them is increasingly becoming of great significance. Computer security is defined as the protection of computing systems against threats to confidentiality, integrity, and availability. There are two types of intruders: the external intruders who are unauthorized users of the machines they attack, and internal intruders, who have permission to access the system with some restrictions. Due to the fact that it is more and more improbable to a system administrator to recognize and manually intervene to stop an attack, there is an increasing recognition that ID systems should have a lot to earn on following its basic principles on the behavior of complex natural systems, namely in what refers to self-organization, allowing for a real distributed and collective perception of this phenomena. With that aim in mind, the present work presents a self-organized ant colony based intrusion detection system (ANTIDS) to detect intrusions in a network infrastructure. The performance is compared among conventional soft computing paradigms like Decision Trees, Support Vector Machines and Linear Genetic Programming to model fast, online and efficient intrusion detection systems."
137,20,5633,1,"On Ants, Bacteria and Dynamic Environments","Wasps, bees, ants and termites all make effective use of their environment and resources by displaying collective âswarmâ intelligence. Termite colonies - for instance - build nests with a complexity far beyond the comprehension of the individual termite, while ant colonies dynamically allocate labor to various vital tasks such as foraging or defense without any central decision-making ability. Recent research suggests that microbial life can be even richer: highly social, intricately networked, and teeming with interactions, as found in bacteria. What strikes from these observations is that both ant colonies and bacteria have similar natural mechanisms based on Stigmergy and Self-Organization in order to emerge coherent and sophisticated patterns of global behaviour. Keeping in mind the above characteristics we will present a simple model to tackle the collective adaptation of a social swarm based on real ant colony behaviors (SSA algorithm) for tracking extrema in dynamic environments and highly multimodal complex functions described in the well-know De Jong test suite. Then, for the purpose of comparison, a recent model of artificial bacterial foraging (BFOA algorithm) based on similar stigmergic features is described and analyzed. Final results indicate that the SSA collective intelligence is able to cope and quickly adapt to unforeseen situations even when over the same cooperative foraging period, the community is requested to deal with two different and contradictory purposes, while outperforming BFOA in adaptive speed."
138,20,5636,1,"On Self-Regulated Swarms, Societal Memory, Speed and Dynamics","Wasps, bees, ants and termites all make effective use of their environment and resources by displaying collective ""swarm"" intelligence. Termite colonies - for instance - build nests with a complexity far beyond the comprehension of the individual termite, while ant colonies dynamically allocate labor to various vital tasks such as foraging or defense without any central decision-making ability. Recent research suggests that microbial life can be even richer: highly social, intricately networked, and teeming with interactions, as found in bacteria. What strikes from these observations is that both ant colonies and bacteria have similar natural mechanisms based on Stigmergy and Self-Organization in order to emerge coherent and sophisticated patterns of global foraging behavior. Keeping in mind the above characteristics we propose a Self-Regulated Swarm (SRS) algorithm which hybridizes the advantageous characteristics of Swarm Intelligence as the emergence of a societal environmental memory or cognitive map via collective pheromone laying in the landscape (properly balancing the exploration/exploitation nature of our dynamic search strategy), with a simple Evolutionary mechanism that trough a direct reproduction procedure linked to local environmental features is able to self-regulate the above exploratory swarm population, speeding it up globally. In order to test his adaptive response and robustness, we have recurred to different dynamic multimodal complex functions as well as to Dynamic Optimization Control problems, measuring reaction speeds and performance. Final comparisons were made with standard Genetic Algorithms (GAs), Bacterial Foraging strategies (BFOA), as well as with recent Co-Evolutionary approaches. SRS's were able to demonstrate quick adaptive responses, while outperforming the results obtained by the other approaches. Additionally, some successful behaviors were found: SRS was able to maintain a number of different solutions, while adapting to unforeseen situations even when over the same cooperative foraging period, the community is requested to deal with two different and contradictory purposes; the possibility to spontaneously create and maintain different subpopulations on different peaks, emerging different exploratory corridors with intelligent path planning capabilities; the ability to request for new agents (division of labor) over dramatic changing periods, and economizing those foraging resources over periods of intermediate stabilization. Finally, results illustrate that the present SRS collective swarm of bio-inspired ant-like agents is able to track about 65% of moving peaks traveling up to ten times faster than the velocity of a single individual composing that precise swarm tracking system. This emerged behavior is probably one of the most interesting ones achieved by the present work."
139,20,5879,1,Gene expression correlates of clinical prostate cancer behavior,"Prostate tumors are among the most heterogeneous of cancers, both histologically and clinically. Microarray expression analysis was used to determine whether global biological differences underlie common pathological features of prostate cancer and to identify genes that might anticipate the clinical behavior of this disease. While no expression correlates of age, serum prostate specific antigen (PSA), and measures of local invasion were found, a set of genes was identified that strongly correlated with the state of tumor differentiation as measured by Gleason score. Moreover, a model using gene expression data alone accurately predicted patient outcome following prostatectomy. These results support the notion that the clinical behavior of prostate cancer is linked to underlying gene expression differences that are detectable at the time of diagnosis."
140,20,5881,1,Image metrics in the statistical analysis of DNA microarray data,"DNA microarrays represent an important new method for determining the complete expression profile of a cell. In ``spotted'' microarrays, slides carrying spots of target DNA are hybridized to fluorescently labeled cDNA from experimental and control cells and the arrays are imaged at two or more wavelengths. In this paper, we perform statistical analysis on images of microarrays and show that quantitating the amount of fluorescent DNA bound to microarrays is subject to considerable uncertainty because of large and small-scale intensity fluctuations within spots, nonadditive background, and fabrication artifacts. Pixel-by-pixel analysis of individual spots can be used to estimate these sources of error and establish the precision and accuracy with which gene expression ratios are determined. Simple weighting schemes based on these estimates are effective in improving significantly the quality of microarray data as it accumulates in a multiexperiment database. We propose that error estimates from image-based metrics should be one component in an explicitly probabilistic scheme for the analysis of DNA microarray data."
141,20,5888,1,Importance of replication in microarray gene expression studies: Statistical methods and evidence from repetitive cDNA hybridizations,"We present statistical methods for analyzing replicated cDNA microarray expression data and report the results of a controlled experiment. The study was conducted to investigate inherent variability in gene expression data and the extent to which replication in an experiment produces more consistent and reliable findings. We introduce a statistical model to describe the probability that mRNA is contained in the target sample tissue, converted to probe, and ultimately detected on the slide. We also introduce a method to analyze the combined data from all replicates. Of the 288 genes considered in this controlled experiment, 32 would be expected to produce strong hybridization signals because of the known presence of repetitive sequences within them. Results based on individual replicates, however, show that there are 55, 36, and 58 highly expressed genes in replicates 1, 2, and 3, respectively. On the other hand, an analysis by using the combined data from all 3 replicates reveals that only 2 of the 288 genes are incorrectly classified as expressed. Our experiment shows that any single microarray output is subject to substantial variability. By pooling data from replicates, we can provide a more reliable analysis of gene expression data. Therefore, we conclude that designing experiments with replications will greatly reduce misclassification rates. We recommend that at least three replicates be used in designing experiments by using cDNA microarrays, particularly when gene expression data from single specimens are being analyzed."
142,20,5889,1,Principal components analysis to summarize microarray experiments: application to sporulation time series,"A series of microarray experiments produces observations of differential expression for thousands of genes across multiple conditions. It is often not clear whether a set of experiments are measuring fundamentally different gene expression states or are measuring similar states created through different mechanisms. It is useful, therefore, to define a core set of independent features for the expression states that allow them to be compared directly. Principal components analysis (PCA) is a statistical technique for determining the key variables in a multidimensional data set that explain the differences in the observations, and can be used to simplify the analysis and visualization of multidimensional data sets. We show that application of PCA to expression data (where the experimental conditions are the variables, and the gene expression measurements are the observations) allows us to summarize the ways in which gene responses vary under different conditions. Examination of the components also provides insight into the underlying factors that are measured in the experiments. We applied PCA to the publicly released yeast sporulation data set (Chu et al. 1998). In that work, 7 different measurements of gene expression were made over time. PCA on the time-points suggests that much of the observed variability in the experiment can be summarized in just 2 components--i.e. 2 variables capture most of the information. These components appear to represent (1) overall induction level and (2) change in induction level over time. We also examined the clusters proposed in the original paper, and show how they are manifested in principal component space. Our results are available on the internet at http: inverted question markwww.smi.stanford.edu/project/helix/PCArray ."
143,20,5897,1,Yeast microarrays for genome wide parallel genetic and gene expression analysis,"{{W}e have developed high-density {DNA} microarrays of yeast {ORF}s. {T}hese microarrays can monitor hybridization to {ORF}s for applications such as quantitative differential gene expression analysis and screening for sequence polymorphisms. {A}utomated scripts retrieved sequence information from public databases to locate predicted {ORF}s and select appropriate primers for amplification. {T}he primers were used to amplify yeast {ORF}s in 96-well plates, and the resulting products were arrayed using an automated micro arraying device. {A}rrays containing up to 2,479 yeast {ORF}s were printed on a single slide. {T}he hybridization of fluorescently labeled samples to the array were detected and quantitated with a laser confocal scanning microscope. {A}pplications of the microarrays are shown for genetic and gene expression analysis at the whole genome level.}"
144,20,5905,1,Practical Methods of Optimization,"Fully describes optimization methods that are currently most valuable in solving real-life problems. Since optimization has applications in almost every branch of science and technology, the text emphasizes their practical aspects in conjunction with the heuristics useful in making them perform more reliably and efficiently. To this end, it presents comparative numerical studies to give readers a feel for possibile applications and to illustrate the problems in assessing evidence. Also provides theoretical background which provides insights into how methods are derived. This edition offers revised coverage of basic theory and standard techniques, with updated discussions of line search methods, Newton and quasi-Newton methods, and conjugate direction methods, as well as a comprehensive treatment of restricted step or trust region methods not commonly found in the literature. Also includes recent developments in hybrid methods for nonlinear least squares; an extended discussion of linear programming, with new methods for stable updating of LU factors; and a completely new section on network programming. Chapters include computer subroutines, worked examples, and study questions."
145,20,7073,1,Background subtraction techniques: a review,"Background subtraction is a widely used approach for detecting moving objects from static cameras. Many different methods have been proposed over the recent years and both the novice and the expert can be confused about their benefits and limitations. In order to overcome this problem, this paper provides a review of the main methods and an original categorisation based on speed, memory requirements and accuracy. Such a review can effectively guide the designer to select the most suitable method for a given application in a principled way. Methods reviewed include parametric and non-parametric background density estimates and spatial correlation approaches."
146,20,10364,1,Clustering by Passing Messages Between Data Points,"Clustering data by identifying a subset of representative examples is important for processing sensory signals and detecting patterns in data. Such {\tt{}""{}}exemplars{\tt{}""{}} can be found by randomly choosing an initial subset of data points and then iteratively refining it, but this works well only if that initial choice is close to a good solution. We devised a method called {\tt{}""{}}affinity propagation,{\tt{}""{}} which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges. We used affinity propagation to cluster images of faces, detect genes in microarray data, identify representative sentences in this manuscript, and identify cities that are efficiently accessed by airline travel. Affinity propagation found clusters with much lower error than other methods, and it did so in less than one-hundredth the amount of time."
147,21,21,1,Functional discovery via a compendium of expression profiles.,"Ascertaining the impact of uncharacterized perturbations on the cell is a fundamental problem in biology. Here, we describe how a single assay can be used to monitor hundreds of different cellular functions simultaneously. We constructed a reference database or âcompendiumâ of expression profiles corresponding to 300 diverse mutations and chemical treatments in S. cerevisiae , and we show that the cellular pathways affected can be determined by pattern matching, even among very subtle profiles. The utility of this approach is validated by examining profiles caused by deletions of uncharacterized genes: we identify and experimentally confirm that eight uncharacterized open reading frames encode proteins required for sterol metabolism, cell wall function, mitochondrial respiration, or protein synthesis. We also show that the compendium can be used to characterize pharmacological perturbations by identifying a novel target of the commonly used drug dyclonine."
148,21,1006,1,Cluster analysis and display of genome-wide expression patterns,"A system of cluster analysis for genome-wide expression data from DNA microarray hybridization is described that uses standard statistical algorithms to arrange genes according to similarity in pattern of gene expression. The output is displayed graphically, conveying the clustering and the underlying expression data simultaneously in a form intuitive for biologists. We have found in the budding yeast Saccharomyces cerevisiae that clustering gene expression data groups together efficiently genes of known similar function, and we find a similar tendency in human data. Thus patterns seen in genome-wide expression experiments can be interpreted as indications of the status of cellular processes. Also, coexpression of genes of known function with poorly characterized or novel genes may provide a simple means of gaining leads to the functions of many genes for which information is not available currently."
149,21,1115,1,Adaptation in Natural and Artificial Systems,"Genetic algorithms are playing an increasingly important role in studies of complex adaptive systems, ranging from adaptive agents in economic theory to the use of machine learning techniques in the design of complex devices such as aircraft turbines and integrated circuits. Adaptation in Natural and Artificial Systems is the book that initiated this field of study, presenting the theoretical foundations and exploring applications.  In its most familiar form, adaptation is a biological process, whereby organisms evolve by rearranging genetic material to survive in environments confronting them. In this now classic work, Holland presents a mathematical model that allows for the nonlinearity of such complex interactions. He demonstrates the model's universality by applying it to economics, physiological psychology, game theory, and artificial intelligence and then outlines the way in which this approach modifies the traditional views of mathematical genetics.  Initially applying his concepts to simply defined artificial systems with limited numbers of parameters, Holland goes on to explore their use in the study of a wide range of complex, naturally occuring processes, concentrating on systems having multiple factors that interact in nonlinear ways. Along the way he accounts for major effects of coadaptation and coevolution: the emergence of building blocks, or schemata, that are recombined and passed on to succeeding generations to provide, innovations and improvements."
150,21,1211,1,Bayesian network classifiers,"Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state-of-the-art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we evaluate approaches for inducing classifiers from data, based on the theory of learning Bayesian networks. These networks are factored representations of probability distributions that generalize the naive Bayesian classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes. We experimentally tested these approaches, using problems from the University of California at Irvine repository, and compared them to C4.5, naive Bayes, and wrapper methods for feature selection."
151,21,1566,1,Molecular classification of cancer: class discovery and class prediction by gene expression monitoring.,"Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge."
152,21,1665,1,Improving support vector machine classifiers by modifying kernel functions,"We propose a method of modifying a kernel function to improve the performance of a support  vector machine classifier. This is based on the Riemannian geometrical structure induced by the  kernel function. The idea is to enlarge the spatial resolution around the separating boundary  surface by a conformal mapping such that the separability between classes is increased. Examples  are given specifically for modifying Gaussian Radial Basis Function kernels. Simulation results for  both artificial and real data show remarkable improvement of generalization errors, supporting our  idea.  1 Introduction  Support Vector Machine (SVM) is a new promising pattern classification technique proposed recently by Vapnik and co-workers (Boser et al., 1992, Cortes and Vapnik, 1995, and Vapnik, 1995). Unlike traditional methods which minimize the empirical training error, SVM aims at minimizing an upper bound of the generalization error through maximizing the margin between the separating hyperplane and..."
153,21,1692,1,The Stanford Microarray Database,"The Stanford Microarray Database (SMD) stores raw and normalized data from microarray experiments, and provides web interfaces for researchers to retrieve, analyze and visualize their data. The two immediate goals for SMD are to serve as a storage site for microarray data from ongoing research at Stanford University, and to facilitate the public dissemination of that data once published, or released by the researcher. Of paramount importance is the connection of microarray data with the biological data that pertains to the DNA deposited on the microarray (genes, clones etc.). SMD makes use of many public resources to connect expression information to the relevant biology, including SGD [Ball,C.A., Dolinski,K., Dwight,S.S., Harris,M.A., Issel-Tarver,L., Kasarskis,A., Scafe,C.R., Sherlock,G., Binkley,G., Jin,H. et al. (2000) Nucleic Acids Res., 28, 77-80], YPD and WormPD [Costanzo,M.C., Hogan,J.D., Cusick,M.E., Davis,B.P., Fancher,A.M., Hodges,P.E., Kondu,P., Lengieza,C., Lew-Smith,J.E., Lingner,C. et al. (2000) Nucleic Acids Res., 28, 73-76], Unigene [Wheeler,D.L., Chappey,C., Lash,A.E., Leipe,D.D., Madden,T.L., Schuler,G.D., Tatusova,T.A. and Rapp,B.A. (2000) Nucleic Acids Res., 28, 10-14], dbEST [Boguski,M.S., Lowe,T.M. and Tolstoshev,C.M. (1993) Nature Genet., 4, 332-333] and SWISS-PROT [Bairoch,A. and Apweiler,R. (2000) Nucleic Acids Res., 28, 45-48] and can be accessed at http://genome-www.stanford.edu/microarray."
154,21,2417,1,Normalization for cDNA microarray data: a robust composite method addressing single and multiple slide systematic variation,"There are many sources of systematic variation in cDNA microarray experiments which affect the measured gene expression levels (e.g. differences in labeling efficiency between the two fluorescent dyes). The term normalization refers to the process of removing such variation. A constant adjustment is often used to force the distribution of the intensity log ratios to have a median of zero for each slide. However, such global normalization approaches are not adequate in situations where dye biases can depend on spot overall intensity and/or spatial location within the array. This article proposes normalization methods that are based on robust local regression and account for intensity and spatial dependence in dye biases for different types of cDNA microarray experiments. The selection of appropriate controls for normalization is discussed and a novel set of controls (microarray sample pool, MSP) is introduced to aid in intensity-dependent normalization. Lastly, to allow for comparisons of expression levels across slides, a robust method based on maximum likelihood estimation is proposed to adjust for scale differences among slides."
155,21,3365,1,Integrative analysis of the cancer transcriptome.,"DNA microarrays have been widely applied to the study of human cancer, delineating myriad molecular subtypes of cancer, many of which are associated with distinct biological underpinnings, disease progression and treatment response. These primary analyses have begun to decipher the molecular heterogeneity of cancer, but integrative analyses that evaluate cancer transcriptome data in the context of other data sources are often capable of extracting deeper biological insight from the data. Here we discuss several such integrative computational and analytical approaches, including meta-analysis, functional enrichment analysis, interactome analysis, transcriptional network analysis and integrative model system analysis."
156,21,3368,1,Array comparative genomic hybridization and its applications in cancer.,"Alteration in DNA copy number is one of the many ways in which gene expression and function may be modified. Some variations are found among normal individuals, others occur in the course of normal processes in some species and still others participate in causing various disease states. For example, many defects in human development are due to gains and losses of chromosomes and chromosomal segments that occur before or shortly after fertilization, and DNA dosage-alteration changes occurring in somatic cells are frequent contributors to cancer. Detecting these aberrations and interpreting them in the context of broader knowledge facilitates the identification of crucial genes and pathways involved in biological processes and disease. Over the past several years, array comparative genomic hybridization has proven its value for analyzing DNA copy-number variations. Here, we discuss the state of the art of array comparative genomic hybridization and its applications in cancer, emphasizing general concepts rather than specific results."
157,21,5876,1,Data Analysis with Bayesian Networks: {A} Bootstrap Approach,"In recent years there has been significant  progress in algorithms and methods for inducing  Bayesian networks from data. However, in complex  data analysis problems, we need to go beyond  being satisfied with inducing networks with  high scores. We need to provide confidence measures  on features of these networks: Is the existence  of an edge between two nodes warranted?  Is the Markov blanket of a given node robust?  Can we say something about the ordering of the  variables? We should be able to address these  questions, even when the amount of data is not  enough to induce a high scoring network. In this  paper we propose Efron's Bootstrap as a computationally  efficient approach for answering these  questions. In addition, we propose to use these  confidence measures to induce better structures  from the data, and to detect the presence of latent  variables."
158,21,5877,1,DNA methylation and human disease," DNA methylation is a crucial epigenetic modification of the genome that is involved in regulating many cellular processes. These include embryonic development, transcription, chromatin structure, X chromosome inactivation, genomic imprinting and chromosome stability. Consistent with these important roles, a growing number of human diseases have been found to be associated with aberrant DNA methylation. The study of these diseases has provided new and fundamental insights into the roles that DNA methylation and other epigenetic modifications have in development and normal cellular homeostasis."
159,21,5885,1,Validating Clustering for Gene Expression Data,"MOTIVATION: Many clustering algorithms have been proposed for the analysis of gene expression data, but little guidance is available to help choose among them. We provide a systematic framework for assessing the results of clustering algorithms. Clustering algorithms attempt to partition the genes into groups exhibiting similar patterns of variation in expression level. Our methodology is to apply a clustering algorithm to the data from all but one experimental condition. The remaining condition is used to assess the predictive power of the resulting clusters-meaningful clusters should exhibit less variation in the remaining condition than clusters formed by chance. RESULTS: We successfully applied our methodology to compare six clustering algorithms on four gene expression data sets. We found our quantitative measures of cluster quality to be positively correlated with external standards of cluster quality."
160,21,5893,1,High density synthetic oligonucleotide arrays,"Experimental genomics involves taking advantage of sequence information to investigate and understand the workings of genes, cells and organisms. We have developed an approach in which sequence information is used directly to design high- density, two-dimensional arrays of synthetic oligonucleotides. The GeneChip(R) probe arrays are made using spatially patterned, light-directed combinatorial chemical synthesis, and contain up to hundreds of thousands of different oligonucleotides on a small glass surface. The arrays have been designed and used for quantitative and highly parallel measurements of gene expression, to discover polymorphic loci and to detect the presence of thousands of alternative alleles. Here, we describe the fabrication of the arrays, their design and some specific: applications to high-throughput genetic and cellular analysis."
161,21,5894,1,Semi-Supervised Support Vector Machines,"We introduce a semi-supervised support vector machine (S  3  VM)  method. Given a training set of labeled data and a working set  of unlabeled data, S  3  VM constructs a support vector machine using  both the training and working sets. We use S  3  VM to solve  the transduction problem using overall risk minimization (ORM)  posed by Vapnik. The transduction problem is to estimate the  value of a classification function at the given points in the working  set. This contrasts with the standard inductive learning problem  of estimating the classification function at all possible values and  then using the fixed function to deduce the classes of the working  set data. We propose a general S  3  VM model that minimizes both  the misclassification error and the function capacity based on all  the available data. We show how the S  3  VM model for 1-norm linear  support vector machines can be converted to a mixed-integer  program and then solved exactly using integer programming. Results  of S  3  VM and the standard 1-norm support vector machine  approach are compared on eleven data sets. Our computational  results support the statistical learning theory results showing that  incorporating working data improves generalization when insu#-  cient training information is available. In every case, S  3  VM either  improved or showed no significant di#erence in generalization compared  to the traditional approach.  # This paper has been accepted for publication in Proceedings of Neural Information Processing Systems, Denver, 1998.  1"
162,21,5901,1,{B}ayesian Interpolation,"Although Bayesian analysis has been in use since Laplace, the Bayesian method of model--comparison has only recently been developed in depth.  In this paper, the Bayesian approach to regularisation and model--comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other problems.  Regularising constants are set by examining their posterior probability distribution. Alternative regularisers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. `Occam&#039;s razor&#039; is automatically embodied by this framework.  The way in which Bayes infers the values of regularising constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling.  1 Data modelling and Occam&#039;s razor In science, a central task is to develop and compare models to a..."
163,21,5903,1,"Light-directed, spatially addressable parallel chemical synthesis","Solid-phase chemistry, photolabile protecting groups, and photolithography have been combined to achieve light-directed, spatially addressable parallel chemical synthesis to yield a highly diverse set of chemical products. Binary masking, one of many possible combinatorial synthesis strategies, yields 2n compounds in n chemical steps. An array of 1024 peptides was synthesized in ten steps, and its interaction with a monoclonal antibody was assayed by epifluorescence microscopy. High-density arrays formed by light-directed synthesis are potentially rich sources of chemical diversity for discovering new ligands that bind to biological receptors and for elucidating principles governing molecular interactions. The generality of this approach is illustrated by the light-directed synthesis of a dinucleotide. Spatially directed synthesis of complex compounds could also be used for microfabrication of devices. 10.1126/science.1990438"
164,22,287,1,Freenet: a distributed anonymous information storage and retrieval system,"Abstract. We describe Freenet, an adaptive peer-to-peer network application that permits the publication, replication, and retrieval of data while protecting the anonymity of both authors and readers. Freenet operates as a network of identical nodes that collectively pool their storage space to store data files and cooperate to route requests to the most likely physical location of data. No broadcast search or centralized location index is employed. Files are referred to in a location-independent manner, and are dynamically replicated in locations near requestors and deleted from locations where there is no interest. It is infeasible to discover the true origin or destination of a file passing through the network, and difficult for a node operator to determine or be held responsible for the actual physical contents of her own node. 1"
165,22,431,1,Kademlia: A peer-to-peer information system based on the XOR metric,"We describe a peer-to-peer distributed hash table with provable consistency and performance in a fault-prone environment. Our system routes queries and locates nodes using a novel XOR-based metric topology that simplifies the algorithm and facilitates our proof. The topology has the property that every message exchanged conveys or reinforces useful contact information. The system exploits this information to send parallel, asynchronous query messages that tolerate node failures without imposing timeout delays on users."
166,22,911,1,Tor: The Second-Generation Onion Router,"We present Tor, a circuit-based low-latency anonymous communication service. This second-generation Onion Routing system addresses limitations in the original design by adding perfect forward secrecy, congestion control, directory servers, integrity checking, configurable exit policies, and a practical design for location-hidden services via rendezvous points. Tor works on the real-world Internet, requires no special privileges or kernel modifications, requires little synchronization or coordination between nodes, and provides a reasonable tradeoff between anonymity, usability, and efficiency. We briefly describe our experiences with an international network of more than 30 nodes. We close with a list of open problems in anonymous communication."
167,22,1359,1,Tarzan: A Peer-to-Peer Anonymizing Network Layer,"Tarzan is a peer-to-peer anonymous IP network overlay. Because it provides IP service, Tarzan is general-purpose and transparent to applications. Organized as a decentralized peer-to-peer overlay, Tarzan is fault-tolerant, highly scalable, and easy to manage. Tarzan achieves its anonymity with layered encryption and multi-hop routing, much like a Chaumian mix. A message initiator chooses a path of peers pseudo-randomly through a restricted topology in a way that adversaries cannot easily influence. Cover traffic prevents a global observer from using traffic analysis to identify an initiator. Protocols toward unbiased peer-selection offer new directions for distributing trust among untrusted entities. Tarzan provides anonymity to either clients or servers, without requiring that both participate. In both cases, Tarzan uses a network address translator (NAT) to bridge between Tarzan hosts and oblivious Internet hosts. Measurements show that Tarzan imposes minimal overhead over a corresponding non-anonymous overlay route."
168,22,1372,1,An Analysis of Internet Content Delivery Systems,"In the span of only a few years, the Internet has experienced an astronomical increase in the use of specialized content delivery systems, such as content delivery networks and peer-to-peer file sharing systems. Therefore, an understanding of content delivery on the Internet now requires a detailed understanding of how these systems are used in practice. This paper examines content delivery from the point of view of four content delivery systems: HTTP web traffic, the Akamai content delivery network, and Kazaa and Gnutella peer-to-peer file sharing traffic. We collected a trace of all incoming and outgoing network traffic at the University of Washington, a large university with over 60,000 students, faculty, and staff. From this trace, we isolated and characterized traffic belonging to each of these four delivery classes. Our results (1) quantify the rapidly increasing importance of new content delivery systems, particularly peerto-peer networks, (2) characterize the behavior of these systems from the perspectives of clients, objects, and servers, and (3) derive implications for caching in these systems. 1"
169,22,2339,1,Tapestry: A Resilient Global-scale Overlay for Service Deployment,"We present Tapestry, a peer-to-peer overlay routing infrastructure offering efficient, scalable, location-independent routing of messages directly to nearby copies of an object or service using only localized resources. Tapestry supports a generic decentralized object location and routing applications programming interface using a self-repairing, soft-state-based routing layer. The paper presents the Tapestry architecture, algorithms, and implementation. It explores the behavior of a Tapestry deployment on PlanetLab, a global testbed of approximately 100 machines. Experimental results show that Tapestry exhibits stable behavior and performance as an overlay, despite the instability of the underlying network layers. Several widely distributed applications have been implemented on Tapestry, illustrating its utility as a deployment infrastructure. ER -"
170,22,2347,1,"Efficient dispersal of information for security, load balancing, and fault tolerance","An Information Dispersal Algorithm (IDA) is developed that breaks a file  F  of length  L  = &uharl;  F &uharr; into  n  pieces  F i , l &le;  i  &le;  n , each of length &uharl; F i &uharr; =  L / m , so that every  m  pieces suffice for reconstructing  F . Dispersal and reconstruction are computationally efficient. The sum of the lengths &uharl; F i &uharr; is ( n / m ) &middot;  L . Since  n / m  can be chosen to be close to l, the IDA is space efficient.  IDA has numerous applications to secure and reliable storage of information in computer networks and even on single disks, to fault-tolerant and efficient transmission of information in networks, and to communications between processors in parallel computers. For the latter problem provably time-efficient and highly fault-tolerant routing on the  n -cube is achieved, using just constant size buffers."
171,22,2984,1,The sybil attack,"Large-scale peer-to-peer systems face security threats from faulty or hostile remote computing elements. To resist these threats, many such systems employ redundancy. However, if a single faulty entity can present multiple identities, it can control a substantial fraction of the system, thereby undermining this redundancy. One approach to preventing these &#034;Sybil attacks&#034; is to have a trusted agency certify identities. This paper shows that, without a logically centralized authority, Sybil attacks are always possible except under extreme and unrealistic assumptions of resource parity and coordination among entities."
172,22,3227,1,Computer Networking: A Top-Down Approach Featuring the Internet,"{Certain data-communication protocols hog the spotlight, but all of them have a lot in common. <I>Computer Networking: A Top-Down Approach Featuring the Internet</I> explains the engineering problems that are inherent in communicating digital information from point to point. The top-down approach mentioned in the subtitle means that the book starts at the top of the protocol stack--at the application layer--and works its way down through the other layers, until it reaches bare wire. <p>  The authors, for the most part, shun the well-known seven-layer Open Systems Interconnection (OSI) protocol stack in favor of their own five-layer (application, transport, network, link, and physical) model. It's an effective approach that helps clear away some of the hand waving traditionally associated with the more obtuse layers in the OSI model. The approach is definitely theoretical--don't look here for instructions on configuring Windows 2000 or a Cisco router--but it's relevant to reality, and should help anyone who needs to understand networking as a programmer, system architect, or even administration guru.<p>  The treatment of the network layer, at which routing takes place, is typical of the overall style. In discussing routing, authors James Kurose and Keith Ross explain (by way of lots of clear, definition-packed text) what routing protocols need to do: find the best route to a destination. Then they present the mathematics that determine the best path, show some code that implements those algorithms, and illustrate the logic by using excellent conceptual diagrams. Real-life implementations of the algorithms--including Internet Protocol (both IPv4 and IPv6) and several popular IP routing protocols--help you to make the transition from pure theory to networking technologies. <I>--David Wall</I><p>  <B>Topics covered:</B> The theory behind data networks, with thorough discussion of the problems that are posed at each level (the application layer gets plenty of attention). For each layer, there's academic coverage of networking problems and solutions, followed by discussion of real technologies. Special sections deal with network security and transmission of digital multimedia. } { The most up-to-date introduction to the field of computer networking, this book's top-down approach starts at the application layer and works down the protocol stack. It also uses the Internet as the main example of networks.  This all creates a book relevant to those interested in networking today.  By starting at the application-layer and working down the protocol stack, this book provides a relevant introduction of important concepts. Based on the rationale that once a reader understands the applications of networks they can understand the network services needed to support these applications, this book takes a ""top-down"" approach that exposes readers first to a concrete application and then draws into some of the deeper issues surrounding networking.  This book focuses on the Internet as opposed to addressing it as one of many computer network technologies, further motivating the study of the material.  This book is designed for programmers who need to learn the fundamentals of computer networking. It also has extensive material making it of great interest to networking professionals. }"
173,22,4462,1,Random Walks in Peer-to-Peer Networks,"We quantify the effectiveness of random walks for searching and construction of unstructured peer-to-peer (P2P) networks. We have identified two cases where the use of random walks for searching achieves better results than flooding: a) when the overlay topology is clustered, and h) when a client re-issues the same query while its horizon does not change much. For construction, we argue that an expander can he maintained dynamically with constant operations per addition. The key technical ingredient of our approach is a deep result of stochastic processes indicating that samples taken from consecutive steps of a random walk can achieve statistical properties similar to independent sampling (if the second eigenvalue of the transition matrix is hounded away from 1, which translates to good expansion of the network; such connectivity is desired, and believed to hold, in every reasonable network and network model). This property has been previously used in complexity theory for construction of pseudorandom number generators. We reveal another facet of this theory and translate savings in random bits to savings in processing overhead."
174,22,4463,1,Making gnutella-like P2P systems scalable,"Napster pioneered the idea of peer-to-peer file sharing, and supported it with a centralized file search facility. Subsequent P2P systems like Gnutella adopted decentralized search algorithms. However, Gnutella&#039;s notoriously poor scaling led some to propose distributed hash table solutions to the wide-area file search problem. Contrary to that trend, we advocate retaining Gnutella&#039;s simplicity while proposing new mechanisms that greatly improve its scalability. Building upon prior research [1, 12, 22], we propose several modifications to Gnutella&#039;s design that dynamically adapt the overlay topology and the search algorithms in order to accommodate the natural heterogeneity present in most peer-to-peer systems. We test our design through simulations and the results show three to five orders of magnitude improvement in total system capacity. We also report on a prototype implementation and its deployment on a testbed. Categories and Subject Descriptors  C.2 [Computer Communication Networks]: Distributed Systems  General Terms  Algorithms, Design, Performance, Experimentation  Keywords  Peer-to-peer, distributed hash tables, Gnutella  1."
175,22,6920,1,Accessing Nearby Copies of Replicated Objects in a Distributed Environment,"Consider a set of shared objects in a distributed network, where several copies of each object may exist at any given time. To ensure both fast access to the objects as well as efficient utilization of network resources, it is desirable that each access request be satisfied by a copy &#034;close&#034; to the requesting node. Unfortunately, it is not clear how to efficiently achieve this goal in a dynamic, distributed environment in which large numbers of objects are continuously being created, replicated, and destroyed. In this paper, we design a simple randomized algorithm for accessing shared objects that tends to satisfy each access request with a nearby copy. The algorithm is based on a novel mechanism to maintain and distribute information about object locations, and requires only a small amount of additional memory at each node. We analyze our access scheme for a class of cost functions that captures the hierarchical nature of wide-area networks. We show that under the particular cost mode..."
176,22,7402,1,"Untraceable electronic mail, return addresses, and digital pseudonyms","A technique based on public key cryptography ispresented that allows an electronic mail system to hidewho a participant communicates with as well as thecontent of the communication--in spite of an unsecuredunderlying telecommunication system. The techniquedoes not require a universally trusted authority. Onecorrespondent can remain anonymous to a second, whileallowing the second to respond via an untraceble returnaddress.The technique can also be used to form rosters ofuntraceable digital pseudonyms from selected applications.Applicants retain the exclusive ability to formdigital signatures corresponding to their pseudonyms.Elections in which any interested party can verify thatthe ballots have been properly counted are possible ifanonymously mailed ballots are signed with pseudonymsfrom a roster of registered voters. Another use allows anindividual to correspond with a record-keeping organizationunder a unique pseudonym which appears in aroster of acceptable clients."
177,22,7438,1,Sybilproof reputation mechanisms,"Due to the open, anonymous nature of many P2P networks, new identities - or sybils - may be created cheaply and in large numbers. Given a reputation system, a peer may attempt to falsely raise its reputation by creating fake links between its sybils. Many existing reputation mechanisms are not resistant to these types of strategies.Using a static graph formulation of reputation, we attempt to formalize the notion of sybilproofness. We show that there is no symmetric sybilproof reputation function. For nonsymmetric reputations, following the notion of reputation propagation along paths, we give a general asymmetric reputation function based on flow and give conditions for sybilproofness."
178,22,9760,1,Analyzing and Improving a BitTorrent Network's Performance Mechanisms,"In recent years, BitTorrent has emerged as a very scalable peer-to-peer file distribution mechanism. While early measurement and analytical studies have verified BitTorrentâs performance, they have also raised questions about various metrics (upload utilization, fairness, etc.), particularly in settings other than those measured. In this paper, we present a simulation-based study of BitTorrent. Our goal is to deconstruct the system and evaluate the impact of its core mechanisms, both individually and in combination, on overall system performance under a variety of workloads. Our evaluation focuses on several important metrics, including peer link utilization, file download time, and fairness amongst peers in terms of volume of content served. Our results confirm that BitTorrent performs near-optimally in terms of uplink bandwidth utilization, and download time except under certain extreme conditions. We also show that low bandwidth peers can download more than they upload to the network when high bandwidth peers are present. We find that the rate-based tit-for-tat policy is not effective in preventing unfairness. We show how simple changes to the tracker and a stricter, block-based tit-for-tat policy, greatly improves fairness."
179,22,11698,1,The state of peer-to-peer simulators and simulations,"In this paper, we discuss the current situation with respect to simulation usage in P2P research, testing the available P2P simulators against a proposed set of requirements, and surveying over 280 papers to discover what simulators are already being used. We found that no simulator currently meets all our requirements, and that simulation results are generally reported in the literature in a fashion that precludes any reproduction of results. We hope that this paper will give rise to further discussion and knowledge sharing among those of the P2P and network simulation research communities, so that a simulator that meets the needs of rigorous P2P research can be developed"
180,23,768,1,Basic local alignment search tool.,"{A new approach to rapid sequence comparison, basic local alignment search tool (BLAST), directly approximates alignments that optimize a measure of local similarity, the maximal segment pair (MSP) score. Recent mathematical results on the stochastic properties of MSP scores allow an analysis of the performance of this method as well as the statistical significance of alignments it generates. The basic algorithm is simple and robust; it can be implemented in a number of ways and applied in a variety of contexts including straightforward DNA and protein sequence database searches, motif searches, gene identification searches, and in the analysis of multiple regions of similarity in long DNA sequences. In addition to its flexibility and tractability to mathematical analysis, BLAST is an order of magnitude faster than existing sequence comparison tools of comparable sensitivity.}"
181,23,1581,1,{Discovering regulatory and signalling circuits in molecular interaction networks},"Motivation: In model organisms such as yeast, large databases of  proteinÃ¢ÂÂprotein and protein-DNA interactions have become an extremely  important resource for the study of protein function, evolution, and gene  regulatory dynamics. In this paper we demonstrate that by integrating these  interactions with widely-available mRNA expression data, it is possible to generate concrete hypotheses for the underlying mechanisms governing the  observed changes in gene expression. To perform this integration  systematically and at large scale, we introduce an approach for screening a  molecular interaction network to identify active subnetworks, i.e.,  connected regions of the network that show significant changes in expression over  particular subsets of conditions. The method we present here combines a  rigorous statistical measure for scoring subnetworks with a search algorithm  for identifying subnetworks with high score.Results: We evaluated our procedure on a small network of 332 genes and 362 interactions and a large network of 4160 genes containing all 7462 proteinÃ¢ÂÂprotein and protein-DNA interactions in the yeast public databases.  In the case of the small network, we identified five significant subnetworks  that covered 41 out of 77 (53%) of all significant changes in expression.  Both network analyses returned several top-scoring subnetworks with good  correspondence to known regulatory mechanisms in the literature. These  results demonstrate how large-scale genomic approaches may be used to  uncover signalling and regulatory pathways in a systematic, integrative  fashion.Availability: The methods presented in this paper are implemented  in the Cytoscape software package which is available to the academic community at  http://www.cytoscape.org.Contact: trey@wi.mit.eduKeywords: molecular interactions; gene expression; data integration; simulated annealing; Monte carlo methods."
182,23,2194,1,A Whole-Genome Assembly of Drosophila,"We report on the quality of a whole-genome assembly of Drosophila melanogaster and the nature of the computer algorithms that accomplished it. Three independent external data sources essentially agree with and support the assembly's sequence and ordering of contigs across the euchromatic portion of the genome. In addition, there are isolated contigs that we believe represent nonrepetitive pockets within the heterochromatin of the centromeres. Comparison with a previously sequenced 2.9- megabase region indicates that sequencing accuracy within nonrepetitive segments is greater than 99. 99% without manual curation. As such, this initial reconstruction of the Drosophila sequence should be of substantial value to the scientific community."
183,23,3692,1,Whole-genome random sequencing and assembly of Haemophilus influenzae Rd.,"An approach for genome analysis based on sequencing and assembly of unselected pieces of {DNA} from the whole chromosome has been applied to obtain the complete nucleotide sequence (1,830,137 base pairs) of the genome from the bacterium {H}aemophilus influenzae {R}d. {T}his approach eliminates the need for initial mapping efforts and is therefore applicable to the vast array of microbial species for which genome maps are unavailable. {T}he {H}. influenzae {R}d genome sequence ({G}enome {S}equence {D}ata{B}ase accession number {L}42023) represents the only complete genome sequence from a free-living organism."
184,23,3963,1,Modeling gene and genome duplications in eukaryotes,"10.1073/pnas.0501102102 Recent analysis of complete eukaryotic genome sequences has revealed that gene duplication has been rampant. Moreover, next to a continuous mode of gene duplication, in many eukaryotic organisms the complete genome has been duplicated in their evolutionary past. Such large-scale gene duplication events have been associated with important evolutionary transitions or major leaps in development and adaptive radiations of species. Here, we present an evolutionary model that simulates the duplication dynamics of genes, considering genome-wide duplication events and a continuous mode of gene duplication. Modeling the evolution of the different functional categories of genes assesses the importance of different duplication events for gene families involved in specific functions or processes. By applying our model to the  genome, for which there is compelling evidence for three whole-genome duplications, we show that gene loss is strikingly different for large-scale and small-scale duplication events and highly biased toward certain functional classes. We provide evidence that some categories of genes were almost exclusively expanded through large-scale gene duplication events. In particular, we show that the three whole-genome duplications in  have been directly responsible for >90% of the increase in transcription factors, signal transducers, and developmental genes in the last 350 million years. Our evolutionary model is widely applicable and can be used to evaluate different assumptions regarding small- or large-scale gene duplication events in eukaryotic genomes."
185,23,4311,1,{Mauve: Multiple Alignment of Conserved Genomic Sequence With Rearrangements},"10.1101/gr.2289704 As genomes evolve, they undergo large-scale evolutionary processes that present a challenge to sequence comparison not posed by short sequences. Recombination causes frequent genome rearrangements, horizontal transfer introduces new sequences into bacterial chromosomes, and deletions remove segments of the genome. Consequently, each genome is a mosaic of unique lineage-specific segments, regions shared with a subset of other genomes and segments conserved among all the genomes under consideration. Furthermore, the linear order of these segments may be shuffled among genomes. We present methods for identification and alignment of conserved genomic DNA in the presence of rearrangements and horizontal transfer. Our methods have been implemented in a software package called Mauve. Mauve has been applied to align nine enterobacterial genomes and to determine global rearrangement structure in three mammalian genomes. We have evaluated the quality of Mauve alignments and drawn comparison to other methods through extensive simulations of genome evolution."
186,23,4649,1,A block-sorting lossless data compression algorithm.,"The charter of SRC is to advance both the state of knowledge and the state of the art in computer systems. From our establishment in 1984, we have performed basic and applied research to support Digitalâs business objectives. Our current work includes exploring distributed personal computing on multiple platforms, networking, programming technology, system modelling and management techniques, and selected applications. Our strategy is to test the technical and practical value of our ideas by building hardware and software prototypes and using them as daily tools. Interesting systems are too complex to be evaluated solely in the abstract; extended use allows us to investigate their properties in depth. This experience is useful in the short term in refining our designs, and invaluable in the long term in advancing our knowledge. Most of the major advances in information systems have come through this strategy, including personal computing, distributed systems, and the Internet. We also perform complementary work of a more mathematical flavor. Some of it is in established fields of theoretical computer science, such as the analysis of algorithms, computational geometry, and logics of programming. Other work explores new ground motivated by problems that arise in our systems research. We have a strong commitment to communicating our results; exposing and testing our ideas in the research and development communities leads to improved understanding. Our research report series supplements publication in professional journals and conferences. We seek users for our prototype systems among those with whom we have common interests, and we encourage collaboration with university researchers."
187,23,5165,1,A single determinant dominates the rate of yeast protein evolution.,"A gene's rate of sequence evolution is among the most fundamental evolutionary quantities in common use, but what determines evolutionary rates has remained unclear. Here, we carry out the first combined analysis of seven predictors (gene expression level, dispensability, protein abundance, codon adaptation index, gene length, number of protein-protein interactions, and the gene's centrality in the interaction network) previously reported to have independent influences on protein evolutionary rates. Strikingly, our analysis reveals a single dominant variable linked to the number of translation events which explains 40-fold more variation in evolutionary rate than any other, suggesting that protein evolutionary rate has a single major determinant among the seven predictors. The dominant variable explains nearly half the variation in the rate of synonymous and protein evolution. We show that the two most commonly used methods to disentangle the determinants of evolutionary rate, partial correlation analysis and ordinary multivariate regression, produce misleading or spurious results when applied to noisy biological data. We overcome these difficulties by employing principal component regression, a multivariate regression of evolutionary rate against the principal components of the predictor variables. Our results support the hypothesis that translational selection governs the rate of synonymous and protein sequence evolution in yeast. 10.1093/molbev/msj038"
188,23,5167,1,ARACHNE: A Whole-Genome Shotgun Assembler,"We describe a new computer system, called ARACHNE, for assembling genome sequence using paired-end whole-genome shotgun reads. ARACHNE has several key features, including an efficient and sensitive procedure for finding read overlaps, a procedure for scoring overlaps that achieves high accuracy by correcting errors before assembly, read merger based on forward-reverse links, and detection of repeat contigs by forward-reverse link inconsistency. To test ARACHNE, we created simulated reads providing approximately 10-fold coverage of the genomes of H. influenzae, S. cerevisiae, and D. melanogaster, as well as human chromosomes 21 and 22. The assemblies of these simulated reads yielded nearly complete coverage of the respective genomes, with a small number of contigs joined into a smaller number of supercontigs (or scaffolds). For example, analysis of the D. melanogaster genome yielded approximately 98% coverage with an N50 contig length of 324 kb and an N50 supercontig length of 5143 kb. The assembly accuracy was high, although not perfect: small errors occurred at a frequency of roughly 1 per 1 Mb (typically, deletion of approximately 1 kb in size), with a very small number of other misassemblies. The assembly was rapid: the Drosophila assembly required only 21 hours on a single 667 MHz processor and used 8.4 Gb of memory."
189,23,5819,1,Improved prediction of signal peptides: SignalP 3.0.,"We describe improvements of the currently most popular method for prediction of classically secreted proteins, SignalP. SignalP consists of two different predictors based on neural network and hidden Markov model algorithms, where both components have been updated. Motivated by the idea that the cleavage site position and the amino acid composition of the signal peptide are correlated, new features have been included as input to the neural network. This addition, combined with a thorough error-correction of a new data set, have improved the performance of the predictor significantly over SignalP version 2. In version 3, correctness of the cleavage site predictions has increased notably for all three organism groups, eukaryotes, Gram-negative and Gram-positive bacteria. The accuracy of cleavage site prediction has increased in the range 6â17\\ over the previous version, whereas the signal peptide discrimination improvement is mainly due to the elimination of false-positive predictions, as well as the introduction of a new discrimination score for the neural network. The new method has been benchmarked against other available methods. Predictions can be made at the publicly available web server http://www.cbs.dtu.dk/services/SignalP/"
190,23,6274,1,Essential genes of a minimal bacterium.,"Mycoplasma genitalium has the smallest genome of any organism that can be grown in pure culture. It has a minimal metabolism and little genomic redundancy. Consequently, its genome is expected to be a close approximation to the minimal set of genes needed to sustain bacterial life. Using global transposon mutagenesis, we isolated and characterized gene disruption mutants for 100 different nonessential protein-coding genes. None of the 43 RNA-coding genes were disrupted. Herein, we identify 382 of the 482 M. genitalium protein-coding genes as essential, plus five sets of disrupted genes that encode proteins with potentially redundant essential functions, such as phosphate transport. Genes encoding proteins of unknown function constitute 28\% of the essential protein-coding genes set. Disruption of some genes accelerated M. genitalium growth."
191,23,7759,1,Duplicated genes evolve slower than singletons despite the initial rate increase.,"Background: Gene duplication is an important mechanism that can lead to the emergence of new functions during evolution. The impact of duplication on the mode of gene evolution has been the subject of several theoretical and empirical comparative-genomic studies. It has been shown that, shortly after the duplication, genes seem to experience a considerable relaxation of purifying selection. Results: Here we demonstrate two opposite effects of gene duplication on evolutionary rates. Sequence comparisons between paralogs show that, in accord with previous observations, a substantial acceleration in the evolution of paralogs occurs after duplication, presumably due to relaxation of purifying selection. The effect of gene duplication on evolutionary rate was also assessed by sequence comparison between orthologs that have paralogs ( duplicates) and those that do not ( singletons). It is shown that, in eukaryotes, duplicates, on average, evolve significantly slower than singletons. Eukaryotic ortholog evolutionary rates for duplicates are also negatively correlated with the number of paralogs per gene and the strength of selection between paralogs. A tally of annotated gene functions shows that duplicates tend to be enriched for proteins with known functions, particularly those involved in signaling and related cellular processes; by contrast, singletons include an over-abundance of poorly characterized proteins. Conclusions: These results suggest that whether or not a gene duplicate is retained by selection depends critically on the pre-existing functional utility of the protein encoded by the ancestral singleton. Duplicates of genes of a higher biological import, which are subject to strong functional constraints on the sequence, are retained relatively more often. Thus, the evolutionary trajectory of duplicated genes appears to be determined by two opposing trends, namely, the post-duplication rate acceleration and the generally slow evolutionary rate owing to the high level of functional constraints."
192,23,9322,1,Symbiosis insights through metagenomic analysis of a microbial consortium,"Symbioses between bacteria and eukaryotes are ubiquitous, yet our understanding of the interactions driving these associations is hampered by our inability to cultivate most host-associated microbes. Here we use a metagenomic approach to describe four co-occurring symbionts from the marine oligochaete Olavius algarvensis, a worm lacking a mouth, gut and nephridia. Shotgun sequencing and metabolic pathway reconstruction revealed that the symbionts are sulphur-oxidizing and sulphate-reducing bacteria, all of which are capable of carbon fixation, thus providing the host with multiple sources of nutrition. Molecular evidence for the uptake and recycling of worm waste products by the symbionts suggests how the worm could eliminate its excretory system, an adaptation unique among annelid worms. We propose a model that describes how the versatile metabolism within this symbiotic consortium provides the host with an optimal energy supply as it shuttles between the upper oxic and lower anoxic coastal sediments that it inhabits."
193,23,9421,1,Phylogenetic footprinting of transcription factor binding sites in proteobacterial genomes.,"Toward the goal of identifying complete sets of transcription factor (TF)-binding sites in the genomes of several gamma proteobacteria, and hence describing their transcription regulatory networks, we present a phylogenetic footprinting method for identifying these sites. Probable transcription regulatory sites upstream of Escherichia coli genes were identified by cross-species comparison using an extended Gibbs sampling algorithm. Close examination of a study set of 184 genes with documented transcription regulatory sites revealed that when orthologous data were available from at least two other gamma proteobacterial species, 81\% of our predictions corresponded with the documented sites, and 67\% corresponded when data from only one other species were available. That the remaining predictions included bona fide TF-binding sites was proven by affinity purification of a putative transcription factor (YijC) bound to such a site upstream of the fabA gene. Predicted regulatory sites for 2097 E.coli genes are available at http://www.wadsworth.org/resnres/bioinfo/."
194,23,9906,1,Proteorhodopsin phototrophy in the ocean.,"Proteorhodopsin, a retinal-containing integral membrane protein that functions as a light-driven proton pump, was discovered in the genome of an uncultivated marine bacterium; however, the prevalence, expression and genetic variability of this protein in native marine microbial populations remain unknown. Here we report that photoactive proteorhodopsin is present in oceanic surface waters. We also provide evidence of an extensive family of globally distributed proteorhodopsin variants. The protein pigments comprising this rhodopsin family seem to be spectrally tuned to different habitats--absorbing light at different wavelengths in accordance with light available in the environment. Together, our data suggest that proteorhodopsin-based phototrophy is a globally significant oceanic microbial process."
195,23,10042,1,Whole-genome re-sequencing.,"{DNA} sequencing can be used to gain important information on genes, genetic variation and gene function for biological and medical studies. The growing collection of publicly available reference genome sequences will underpin a new era of whole genome re-sequencing, but sequencing costs need to fall and throughput needs to rise by several orders of magnitude. Novel technologies are being developed to meet this need by generating massive amounts of sequence that can be aligned to the reference sequence. The challenge is to maintain the high standards of accuracy and completeness that are hallmarks of the previous genome projects. One or more new sequencing technologies are expected to become the mainstay of future research, and to make {DNA} sequencing centre stage as a routine tool in genetic research in the coming years."
196,23,10569,1,Metaproteomics approach to study the functionality of the microbiota in the human infant gastrointestinal tract.,"A metaproteomics approach comprising two-dimensional gel electrophoresis and matrix-assisted laser desorption ionization-time of flight (mass spectrometry) was applied to the largely uncultured infant fecal microbiota for the first time. The fecal microbial metaproteome profiles changed over time, and one protein spot contained a peptide sequence that showed high similarity to those of bifidobacterial transaldolases."
197,23,10651,1,"Minimus: a fast, lightweight genome assembler","BACKGROUND: Genome assemblers have grown very large and complex in response to the need for algorithms to handle the challenges of large whole-genome sequencing projects. Many of the most common uses of assemblers, however, are best served by a simpler type of assembler that requires fewer software components, uses less memory, and is far easier to install and run. RESULTS: We have developed the Minimus assembler to address these issues, and tested it on a range of assembly problems. We show that Minimus performs well on several small assembly tasks, including the assembly of viral genomes, individual genes, and BAC clones. In addition, we evaluate Minimus' performance in assembling bacterial genomes in order to assess its suitability as a component of a larger assembly pipeline. We show that, unlike other software currently used for these tasks, Minimus produces significantly fewer assembly errors, at the cost of generating a more fragmented assembly. CONCLUSION: We find that for small genomes and other small assembly tasks, Minimus is faster and far more flexible than existing tools. Due to its small size and modular design Minimus is perfectly suited to be a component of complex assembly pipelines. Minimus is released as an open-source software project and the code is available as part of the AMOS project at Sourceforge."
198,23,10717,1,Identifying bacterial genes and endosymbiont DNA with Glimmer,"Motivation: The Glimmer gene-finding software has been successfully used for finding genes in bacteria, archaea, and viruses representing hundreds of species. We describe several major changes to the Glimmer system, including improved methods for identifying both coding regions and start codons. We also describe a new module of Glimmer that can distinguish host and endosymbiont DNA. This module was developed in response to the discovery that eukaryotic genome sequencing projects sometimes inadvertently capture the DNA of intracellular bacteria living in the host.  Results: The new methods dramatically reduce the rate of false-positive predictions, while maintaining Glimmer's 99% sensitivity rate at detecting genes in most species, and they find substantially more correct start sites, as measured by comparisons to known and well-curated genes. We show that our interpolated Markov model (IMM) DNA discriminator correctly separated 99% of the sequences in a recent genome project that produced a mixture of sequences from the bacterium Prochloron didemni and its sea squirt host, Lissoclinum patella.  Availability: Glimmer is OSI Certified Open Source and available at http://cbcb.umd.edu/software/glimmer 10.1093/bioinformatics/btm009"
199,23,10769,1,The Sorcerer II Global Ocean Sampling expedition: northwest Atlantic through eastern tropical Pacific.,"The world's oceans contain a complex mixture of micro-organisms that are for the most part, uncharacterized both genetically and biochemically. We report here a metagenomic study of the marine planktonic microbiota in which surface (mostly marine) water samples were analyzed as part of the Sorcerer II Global Ocean Sampling expedition. These samples, collected across a several-thousand km transect from the North Atlantic through the Panama Canal and ending in the South Pacific yielded an extensive dataset consisting of 7.7 million sequencing reads (6.3 billion bp). Though a few major microbial clades dominate the planktonic marine niche, the dataset contains great diversity with 85% of the assembled sequence and 57% of the unassembled data being unique at a 98% sequence identity cutoff. Using the metadata associated with each sample and sequencing library, we developed new comparative genomic and assembly methods. One comparative genomic method, termed ""fragment recruitment,"" addressed questions of genome structure, evolution, and taxonomic or phylogenetic diversity, as well as the biochemical diversity of genes and gene families. A second method, termed ""extreme assembly,"" made possible the assembly and reconstruction of large segments of abundant but clearly nonclonal organisms. Within all abundant populations analyzed, we found extensive intra-ribotype diversity in several forms: (1) extensive sequence variation within orthologous regions throughout a given genome; despite coverage of individual ribotypes approaching 500-fold, most individual sequencing reads are unique; (2) numerous changes in gene content some with direct adaptive implications; and (3) hypervariable genomic islands that are too variable to assemble. The intra-ribotype diversity is organized into genetically isolated populations that have overlapping but independent distributions, implying distinct environmental preference. We present novel methods for measuring the genomic similarity between metagenomic samples and show how they may be grouped into several community types. Specific functional adaptations can be identified both within individual ribotypes and across the entire community, including proteorhodopsin spectral tuning and the presence or absence of the phosphate-binding gene PstS."
200,23,10951,1,Comparative analysis indicates regulatory neofunctionalization of yeast duplicates.,"ABSTRACT: BACKGROUND: Gene duplication provides raw material for the generation of new functions, but most duplicates are rapidly lost due to the initial redundancy in gene function. How gene function diversifies following duplication is largely unclear. Previous studies analyzed the diversification of duplicates by characterizing their coding sequence divergence. However, functional divergence can also be attributed to changes in regulatory properties, such as protein localization or expression, which require only minor changes in gene sequence. RESULTS: We developed a novel method to compare expression profiles from different organisms and applied it to analyze the expression divergence of yeast duplicated genes. The expression profiles of Saccharomyces cerevisiae duplicate pairs were compared with those of their pre-duplication orthologs in Candida albicans. Duplicate pairs were classified into two classes, corresponding to symmetric versus asymmetric rates of expression divergence. The latter class includes 43 duplicate pairs in which only one copy has a significant expression similarity to the C. albicans ortholog. These may present cases of regulatory neofunctionalization, as supported also by their dispensability and variability. CONCLUSION: Duplicated genes may diversify through regulatory neofunctionalization. Notably, the asymmetry of gene sequence evolution and the asymmetry of gene expression evolution are only weakly correlated, underscoring the importance of expression analysis to elucidate the evolution of novel functions."
201,23,11583,1,"Locating proteins in the cell using TargetP, SignalP and related tools.","Determining the subcellular localization of a protein is an important first step toward understanding its function. Here, we describe the properties of three well-known N-terminal sequence motifs directing proteins to the secretory pathway, mitochondria and chloroplasts, and sketch a brief history of methods to predict subcellular localization based on these sorting signals and other sequence properties. We then outline how to use a number of internet-accessible tools to arrive at a reliable subcellular localization prediction for eukaryotic and prokaryotic proteins. In particular, we provide detailed step-by-step instructions for the coupled use of the amino-acid sequence-based predictors TargetP, SignalP, ChloroP and TMHMM, which are all hosted at the Center for Biological Sequence Analysis, Technical University of Denmark. In addition, we describe and provide web references to other useful subcellular localization predictors. Finally, we discuss predictive performance measures in general and the performance of TargetP and SignalP in particular."
202,23,11957,1,"Whole-genome sequencing and assembly with high-throughput, short-read technologies.","While recently developed short-read sequencing technologies may dramatically reduce the sequencing cost and eventually achieve the $1000 goal for re-sequencing, their limitations prevent the de novo sequencing of eukaryotic genomes with the standard shotgun sequencing protocol. We present SHRAP (SHort Read Assembly Protocol), a sequencing protocol and assembly methodology that utilizes high-throughput short-read technologies. We describe a variation on hierarchical sequencing with two crucial differences: (1) we select a clone library from the genome randomly rather than as a tiling path and (2) we sample clones from the genome at high coverage and reads from the clones at low coverage. We assume that 200 bp read lengths with a 1% error rate and inexpensive random fragment cloning on whole mammalian genomes is feasible. Our assembly methodology is based on first ordering the clones and subsequently performing read assembly in three stages: (1) local assemblies of regions significantly smaller than a clone size, (2) clone-sized assemblies of the results of stage 1, and (3) chromosome-sized assemblies. By aggressively localizing the assembly problem during the first stage, our method succeeds in assembling short, unpaired reads sampled from repetitive genomes. We tested our assembler using simulated reads from D. melanogaster and human chromosomes 1, 11, and 21, and produced assemblies with large sets of contiguous sequence and a misassembly rate comparable to other draft assemblies. Tested on D. melanogaster and the entire human genome, our clone-ordering method produces accurate maps, thereby localizing fragment assembly and enabling the parallelization of the subsequent steps of our pipeline. Thus, we have demonstrated that truly inexpensive de novo sequencing of mammalian genomes will soon be possible with high-throughput, short-read technologies using our methodology."
203,23,12510,1,"SHARCGS, a fast and highly accurate short-read assembly algorithm for de novo genomic sequencing","10.1101/gr.6435207 The latest revolution in the DNA sequencing field has been brought about by the development of automated sequencers that are capable of generating giga base pair data sets quickly and at low cost. Applications of such technologies seem to be limited to resequencing and transcript discovery, due to the shortness of the generated reads. In order to extend the fields of application to de novo sequencing, we developed the SHARCGS algorithm to assemble short-read (25â40-mer) data with high accuracy and speed. The efficiency of SHARCGS was tested on BAC inserts from three eukaryotic species, on two yeast chromosomes, and on two bacterial genomes (Haemophilus influenzae, Escherichia coli). We show that 30-mer-based BAC assemblies have N50 sizes >20 kbp for Drosophila and Arabidopsis and >4 kbp for human in simulations taking missing reads and wrong base calls into account. We assembled 949,974 contigs with length >50 bp, and only one single contig could not be aligned error-free against the reference sequences. We generated 36-mer reads for the genome of Helicobacter acinonychis on the Illumina 1G sequencing instrument and assembled 937 contigs covering 98% of the genome with an N50 size of 3.7 kbp. With the exception of five contigs that differ in 1â4 positions relative to the reference sequence, all contigs matched the genome error-free. Thus, SHARCGS is a suitable tool for fully exploiting novel sequencing technologies by assembling sequence contigs de novo with high confidence and by outperforming existing assembly algorithms in terms of speed and accuracy."
204,23,12644,1,Metagenomic and functional analysis of hindgut microbiota of a wood-feeding higher termite,"From the standpoints of both basic research and biotechnology, there is considerable interest in reaching a clearer understanding of the diversity of biological mechanisms employed during lignocellulose degradation. Globally, termites are an extremely successful group of wood-degrading organisms and are therefore important both for their roles in carbon turnover in the environment and as potential sources of biochemical catalysts for efforts aimed at converting wood into biofuels. Only recently have data supported any direct role for the symbiotic bacteria in the gut of the termite in cellulose and xylan hydrolysis. Here we use a metagenomic analysis of the bacterial community resident in the hindgut paunch of a wood-feeding 'higher' Nasutitermes species (which do not contain cellulose-fermenting protozoa) to show the presence of a large, diverse set of bacterial genes for cellulose and xylan hydrolysis. Many of these genes were expressed in vivo or had cellulase activity in vitro, and further analyses implicate spirochete and fibrobacter species in gut lignocellulose degradation. New insights into other important symbiotic functions including H2 metabolism, CO2-reductive acetogenesis and N2 fixation are also provided by this first system-wide gene analysis of a microbial community specialized towards plant lignocellulose degradation. Our results underscore how complex even a 1-microl environment can be."
205,23,13093,1,The RAST Server: rapid annotations using subsystems technology.,"ABSTRACT: BACKGROUND: The number of prokaryotic genome sequences becoming available is growing steadily and is growing faster than our ability to accurately annotate them. Description: We describe a fully automated service for annotating bacterial and archaeal genomes. The service identifies protein-encoding, rRNA and tRNA genes, assigns functions to the genes, predicts which subsystems are represented in the genome, uses this information to reconstruct the metabolic network and makes the output easily downloadable for the user. In addition, the annotated genome can be browsed in an environment that supports comparative analysis with the annotated genomes maintained in the SEED environment. The service normally makes the annotated genome available within 12-24 hours of submission, but ultimately the quality of such a service will be judged in terms of accuracy, consistency, and completeness of the produced annotations. We summarize our attempts to address these issues and discuss plans for incrementally enhancing the service. CONCLUSIONS: By providing accurate, rapid annotation freely to the community we have created an important community resource. The service has now been utilized by over 120 external users annotating over 350 distinct genomes."
206,23,13111,1,The impact of next-generation sequencing technology on genetics," If one accepts that the fundamental pursuit of genetics is to determine the genotypes that explain phenotypes, the meteoric increase of DNA sequence information applied toward that pursuit has nowhere to go but up. The recent introduction of instruments capable of producing millions of DNA sequence reads in a single run is rapidly changing the landscape of genetics, providing the ability to answer questions with heretofore unimaginable speed. These technologies will provide an inexpensive, genome-wide sequence readout as an endpoint to applications ranging from chromatin immunoprecipitation, mutation mapping and polymorphism discovery to noncoding RNA discovery. Here I survey next-generation sequencing technologies and consider how they can provide a more complete picture of how the genome shapes the organism."
207,23,13358,1,Toward a census of bacteria in soil.,"For more than a century, microbiologists have sought to determine the species richness of bacteria in soil, but the extreme complexity and unknown structure of soil microbial communities have obscured the answer. We developed a statistical model that makes the problem of estimating richness statistically accessible by evaluating the characteristics of samples drawn from simulated communities with parametric community distributions. We identified simulated communities with rank-abundance distributions that followed a truncated lognormal distribution whose samples resembled the structure of 16S rRNA gene sequence collections made using Alaskan and Minnesotan soils. The simulated communities constructed based on the distribution of 16S rRNA gene sequences sampled from the Alaskan and Minnesotan soils had a richness of 5,000 and 2,000 operational taxonomic units (OTUs), respectively, where an OTU represents a collection of sequences not more than 3% distant from each other. To sample each of these OTUs in the Alaskan 16S rRNA gene library at least twice, 480,000 sequences would be required; however, to estimate the richness of the simulated communities using nonparametric richness estimators would require only 18,000 sequences. Quantifying the richness of complex environments such as soil is an important step in building an ecological framework. We have shown that generating sufficient sequence data to do so requires less sequencing effort than completely sequencing a bacterial genome."
208,23,13887,1,Historical contingency and the evolution of a key innovation in an experimental population of Escherichia coli,"10.1073/pnas.0803151105 The role of historical contingency in evolution has been much debated, but rarely tested. Twelve initially identical populations of Escherichia coli were founded in 1988 to investigate this issue. They have since evolved in a glucose-limited medium that also contains citrate, which E. coli cannot use as a carbon source under oxic conditions. No population evolved the capacity to exploit citrate for >30,000 generations, although each population tested billions of mutations. A citrate-using (Cit+) variant finally evolved in one population by 31,500 generations, causing an increase in population size and diversity. The long-delayed and unique evolution of this function might indicate the involvement of some extremely rare mutation. Alternately, it may involve an ordinary mutation, but one whose physical occurrence or phenotypic expression is contingent on prior mutations in that population. We tested these hypotheses in experiments that âreplayedâ evolution from different points in that population's history. We observed no Cit+ mutants among 8.4 Ã 1012 ancestral cells, nor among 9 Ã 1012 cells from 60 clones sampled in the first 15,000 generations. However, we observed a significantly greater tendency for later clones to evolve Cit+, indicating that some potentiating mutation arose by 20,000 generations. This potentiating change increased the mutation rate to Cit+ but did not cause generalized hypermutability. Thus, the evolution of this phenotype was contingent on the particular history of that population. More generally, we suggest that historical contingency is especially important when it facilitates the evolution of key innovations that are not easily evolved by gradual, cumulative selection."
209,23,13901,1,k-means++: the advantages of careful seeding,"The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is &Theta;(log k )-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically."
210,23,14511,1,Efficient mapping of Applied Biosystems SOLiD sequence data to a reference genome for functional genomic applications,"Summary: Here, we report the development of SOCS (short oligonucleotide color space), a program designed for efficient and flexible mapping of Applied Biosystems SOLiD sequence data onto a reference genome. SOCS performs its mapping within the context of  color space', and it maximizes usable data by allowing a user-specified number of mismatches. Sequence census functions facilitate a variety of functional genomics applications, including transcriptome mapping and profiling, as well as ChIP-Seq.  Availability: Executables, source code, and sample data are available at http://socs.biology.gatech.edu/  Contact: nickbergman@gatech.edu  Supplementary information: Supplementary data are available at Bioinformatics Online. 10.1093/bioinformatics/btn512"
211,23,14557,1,"Deep sequencing-based expression analysis shows major advances in robustness, resolution and inter-lab portability over five microarray platforms","The hippocampal expression profiles of wild-type mice and mice transgenic for {delta}C-doublecortin-like kinase were compared with Solexa/Illumina deep sequencing technology and five different microarray platforms. With Illumina's digital gene expression assay, we obtained [~]2.4 million sequence tags per sample, their abundance spanning four orders of magnitude. Results were highly reproducible, even across laboratories. With a dedicated Bayesian model, we found differential expression of 3179 transcripts with an estimated false-discovery rate of 8.5%. This is a much higher figure than found for microarrays. The overlap in differentially expressed transcripts found with deep sequencing and microarrays was most significant for Affymetrix. The changes in expression observed by deep sequencing were larger than observed by microarrays or quantitative PCR. Relevant processes such as calmodulin-dependent protein kinase activity and vesicle transport along microtubules were found affected by deep sequencing but not by microarrays. While undetectable by microarrays, antisense transcription was found for 51% of all genes and alternative polyadenylation for 47%. We conclude that deep sequencing provides a major advance in robustness, comparability and richness of expression profiling data and is expected to boost collaborative, comparative and integrative genomics studies. 10.1093/nar/gkn705"
212,23,14704,1,Turning a hobby into a job: How duplicated genes find new functions,"Gene duplication provides raw material for functional innovation. Recent advances have shed light on two fundamental questions regarding gene duplication: which genes tend to undergo duplication? And how does natural selection subsequently act on them? Genomic data suggest that different gene classes tend to be retained after single-gene and whole-genome duplications. We also know that functional differences between duplicate genes can originate in several different ways, including mutations that directly impart new functions, subdivision of ancestral functions and selection for changes in gene dosage. Interestingly, in many cases the 'new' function of one copy is a secondary property that was always present, but that has been co-opted to a primary role after the duplication."
213,23,14730,1,Exploring Microbial Diversity and Taxonomy Using SSU rRNA Hypervariable Tag Sequencing,"Massively parallel pyrosequencing of hypervariable regions from small subunit ribosomal RNA (SSU rRNA) genes can sample a microbial community two or three orders of magnitude more deeply per dollar and per hour than capillary sequencing of full-length SSU rRNA. As with full-length rRNA surveys, each sequence read is a tag surrogate for a single microbe. However, rather than assigning taxonomy by creating gene trees de novo that include all experimental sequences and certain reference taxa, we compare the hypervariable region tags to an extensive database of rRNA sequences and assign taxonomy based on the best match in a Global Alignment for Sequence Taxonomy (GAST) process. The resulting taxonomic census provides information on both composition and diversity of the microbial community. To determine the effectiveness of using only hypervariable region tags for assessing microbial community membership, we compared the taxonomy assigned to the V3 and V6 hypervariable regions with the taxonomy assigned to full-length SSU rRNA sequences isolated from both the human gut and a deep-sea hydrothermal vent. The hypervariable region tags and full-length rRNA sequences provided equivalent taxonomy and measures of relative abundance of microbial communities, even for tags up to 15% divergent from their nearest reference match. The greater sampling depth per dollar afforded by massively parallel pyrosequencing reveals many more members of the Ã¢â¬Årare biosphereÃ¢â¬ï¿½ than does capillary sequencing of the full-length gene. In addition, tag sequencing eliminates cloning bias and the sequences are short enough to be completely sequenced in a single read, maximizing the number of organisms sampled in a run while minimizing chimera formation. This technique allows the cost-effective exploration of changes in microbial community structure, including the rare biosphere, over space and time and can be applied immediately to initiatives, such as the Human Microbiome Project."
214,23,15031,1,Multiple whole-genome alignments without a reference organism,"10.1101/gr.081778.108 Multiple sequence alignments have become one of the most commonly used resources in genomics research. Most algorithms for multiple alignment of whole genomes rely either on a reference genome, against which all of the other sequences are laid out, or require a one-to-one mapping between the nucleotides of the genomes, preventing the alignment of recently duplicated regions. Both approaches have drawbacks for whole-genome comparisons. In this paper we present a novel symmetric alignment algorithm. The resulting alignments not only represent all of the genomes equally well, but also include all relevant duplications that occurred since the divergence from the last common ancestor. Our algorithm, implemented as a part of the VISTA Genome Pipeline (VGP), was used to align seven vertebrate and six  genomes. The resulting whole-genome alignments demonstrate a higher sensitivity and specificity than the pairwise alignments previously available through the VGP and have higher exon alignment accuracy than comparable public whole-genome alignments. Of the multiple alignment methods tested, ours performed the best at aligning genes from multigene familiesâperhaps the most challenging test for whole-genome alignments. Our whole-genome multiple alignments are available through the VISTA Browser at ."
215,23,15226,1,A consistency-based consensus algorithm for de novo and reference-guided sequence assembly of short reads,"Motivation: Novel high-throughput sequencing technologies pose new algorithmic challenges in handling massive amounts of shortread, high-coverage data. A robust and versatile consensus tool is of particular interest for such data since a sound multi-read alignment is a prerequisite for variation analyses, accurate genome assemblies and insert sequencing. Results: A multi-read alignment algorithm for de novo or reference-guided genome assembly is presented. The program identifies segments shared by multiple reads and then aligns these segments using a consistency-enhanced alignment graph. On real de novo sequencing data, obtained from the newly established NCBI Short Read Archive, the program performs similarly in quality to other comparable programs. On more challenging simulated data sets for insert sequencing and variation analyses our program outperforms the other tools. Availability: Availability: The consensus program can be downloaded from http://www.seqan.de/projects/consensus.html. It can be used stand-alone or in conjunction with the Celera Assembler. Both application scenarios as well as the usage of the tool are described in the documentation. Contact: rausch{\\char64}inf.fu-berlin.de"
216,23,15413,1,"Assembling the marine metagenome, one cell at a time.","The difficulty associated with the cultivation of most microorganisms and the complexity of natural microbial assemblages, such as marine plankton or human microbiome, hinder genome reconstruction of representative taxa using cultivation or metagenomic approaches. Here we used an alternative, single cell sequencing approach to obtain high-quality genome assemblies of two uncultured, numerically significant marine microorganisms. We employed fluorescence-activated cell sorting and multiple displacement amplification to obtain hundreds of micrograms of genomic DNA from individual, uncultured cells of two marine flavobacteria from the Gulf of Maine that were phylogenetically distant from existing cultured strains. Shotgun sequencing and genome finishing yielded 1.9 Mbp in 17 contigs and 1.5 Mbp in 21 contigs for the two flavobacteria, with estimated genome recoveries of about 91% and 78%, respectively. Only 0.24% of the assembling sequences were contaminants and were removed from further analysis using rigorous quality control. In contrast to all cultured strains of marine flavobacteria, the two single cell genomes were excellent Global Ocean Sampling (GOS) metagenome fragment recruiters, demonstrating their numerical significance in the ocean. The geographic distribution of GOS recruits along the Northwest Atlantic coast coincided with ocean surface currents. Metabolic reconstruction indicated diverse potential energy sources, including biopolymer degradation, proteorhodopsin photometabolism, and hydrogen oxidation. Compared to cultured relatives, the two uncultured flavobacteria have small genome sizes, few non-coding nucleotides, and few paralogous genes, suggesting adaptations to narrow ecological niches. These features may have contributed to the abundance of the two taxa in specific regions of the ocean, and may have hindered their cultivation. We demonstrate the power of single cell DNA sequencing to generate reference genomes of uncultured taxa from a complex microbial community of marine bacterioplankton. A combination of single cell genomics and metagenomics enabled us to analyze the genome content, metabolic adaptations, and biogeography of these taxa."
217,23,16135,1,The Universal Protein Resource (UniProt) in 2010,"The primary mission of UniProt is to support biological research by maintaining a stable, comprehensive, fully classified, richly and accurately annotated protein sequence knowledgebase, with extensive cross-references and querying interfaces freely accessible to the scientific community. UniProt is produced by the UniProt Consortium which consists of groups from the European Bioinformatics Institute (EBI), the Swiss Institute of Bioinformatics (SIB) and the Protein Information Resource (PIR). UniProt is comprised of four major components, each optimized for different uses: the UniProt Archive, the UniProt Knowledgebase, the UniProt Reference Clusters and the UniProt Metagenomic and Environmental Sequence Database. UniProt is updated and distributed every 3 weeks and can be accessed online for searches or download at http://www.uniprot.org."
218,23,16494,1,A human gut microbial gene catalogue established by metagenomic sequencing,"To understand the impact of gut microbes on human health and well-being it is crucial to assess their genetic potential. Here we describe the Illumina-based metagenomic sequencing, assembly and characterization of 3.3 million non-redundant microbial genes, derived from 576.7 gigabases of sequence, from faecal samples of 124 European individuals. The gene set, approximately 150 times larger than the human gene complement, contains an overwhelming majority of the prevalent (more frequent) microbial genes of the cohort and probably includes a large proportion of the prevalent human intestinal microbial genes. The genes are largely shared among individuals of the cohort. Over 99% of the genes are bacterial, indicating that the entire cohort harbours between 1,000 and 1,150 prevalent bacterial species and each individual at least 160 such species, which are also largely shared. We define and describe the minimal gut metagenome and the minimal gut bacterial genome in terms of functions present in all individuals and most bacteria, respectively."
219,23,16744,1,Efficient construction of an assembly string graph using the FM-index,"Motivation: Sequence assembly is a difficult problem whose importance has grown again recently as the cost of sequencing has dramatically dropped. Most new sequence assembly software has started by building a de Bruijn graph, avoiding the overlap-based methods used previously because of the computational cost and complexity of these with very large numbers of short reads. Here, we show how to use suffix array-based methods that have formed the basis of recent very fast sequence mapping algorithms to find overlaps and generate assembly string graphs asymptotically faster than previously described algorithms.Results: Standard overlap assembly methods have time complexity O(N2), where N is the sum of the lengths of the reads. We use the FerraginaÃ¢ÂÂManzini index (FM-index) derived from the BurrowsÃ¢ÂÂWheeler transform to find overlaps of length at least ÃÂ among a set of reads. As well as an approach that finds all overlaps then implements transitive reduction to produce a string graph, we show how to output directly only the irreducible overlaps, significantly shrinking memory requirements and reducing compute time to O(N), independent of depth. Overlap-based assembly methods naturally handle mixed length read sets, including capillary reads or long reads promised by the third generation sequencing technologies. The algorithms we present here pave the way for overlap-based assembly approaches to be developed that scale to whole vertebrate genome de novo assembly.Contact: js18@sanger.ac.uk"
220,24,29,1,The structure and function of complex networks,"Inspired by empirical studies of networked systems such as the Internet, social networks, and bio- logical networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks."
221,24,10920,1,Generalizations of the clustering coefficient to weighted complex networks,"The recent high level of interest in weighted complex networks gives rise to a need to develop new measures and to generalize existing ones to take the weights of links into account. Here we focus on various generalizations of the clustering coefficient, which is one of the central characteristics in the complex network theory. We present a comparative study of the several suggestions introduced in the literature, and point out their advantages and limitations. The concepts are illustrated by simple examples as well as by empirical data of the world trade and weighted coauthorship networks."
222,24,10945,1,Quantifying social group evolution,"The rich set of interactions between individuals in society1, 2, 3, 4, 5, 6, 7 results in complex community structure, capturing highly connected circles of friends, families or professional cliques in a social network3, 7, 8, 9, 10. Thanks to frequent changes in the activity and communication patterns of individuals, the associated social and communication network is subject to constant evolution7, 11, 12, 13, 14, 15, 16. Our knowledge of the mechanisms governing the underlying community dynamics is limited, but is essential for a deeper understanding of the development and self-optimization of society as a whole17, 18, 19, 20, 21, 22. We have developed an algorithm based on clique percolation23, 24 that allows us to investigate the time dependence of overlapping communities on a large scale, and thus uncover basic relationships characterizing community evolution. Our focus is on networks capturing the collaboration between scientists and the calls between mobile phone users. We find that large groups persist for longer if they are capable of dynamically altering their membership, suggesting that an ability to change the group composition results in better adaptability. The behaviour of small groups displays the opposite tendencyâthe condition for stability is that their composition remains unchanged. We also show that knowledge of the time commitment of members to a given community can be used for estimating the communityâs lifetime. These findings offer insight into the fundamental differences between the dynamics of small groups and large institutions."
223,24,14263,1,The Collective Dynamics of Smoking in a Large Social Network,"Background The prevalence of smoking has decreased substantially in the United States over the past 30 years. We examined the extent of the person-to-person spread of smoking behavior and the extent to which groups of widely connected people quit together.  Methods We studied a densely interconnected social network of 12,067 people assessed repeatedly from 1971 to 2003 as part of the Framingham Heart Study. We used network analytic methods and longitudinal statistical models.  Results Discernible clusters of smokers and nonsmokers were present in the network, and the clusters extended to three degrees of separation. Despite the decrease in smoking in the overall population, the size of the clusters of smokers remained the same across time, suggesting that whole groups of people were quitting in concert. Smokers were also progressively found in the periphery of the social network. Smoking cessation by a spouse decreased a person's chances of smoking by 67% (95% confidence interval [CI], 59 to 73). Smoking cessation by a sibling decreased the chances by 25% (95% CI, 14 to 35). Smoking cessation by a friend decreased the chances by 36% (95% CI, 12 to 55 ). Among persons working in small firms, smoking cessation by a coworker decreased the chances by 34% (95% CI, 5 to 56). Friends with more education influenced one another more than those with less education. These effects were not seen among neighbors in the immediate geographic area.  Conclusions Network phenomena appear to be relevant to smoking cessation. Smoking behavior spreads through close and distant social ties, groups of interconnected people stop smoking in concert, and smokers are increasingly marginalized socially. These findings have implications for clinical and public health interventions to reduce and prevent smoking. 10.1056/NEJMsa0706154"
224,24,15349,1,Extracting the multiscale backbone of complex weighted networks,"10.1073/pnas.0808904106 A large number of complex systems find a natural abstraction in the form of weighted networks whose nodes represent the elements of the system and the weighted edges identify the presence of an interaction and its relative strength. In recent years, the study of an increasing number of large-scale networks has highlighted the statistical heterogeneity of their interaction pattern, with degree and weight distributions that vary over many orders of magnitude. These features, along with the large number of elements and links, make the extraction of the truly relevant connections forming the network's backbone a very challenging problem. More specifically, coarse-graining approaches and filtering techniques come into conflict with the multiscale nature of large-scale systems. Here, we define a filtering method that offers a practical procedure to extract the relevant connection backbone in complex multiscale networks, preserving the edges that represent statistically significant deviations with respect to a null model for the local assignment of weights to edges. An important aspect of the method is that it does not belittle small-scale interactions and operates at all scales defined by the weight distribution. We apply our method to real-world network instances and compare the obtained results with alternative backbone extraction techniques."
225,24,15585,1,Brain Anatomical Network and Intelligence,"Intuitively, higher intelligence might be assumed to correspond to more efficient information transfer in the brain, but no direct evidence has been reported from the perspective of brain networks. In this study, we performed extensive analyses to test the hypothesis that individual differences in intelligence are associated with brain structural organization, and in particular that higher scores on intelligence tests are related to greater global efficiency of the brain anatomical network. We constructed binary and weighted brain anatomical networks in each of 79 healthy young adults utilizing diffusion tensor tractography and calculated topological properties of the networks using a graph theoretical method. Based on their IQ test scores, all subjects were divided into general and high intelligence groups and significantly higher global efficiencies were found in the networks of the latter group. Moreover, we showed significant correlations between IQ scores and network properties across all subjects while controlling for age and gender. Specifically, higher intelligence scores corresponded to a shorter characteristic path length and a higher global efficiency of the networks, indicating a more efficient parallel information transfer in the brain. The results were consistently observed not only in the binary but also in the weighted networks, which together provide convergent evidence for our hypothesis. Our findings suggest that the efficiency of brain structural organization may be an important biological basis for intelligence."
226,25,827,1,A Practical Guide to Splines,"This book (seventh printing) is based on the author's experience with calculations involving polynomial splines. It presents those parts of the theory which are especially useful in calculations and stresses the representation of splines as linear combinations of B-splines.  After two chapters summarizing polynomial approximation, a rigorous discussion of elementary spline theory is given involving linear, cubic and parabolic splines. The computational handling of piecewise polynomial functions (of one variable) of arbitrary order is the subject of chapters 7 and 8, while chapters 9, 10, and 11 are devoted to B-splines. The distances from splines with fixed and with variable knots is discussed in chapter 12. The remaining five chapters concern specific approximation methods, interpolation, smoothing and least-squares approximation, the solution of an ordinary differential equation by collocation, curve fitting, and surface fitting."
227,25,1714,1,Real Analysis,"This course in real analysis is directed at advanced undergraduates andbeginning graduate students in mathematics and related fields. Presupposingonly a modest background in real analysis or advanced calculus, the bookoffers something of value to specialists and nonspecialists alike. The textcovers three major topics: metric and normed linear spaces, function spaces,and Lebesgue measure and integration on the line. In an informal, down-to-earth style, the author gives motivation and overview of new ideas, whilestill supplying full details and complete proofs. He provides a great manyexercises and suggestions for further study."
228,25,2212,1,Convex Optimization,"Convex optimization problems arise frequently in many different fields. This book provides a comprehensive introduction to the subject, and shows in detail how such problems can be solved numerically with great efficiency. The book begins with the basic elements of convex sets and functions, and then describes various classes of convex optimization problems. Duality and approximation techniques are then covered, as are statistical estimation techniques. Various geometrical problems are then presented, and there is detailed discussion of unconstrained and constrained minimization problems, and interior-point methods. The focus of the book is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. It contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance and economics.  Ã¢â¬Â¢ Gives comprehensive details on how to recognize convex optimization problems in a wide variety of settings Ã¢â¬Â¢ Provides a broad range of practical algorithms for solving real problems Ã¢â¬Â¢ Contains hundreds of worked examples and homework exercises"
229,25,5551,1,"{Petri} nets: properties, analysis, and applications","Starts with a brief review of the history and the application areas considered in the literature. The author then proceeds with introductory modeling examples, behavioral and structural properties, three methods of analysis, subclasses of Petri nets and their analysis. In particular, one section is devoted to marked graphs, the concurrent system model most amenable to analysis. Introductory discussions on stochastic nets with their application to performance modeling, and on high-level nets with their application to logic programming, are provided. Also included are recent results on reachability criteria. Suggestions are provided for further reading on many subject areas of Petri nets."
230,25,7556,1,Constrained model predictive control: stability and optimality,"Model predictive control is a form of control in which the current control action is obtained by solving, at each sampling instant, a finite horizon open-loop optimal control problem, using the current state of the plant as the initial state; the optimization yields an optimal control sequence and the first control in this sequence is applied to the plant. An important advantage of this type of control is its ability to cope with hard constraints on controls and states. It has, therefore, been widely applied in petro-chemical and related industries where satisfaction of constraints is particularly important because efficiency demands operating points on or close to the boundary of the set of admissible states and controls. In this review, we focus on model predictive control of constrained systems, both linear and nonlinear and discuss only briefly model predictive control of unconstrained nonlinear and/or time-varying systems. We concentrate our attention on research dealing with stability and optimality; in these areas the subject has developed, in our opinion, to a stage where it has achieved sufficient maturity to warrant the active interest of researchers in nonlinear control. We distill from an extensive literature essential principles that ensure stability and use these to present a concise characterization of most of the model predictive controllers that have been proposed in the literature. In some cases the finite horizon optimal control problem solved on-line is exactly equivalent to the same problem with an infinite horizon; in other cases it is equivalent to a modified infinite horizon optimal control problem. In both situations, known advantages of infinite horizon optimal control accrue."
231,26,4407,1,Development and Testing of the OPLS All-Atom Force Field on Conformational Energetics and Properties of Organic Liquids,"Abstract: The parametrization and testing of the OPLS all-atom force field for organic molecules and peptides are described. Parameters for both torsional and nonbonded energetics have been derived, while the bond stretching and angle bending parameters have been adopted mostly from the AMBER all-atom force field. The torsional parameters were determined by fitting to rotational energy profiles obtained from ab initio molecular orbital calculations at the RHF/6-31G*//RHF/6-31G* level for more than 50 organic molecules and ions. The quality of the fits was high with average errors for conformational energies of less than 0.2 kcal/mol. The force-field results for molecular structures are also demonstrated to closely match the ab initio predictions. The nonbonded parameters were developed in conjunction with Monte Carlo statistical mechanics simulations by computing thermodynamic and structural properties for 34 pure organic liquids including alkanes, alkenes, alcohols, ethers, acetals, thiols, sulfides, disulfides, aldehydes, ketones, and amides. Average errors in comparison with experimental data are 2% for heats of vaporization and densities. The Monte Carlo simulations included sampling all internal and intermolecular degrees of freedom. It is found that such non-polar and monofunctional systems do not show significant condensed-phase effects on internal energies in going from the gas phase to the pure liquids."
232,26,5978,1,"GROMACS: Fast, flexible, and free","Abstract 10.1002/jcc.20291.abs This article describes the software suite GROMACS (Groningen MAchine for Chemical Simulation) that was developed at the University of Groningen, The Netherlands, in the early 1990s. The software, written in ANSI C, originates from a parallel hardware project, and is well suited for parallelization on processor clusters. By careful optimization of neighbor searching and of inner loop performance, GROMACS is a very fast program for molecular dynamics simulation. It does not have a force field of its own, but is compatible with GROMOS, OPLS, AMBER, and ENCAD force fields. In addition, it can handle polarizable shell models and flexible constraints. The program is versatile, as force routines can be added by the user, tabulated functions can be specified, and analyses can be easily customized. Nonequilibrium dynamics and free energy determinations are incorporated. Interfaces with popular quantum-chemical packages (MOPAC, GAMES-UK, GAUSSIAN) are provided to perform mixed MM/QM simulations. The package includes about 100 utility and analysis programs. GROMACS is in the public domain and distributed (with source code and documentation) under the GNU General Public License. It is maintained by a group of developers from the Universities of Groningen, Uppsala, and Stockholm, and the Max Planck Institute for Polymer Research in Mainz. Its Web site is http://www.gromacs.org. Â© 2005 Wiley Periodicals, Inc. J Comput Chem 26: 1701â1718, 2005"
233,26,9843,1,Simulating microscopic hydrodynamic phenomena with dissipative particle dynamics,"We present a novel method for simulating hydrodynamic phenomena. This particle-based method combines features from molecular dynamics and lattice-gas automata. It is shown theoretically as well as in simulations that a quantitative description of isothermal Navier-Stokes flow is obtained with relatively few particles. Computationally, the method is much faster than molecular dynamics, and the at same time it is much more flexible than lattice-gas automata schemes."
234,26,12771,1,Dissipative particle dynamics: Bridging the gap between atomistic and mesoscopic simulation,"We critically review dissipative particle dynamics (DPD) as a mesoscopic simulation method. We have established useful parameter ranges for simulations, and have made a link between these parameters and -parameters in Flory-Huggins-type models. This is possible because the equation of state of the DPD fluid is essentially quadratic in density. This link opens the way to do large scale simulations, effectively describing millions of atoms, by firstly performing simulations of molecular fragments retaining all atomistic details to derive -parameters, then secondly using these results as input to a DPD simulation to study the formation of micelles, networks, mesophases and so forth. As an example application, we have calculated the interfacial tension between homopolymer melts as a function of and N and have found a universal scaling collapse when /kBT0.4 is plotted against N for N>1. We also discuss the use of DPD to simulate the dynamics of mesoscopic systems, and indicate a possible problem with the timescale separation between particle diffusion and momentum diffusion (viscosity). {\\copyright}1997 American Institute of Physics."
235,27,101,1,The long memory of the efficient market,"For the London Stock Exchange we demonstrate that the signs of orders obey a long-memory process. The autocorrelation function decays roughly as $\tau^{-\alpha}$ with $\alpha \approx 0.6$, corresponding to a Hurst exponent $H \approx 0.7$. This implies that the signs of future orders are quite predictable from the signs of past orders; all else being equal, this would suggest a very strong market inefficiency. We demonstrate, however, that fluctuations in order signs are compensated for by anti-correlated fluctuations in transaction size and liquidity, which are also long-memory processes. This tends to make the returns whiter. We show that some institutions display long-range memory and others don't."
236,27,478,1,Emergence of scaling in random networks,"Recently retired as head of the Global Alliance for Vaccines and Immunization (GAVI) secretariat and as a health advisor to leading global entities, Tore Godal is now a Special Advisor to the Norwegian Prime Minister. He is nevertheless continuing to fight for better global health, cogently articulating the needs of the world's poor and disadvantaged. He is a leading leprosy expert, ex-director of the world's premier agency for research and training in tropical diseases, instigator and prime mover of some global innovative public-private health sector partnerships, adept fund mobilizer, and advocate of the `let's get it done' school of leadership. Few individuals are, therefore, more experienced or better suited for such a crucial and much-needed role"
237,27,2692,1,The evolution of cooperation,"Darwin recognized that natural selection could not favor a trait in one species solely for the benefit of another species. The modern, selfish-gene view of the world suggests that cooperation between individuals, whether of the same species or different species, should be especially vulnerable to the evolution of noncooperators. Yet, cooperation is prevalent in nature both within and between species. What special circumstances or mechanisms thus favor cooperation? Currently, evolutionary biology offers a set of disparate explanations, and a general framework for this breadth of models has not emerged. Here, we offer a tripartite structure that links previously disconnected views of cooperation. We distinguish three general models by which cooperation can evolve and be maintained: (i) directed reciprocation â cooperation with individuals who give in return; (ii) shared genes â cooperation with relatives (e.g., kin selection); and (iii) byproduct benefits â cooperation as an incidental consequence of selfish action. Each general model is further subdivided. Several renowned examples of cooperation that have lacked explanation until recently â plant-rhizobium symbioses and bacteria-squid light organs â fit squarely within this framework. Natural systems of cooperation often involve more than one model, and a fruitful direction for future research is to understand how these models interact to maintain cooperation in the long term."
238,27,2823,1,The Pricing of Options and Corporate Liabilities,"If options are correctly priced in the market, it should not be possible to make sure profits by creating portfolios of long and short positions in options and their underlying stocks. Using this principle, a theoretical valuation formula for options is derived. Since almost all corporate liabilities can be viewed as combinations of options, the formula and the analysis that led to it are also applicable to corporate liabilities such as common stock, corporate bonds, and warrants. In particular, the formula can be used to derive the discount that should be applied to a corporate bond because of the possibility of default."
239,27,5942,1,"The General Theory of Employment, Interest and Money","John Maynard Keynes is the great British economist of the twentieth century whose hugely influential work The General Theory of Employment, Interest and Money is undoubtedly the century s most important book on economics--strongly influencing economic theory and practice, particularly with regard to the role of government in stimulating and regulating a nation's economic life. Keynes's work has undergone significant revaluation in recent years, and ""Keynesian"" views which have been widely defended for so long are now perceived as at odds with Keynes's own thinking. Recent scholarship and research has demonstrated considerable rivalry and controversy concerning the proper interpretation of Keynes's works, such that recourse to the original text is all the more important. Although considered by a few critics that the sentence structures of the book are quite incomprehensible and almost unbearable to read, the book is an essential reading for all those who desire a basic education in economics. The key to understanding Keynes is the notion that at particular times in the business cycle, an economy can become over-productive (or under- consumptive) and thus, a vicious spiral is begun that results in massive layoffs and cuts in production as businesses attempt to equilibrate aggregate supply and demand. Thus, full employment is only one of many or multiple macro equilibria. If an economy reaches an underemployment equilibrium, something is necessary to boost or stimulate demand to produce full employment. This something could be business investment but because of the logic and individualist nature of investment decisions, it is unlikely to rapidly restore full employment. Keynes logically seizes upon the public budget and government expenditures as the quickest way to restore full employment. Borrowing the money to finance the deficit from private households and businesses is a quick, direct way to restore full employment while at the same time, redirecting or siphoning"
240,27,6807,1,Market Microstructure Theory,"{Written by one of the leading authorities in market microstructure research, this book provides a comprehensive guide to the theoretical work in this important area of finance.After an introduction to the general issues and problems in market microstructure, the book examines the main theoretical models developed to address inventory-based issues. There is then an extensive examination and discussion of the information-based models, with particular attention paid to the linkage with rational expectations model and learning models. The concluding chapters are concerned with price dynamics and with applications of the various models to specific microstructure problems including:- Liquidity.- Multi-market trading.- Market structure.- Market Design Market Microstructure Theory includes extensive appendices developing Bayesian learning and the rational expectations framework.}"
241,27,6808,1,Allocative Efficiency of Markets with Zero-Intelligence Traders: Market as a Partial Substitute for Individual Rationality,"This paper reports market experiments in which human traders are replaced by 'zero-intelligence' programs that submit random bids and offers. Imposing a budget constraint (i.e., n ot permitting traders to sell below their costs or buy above their valu es) is sufficient to raise the allocative efficiency of these auctions close to 100 percent. Allocative efficiency of a double auction deri ves largely from its structure, independent of traders' motivation, intelligence, or learning. Adam Smith's invisible hand may be more powerful than some may have thought; it can generate aggregate rationality not only from individual rationality but also from individual irrationality."
242,27,10935,1,Continuous auctions and insider trading,"A dynamic model of insider trading with sequential auctions, structured to resemble a sequential equilibrium, is used to examine the informational content of prices, the liquidity characteristics of a speculative market, and the value of private information to an insider. The model has three kinds of traders: a single risk neutral insider, random noise traders, and competitive risk neutral market makers. The insider makes positive profits by exploiting his monopoly power optimally in a dynamic context, where noise trading provides camouflage which conceals his trading from market makers. As the time interval between auctions goes to zero, a limiting model of continuous trading is obtained. In this equilibrium, prices follow Brownian motion, the depth of the market is constant over time, and all private information is incorporated into prices by the end of trading."
243,28,227,1,The PageRank Citation Ranking: Bringing Order to the Web,"The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation. 1"
244,28,4344,1,Named Entity recognition without gazetteers,"It is often claimed that Named Entity  recognition systems need extensive  gazetteers lists of names of people, organisations,  locations, and other named  entities. Indeed, the compilation of such  gazetteers is sometimes mentioned as a  bottleneck in the design of Named Entity  recognition systems.  We report on a Named Entity recognition  system which combines rule-based  grammars with statistical (maximum entropy)  models. We report on the system  &#039;s performance with gazetteers of different  types and dierent sizes, using test  material from the muc{7 competition.  We show that, for the text type and task  of this competition, it is sucient to use  relatively small gazetteers of well-known  names, rather than large gazetteers of  low-frequency names. We conclude with  observations about the domain independence  of the competition and of our experiments.  1 Introduction  Named Entity recognition involves processing a text and identifying certain occurrences of words or expressions ..."
245,28,7659,1,A statistical interpretation of term specificity and its application in retrieval,"The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing, in particular, that frequently-occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure."
246,29,1916,1,New Directions in Cryptography,"The authors discuss some of the recent results in communications theory that have arisen out of the need for security in the key distribution channels. They concentrate on the use of ciphers to restrict the extraction of information from a communication over an insecure. As is well known, the transmission and distribution is then likely to become a problem, in efficiency if not in security. The authors suggest various possible approaches to avoid these further problems that arise. The first they call a ""public key distribution system"", which has the feature that an unauthorized ""eavesdropper"" will find it computationally infeasible to decipher the message since the enciphering and deciphering are governed by distinct keys. They propose a couple of techniques for implementing the system, but the reviewer was unconvinced. A further system is designed to treat a problem that arises in business transactions, that of transmitting a legally valid signature whose authenticity can be checked by anyone; this necessitates the signature's being message-dependent. Finally, they treat the problem of ""trap-doors"" (a trap-door cipher is one that strongly resists decipherment by anyone unauthorized and not in possession of particular information used in the design of the cipher) and relate their concerns to various questions that arise in problems of computational complexity."
247,29,2610,1,"Introduction to Algorithms, Second Edition","{Aimed at any serious programmer or computer science student,  the new second edition of <I>Introduction to Algorithms</I> builds on  the tradition of the original with a truly magisterial guide to the  world of algorithms. Clearly presented, mathematically rigorous, and  yet approachable even for the math-averse, this title sets a high  standard for a textbook and reference to the best algorithms for  solving a wide range of computing problems.<p>With sample problems and  mathematical proofs demonstrating the correctness of each algorithm,  this book is ideal as a textbook for classroom study, but its reach  doesn't end there. The authors do a fine job of explaining each  algorithm. (Reference sections on basic mathematical notation will help  readers bridge the gap, but it will help to have some math background  to appreciate the full achievement of this handsome hardcover volume.)  Every algorithm is presented in pseudo-code, which can be implemented  in any computer language, including C/C++ and Java. This ecumenical  approach is one of the book's strengths. When it comes to sorting and  common data structures, from basic linked lists to trees (including  binary trees, red-black, and B-trees), this title really shines, with  clear diagrams that show algorithms in operation. Even if you just  glance over the mathematical notation here, you can definitely benefit  from this text in other ways.<p>The book moves forward with more  advanced algorithms that implement strategies for solving more  complicated problems (including dynamic programming techniques, greedy  algorithms, and amortized analysis). Algorithms for graphing problems  (used in such real-world business problems as optimizing flight  schedules or flow through pipelines) come next. In each case, the  authors provide the best from current research in each topic, along  with sample solutions.<p>This text closes with a grab bag of useful  algorithms including matrix operations and linear programming,  evaluating polynomials, and the well-known Fast Fourier Transformation  (FFT) (useful in signal processing and engineering). Final sections on  ""NP-complete"" problems, like the well-known traveling salesman problem,  show off that while not all problems have a demonstrably final and best  answer, algorithms that generate acceptable approximate solutions can  still be used to generate useful, real-world answers.<p>Throughout this  text, the authors anchor their discussion of algorithms with current  examples drawn from molecular biology (like the Human Genome Project),  business, and engineering. Each section ends with short discussions of  related historical material, often discussing original research in each  area of algorithms. On the whole, they argue successfully that  algorithms are a ""technology"" just like hardware and software that can  be used to write better software that does more, with better  performance. Along with classic books on algorithms (like Donald  Knuth's three-volume set, <I>The Art of Computer  Programming</I>), this title sets a new standard for compiling the  best research in algorithms. For any experienced developer, regardless  of their chosen language, this text deserves a close look for extending  the range and performance of real-world software. <I>--Richard  Dragan</I> <p> <B>Topics covered:</B> Overview of algorithms (including algorithms as  a technology); designing and analyzing algorithms; asymptotic notation;  recurrences and recursion; probabilistic analysis and randomized  algorithms; heapsort algorithms; priority queues; quicksort algorithms;  linear time sorting (including radix and bucket sort); medians and  order statistics (including minimum and maximum); introduction to data  structures (stacks, queues, linked lists, and rooted trees); hash  tables (including hash functions); binary search trees; red-black  trees; augmenting data structures for custom applications; dynamic  programming explained (including assembly-line scheduling, matrix-chain  multiplication, and optimal binary search trees); greedy algorithms  (including Huffman codes and task-scheduling problems); amortized  analysis (the accounting and potential methods); advanced data  structures (including B-trees, binomial and Fibonacci heaps,  representing disjoint sets in data structures); graph algorithms  (representing graphs, minimum spanning trees, single-source shortest  paths, all-pairs shortest paths, and maximum flow algorithms); sorting  networks; matrix operations; linear programming (standard and slack  forms); polynomials and the Fast Fourier Transformation (FFT); number  theoretic algorithms (including greatest common divisor, modular  arithmetic, the Chinese remainder theorem, RSA public-key encryption,  primality testing, integer factorization); string matching;  computational geometry (including finding the convex hull);  NP-completeness (including sample real-world NP-complete problems and  their insolvability); approximation algorithms for NP-complete problems  (including the traveling salesman problem); reference sections for  summations and other mathematical notation, sets, relations, functions,  graphs and trees, as well as counting and probability backgrounder  (plus geometric and binomial distributions).}"
248,29,6928,1,Network information flow,"AbstractÃWe introduce a new class of problems called network information flow which is inspired by computer network applications. Consider a point-to-point communication network on which a number of information sources are to be mulitcast to certain sets of destinations. We assume that the information sources are mutually independent. The problem is to characterize the admissible coding rate region. This model subsumes all previously studied models along the same line. In this paper, we study the problem with one information source, and we have obtained a simple characterization of the admissible coding rate region. Our result can be regarded as the Max-flow Min-cut Theorem for network information flow. Contrary to oneÃs intuition, our work reveals that it is in general not optimal to regard the information to be multicast as a ÃfluidÃ which can simply be routed or replicated. Rather, by employing coding at the nodes, which we refer to as network coding bandwidth can in general be saved. This finding may have significant impact on future design of switching systems. Index TermsÃDiversity coding, multicast, network coding switching, multiterminal source coding."
249,30,108,1,The systems biology markup language (SBML): a medium for representation and exchange of biochemical network models,"Motivation: Molecular biotechnology now makes it possible to build elaborate systems models, but the systems biology community needs information standards if models are to be shared, evaluated and developed cooperatively.  Results: We summarize the Systems Biology Markup Language (SBML) Level 1, a free, open, XML-based format for representing biochemical reaction networks. SBML is a software-independent language for describing models common to research in many areas of computational biology, including cell signaling pathways, metabolic pathways, gene regulation, and others.  Availability: The specification of SBML Level 1 is freely available from http://www.sbml.org/  Contact: sysbio-team@caltech.edu 10.1093/bioinformatics/btg015"
250,30,1551,1,{Genome-scale reconstruction of the Saccharomyces cerevisiae metabolic network},"10.1101/gr.234503 The metabolic network in the yeast was reconstructed using currently available genomic, biochemical, and physiological information. The metabolic reactions were compartmentalized between the cytosol and the mitochondria, and transport steps between the compartments and the environment were included. A total of 708 structural open reading frames (ORFs) were accounted for in the reconstructed network, corresponding to 1035 metabolic reactions. Further, 140 reactions were included on the basis of biochemical evidence resulting in a genome-scale reconstructed metabolic network containing 1175 metabolic reactions and 584 metabolites. The number of gene functions included in the reconstructed network corresponds to Ã¢ÂÂ¼16% of all characterized ORFs in . Using the reconstructed network, the metabolic capabilities of  were calculated and compared with . The reconstructed metabolic network is the first comprehensive network for a eukaryotic organism, and it may be used as the basis for in silico analysis of phenotypic functions. [Supplemental material is available online at . The detailed genome-scale reconstructed model of can be found at or.]"
251,30,2025,1,An expanded genome-scale model of Escherichia coli K-12 (iJR904 GSM/GPR),"BACKGROUND: Diverse datasets, including genomic, transcriptomic, proteomic and metabolomic data, are becoming readily available for specific organisms. There is currently a need to integrate these datasets within an in silico modeling framework. Constraint-based models of Escherichia coli K-12 MG1655 have been developed and used to study the bacterium's metabolism and phenotypic behavior. The most comprehensive E. coli model to date (E. coli iJE660a GSM) accounts for 660 genes and includes 627 unique biochemical reactions. RESULTS: An expanded genome-scale metabolic model of E. coli (iJR904 GSM/GPR) has been reconstructed which includes 904 genes and 931 unique biochemical reactions. The reactions in the expanded model are both elementally and charge balanced. Network gap analysis led to putative assignments for 55 open reading frames (ORFs). Gene to protein to reaction associations (GPR) are now directly included in the model. Comparisons between predictions made by iJR904 and iJE660a models show that they are generally similar but differ under certain circumstances. Analysis of genome-scale proton balancing shows how the flux of protons into and out of the medium is important for maximizing cellular growth. CONCLUSIONS: E. coli iJR904 has improved capabilities over iJE660a. iJR904 is a more complete and chemically accurate description of E. coli metabolism than iJE660a. Perhaps most importantly, iJR904 can be used for analyzing and integrating the diverse datasets. iJR904 will help to outline the genotype-phenotype relationship for E. coli K-12, as it can account for genomic, transcriptomic, proteomic and fluxomic data simultaneously."
252,30,3171,1,Stochastic reaction-diffusion simulation with MesoRD,"Summary: MesoRD is a tool for stochastic simulation of chemical reactions and diffusion. In particular, it is an implementation of the next subvolume method, which is an exact method to simulate the Markov process corresponding to the reaction-diffusion master equation.  Availability: MesoRD is free software, written in C++ and licensed under the GNU general public license (GPL). MesoRD runs on Linux, Mac OS X, NetBSD, Solaris and Windows XP. It can be downloaded from http://mesord.sourceforge.net.  Contact: johan.elf@icm.uu.se; johan.hattne@embl-hamburg.de  Supplementary information:  MesoRD User's Guide' and other documents are available at http://mesord.sourceforge.net. 10.1093/bioinformatics/bti431"
253,30,4762,1,Simulated Diffusion of Phosphorylated {C}he{Y} through the Cytoplasm of {E}scherichia coli,"We describe the use of a computational model to study the effects of cellular architecture and macromolecular crowding on signal transduction in Escherichia coli chemotaxis. A newly developed program, Smoldyn, allows the movement and interaction of a large number of individual molecules in a structured environment to be simulated (S. S. Andrews and D. Bray, Phys. Biol., in press). With Smoldyn, we constructed a three-dimensional model of an E. coli cell and examined the diffusion of CheYp from the cluster of receptors to the flagellar motors under control conditions and in response to attractant and repellent stimuli. Our simulations agree well with experimental observations of cell swimming responses and are consistent with the diffusive behavior expected in wild-type and mutant cells. The high resolution available to us in the new program allows us to calculate the loci of individual CheYp molecules in a cell and the distribution of their lifetimes under different cellular conditions. We find that the time delay between stimulus and response differs for flagellar motors located at different positions in the cell. We explore different possible locations for the phosphatase CheZ and show conditions under which a gradient of CheYp exists in the cell. The introduction of inert blocks into the cytoplasm, representing impenetrable structures such as the nucleoid and large protein complexes, produces a fall in the apparent diffusion coefficient of CheYp and enhances the differences between motors. These and other results are left as predictions for future experiments."
254,30,5234,1,A protein interaction network of the malaria parasite Plasmodium falciparum,"Plasmodium falciparum causes the most severe form of malaria and kills up to 2.7 million people annually1. Despite the global importance of P. falciparum, the vast majority of its proteins have not been characterized experimentally. Here we identify P. falciparum proteinâprotein interactions using a high-throughput version of the yeast two-hybrid assay that circumvents the difficulties in expressing P. falciparum proteins in Saccharomyces cerevisiae. From more than 32,000 yeast two-hybrid screens with P. falciparum protein fragments, we identified 2,846 unique interactions, most of which include at least one previously uncharacterized protein. Informatic analyses of network connectivity, coexpression of the genes encoding interacting fragments, and enrichment of specific protein domains or Gene Ontology annotations2 were used to identify groups of interacting proteins, including one implicated in chromatin modification, transcription, messenger RNA stability and ubiquitination, and another implicated in the invasion of host cells. These data constitute the first extensive description of the protein interaction network for this important human pathogen."
255,30,10781,1,S{{TOCHSIM}}: modelling of stochastic biomolecular processes.,"Summary: STOCHSIM is a stochastic simulator for chemical reactions. Molecules are represented as individual software objects that react according to probabilities derived from concentrations and rate constants. Version 1.2 of STOCHSIM provides a novel cross-platform graphical interface written in Perl/Tk. A simple two-dimensional spatial structure has also been implemented, in which nearest-neighbour interactions of molecules in a 2-D lattice can be simulated.  Availability: Various ports of the program can be retrieved at ftp://ftp.cds.caltech.edu/pub/dbray/  Contact: nl223@cus.cam.ac.uk; tss26@cam.ac.uk 10.1093/bioinformatics/17.6.575"
256,31,3101,1,Neuronal Population Coding of Movement Direction,"Although individual neurons in the arm area of the primate motor cortex are only broadly tuned to a particular direction in three-dimensional space, the animal can very precisely control the movement of its arm. The direction of movement was found to be uniquely predicted by the action of a population of motor cortical neurons. When individual cells were represented as vectors that make weighted contributions along the axis of their preferred direction (according to changes in their activity during the movement under consideration) the resulting vector sum of all cell vectors (population vector) was in a direction congruent with the direction of movement. This population vector can be monitored during various tasks, and similar measures in other neuronal populations could be of heuristic value where there is a neural representation of variables with vectorial attributes."
257,31,3735,1,Multiple neural spike train data analysis: state-of-the-art and future challenges,Multiple electrodes are anow a standard tool in neuroscience research that make it possible to study the simultaneous activity of several neurons in a given brain region or across different regions. The data from multi-electrode studies present important analysis challenges that must be resolved for optimal use of these neurophysiological measurements to answer questions about how the brain works. Here we review statistical methods for the analysis of multiple neural spike-train data and discuss future challenges for methodology research.
258,31,14106,1,Internal brain state regulates membrane potential synchrony in barrel cortex of behaving mice.,"Internal brain states form key determinants for sensory perception, sensorimotor coordination and learning. A prominent reflection of different brain states in the mammalian central nervous system is the presence of distinct patterns of cortical synchrony, as revealed by extracellular recordings of the electroencephalogram, local field potential and action potentials. Such temporal correlations of cortical activity are thought to be fundamental mechanisms of neuronal computation. However, it is unknown how cortical synchrony is reflected in the intracellular membrane potential (V(m)) dynamics of behaving animals. Here we show, using dual whole-cell recordings from layer 2/3 primary somatosensory barrel cortex in behaving mice, that the V(m) of nearby neurons is highly correlated during quiet wakefulness. However, when the mouse is whisking, an internally generated state change reduces the V(m) correlation, resulting in a desynchronized local field potential and electroencephalogram. Action potential activity was sparse during both quiet wakefulness and active whisking. Single action potentials were driven by a large, brief and specific excitatory input that was not present in the V(m) of neighbouring cells. Action potential initiation occurs with a higher signal-to-noise ratio during active whisking than during quiet periods. Therefore, we show that an internal brain state dynamically regulates cortical membrane potential synchrony during behaviour and defines different modes of cortical processing."
259,31,14380,1,Unsupervised natural experience rapidly alters invariant object representation in visual cortex.,"Object recognition is challenging because each object produces myriad retinal images. Responses of neurons from the inferior temporal cortex (IT) are selective to different objects, yet tolerant (""invariant"") to changes in object position, scale, and pose. How does the brain construct this neuronal tolerance? We report a form of neuronal learning that suggests the underlying solution. Targeted alteration of the natural temporal contiguity of visual experience caused specific changes in IT position tolerance. This unsupervised temporal slowness learning (UTL) was substantial, increased with experience, and was significant in single IT neurons after just 1 hour. Together with previous theoretical work and human object perception experiments, we speculate that UTL may reflect the mechanism by which the visual stream builds and maintains tolerant object representations. 10.1126/science.1160028"
260,32,8421,1,Automated Tag Clustering: Improving search and exploration in the tag space,"prima di tutto, prende solo i tag che hanno una co-occorrenza rilevante, trovando un punto di taglio nella classifica delle co-occorrenze ordinate di un tag con i suoi co-tags per escludere quelli non significativi. (e gli altri? dove li piazzo nel clustering?). poi fa un grafo e successivamente ci implementa un algoritmo di clustering, basato sullo spectral clustering + modularity utile per fare suggest di tag simili. lavora sempre su matrici con pesi unitari, non gestisce i pesi calcolo della modularit{Ã¡}: A(Vc, Vc) = A(V, V) A(Vc, V) A(V', V'') = somma dei pesi degli archi tra i nodi , uno di V' e l'altro di V''"
261,32,9165,1,Harvesting social knowledge from folksonomies,"Collaborative tagging systems, or folksonomies, have the potential of becoming technological infrastructure to support knowledge management activities in an organization or a society. There are many challenges, however. This paper presents designs that enhance collaborative tagging systems to meet some key challenges: community identification, ontology generation, user and document recommendation. Design prototypes, evaluation methodology and selected preliminary results are presented."
262,32,10496,1,The Social Structure of Tagging Internet Video on del.icio.us,"The ability to tag resources with uncontrolled metadata or ""folksonomies"" is often characterized as one of the central features of ""Web 2.0"" applications. Folksonomies are said to support emergent classification, where the semantic value of the tags and their relation to one another is worked out through a negotiated process of users applying their selected tags and seeing what others have tagged the same way. Few studies exist to show how folksonomic tagging is actually done, and to what extent users share each otherÃÂ¿s tagging patterns. In this paper, we present the results of a social network analysis of two months worth of tagging Internet video on the social bookmarking system del.icio.us. The analysis reveals that specific videos are tagged in fairly coherent ways by a relatively tight group of users. However, contrary to our expectations, there does not appear to be much re-use of tags across different content, or even very many users tagging more than a few similar items. Overwhelmingly, specific clusters of tags and users are associated with individual video links. This result suggests that tagging bookmarks is highly local, and the overall collection of tags is unlikely to result in a coherent globally navigable classification system."
263,33,3647,1,Computational cluster validation in post-genomic data analysis,"Motivation: The discovery of novel biological knowledge from the ab initio analysis of post-genomic data relies upon the use of unsupervised processing methods, in particular clustering techniques. Much recent research in bioinformatics has therefore been focused on the transfer of clustering methods introduced in other scientific fields and on the development of novel algorithms specifically designed to tackle the challenges posed by post-genomic data. The partitions returned by a clustering algorithm are commonly validated using visual inspection and concordance with prior biological knowledge--whether the clusters actually correspond to the real structure in the data is somewhat less frequently considered. Suitable computational cluster validation techniques are available in the general data-mining literature, but have been given only a fraction of the same attention in bioinformatics. Results: This review paper aims to familiarize the reader with the battery of techniques available for the validation of clustering results, with a particular focus on their application to post-genomic data analysis. Synthetic and real biological datasets are used to demonstrate the benefits, and also some of the perils, of analytical clustervalidation. Availability: The software used in the experiments is available at http://dbkweb.ch.umist.ac.uk/handl/clustervalidation/ Contact: J.Handl@postgrad.manchester.ac.uk Supplementary information: Enlarged colour plots are provided in the Supplementary Material, which is available at http://dbkweb.ch.umist.ac.uk/handl/clustervalidation/"
264,33,10183,1,Semi-supervised methods to predict patient survival from gene expression data.,"An important goal of DNA microarray research is to develop tools to diagnose cancer more accurately based on the genetic profile of a tumor. There are several existing techniques in the literature for performing this type of diagnosis. Unfortunately, most of these techniques assume that different subtypes of cancer are already known to exist. Their utility is limited when such subtypes have not been previously identified. Although methods for identifying such subtypes exist, these methods do not work well for all datasets. It would be desirable to develop a procedure to find such subtypes that is applicable in a wide variety of circumstances. Even if no information is known about possible subtypes of a certain form of cancer, clinical information about the patients, such as their survival time, is often available. In this study, we develop some procedures that utilize both the gene expression data and the clinical data to identify subtypes of cancer and use this knowledge to diagnose future patients. These procedures were successfully applied to several publicly available datasets. We present diagnostic procedures that accurately predict the survival of future patients based on the gene expression profile and survival times of previous patients. This has the potential to be a powerful tool for diagnosing and treating cancer."
265,33,12531,1,Short pyrosequencing reads suffice for accurate microbial community analysis,"Pyrosequencing technology allows us to characterize microbial communities using 16S ribosomal RNA (rRNA) sequences orders of magnitude faster and more cheaply than has previously been possible. However, results from different studies using pyrosequencing and traditional sequencing are often difficult to compare, because amplicons covering different regions of the rRNA might yield different conclusions. We used sequences from over 200 globally dispersed environments to test whether studies that used similar primers clustered together mistakenly, without regard to environment. We then tested whether primer choice affects sequence-based community analyses using UniFrac, our recently-developed method for comparing microbial communities. We performed three tests of primer effects. We tested whether different simulated amplicons generated the same UniFrac clustering results as near-full-length sequences for three recent large-scale studies of microbial communities in the mouse and human gut, and the Guerrero Negro microbial mat. We then repeated this analysis for short sequences (100-, 150-, 200- and 250-base reads) resembling those produced by pyrosequencing. The results show that sequencing effort is best focused on gathering more short sequences rather than fewer longer ones, provided that the primers are chosen wisely, and that community comparison methods such as UniFrac are surprisingly robust to variation in the region sequenced. 10.1093/nar/gkm541"
266,33,13411,1,Velvet: algorithms for de novo short read assembly using de Bruijn graphs.,"10.1101/gr.074492.107 We have developed a new set of algorithms, collectively called â Velvet,â to manipulate de Bruijn graphs for genomic sequence assembly. A de Bruijn graph is a compact representation based on short words (-mers) that is ideal for high coverage, very short read (25â50 bp) data sets. Applying Velvet to very short reads and paired-ends information only, one can produce contigs of significant length, up to 50-kb N50 length in simulations of prokaryotic data and 3-kb N50 on simulated mammalian BACs. When applied to real Solexa data sets without read pairs, Velvet generated contigs of â¼8 kb in a prokaryote and 2 kb in a mammalian BAC, in close agreement with our simulated results without read-pair information. Velvet represents a new approach to assembly that can leverage very short reads in combination with read pairs to produce useful assemblies."
267,33,14706,1,"The Pervasive Effects of an Antibiotic on the Human Gut Microbiota, as Revealed by Deep 16S rRNA Sequencing","The human intestinal microbiota is essential to the health of the host and plays a role in nutrition, development, metabolism, pathogen resistance, and regulation of immune responses. Antibiotics may disrupt these coevolved interactions, leading to acute or chronic disease in some individuals. Our understanding of antibiotic-associated disturbance of the microbiota has been limited by the poor sensitivity, inadequate resolution, and significant cost of current research methods. The use of pyrosequencing technology to generate large numbers of 16S rDNA sequence tags circumvents these limitations and has been shown to reveal previously unexplored aspects of the Ã¢â¬Årare biosphere.Ã¢â¬ï¿½ We investigated the distal gut bacterial communities of three healthy humans before and after treatment with ciprofloxacin, obtaining more than 7,000 full-length rRNA sequences and over 900,000 pyrosequencing reads from two hypervariable regions of the rRNA gene. A companion paper in PLoS Genetics (see Huse et al., doi: 10.1371/journal.pgen.1000255 ) shows that the taxonomic information obtained with these methods is concordant. Pyrosequencing of the V6 and V3 variable regions identified 3,300Ã¢â¬â5,700 taxa that collectively accounted for over 99% of the variable region sequence tags that could be obtained from these samples. Ciprofloxacin treatment influenced the abundance of about a third of the bacterial taxa in the gut, decreasing the taxonomic richness, diversity, and evenness of the community. However, the magnitude of this effect varied among individuals, and some taxa showed interindividual variation in the response to ciprofloxacin. While differences of community composition between individuals were the largest source of variability between samples, we found that two unrelated individuals shared a surprising degree of community similarity. In all three individuals, the taxonomic composition of the community closely resembled its pretreatment state by 4 weeks after the end of treatment, but several taxa failed to recover within 6 months. These pervasive effects of ciprofloxacin on community composition contrast with the reports by participants of normal intestinal function and with prior assumptions of only modest effects of ciprofloxacin on the intestinal microbiota. These observations support the hypothesis of functional redundancy in the human gut microbiota. The rapid return to the pretreatment community composition is indicative of factors promoting community resilience, the nature of which deserves future investigation."
268,33,15615,1,The Sequence Alignment/Map format and SAMtools,"Summary: The Sequence Alignment/Map (SAM) format is a generic alignment format for storing read alignments against reference sequences, supporting short and long reads (up to 128 Mbp) produced by different sequencing platforms. It is flexible in style, compact in size, efficient in random access and is the format in which alignments from the 1000 Genomes Project are released. SAMtools implements various utilities for post-processing alignments in the SAM format, such as indexing, variant caller and alignment viewer, and thus provides universal tools for processing read alignments.Availability: http://samtools.sourceforge.netContact: rd@sanger.ac.uk"
269,33,16610,1,Detection and characterization of novel sequence insertions using paired-end next-generation sequencing,"Motivation: In the past few years, human genome structural variation discovery has enjoyed increased attention from the genomics research community. Many studies were published to characterize short insertions, deletions, duplications and inversions, and associate copy number variants (CNVs) with disease. Detection of new sequence insertions requires sequence data, however, the Ã¢ÂÂdetectableÃ¢ÂÂ sequence length with read-pair analysis is limited by the insert size. Thus, longer sequence insertions that contribute to our genetic makeup are not extensively researched.Results: We present NovelSeq: a computational framework to discover the content and location of long novel sequence insertions using paired-end sequencing data generated by the next-generation sequencing platforms. Our framework can be built as part of a general sequence analysis pipeline to discover multiple types of genetic variation (SNPs, structural variation, etc.), thus it requires significantly less-computational resources than de novo sequence assembly. We apply our methods to detect novel sequence insertions in the genome of an anonymous donor and validate our results by comparing with the insertions discovered in the same genome using various sources of sequence data.Availability: The implementation of the NovelSeq pipeline is available at http://compbio.cs.sfu.ca/strvar.htmContact:eee@gs.washington.edu; cenk@cs.sfu.ca"
270,34,30,1,Matching words and pictures,"We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-moda  and correspondence extensions to Hofmannâs hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation (MoM-LDA). All models are assessed using a large collection of annotated images of real scenes. We study in depth the difficult problem of measuring performance. For the annotation task, we look at prediction performance on held out data. We present three alternative measures, oriented toward different types of task. Measuring the performance of correspondence methods is harder, because one must determine whether a word has been placed on the right region of an image. We can use annotation performance as a proxy measure, but accurate measurement requires hand labeled data, and thus must occur on a smaller scale. We show results using both an annotation proxy, and manually labeled data."
271,34,1204,1,{Maximum likelihood from incomplete data via the EM algorithm},"{A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.}"
272,34,2501,1,A Unifying Review of Linear Gaussian Models,"Factor analysis, principal component analysis, mixtures of gaussian  clusters, vector quantization, Kalman filter models, and hidden Markov  models can all be unified as variations of unsupervised learning under  a single basic generative model. This is achieved by collecting  together disparate observations and derivations made by many previous  authors and introducing a new way of linking discrete and continuous  state models using a simple nonlinearity. Through the use of other  nonlinearities, we show how independent component analysis is also a  variation of the same basic generative model. We show that factor  analysis and mixtures of gaussians can be implemented in autoencoder  neural networks and learned using squared error plus the same  regularization term. We introduce a new model for static data, known as  sensible principal component analysis, as well as a novel concept of  spatially adaptive observation noise. We also review some of the  literature involving global and local mixtures of the basic models and  provide pseudocode for inference and learning for all the basic models."
273,34,2836,1,Support vector machine active learning for image retrieval,"Relevance feedback is often a critical component when designing image databases. With these databases it is difficult to specify queries directly and explicitly. Relevance feedback interactively determinines a user's desired output or  query concept  by asking the user whether certain proposed images are relevant or not. For a relevance feedback algorithm to be effective, it must grasp a user's query concept accurately and quickly, while also only asking the user to label a small number of images. We propose the use of a  support vector machine active learning  algorithm for conducting effective relevance feedback for image retrieval. The algorithm selects the most informative images to query a user and quickly learns a boundary that separates the images that satisfy the user's query concept from the rest of the dataset. Experimental results show that our algorithm achieves significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback."
274,34,3778,1,Auditory Scene Analysis: The Perceptual Organization of Sound,"{""Bregman has written a major book, a unique and important contribution to the rapidly expanding field of complex auditory perception. This is a big, rich, and fulfilling piece of work that deserves the wide audience it is sure to attract."" -- Stewart H. Hulse, <i>Science</i>  <P>Auditory Scene Analysis addresses the problem of hearing complex auditory environments, using a series of creative analogies to describe the process required of the human auditory system as it analyzes mixtures of sounds to recover descriptions of individual sounds. In a unified and comprehensive way, Bregman establishes a theoretical framework that integrates his findings with an unusually wide range of previous research in psychoacoustics, speech perception, music theory and composition, and computer modeling.}"
275,34,4113,1,An information-maximization approach to blind separation and blind deconvolution,"We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in ""blind"" signal processing. [Journal Article; In English; United States; MEDLINE]"
276,34,6640,1,Musical Genre Classification of Audio Signals,"Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content- based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61% for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification."
277,34,7201,1,Finding Structure in Time,"Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by [Jordan, 1986] which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
278,34,8158,1,Linear prediction: A tutorial review,"This paper gives an exposition of linear prediction in the analysis of discrete signals. The signal is modeled as a linear combination of its past values and present and past values of a hypothetical input to a system whose output is the given signal. In the frequency domain, this is equivalent to modeling the signal spectrum by a pole-zero spectrum. The major part of the paper is devoted to all-pole models. The model parameters are obtained by a least squares analysis in the time domain. Two methods result, depending on whether the signal is assumed to be stationary or nonstationary. The same results are then derived in the frequency domain. The resulting spectral matching formulation allows for the modeling of selected portions of a spectrum, for arbitrary spectral shaping in the frequency domain, and for the modeling of continuous as well as discrete spectra. This also leads to a discussion of the advantages and disadvantages of the least squares error criterion. A spectral interpretation is given to the normalized minimum prediction error. Applications of the normalized error are given, including the determination of an ""optimal"" number of poles. The use of linear prediction in data compression is reviewed. For purposes of transmission, particular attention is given to the quantization and encoding of the reflection (or partial correlation) coefficients. Finally, a brief introduction to pole-zero modeling is given."
279,34,9500,1,Putting Objects in Perspective,"Image understanding requires not only individually esti- mating elements of the visual world but also capturing the interplay among them. In this paper, we provide a frame- work for placing local object detection in the context of the overall 3D scene by modeling the interdependence of ob- jects, surface orientations, and camera viewpoint. Most object detection methods consider all scales and locations in the image as equally likely. We show that with probabilistic estimates of 3D geometry, both in terms of surfaces and world coordinates, we can put objects into perspective and model the scale and location variance in the image. Our approach reflects the cyclical nature of the problem by allowing probabilistic object hypotheses to re- fine geometry and vice-versa. Our framework allows pain- less substitution of almost any object detector and is easily extended to include other aspects of image understanding. Our results confirm the benefits of our integrated approach."
280,34,11209,1,Phoneme recognition using time-delay neural networks,"In this paper we present a Time-Delay Neural Network (TDNN) approach to phoneme recognition which is characterized by two important properties. 1) Using a 3 layer arrangement of simple computing units, a hierarchy can be constructed that allows for the formation of arbitrary nonlinear decision surfaces. The TDNN learns these decision surfaces automatically using error backpropagation [1]. 2) The time-delay arrangement enables the network to discover acoustic-phonetic features and the temporal relationships between them independent of position in time and hence not blurred by temporal shifts in the input. As a recognition task, the speaker-dependent recognition of the phonemes ``B,'' ``D,'' and ``G'' in varying phonetic contexts was chosen. For comparison, several discrete Hidden Markov Models (HMM) were trained to perform the same task. Performance evaluation over 1946 testing tokens from three speakers showed that the TDNN achieves a recognition rate of 98.5 percent correct while the rate obtained by the best of our HMM's was only 93.7 percent. Closer inspection reveals that the network ``invented'' well-known acoustic-phonetic features (e.g., F2-rise, F2-fall, vowel-onset) as useful abstractions. It also developed alternate internal representations to link different acoustic realizations to the same concept."
281,34,13125,1,Long Short-Term Memory,"Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
282,34,13874,1,Object-based auditory and visual attention," Theories of visual attention argue that attention operates on perceptual objects, and thus that interactions between object formation and selective attention determine how competing sources interfere with perception. In auditory perception, theories of attention are less mature and no comprehensive framework exists to explain how attention influences perceptual abilities. However, the same principles that govern visual perception can explain many seemingly disparate auditory phenomena. In particular, many recent studies of âinformational maskingâ can be explained by failures of either auditory object formation or auditory object selection. This similarity suggests that the same neural mechanisms control attention and influence perception across different sensory modalities."
283,34,15932,1,Generating Coherent Patterns of Activity from Chaotic Neural Networks," Summary Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated."
284,35,940,1,"The Power of Identity: The Information Age: Economy, Society and Culture, Volume II (The Information Age) 2nd Edition","{The Power of Identity is the second volume of Manuel Castells&#146;s trilogy, The Information Age: Economy, Society, and Culture. It deals with the social, political, and cultural dynamics associated with the technological transformation of our societies and with the globalization of the economy. It analyzes the importance of cultural, religious, and national identities as sources of meaning for people, and the implications of these identities for social movements. It studies grassroots mobilizations against the unfettered globalization of wealth and power, and considers the formation of alternative projects of social organization, as represented by the environmental movement and the women&#146;s movement. It also analyzes the crisis of the nation-state and its transformation into a network state, and the effects on political democracies of the difficulties of international governance and the submission of political representation to the dictates of media politics and the!  politics of scandal.    <P>This substantially expanded second edition updates and elaborates the analysis of these themes, adding new sections on al-Qaeda and global terrorist networks, on the anti-globalization movement, on American unilateralism and the conflicts of global governance, on the crisis of political legitimacy throughout the world, and on the theory of the network state.}"
285,35,4874,1,The mechanics of trust: a framework for research and design,"With an increasing number of technologies supporting transactions over distance and replacing traditional forms of interaction, designing for trust in mediated interactions has become a key concern for researchers in human computer interaction {(HCI).} While much of this research focuses on increasing users' trust, we present a framework that shifts the perspective towards factors that support trustworthy behavior. In a second step, we analyze how the presence of these factors can be signalled. We argue that it is essential to take a systemic perspective for enabling well-placed trust and trustworthy behavior in the long term. For our analysis we draw on relevant research from sociology, economics, and psychology, as well as {HCI.} We identify contextual properties (motivation based on temporal, social, and institutional embeddedness) and the actor's intrinsic properties (ability, and motivation based on internalized norms and benevolence) that form the basis of trustworthy behavior. Our analysis provides a frame of reference for the design of studies on trust in technology-mediated interactions, as well as a guide for identifying trust requirements in design processes. We demonstrate the application of the framework in three scenarios: call centre interactions, {B2C} e-commerce, and voice-enabled on-line gaming."
286,35,5266,1,Technology as Experience,"{In <i>Technology as Experience</i>, John McCarthy and Peter Wright argue that any account of what is often called the user experience must take into consideration the emotional, intellectual, and sensual aspects of our interactions with technology. We don't just use technology, they point out; we live with it. They offer a new approach to understanding human-computer interaction through examining the felt experience of technology. Drawing on the pragmatism of such philosophers as John Dewey and Mikhail Bakhtin, they provide a framework for a clearer analysis of technology as experience.<br /> <br /> Just as Dewey, in <i>Art as Experience</i>, argued that art is part of everyday lived experience and not isolated in a museum, McCarthy and Wright show how technology is deeply embedded in everyday life. The ""zestful integration"" or transcendent nature of the aesthetic experience, they say, is a model of what human experience with technology might become.<br /> <br /> McCarthy and Wright illustrate their theoretical framework with real-world examples that range from online shopping to ambulance dispatch. Their approach to understanding human computer interaction -- seeing it as creative, open, and relational, part of felt experience -- is a measure of the fullness of technology's potential to be more than merely functional.}"
287,35,8594,1,A Collaborative Web Browsing System for Multiple Mobile Users,"In mobile computing environments, handheld devices with low functionality restrict the services provided for mobile users. We propose a new concept of collaborative browsing, where mobile users collaboratively browse web pages designed for desktop PC. In collaborative browsing, a web page is divided into multiple components, and each is distributed to a different device. In mobile computing environments, the number of handheld devices, their capabilities, and other conditions can vary widely amongst mobile users who want to browse content. Therefore, we developed a page partitioning method for collaborative browsing, which divides a web page into multiple components. Moreover, we designed and implemented a collaborative web browsing system in which users can search and browse their target information by discussing and watching partial pages displayed on multiple devices."
288,35,10862,1,A large scale study of wireless search behavior: Google mobile search,"We present a large scale study of search patterns on Google's mobile search interface. Our goal is to understand the current state of wireless search by analyzing over 1 Million hits to Google's mobile search sites. Our study also includes the examination of search queries and the general categories under which they fall. We follow users throughout multiple interactions to determine search behavior; we estimate how long they spend inputting a query, viewing the search results, and how often they click on a search result. We also compare and contrast search patterns between 12-key keypad phones (cellphones), phones with QWERTY keyboards (PDAs) and conventional computers."
289,35,11101,1,Wikinomics: How Mass Collaboration Changes Everything,"{<B> In just the last few years, traditional collaboration&#151;in a meeting room, a conference call, even a convention center&#151;has been superseded by collaborations on an astronomical scale.  </B> <P> Today, encyclopedias, jetliners, operating systems, mutual funds, and many other items are being created by teams numbering in the thousands or even millions. While some leaders fear the heaving growth of these massive online communities, <I>Wikinomics</I> proves this fear is folly. Smart firms can harness collective capability and genius to spur innovation, growth, and success. <P> A brilliant guide to one of the most profound changes of our time, <I>Wikinomics</I> challenges our most deeply-rooted assumptions about business and will prove indispensable to anyone who wants to understand competitiveness in the twenty-first century. <P> Based on a $9 million research project led by bestselling author Don Tapscott, <I>Wikinomics</I> shows how masses of people can participate in the economy like never before. They are creating TV news stories, sequencing the human genome, remixing their favorite music, designing software, finding a cure for disease, editing school texts, inventing new cosmetics, or even building motorcycles.  You'll read about: <BR> &#149; Rob McEwen, the Goldcorp, Inc. CEO who used open source tactics and an online competition to save his company and breathe new life into an old-fashioned industry.<BR> &#149; Flickr, Second Life, YouTube, and other thriving online communities that transcend social networking to pioneer a new form of collaborative production.<BR> &#149; Mature companies like Procter & Gamble that cultivate nimble, trust-based relationships with external collaborators to form vibrant business ecosystems. <BR> <BR> An important look into the future, <I>Wikinomics</I> will be your road map for doing business in the twenty-first century.}"
290,36,1131,1,Workflow Patterns,"Differences in features supported by the various contemporary commercial workflow management systems point to different insights of suitability and different levels of expressive power. The challenge, which we undertake in this paper, is to systematically address workflow requirements, from basic to complex. Many of the more complex requirements identified, recur quite frequently in the analysis phases of workflow projects, however their implementation is uncertain in current products. Requirements for workflow languages are indicated through workflow patterns. In this context, patterns address business requirements in an imperative workflow style expression, but are removed from specific workflow languages. The paper describes a number of workflow patterns addressing what we believe identify comprehensive workflow functionality. These patterns provide the basis for an in-depth comparison of a number of commercially availablework flow management systems. As such, this paper can be seen as the academic response to evaluations made by prestigious consulting companies. Typically, these evaluations hardly consider the workflow modeling language and routing capabilities, and focus more on the purely technical and commercial aspects."
291,36,3231,1,"A Taxonomy of Data Grids for Distributed Data Sharing, Management and Processing","Data Grids have been adopted as the platform for scientific communities that need to share, access, transport, process and manage large data collections distributed worldwide. They combine high-end computing technologies with high-performance networking and wide-area storage management techniques. In this paper, we discuss the key concepts behind Data Grids and compare them with other data sharing and distribution paradigms such as content delivery networks, peer-to-peer networks and distributed databases. We then provide comprehensive taxonomies that cover various aspects of architecture, data transportation, data replication and resource allocation and scheduling. Finally, we map the proposed taxonomy to various Data Grid systems not only to validate the taxonomy but also to identify areas for future exploration. Through this taxonomy, we aim to categorise existing systems to better understand their goals and their methodology. This would help evaluate their applicability for solving similar problems. This taxonomy also provides a \\gap analysis\\ of this area through which researchers can potentially identify new issues for investigation. Finally, we hope that the proposed taxonomy and mapping also helps to provide an easy way for new practitioners to understand this complex area of research."
292,36,11978,1,Improving life sciences information retrieval using semantic web technology.,"The ability to retrieve relevant information is at the heart of every aspect of research and development in the life sciences industry. Information is often distributed across multiple systems and recorded in a way that makes it difficult to piece together the complete picture. Differences in data formats, naming schemes and network protocols amongst information sources, both public and private, must be overcome, and user interfaces not only need to be able to tap into these diverse information sources but must also assist users in filtering out extraneous information and highlighting the key relationships hidden within an aggregated set of information. The Semantic Web community has made great strides in proposing solutions to these problems, and many efforts are underway to apply Semantic Web techniques to the problem of information retrieval in the life sciences space. This article gives an overview of the principles underlying a Semantic Web-enabled information retrieval system: creating a unified abstraction for knowledge using the RDF semantic network model; designing semantic lenses that extract contextually relevant subsets of information; and assembling semantic lenses into powerful information displays. Furthermore, concrete examples of how these principles can be applied to life science problems including a scenario involving a drug discovery dashboard prototype called BioDash are provided."
293,37,136,1,Understanding and Using Context,"Context is a poorly used source of information in our computing environments. As a result, we have an impoverished understanding of what context is and how it can be used. In this paper, we provide an operational definition of context and discuss the different ways that context can be used by context-aware applications. We also present the Context Toolkit, an architecture that supports the building of these context-aware applications. We discuss the features and abstractions in the toolkit that make the task of building applications easier. Finally, we introduce a new abstraction, a situation, which we believe will provide additional support to application designers.  1. Introduction  Humans are quite successful at conveying ideas to each other and reacting appropriately. This is due to many factors: the richness of the language they share, the common understanding of how the world works, and an implicit understanding of everyday situations. When humans talk with humans, they are able..."
294,37,1746,1,Ambiguity as a resource for design,"Ambiguity is usually considered anathema in Human Computer Interaction. We argue, in contrast, that it is a resource for design that can be used to encourage close personal engagement with systems. We illustrate this with examples from contemporary arts and design practice, and distinguish three broad classes of ambiguity according to where uncertainty is located in the interpretative relationship linking person and artefact. Ambiguity of information finds its source in the artefact itself, ambiguity of context in the sociocultural discourses that are used to interpret it, and ambiguity of relationship in the interpretative and evaluative stance of the individual. For each of these categories, we describe tactics for emphasising ambiguity that may help designers and other practitioners understand and craft its use."
295,37,2830,1,Pervasive computing: vision and challenges,"This article discusses the challenges in computer systems research posed by the emerging field of pervasive computing. It first examines the relationship of this new field to its predecessors: distributed systems and mobile computing. It then identifies four new research thrusts: effective use of smart spaces, invisibility, localized scalability, and masking uneven conditioning. Next, it sketches a couple of hypothetical pervasive computing scenarios, and uses them to identify key capabilities missing from today's systems. The article closes with a discussion of the research necessary to develop these capabilities."
296,37,3311,1,The Uses of Argument,"{This reissue of the modern classic on the study of argumentation features a new Introduction by the author.} {A central theme throughout the impressive series of philosophical books and articles Stephen Toulmin has published since 1948 is the way in which assertions and opinions concerning all sorts of topics, brought up in everyday life or in academic research, can be rationally justified. Is there one universal system of norms, by which all sorts of arguments in all sorts of fields must be judged, or must each sort of argument be judged according to its own norms? In The Uses of Argument (1958) Toulmin sets out his views on these questions for the first time. In spite of initial criticisms from logicians and fellow philosophers, The Uses of Argument has been an enduring source of inspiration and discussion to students of argumentation from all kinds of disciplinary background for more than forty years.}"
297,37,3974,1,Agent-based software engineering,"The technology of intelligent agents and multi-agent systems seems set to radically alter the way in which complex, distributed, open systems are conceptualized and implemented. The purpose of this paper is to consider the problem of building a multi-agent system as a software engineering enterprise. The article focuses on three issues: (i) how agents might be specified; (ii) how these specifications might be refined or otherwise transformed into efficient implementations; and (iii) how implemented agents and multi-agent systems might subsequently be verified, in order to show that they are correct with respect to their specifications. These issues are discussed with reference to a number of casestudies. The article concludes by setting out some issues and open problems for future research. 1 Introduction  Intelligent agents are ninety-nine percent computer science and one percent AI.  Oren Etzioni [12] Over its 40-year history, Artificial Intelligence (AI) has been subject to many and..."
298,37,5169,1,Formalising Trust as a Computational Concept,"Trust is a judgement of unquestionable utility â as humans we use it every day of our lives. However, trust has suffered from an imperfect understanding, a plethora of definitions, and informal use in the literature and in everyday life. It is common to say âI trust you, â but what does that mean? This thesis provides a clarification of trust. We present a formalism for trust which provides us with a tool for precise discussion. The formalism is implementable: it can be embedded in an artificial agent, enabling the agent to make trust-based decisions. Its applicability in the domain of Distributed Artificial Intelligence (DAI) is raised. The thesis presents a testbed populated by simple trusting agents which substantiates the utility of the formalism. The formalism provides a step in the direction of a proper understanding and definition of human trust. A contribution of the thesis is its detailed exploration of the possibilities of future work in the area. Summary 1. Overview This thesis presents an overview of trust as a social phenomenon and discusses it formally. It argues that trust is: â¢ A means for understanding and adapting to the complexity of the environment. â¢ A means of providing added robustness to independent agents. â¢ A useful judgement in the light of experience of the behaviour of others. â¢ Applicable to inanimate others. The thesis argues these points from the point of view of artificial agents. Trust in an artificial agent is a means of providing an additional tool for the consideration of other agents and the environment in which it exists. Moreover, a formalisation of trust enables the embedding of the concept into an artificial agent. This has been done, and is documented in the thesis. 2. Exposition There are places in the thesis where it is necessary to give a broad outline before going deeper. In consequence it may seem that the subject is not receiving a thorough treatment, or that too much is being discussed at one time! (This is particularly apparent in the first and second chapters.) To present a thorough understanding of trust, we have proceeded breadth first in the introductory chapters. Chapter 3 expands, depth first, presenting critical views of established researchers."
299,37,6549,1,Intention is choice with commitment,"This paper explores principles governing the rational balance among an agent's beliefs, goals, actions, and intentions. Such principles provide specifications for artificial agents, and approximate a theory of human action (as philosophers use the term). By making explicit the conditions under which an agent can drop his goals, i.e., by specifying how the agent is committed to his goals, the formalism captures a number of important properties of intention. Specifically, the formalism provides analyses for Bratman's three characteristic functional roles played by intentions [7, 9], and shows how agents can avoid intending all the foreseen side-effects of what they actually intend. Finally, the analysis shows how intentions can be adopted relative to a background of relevant beliefs and other intentions or goals. By relativizing one agent's intentions in terms of beliefs about another agent's intentions (or beliefs), we derive a preliminary account of interpersonal commitments."
300,37,9294,1,The Distributed Constraint Satisfaction Problem: Formalization and Algorithms,"In this paper, we develop a formalism called a distributed constraint satisfaction problem (distributed CSP) and algorithms for solving distributed CSPs. A distributed CSP is a constraint satisfaction problem in which variables and constraints are distributed among multiple agents. Various application problems in Distributed Artificial Intelligence can be formalized as distributed CSPs. We present our newly developed technique called asynchronous backtracking that allows agents to act asynchronously and concurrently without any global control, while guaranteeing the completeness of the algorithm. Furthermore, we describe how the asynchronous backtracking algorithm can be modified into a more efficient algorithm called an asynchronous weak-commitment search, which can revise a bad decision without exhaustive search by changing the priority order of agents dynamically. The experimental results on various example problems show that the asynchronous weak-commitment search algorithm is, by far more, efficient than the asynchronous backtracking algorithm and can solve fairly large-scale problems."
301,38,13280,1,"I tube, you tube, everybody tubes: analyzing the world's largest user generated content video system","User Generated Content (UGC) is re-shaping the way people watch video and TV, with millions of video producers and consumers. In particular, UGC sites are creating new viewing patterns and social interactions, empowering users to be more creative, and developing new business opportunities. To better understand the impact of UGC systems, we have analyzed YouTube, the world's largest UGC VoD system. Based on a large amount of data collected, we provide an in-depth study of YouTube and other similar UGC systems. In particular, we study the popularity life-cycle of videos, the intrinsic statistical properties of requests and their relationship with video age, and the level of content aliasing or of illegal content in the system. We also provide insights on the potential for more efficient UGC VoD systems (e.g. utilizing P2P techniques or making better use of caching). Finally, we discuss the opportunities to leverage the latent demand for niche videos that are not reached today due to information filtering effects or other system scarcity distortions. Overall, we believe that the results presented in this paper are crucial in understanding UGC systems and can provide valuable information to ISPs, site administrators, and content owners with major commercial and technical implications."
302,38,13535,1,A systemic and cognitive view on collaborative knowledge building with wikis,"Abstract&nbsp;&nbsp;Wikis provide new opportunities for learning and for collaborative knowledge building as well as for understanding these processes. This article presents a theoretical framework for describing how learning and collaborative knowledge building take place. In order to understand these processes, three aspects need to be considered: the social processes facilitated by a wiki, the cognitive processes of the users, and how both processes influence each other mutually. For this purpose, the model presented in this article borrows from the systemic approach of Luhmann as well as from Piaget's theory of equilibration and combines these approaches. The model analyzes processes which take place in the social system of a wiki as well as in the cognitive systems of the users. The model also describes learning activities as processes of externalization and internalization. Individual learning happens through internal processes of assimilation and accommodation, whereas changes in a wiki are due to activities of external assimilation and accommodation which in turn lead to collaborative knowledge building. This article provides empirical examples for these equilibration activities by analyzing Wikipedia articles. Equilibration activities are described as being caused by subjectively perceived incongruities between an individuals' knowledge and the information provided by a wiki. Incongruities of medium level cause cognitive conflicts which in turn activate the described processes of equilibration and facilitate individual learning and collaborative knowledge building."
303,39,5617,1,Less is More - Genetic Optimisation of Nearest Neighbour Classifiers,"The present paper deals with optimisation of Nearest Neighbour rule Classifiers via Genetic Algorithms. The methodology consists on implement a Genetic Algorithm capable of search the input feature space used by the NNR classifier. Results show that is adequate to perform feature reduction and simultaneous improve the Recognition Rate. Some practical examples prove that is possible to Recognise Portuguese Granites in 100%, with only 3 morphological features (from an original set of 117..."
304,39,5625,1,Artificial Neoteny In Evolutionary Image Segmentation,"Neoteny, also spelled Paedomorphosis, can be defined in biological terms as the retention by an organism of juvenile or even larval traits into later life. In some species, all morphological development is retarded; the organism is juvenilized but sexually mature. Such shifts of reproductive capability would appear to have adaptive significance to organisms that exhibit it. In terms of evolutionary theory, the process of paedomorphosis suggests that larval stages and developmental phases of..."
305,39,5626,1,Intrusion Detection Systems using Adaptive Regression Splines,"Past few years have witnessed a growing recognition of soft computing technologies for the construction of intelligent and reliable intrusion detection systems. Due to increasing incidents of cyber attacks, building effective intrusion detection systems (IDSs) are essential for protecting information systems security, and yet it remains an elusive goal and a great challenge. In this paper, we report a performance analysis between Multivariate Adaptive Regression Splines (MARS), neural networks and support vector machines. The MARS procedure builds flexible regression models by fitting separate splines to distinct intervals of the predictor variables. A brief comparison of different neural network learning algorithms is also given."
306,39,5633,1,"On Ants, Bacteria and Dynamic Environments","Wasps, bees, ants and termites all make effective use of their environment and resources by displaying collective âswarmâ intelligence. Termite colonies - for instance - build nests with a complexity far beyond the comprehension of the individual termite, while ant colonies dynamically allocate labor to various vital tasks such as foraging or defense without any central decision-making ability. Recent research suggests that microbial life can be even richer: highly social, intricately networked, and teeming with interactions, as found in bacteria. What strikes from these observations is that both ant colonies and bacteria have similar natural mechanisms based on Stigmergy and Self-Organization in order to emerge coherent and sophisticated patterns of global behaviour. Keeping in mind the above characteristics we will present a simple model to tackle the collective adaptation of a social swarm based on real ant colony behaviors (SSA algorithm) for tracking extrema in dynamic environments and highly multimodal complex functions described in the well-know De Jong test suite. Then, for the purpose of comparison, a recent model of artificial bacterial foraging (BFOA algorithm) based on similar stigmergic features is described and analyzed. Final results indicate that the SSA collective intelligence is able to cope and quickly adapt to unforeseen situations even when over the same cooperative foraging period, the community is requested to deal with two different and contradictory purposes, while outperforming BFOA in adaptive speed."
307,39,5635,1,Societal Implicit Memory and his Speed on Tracking Extrema over Dynamic Environments using Self-Regulatory Swarms,"In order to overcome difficult dynamic optimization and environment extrema tracking problems, we propose a Self-Regulated Swarm (SRS) algorithm which hybridizes the advantageous characteristics of Swarm Intelligence as the emergence of a societal environmental memory or cognitive map via collective pheromone laying in the landscape (properly balancing the exploration/exploitation nature of the search strategy), with a simple Evolutionary mechanism that trough a direct reproduction procedure linked to local environmental features is able to self-regulate the above exploratory swarm population, speeding it up globally. In order to test his adaptive response and robustness, we have recurred to different dynamic multimodal complex functions as well as to Dynamic Optimization Control (DOC) problems. Measures were made for different dynamic settings and parameters such as, environmental upgrade frequencies, landscape changing speed severity, type of dynamic (linear or circular), and to dramatic changes on the algorithmic search purpose over each test environment (e.g. shifting the extrema). Finally, comparisons were made with traditional Genetic Algorithms (GA), Bacterial Foraging Optimization Algorithms (BFOA), as well as with more recently proposed Co-Evolutionary approaches. SRS, were able to demonstrate quick adaptive responses, while outperforming the results obtained by the other approaches. Additionally, some successful behaviors were found: SRS was able not only to achieve quick adaptive responses, as to maintaining a number of different solutions, while adapting to new unforeseen extrema; the possibility to spontaneously create and maintain different subpopulations on different peaks, emerging different exploratory corridors with intelligent path planning capabilities; the ability to request for new agents over dramatic changing periods, and economizing those foraging resources over periods of stabilization. Finally, results prove that the present SRS collective swarm of bio-inspired agents is able to track about 65% of moving peaks traveling up to ten times faster than the velocity of a single individual composing that precise swarm tracking system. This emerged behavior is probably one of the most interesting ones achieved by the present work."
308,40,2420,1,Chat circles,"Although current online chat environments provide new opportunities for communication, they are quite constrained in their ability to convey many important pieces of social information, ranging from the number of participants in a conversation to the subtle nuances of expression that enrich face to face speech. In this paper we present Chat Circles, an abstract graphical interface for synchronous conversa- tion. Here, presence and activity are made manifest by changes in color and form, proximity-based filtering intu- itively breaks large groups into conversational clusters, and the archives of a conversation are made visible through an integrated history interface. Our goal in this work is to cre- ate a richer environment for online discussions. There are currently a wide variety of tools that allow for synchronous communication over a computer network. Internet Relay Chat (IRC), for instance, is one of the Inter- net's most popular applications for interpersonal communi- cation. And, although the World Wide Web's initial protocols were not conducive to live interaction, the advent of Java has made Web-based chatrooms increasingly popu- lar. When email, newsgroups and chatrooms were first devel- oped, ASCII interfaces were the norm: most systems lacked both the power and the infrastructure for more elaborate graphical interfaces. Today, although faster computers and networks as well as support for visual routines make graphi-"
309,40,3092,1,The Discovery of Grounded Theory: Strategies for Qualitative Research,"One should not be dissuaded from this book by its publication date. The material is as relevant now the day it was published, making it an essential classic now for over 30 years. Readers of this book will be introduced into an entirely new paradigm for doing rigorous research based on a qualitative methodology. Particularly if you are only familiar with the traditional scientific method and experimentalism, this book open up a whole new world. Glaser & Strauss show how theory emerges from the data, as an ever improving understanding of the signifiance of what is discovered. This book really presents a robust way for a researcher in any field, but particularly in the human sciences, to approach day to day research. It is the living method of creativity and innovation, presenting a system for understanding one's discoveries and framing them to producing meaningful knowledge. Everyone is approaching life this way, living out experiences and drawing conclusions from them, including the most mainstream of scientists. But Glaser and Strauss show how that process can be transformed from a willy-nilly gut feel into something that more reliably produces defensible knowledge claims and builds a substantive theoretical network."
310,40,10099,1,"Revisiting the Commons: Local Lessons, Global Challenges","In a seminal paper, Garrett Hardin argued in 1968 that users of a commons are caught in an inevitable process that leads to the destruction of the resources on which they depend. This article discusses new insights about such problems and the conditions most likely to favor sustainable uses of common-pool resources. Some of the most difficult challenges concern the management of large-scale resources that depend on international cooperation, such as fresh water in international basins or large marine ecosystems. Institutional diversity may be as important as biological diversity for our long-term survival."
311,41,1148,1,Consciousness Explained,"{Consciousness is notoriously difficult to explain. On one hand, there are facts about conscious experience--the way clarinets sound, the way lemonade tastes--that we know subjectively, from the inside. On the other hand, such facts are not readily accommodated in the objective world described by science. How, after all, could the reediness of clarinets or the tartness of lemonade be predicted in advance? Central to Daniel C. Dennett's attempt to resolve this dilemma is the ""heterophenomenological"" method, which treats reports of introspection nontraditionally--not as evidence to be used in explaining consciousness, but as data to be explained. Using this method, Dennett argues against the myth of the Cartesian theater--the idea that consciousness can be precisely located in space or in time. To replace the Cartesian theater, he introduces his own multiple drafts model of consciousness, in which the mind is a bubbling congeries of unsupervised parallel processing. Finally, Dennett tackles the conventional philosophical questions about consciousness, taking issue not only with the traditional answers but also with the traditional methodology by which they were reached.<p>  Dennett's writing, while always serious, is never solemn; who would have thought that combining philosophy, psychology, and neuroscience could be such fun? Not every reader will be convinced that Dennett has succeeded in explaining consciousness; many will feel that his account fails to capture essential features of conscious experience. But none will want to deny that the attempt was well worth making. <I>--Glenn Branch</I>}"
312,41,1443,1,Aspects of the Theory of Syntax,"Beginning in the mid-fifties and emanating largely form MIT, and approach was developed to linguistic theory and to the study of the structure of particular languages that diverges in many respects from modern linguistics. Although this approach is connected to the traditional study of languages, it differs enough in its specific conclusions about the structure and in its specific conclusions about the structure of language to warrant a name, ""generative grammar.""  Various deficiencies have been discovered in the first attempts to formulate a theory of transformational generative grammar and in the descriptive analysis of particular languages that motivated these formulations. At the same time, it has become apparent that these formulations can be extended and deepened.  The major purpose of this book is to review these developments and to propose a reformulation of the theory of transformational generative grammar that takes them into account. The emphasis in this study is syntax; semantic and phonological aspects of the language structure are discussed only insofar as they bear on syntactic theory."
313,41,1760,1,The Modularity of Mind,{This study synthesizes current information from the various fields of cognitive science in support of a new and exciting theory of mind. Most psychologists study horizontal processes like memory and information flow; Fodor postulates a vertical and modular psychological organization underlying biologically coherent behaviors. This view of mental architecture is consistent with the historical tradition of faculty psychology while integrating a computational approach to mental processes. One of the most notable aspects of Fodor's work is that it articulates features not only of speculative cognitive architectures but also of current research in artificial intelligence.<br /> <br /> Jerry A. Fodor is Professor of Psychology and Chairman of the Department of Philosophy at MIT.}
314,41,2687,1,A Theory of Justice,"{Since it appeared in 1971, John Rawls's <I>A Theory of Justice</I> has become a classic. The author has now revised the original edition to clear up a number of difficulties he and others have found in the original book. <P> Rawls aims to express an essential part of the common core of the democratic tradition--justice as fairness--and to provide an alternative to utilitarianism, which had dominated the Anglo-Saxon tradition of political thought since the nineteenth century. Rawls substitutes the ideal of the social contract as a more satisfactory account of the basic rights and liberties of citizens as free and equal persons. ""Each person,"" writes Rawls, ""possesses an inviolability founded on justice that even the welfare of society as a whole cannot override."" Advancing the ideas of Rousseau, Kant, Emerson, and Lincoln, Rawls's theory is as powerful today as it was when first published.}"
315,41,6449,1,Scientific Reasoning: The Bayesian Approach,"{In this clearly reasoned defense of Bayes's Theorem &#151; that probability can be used to reasonably justify scientific theories &#151; Colin Howson and Peter Urbach examine the way in which scientists appeal to probability arguments, and demonstrate that the classical approach to statistical inference is full of flaws. Arguing the case for the Bayesian method with little more than basic algebra, the authors show that it avoids the difficulties of the classical system. The book also refutes the major criticisms leveled against Bayesian logic, especially that it is too subjective. This newly updated edition of this classic textbook is also suitable for college courses.}"
316,41,7209,1,Collected Papers,"{John Rawls's work on justice has drawn more commentary and aroused wider attention than any other work in moral or political philosophy in the twentieth century. Rawls is the author of two major treatises, <i>A Theory     of Justice</i> (1971) and <i>Political Liberalism</i> (1993); it is said that <i>A Theory of Justice</i> revived political philosophy in the English-speaking world.     But before and after writing his great treatises Rawls produced a steady stream of essays. Some of these essays articulate views of justice and liberalism distinct from those found in the two books. They are important in and of     themselves because of the deep issues about the nature of justice, moral reasoning, and liberalism they raise as well as for the light they shed on the evolution of Rawls's views. Some of the articles tackle issues not addressed in     either book. They help identify some of the paths open to liberal theorists of justice and some of the knotty problems which liberal theorists must seek to resolve. A complete collection of John Rawls's essays is long overdue.}"
317,41,10791,1,Visual routines,"This paper examines the processing of visual information beyond the creation of the early representations. A fundamental requirement at this level is the capacity to establish visually abstract shape properties and spatial relations. This capacity plays a major role in object recognition, visually guided manipulation, and more abstract visual thinking. For the human visual system, the perception of spatial properties and relations that are complex from a computational standpoint nevertheless often appears deceivingly immediate and effortless. The proficiency of the human system in analyzing spatial information far surpasses the capacities of current artificial systems. The study of the computations that underlie this competence may therefore lead to the development of new more efficient methods for the spatial analysis of visual information. The perception of abstract shape properties and spatial relations raises fundamental difficulties with major implications for the overall processing of visual information. It will be argued that the computation of spatial relations divides the analysis of visual information into two main stages. The first is the bottom-up creation of certain representations of the visible environment. The second stage involves the application of process called âvisual routinesâ to the representations constructed in the first stage. These routines can establish properties and relations that cannot be represented explicitly in the initial representations. Visual routines are composed of sequences of elemental operations. Routines for different properties and relations share elemental operations. Using a fixed set of basic operations, the visual system can assemble different routines to extract an unbounded variety of shape properties and spatial relations."
318,41,11967,1,Knowledge and its Limits,"Knowledge and Its Limits presents a systematic new conception of knowledge as a fundamental kind of mental state sensitive to the knower's environment. It makes a major contribution to the debate between externalist ad internalist philosophies of mind, and breaks radically with the epistemological tradition of analysing knowledge in terms of true belief. The theory casts light on a wide variety of philosophical issues: the problem of scepticism, the nature of evidence, probability and assertion, the dispute between realism and anti-realism and the paradox of the surprise examination. Williamson relates the new conception to structural limits on knowledge which imply that what can be known never exhausts what is true. The arguments are illustrated by rigorous models based on epistemic logic and probability theory. The result is a new way of doing epistemology for the twenty-first century."
319,42,1788,1,"Generative Programming: Methods, Tools, and Applications","{The authors present a grand tour of Generative Programming that is bound to become a classic. They . . . focus on the generally unappreciated connection between Domain Specific Languages and Generative Programming as a motivation for future development. Their wide-ranging and practical methods for Domain Analysis and Domain Engineering describe the first steps that developers can take right now . . . and are valuable both when existing systems are used or in preparation for emerging new generative technologies."" --Charles Simonyi, Chief Architect at Microsoft Research and the inventor of Intentional Programming  ""The book develops strong themes around unifying principles that tie the pieces together, most notably domain engineering and metaprogramming. It is crucial to understand that this book is not just some refreshing diversion, nor just an exposition of some noteworthy niche techniques: It is a harbinger of a broader enlightenment that opens the door to a new age."" --From the Foreword by James Coplien, a Distinguished Member of Technical Staff at Lucent Technologies, Bell Laboratories  <P>Generative Programming (GP) offers great promise to application developers. It makes the idea of moving from one-of-a-kind software systems to the semi-automated manufacture of wide varieties of software quite real. In short, GP is about recognizing the benefits of automation in software development. Generative Programming covers methods and tools that will help you design and implement the right components for a system family and automate component assembly. The methods presented here are applicable for all commercial development--from ""programming in the small,"" at the level of classes and procedures--to ""programming in the large,"" or developing families of large systems.   <P>Generative Programming is your complete guide and reference to this emerging discipline. It provides in-depth treatment of critical technologies and topics including:  Domain Engineering  Feature Modeling  Generic Programming  Aspect-Oriented Programming  Template Metaprogramming in C++  Generators  Microsoft's Intentional Programming  Using this book you will learn how these techniques fit together and, more importantly, how to apply them in practice. The text contains three comprehensive case studies in three different domains: programming domain (container data structures), business domain (banking), and scientific computing (matrix computations).}"
320,42,4134,1,"Feature Models, Grammars, and Propositional Formulas","Feature models are used to specify members of a product-line. Despite years of progress, contemporary tools often provide limited support for feature constraints and offer little or no support for debugging feature models. We integrate prior results to connect feature models, grammars, and propositional formulas. This connection allows arbitrary propositional constraints to be defined among features and enables off-the-shelf satisfiability solvers to debug feature models. We also show how our ideas can generalize recent results on the staged configuration of feature models."
321,42,4252,1,{Variability Issues in Software Product Lines},"Software product lines (or system families) have achieved considerable adoption by the software industry. A software product line captures the commonalities between a set of products while providing for the differences. Differences are managed by delaying design decisions, thereby introducing variation points. The whole of variation points is typically referred to as the variability of the software product line. Variability management is, however, not a trivial activity and several issues exist, both in general as well as specific to individual phases in the lifecycle. This paper identifies and describes several variability issues based on practical experiences and theoretical understanding of the problem domain."
322,42,4792,1,Concern Graphs: Finding and Describing Concerns Using Structural Program Dependencies,"Many maintenance tasks address concerns, or features, that are not well modularized in the source code comprising a system. Existing approaches available to help software developers locate and manage scattered concerns use a representation based on lines of source code, complicating the analysis of the concerns. In this paper, we introduce the Concern Graph representation that abstracts the implementation details of a concern and makes explicit the relationships between different parts of the concern. The abstraction used in a Concern Graph has been designed to allow an obvious and inexpensive mapping back to the corresponding source code. To investigate the practical tradeoffs related to this approach, we have built the Feature Exploration and Analysis tool (FEAT) that allows a developer to manipulate a concern representation extracted from a Java system, and to analyze the relationships of that concern to the code base. We have used this tool to find and describe concerns related to software change tasks. We have performed case studies to evaluate the feasibility, usability, and scalability of the approach. Our results indicate that Concern Graphs can be used to document a concern for change, that developers unfamiliar with Concern Graphs can use them effectively, and that the underlying technology scales to industrial-sized programs."
323,42,5390,1,Version models for software configuration management,"After more than 20 years of research and practice in software configuration management (SCM), constructing consistent configurations of versioned software products still remains a challenge. This article focuses on the version models underlying both commercial systems and research prototypes. It provides an overview and classification of different versioning paradigms and defines and relates fundamental concepts such as revisions, variants, configurations, and changes. In particular, we focus on intensional versioning, that is, construction of versions based on configuration rules. Finally, we provide an overview of systems that have had significant impact on the development of the SCM discipline and classify them according to a detailed taxonomy."
324,42,8871,1,Agile Modeling: Effective Practices for Extreme Programming and the Unified Process,"{The first book to cover Agile Modeling, a new modeling technique created specifically for XP projects eXtreme Programming (XP) has created a buzz in the software development community-much like Design Patterns did several years ago. Although XP presents a methodology for faster software development, many developers find that XP does not allow for modeling time, which is critical to ensure that a project meets its proposed requirements. They have also found that standard modeling techniques that use the Unified Modeling Language (UML) often do not work with this methodology. In this innovative book, Software Development columnist Scott Ambler presents Agile Modeling (AM)-a technique that he created for modeling XP projects using pieces of the UML and Rational's Unified Process (RUP). Ambler clearly explains AM, and shows readers how to incorporate AM, UML, and RUP into their development projects with the help of numerous case studies integrated throughout the book. <ul> <li>AM was created by the author for modeling XP projects-an element lacking in the original XP design <li>The XP community and its creator have embraced AM, which should give this book strong market acceptance </ul> <p> Companion Web site at www.agilemodeling.com features updates, links to XP and AM resources, and ongoing case studies about agile modeling.}"
325,42,10617,1,Representing concerns in source code,"A software modification task often addresses several concerns . A concern is anything a stakeholder may want to consider as a conceptual unit, including features, nonfunctional requirements, and design idioms. In many cases, the source code implementing a concern is not encapsulated in a single programming language module, and is instead scattered and tangled throughout a system. Inadequate separation of concerns increases the difficulty of evolving software in a correct and cost-effective manner. To make it easier to modify concerns that are not well modularized, we propose an approach in which the implementation of concerns is documented in artifacts, called concern graphs. Concern graphs are abstract models that describe which parts of the source code are relevant to different concerns. We present a formal model for concern graphs and the tool support we developed to enable software developers to create and use concern graphs during software evolution tasks. We report on five empirical studies, providing evidence that concern graphs support views and operations that facilitate the task of modifying the code implementing scattered concerns, are cost-effective to create and use, and robust enough to be used with different versions of a software system."
326,42,11949,1,Source Code Analysis: A Road Map,"The automated and semi-automated analysis of source code has remained a topic of intense research for more than thirty years. During this period, algorithms and techniques for source-code analysis have changed, sometimes dramatically. The abilities of the tools that implement them have also expanded to meet new and diverse challenges. This paper surveys current work on source-code analysis. It also provides a road map for future work over the next five-year period and speculates on the development of source-code analysis applications, techniques, and challenges over the next 10, 20, and 50 years."
327,43,250,1,Mining Association Rules between Sets of Items in Large Databases,"We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm."
328,43,934,1,A simple rule-based part of speech tagger,"Automatic part of speech tagging is an area of natural language processing where statistical techniques have been more successful than rule-based methods. In this paper, we present a simple rule-based part of speech tagger which automatically acquires its rules and tags with accuracy comparable to stochastic taggers. The rule-based tagger has many advantages over these taggers, including: a vast reduction in stored information required, the perspicuity of a small set of meaningful rules, ease of finding and implementing improvements to the tagger, and better portability from one tag set, corpus genre or language to another. Perhaps the biggest contribution of this work is in demonstrating that the stochastic method is not the only viable method for part of speech tagging. The fact that a simple rule-based tagger that automatically learns its rules can perform so well should offer encouragement for researchers to further explore rulebased tagging, searching for a better and more expressive set of rule templates and other variations on the simple but effective theme described below. 1."
329,43,1105,1,Elements of Information Theory,"{Following a brief introduction and overview, early chapters cover the basic algebraic relationships of entropy, relative entropy and mutual information, AEP, entropy rates of stochastics processes and data compression, duality of data compression and the growth rate of wealth. Later chapters explore Kolmogorov complexity, channel capacity, differential entropy, the capacity of the fundamental Gaussian channel, the relationship between information theory and statistics, rate distortion and network information theories. The final two chapters examine the stock market and inequalities in information theory. In many cases the authors actually describe the properties of the solutions before the presented problems.}"
330,43,1677,1,Text Classification using String Kernels,"We propose a novel approach for categorizing text documents based on the use of a special kernel. The kernel is an inner product in the feature space generated by all subsequences of length &lt;em&gt;k&lt;/em&gt;. A subsequence is any ordered sequence of &lt;em&gt;k&lt;/em&gt; characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences that are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of &lt;em&gt;k&lt;/em&gt;, since the dimension of the feature space grows exponentially with &lt;em&gt;k&lt;/em&gt;. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. Experimental comparisons of the performance of the kernel compared with a standard word feature space kernel (Joachims, 1998) show positive results on modestly sized datasets. The case of contiguous subsequences is also considered for comparison with the subsequences kernel with different decay factors. For larger documents and datasets the paper introduces an approximation technique that is shown to deliver good approximations efficiently for large datasets."
331,43,2303,1,A Method for Semi-Automatic Ontology Acquisition from a Corporate Intranet,"This paper describes our actual and ongoing work in supporting semiautomatic  ontology acquisition from a corporate intranet of an insurance company.  A comprehensive architecture and a system for semi-automatic ontology  acquisition supports processing semi-structured information (e.g. contained in  dictionaries) and natural language documents and including existing core ontologies  (e.g. GermaNet, WordNet). We present a method for acquiring a applicationtailored  domain ontology from given..."
332,43,2324,1,Text categorization with support vector machines: learning with many relevant features,"This paper explores the user of Support Vector machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifies, why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and they behave robustly over a variety of different learning tasks. Furthermore, they are fully automatic, eliminating the need for manuar parameter tuning."
333,43,3188,1,Support Vector Machine Active Learning with Applications to Text Classification,". Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.  Keywords: Active Learning, Selective Sampling, Support Vector Machines, Classification, Relevance Feedback  Abbreviations: SVM -- Support Vector Machine; TSVM -- Transductive Support Vector Machine 1."
334,43,3214,1,Making large-scale support vector machine learning practical,"Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, off-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVMlight is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVMlight V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains."
335,43,4198,1,Unsupervised learning of natural languages,"Edited by James L. McClelland, Carnegie Mellon University, Pittsburgh, PA, and approved June 14, 2005 (received for review December 25, 2004)We address the problem, fundamental to linguistics, bioinformatics, and certain other disciplines, of using corpora of raw symbolic sequential data to infer underlying rules that govern their production. Given a corpus of strings (such as text, transcribed speech, chromosome or protein sequence data, sheet music, etc.), our unsupervised algorithm recursively distills from it hierarchically structured patterns. The ADIOS (automatic distillation of structure) algorithm relies on a statistical method for pattern extraction and on structured generalization, two processes that have been implicated in language acquisition. It has been evaluated on artificial context-free grammars with thousands of rules, on natural languages as diverse as English and Chinese, and on protein data correlating sequence with function. This unsupervised algorithm is capable of learning complex syntax, generating grammatical novel sentences, and proving useful in other fields that call for structure discovery from raw data, such as bioinformatics."
336,43,6881,1,Inductive learning algorithms and representations for text categorization,"Text categorization â the assignment of natural language texts to one or more predefined categories based on their content â is an important component in many information organization and management tasks. We compare the effectiveness of five different automatic learning algorithms for text categorization in terms of learning speed, realtime classification speed, and classification accuracy. We also examine training set size, and alternative document representations. Very accurate text classifiers can be learned automatically from training examples. Linear Support Vector Machines (SVMs) are particularly promising because they are very accurate, quick to train, and quick to evaluate. 1.1 Keywords Text categorization, classification, support vector machines, machine learning, information management."
337,43,7044,1,Error bounds for convolutional codes and an asymptotically optimum decoding algorithm,"The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above<tex>R_{0}</tex>, the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above<tex>R_{0}</tex>and whose performance bears certain similarities to that of sequential decoding algorithms."
338,43,7541,1,Fast and effective text mining using linear-time document clustering,"Clustering is a powerful technique for large-scale topic discovery from text. It involves two phases: first, feature extraction maps each document or record to a point in a high dimensional space, then clustering algorithms automatically group the points into a hierarchy of clusters. We describe an unsupervised, near-linear time text clustering system that offers a number of algorithm choices for each phase. We introduce a methodology for measuring the quality of a cluster hierarchy in terms of F-Measure, and present the results of experiments comparing different algorithms. The evaluation considers some feature selection parameters (\\textif{tfidf} and feature vector lenght) but focuses on the clustering algorithms, namely techniques from Scatter/Gather (buckshot, fractionation, and split/join) and \\textit{k}-means. Our experiments suggest that continuos center adjustement contributes more to cluster quality than seed selection does. It follows that using a simpler seed selection algorithm gives a better time/quality tradeoff. We describe a refinement to center adjustement, ``vector average damping'', that further improves cluster quality. We also compare the near-linear time algorithms to a group average greedy agglomerative clustering algorithm to demonstrate the time/quality tradeoff quantitatively."
339,43,7732,1,Accurate Unlexicalized Parsing,"We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F 1 ) is better than that of early  lexicalized  PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize."
340,43,8570,1,Accurate methods for the statistics of surprise and coincidence,"Much work has been done on the statistical analysis of text. In some cases reported in the literature, inappropriate statistical methods have been used, and statistical significance of results have not been addressed. In particular, asymptotic normality assumptions have often been used unjustifiably, leading to flawed results. This assumption of normal distribution limits the ability to analyze rare events. Unfortunately rare events do make up a large fraction of real text. However, more applicable methods based on likelihood ratio tests are available which yield good results with relatively small samples. These tests can be implemented efficiently, and have been used for the detection of composite terms, and for the determination of domain-specific terms. In some cases, these measures perform much better than the methods previously used. In cases where traditional contingency table methods work well, the likelihood ratio tests described here are nearly identical. This paper describes the basis of a measure based on likelihood ratios which can be applied to the analysis of text."
341,43,9326,1,Automatic ontology-based knowledge extraction from Web documents,"To bring the Semantic Web to life and provide advanced knowledge services, we need efficient ways to access and extract knowledge from Web documents. Although Web page annotations could facilitate such knowledge gathering, annotations are rare and will probably never be rich or detailed enough to cover all the knowledge these documents contain. Manual annotation is impractical and unscalable, and automatic annotation tools remain largely undeveloped. Specialized knowledge services therefore require tools that can search and extract specific knowledge directly from unstructured text on the Web, guided by an ontology that details what type of knowledge to harvest. An ontology uses concepts and relations to classify domain knowledge. Other researchers have used ontologies to support knowledge extraction, but few have explored their full potential in this domain. The paper considers the Artequakt project which links a knowledge extraction tool with an ontology to achieve continuous knowledge support and guide information extraction. The extraction tool searches online documents and extracts knowledge that matches the given classification structure. It provides this knowledge in a machine-readable format that will be automatically maintained in a knowledge base (KB). Knowledge extraction is further enhanced using a lexicon-based term expansion mechanism that provides extended ontology terminology."
342,43,10355,1,Ontology-based knowledge representation for bioinformatics,"Much of biology works by applying prior knowledge ( what is known') to an unknown entity, rather than the application of a set of axioms that will elicit knowledge. In addition, the complex biological data stored in bioinformatics databases often require the addition of knowledge to specify and constrain the values held in that database. One way of capturing knowledge within bioinformatics applications and databases is the use of ontologies. An ontology is the concrete form of a conceptualisation of a community's knowledge of a domain  This paper aims to introduce the reader to the use of ontologies within bioinformatics. A description of the type of knowledge held in an ontology will be given. the paper will be illustrated throughout with examples taken from bioinformatics and molecular biology, and a survey of current biological ontologies will be presented. From this it will be seen that the use to which the ontology is put largely determines the content of the ontology. Finally, the paper will describe the process of building an ontology, introducing the reader to the techniques and methods currently in use and the open research questions in ontology development. 10.1093/bib/1.4.398"
343,43,10556,1,Open Information Extraction from the Web,"Traditionally, Information Extraction (IE) has fo- cused on satisfying precise, narrow, pre-speciï¬ed requests from small homogeneous corpora (e.g., extract the location and time of seminars from a set of announcements). Shifting to a new domain requires the user to name the target relations and to manually create new extraction rules or hand-tag new training examples. This manual labor scales linearly with the number of target relations. This paper introduces Open IE (OIE), a new ex- traction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input. The paper also introduces T EXT RUNNER, a fully implemented, highly scalable OIE system where the tuples are assigned a probability and indexed to support efï¬cient extraction and explo- ration via user queries. We report on experiments over a 9,000,000 Web page corpus that compare T EXT RUNNER with K NOW I TA LL, a state-of-the-art Web IE system. T EXT RUNNER achieves an error reduction of 33% on a comparable set of extractions. Furthermore, in the amount of time it takes K NOW I TA LL to per- form extraction for a handful of pre-speciï¬ed re- lations, T EXT RUNNER extracts a far broader set of facts reï¬ecting orders of magnitude more rela- tions, discovered on the ï¬y. We report statistics on T EXT RUNNERâs 11,000,000 highest probability tuples, and show that they contain over 1,000,000 concrete facts and over 6,500,000 more abstract as- sertions."
344,43,11364,1,Frustratingly Easy Domain Adaptation,"We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough âtargetâ data to do slightly better than just using only âsourceâ data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. The technique comes with several simple theoretical guarantees. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains."
345,43,13594,1,Contextual correlates of semantic similarity,"Investigated the relationship between semantic and contextual similarity for pairs of nouns that vary from high to low semantic similarity in 86 undergraduates in 3 experiments. Semantic similarity was estimated by subjective ratings; contextual similarity was estimated by the method of sorting sentential contexts. Results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation is obtained for 2 separate corpora of sentence contexts. It is concluded that for words in the same language drawn from the same syntactic and semantic categories, the more often 2 words can be substituted into the same contexts the more similar in meaning they are judged to be. ((c) 1997 APA/PsycINFO, all rights reserved)"
346,44,15,1,Collective dynamics of 'small-world' networks.,"Networks of coupled dynamical systems have been used to model biological oscillators1, 2, 3, 4, Josephson junction arrays5,6, excitable media7, neural networks8, 9, 10, spatial games11, genetic control networks12 and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks 'rewired' to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them 'small-world' networks, by analogy with the small-world phenomenon13,14 (popularly known as six degrees of separation15). The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices."
347,44,478,1,Emergence of scaling in random networks,"Recently retired as head of the Global Alliance for Vaccines and Immunization (GAVI) secretariat and as a health advisor to leading global entities, Tore Godal is now a Special Advisor to the Norwegian Prime Minister. He is nevertheless continuing to fight for better global health, cogently articulating the needs of the world's poor and disadvantaged. He is a leading leprosy expert, ex-director of the world's premier agency for research and training in tropical diseases, instigator and prime mover of some global innovative public-private health sector partnerships, adept fund mobilizer, and advocate of the `let's get it done' school of leadership. Few individuals are, therefore, more experienced or better suited for such a crucial and much-needed role"
348,44,1201,1,Statistical mechanics of complex networks,"The science of complex networks is a new interdisciplinary branch of science which has arisen recently on the interface of physics, biology, social and computer sciences, and others. Its main goal is to discover general laws governing the creation and growth as well as processes taking place on networks, like e.g. the Internet, transportation or neural networks. It turned out that most real-world networks cannot be simply reduced to a compound of some individual components. Fortunately, the statistical mechanics, being one of pillars of modern physics, provides us with a very powerful set of tools and methods for describing and understanding these systems. In this thesis, we would like to present a consistent approach to complex networks based on statistical mechanics, with the central role played by the concept of statistical ensemble of networks. We show how to construct such a theory and present some practical problems where it can be applied. Among them, we pay attention to the problem of finite-size corrections and the dynamics of a simple model of mass transport on networks."
349,44,3420,1,How dynamic is the web?,"Recent experiments and analysis suggest that there are about 800 million publicly-indexable Web pages. However, unlike books in a traditional library, Web pages continue to change even after they are initially published by their authors and indexed by search engines. This paper describes preliminary data on and statistical analysis of the frequency and nature of Web page modifications. Using empirical models and a novel analytic metric of âup-to-dateness', we estimate the rate at which Web search engines must re-index the Web to remain current."
350,44,3423,1,Stochastic Models for the Web Graph,"The Web may be viewed as a directed graph each of whose vertices is a static HTML Web page, and each of whose edges corresponds to a hyperlink from one Web page to another. We propose and analyze random graph models inspired by a series of empirical observations on the Web. Our graph models differ from the traditional G<sub>n,p</sub> models in two ways: 1. Independently chosen edges do not result in the statistics (degree distributions, clique multitudes) observed on the Web. Thus, edges in our model are statistically dependent on each other. 2. Our model introduces new vertices in the graph as time evolves. This captures the fact that the Web is changing with time. Our results are two fold: we show that graphs generated using our model exhibit the statistics observed on the Web graph, and additionally, that natural graph models proposed earlier do not exhibit them. This remains true even when these earlier models are generalized to account for the arrival of vertices over time. In particular, the sparse random graphs in our models exhibit properties that do not arise in far denser random graphs generated by Erdos-Renyi models"
351,44,5274,1,The {W}eb as a Graph,"The pages and hyperlinks of the World-Wide Web may be viewed as nodes and edges in a directed graph. This graph has about a billion nodes today, several billion links, and appears to grow exponentially with time. There are many reasons---mathematical, sociological, and commercial---for studying the evolution of this graph. We first review a set of algorithms that operate on the Web graph, addressing problems from Web search, automatic community discovery, and classification. We then recall a number of measurements and properties of the Web graph. Noting that traditional random graph models do not explain these observations, we propose a new family of random graph models."
352,44,11165,1,A Large-scale Evaluation and Analysis of Personalized Search Strategies,"Although personalized search has been proposed for many years and many personalization strategies have been investigated, it is still unclear whether personalization is consistently effective on different queries for different users, and under different search contexts. In this paper, we study this problem and get some preliminary conclusions. We present a large-scale evaluation framework for personalized search based on query logs, and then evaluate five personalized search strategies (including two click-based and three profile-based ones) using 12-day MSN query logs. By analyzing the results, we reveal that personalized search has significant improvement over common web search on some queries but it also has little effect on other queries (e.g., queries with small click entropy). It even harms search accuracy under some situations. Furthermore, we show that straightforward click-based personalization strategies perform consistently and considerably well, while profile-based ones are unstable in our experiments. We also reveal that both long-term and short-term contexts are very important in improving search performance for profile-based personalized search strategies."
353,44,11206,1,Optimizing web search using social annotations,"This paper explores the use of social annotations to improve web search. Nowadays, many services, e.g. del.icio.us, have been developed for web users to organize and share their favorite web pages on line by using social annotations. We observe that the social annotations can benefit web  search in two aspects: 1) the annotations are usually good summaries of corresponding web pages; 2) the count of annotations indicates the popularity of web pages. Two novel algorithms are proposed to incorporate the above information into page ranking: 1) SocialSimRank (SSR) calculates the similarity between social annotations and web queries; 2) SocialPageRank (SPR) captures the popularity of web pages. Preliminary experimental results show that SSR can find the latent semantic association between queries and annotations, while SPR successfully measures the quality (popularity) of a web page from the web usersâ perspective. We further evaluate the proposed methods empirically with 50 manually constructed queries and 3000 auto-generated queries on a  dataset crawled from del.icio.us. Experiments show that both SSR and SPR benefit web search significantly"
354,45,1649,1,Applied bioinformatics for the identification of regulatory elements.," The compilation of multiple metazoan genome sequences and the deluge of large-scale expression data have combined to motivate the maturation of bioinformatics methods for the analysis of sequences that regulate gene transcription. Historically, these bioinformatics methods have been plagued by poor predictive specificity, but new bioinformatics algorithms that accelerate the identification of regulatory regions are drawing disgruntled users back to their keyboards. However, these new approaches and software are not without problems. Here, we introduce the purpose and mechanisms of the leading algorithms, with a particular emphasis on metazoan sequence analysis. We identify key issues that users should take into consideration in interpreting the results and provide an online training example to help researchers who wish to test online tools before taking an independent foray into the bioinformatics of transcription regulation."
355,45,1885,1,Multimeric threading-based prediction of protein-protein interactions on a genomic scale: application to the Saccharomyces cerevisiae proteome.,"MULTIPROSPECTOR, a multimeric threading algorithm for the prediction of protein-protein interactions, is applied to the genome of Saccharomyces cerevisiae. Each possible pairwise interaction among more than 6000 encoded proteins is evaluated against a dimer database of 768 complex structures by using a confidence estimate of the fold assignment and the magnitude of the statistical interfacial potentials. In total, 7321 interactions between pairs of different proteins are predicted, based on 304 complex structures. Quality estimation based on the coincidence of subcellular localizations and biological functions of the predicted interactors shows that our approach ranks third when compared with all other large-scale methods. Unlike other in silico methods, MULTIPROSPECTOR is able to identify the residues that participate directly in the interaction. Three hundred seventy-four of our predictions can be found by at least one of the other studies, which is compatible with the overlap between two different other methods. From the analysis of the mRNA abundance data, our method does not bias towards proteins with high abundance. Finally, several relevant predictions involved in various functions are presented. In summary, we provide a novel approach to predict protein-protein interactions on a genomic scale that is a useful complement to experimental methods."
356,45,3443,1,Automated ortholog inference from phylogenetic trees and  calculation of orthology reliability,"Motivation: Orthologous proteins in different species are likely   to have similar biochemical function and biological role. When   annotating a newly sequenced genome by sequencehomology, the most precise and reliable functional information can thus be derived from orthologs in other species. A standard method of finding orthologs is to compare the sequence tree with the species tree. However, since the topology of phylogenetic tree is not always reliable one might get incorrect assignments.  Results: Here we present a novel method that resolves this   problem by analyzing a set of bootstrap trees instead of the optimal tree. The frequency of orthology assignments in the bootstrap trees can be interpreted as a support value for the possible orthology of the sequences. Our method is efficient enough to analyze data in the scale of whole genomes. It is implemented in Java and calculates orthology support levels for all pairwise combinations of homologous sequences of two species. The method was tested on simulated datasets and on real data of homologous proteins.  Availability: Downloadable free of charge from ftp://ftp.cgb.ki.se/pub/prog/orthostrapper/or   on request from the authors.  Contact: christian.storm@cgb.ki.se 10.1093/bioinformatics/18.1.92"
357,45,3759,1,Protein function prediction using local 3D templates.,"The prediction of a protein's function from its 3D structure is becoming more and more important as the worldwide structural genomics initiatives gather pace and continue to solve 3D structures, many of which are of proteins of unknown function. Here, we present a methodology for predicting function from structure that shows great promise. It is based on 3D templates that are defined as specific 3D conformations of small numbers of residues. We use four types of template, covering enzyme active sites, ligand-binding residues, DNA-binding residues and reverse templates. The latter are templates generated from the target structure itself and scanned against a representative subset of all known protein structures. Together, the templates provide a fairly thorough coverage of the known structures and ensure that if there is a match to a known structure it is unlikely to be missed. A new scoring scheme provides a highly sensitive means of discriminating between true positive and false positive template matches. In all, the methodology provides a powerful new tool for function prediction to complement those already in use. (c) 2005 Elsevier Ltd. All rights reserved."
358,46,132,1,Advances to Bayesian network inference for generating causal networks from observational biological data,"Motivation: Network inference algorithms are powerful computational tools for identifying putative causal interactions among variables from observational data. Bayesian network inference algorithms hold particular promise in that they can capture linear, non-linear, combinatorial, stochastic and other types of relationships among variables across multiple levels of biological organization. However, challenges remain when applying these algorithms to limited quantities of experimental data collected from biological systems. Here, we use a simulation approach to make advances in our dynamic Bayesian network (DBN) inference algorithm, especially in the context of limited quantities of biological data.  Results: We test a range of scoring metrics and search heuristics to find an effective algorithm configuration for evaluating our methodological advances. We also identify sampling intervals and levels of data discretization that allow the best recovery of the simulated networks. We develop a novel influence score for DBNs that attempts to estimate both the sign (activation or repression) and relative magnitude of interactions among variables. When faced with limited quantities of observational data, combining our influence score with moderate data interpolation reduces a significant portion of false positive interactions in the recovered networks. Together, our advances allow DBN inference algorithms to be more effective in recovering biological networks from experimentally collected data.  Availability: Source code and simulated data are available upon request.  Supplementary information: http://www.jarvislab.net/Bioinformatics/BNAdvances/ 10.1093/bioinformatics/bth448"
359,46,1632,1,Cytoscape: a software environment for integrated models of biomolecular interaction networks.,"Cytoscape is an open source software project for integrating biomolecular interaction networks with high-throughput expression data and other molecular states into a unified conceptual framework. Although applicable to any system of molecular components and interactions, Cytoscape is most powerful when used in conjunction with large databases of protein-protein, protein-DNA, and genetic interactions that are increasingly available for humans and model organisms. Cytoscape's software Core provides basic functionality to layout and query the network; to visually integrate the network with expression profiles, phenotypes, and other molecular states; and to link the network to databases of functional annotations. The Core is extensible through a straightforward plug-in architecture, allowing rapid development of additional computational analyses and features. Several case studies of Cytoscape plug-ins are surveyed, including a search for interaction pathways correlating with changes in gene expression, a study of protein complexes involved in cellular recovery to DNA damage, inference of a combined physical/functional interaction network for Halobacterium, and an interface to detailed stochastic/kinetic gene regulatory models."
360,46,4047,1,BiNGO: a Cytoscape plugin to assess overrepresentation of Gene Ontology categories in Biological Networks,"Summary: The Biological Networks Gene Ontology tool (BiNGO) is an open-source Java tool to determine which Gene Ontology (GO) terms are significantly overrepresented in a set of genes. BiNGO can be used either on a list of genes, pasted as text, or interactively on subgraphs of biological networks visualized in Cytoscape. BiNGO maps the predominant functional themes of the tested gene set on the GO hierarchy, and takes advantage of Cytoscape's versatile visualization environment to produce an intuitive and customizable visual representation of the results. Availability: http://www.psb.ugent.be/cbd/papers/BiNGO/ Contact: martin.kuiper@psb.ugent.be"
361,46,8303,1,ARACNE: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context.,"BACKGROUND: Elucidating gene regulatory networks is crucial for understanding normal cell physiology and complex pathologic phenotypes. Existing computational methods for the genome-wide ""reverse engineering"" of such networks have been successful only for lower eukaryotes with simple genomes. Here we present ARACNE, a novel algorithm, using microarray expression profiles, specifically designed to scale up to the complexity of regulatory networks in mammalian cells, yet general enough to address a wider range of network deconvolution problems. This method uses an information theoretic approach to eliminate the majority of indirect interactions inferred by co-expression methods. RESULTS: We prove that ARACNE reconstructs the network exactly (asymptotically) if the effect of loops in the network topology is negligible, and we show that the algorithm works well in practice, even in the presence of numerous loops and complex topologies. We assess ARACNE's ability to reconstruct transcriptional regulatory networks using both a realistic synthetic dataset and a microarray dataset from human B cells. On synthetic datasets ARACNE achieves very low error rates and outperforms established methods, such as Relevance Networks and Bayesian Networks. Application to the deconvolution of genetic networks in human B cells demonstrates ARACNE's ability to infer validated transcriptional targets of the cMYC proto-oncogene. We also study the effects of misestimation of mutual information on network reconstruction, and show that algorithms based on mutual information ranking are more resilient to estimation errors. CONCLUSION: ARACNE shows promise in identifying direct transcriptional interactions in mammalian cellular networks, a problem that has challenged existing reverse engineering algorithms. This approach should enhance our ability to use microarray data to elucidate functional mechanisms that underlie cellular processes and to identify molecular targets of pharmacological compounds in mammalian cellular networks."
362,46,10126,1,Structural Dynamics of Eukaryotic Chromosome Evolution,"Large-scale genome sequencing is providing a comprehensive view of the complex evolutionary forces that have shaped the structure of eukaryotic chromosomes. Comparative sequence analyses reveal patterns of apparently random rearrangement interspersed with regions of extraordinarily rapid, localized genome evolution. Numerous subtle rearrangements near centromeres, telomeres, duplications, and interspersed repeats suggest hotspots for eukaryotic chromosome evolution. This localized chromosomal instability may play a role in rapidly evolving lineage-specific gene families and in fostering large-scale changes in gene order. Computational algorithms that take into account these dynamic forces along with traditional models of chromosomal rearrangement show promise for reconstructing the natural history of eukaryotic chromosomes."
363,46,12155,1,The population genetics of structural variation,"Population genetics is central to our understanding of human variation, and by linking medical and evolutionary themes, it enables us to understand the origins and impacts of our genomic differences. Despite current limitations in our knowledge of the locations, sizes and mutational origins of structural variants, our characterization of their population genetics is developing apace, bringing new insights into recent human adaptation, genome biology and disease. We summarize recent dramatic advances, describe the diverse mutational origins of chromosomal rearrangements and argue that their complexity necessitates a re-evaluation of existing population genetic methods."
364,46,12517,1,Population Genomics: Whole-Genome Analysis of Polymorphism and Divergence in Drosophila simulans,"The population genetic perspective is that the processes shaping genomic variation can be revealed only through simultaneous investigation of sequence polymorphism and divergence within and between closely related species. Here we present a population genetic analysis of Drosophila simulans based on whole-genome shotgun sequencing of multiple inbred lines and comparison of the resulting data to genome assemblies of the closely related species, D. melanogaster and D. yakuba . We discovered previously unknown, large-scale fluctuations of polymorphism and divergence along chromosome arms, and significantly less polymorphism and faster divergence on the X chromosome. We generated a comprehensive list of functional elements in the D. simulans genome influenced by adaptive evolution. Finally, we characterized genomic patterns of base composition for coding and noncoding sequence. These results suggest several new hypotheses regarding the genetic and biological mechanisms controlling polymorphism and divergence across the Drosophila genome, and provide a rich resource for the investigation of adaptive evolution and functional variation in D. simulans ."
365,46,13354,1,Predicting biological networks from genomic data.,"Continuing improvements in DNA sequencing technologies are providing us with vast amounts of genomic data from an ever-widening range of organisms. The resulting challenge for bioinformatics is to interpret this deluge of data and place it back into its biological context. Biological networks provide a conceptual framework with which we can describe part of this context, namely the different interactions that occur between the molecular components of a cell. Here, we review the computational methods available to predict biological networks from genomic sequence data and discuss how they relate to high-throughput experimental methods."
366,46,13411,1,Velvet: algorithms for de novo short read assembly using de Bruijn graphs.,"10.1101/gr.074492.107 We have developed a new set of algorithms, collectively called â Velvet,â to manipulate de Bruijn graphs for genomic sequence assembly. A de Bruijn graph is a compact representation based on short words (-mers) that is ideal for high coverage, very short read (25â50 bp) data sets. Applying Velvet to very short reads and paired-ends information only, one can produce contigs of significant length, up to 50-kb N50 length in simulations of prokaryotic data and 3-kb N50 on simulated mammalian BACs. When applied to real Solexa data sets without read pairs, Velvet generated contigs of â¼8 kb in a prokaryote and 2 kb in a mammalian BAC, in close agreement with our simulated results without read-pair information. Velvet represents a new approach to assembly that can leverage very short reads in combination with read pairs to produce useful assemblies."
367,46,14063,1,Consensus generation and variant detection by Celera Assembler,"Motivation: We present an algorithm to identify allelic variation given a Whole Genome Shotgun (WGS) assembly of haploid sequences, and to produce a set of haploid consensus sequences rather than a single consensus sequence. Existing WGS assemblers take a column-by-column approach to consensus generation, and produce a single consensus sequence which can be inconsistent with the underlying haploid alleles, and inconsistent with any of the aligned sequence reads. Our new algorithm uses a dynamic windowing approach. It detects alleles by simultaneously processing the portions of aligned reads spanning a region of sequence variation, assigns reads to their respective alleles, phases adjacent variant alleles and generates a consensus sequence corresponding to each confirmed allele. This algorithm was used to produce the first diploid genome sequence of an individual human. It can also be applied to assemblies of multiple diploid individuals and hybrid assemblies of multiple haploid organisms.  Results: Being applied to the individual human genome assembly, the new algorithm detects exactly two confirmed alleles and reports two consensus sequences in 98.98% of the total number 2 033 311 detected regions of sequence variation. In 33 269 out of 460 373 detected regions of size >1 bp, it fixes the constructed errors of a mosaic haploid representation of a diploid locus as produced by the original Celera Assembler consensus algorithm. Using an optimized procedure calibrated against 1 506 344 known SNPs, it detects 438 814 new heterozygous SNPs with false positive rate 12%.  Availability: The open source code is available at: http://wgs-assembler.cvs.sourceforge.net/wgs-assembler/  Contact: gdenisov@jcvi.org 10.1093/bioinformatics/btn074"
368,46,14581,1,Aggressive assembly of pyrosequencing reads with mates,"Motivation: DNA sequence reads from Sanger and pyrosequencing platforms differ in cost, accuracy, typical coverage, average read length and the variety of available paired-end protocols. Both read types can complement one another in a  hybrid' approach to whole-genome shotgun sequencing projects, but assembly software must be modified to accommodate their different characteristics. This is true even of pyrosequencing mated and unmated read combinations. Without special modifications, assemblers tuned for homogeneous sequence data may perform poorly on hybrid data.  Results: Celera Assembler was modified for combinations of ABI 3730 and 454 FLX reads. The revised pipeline called CABOG (Celera Assembler with the Best Overlap Graph) is robust to homopolymer run length uncertainty, high read coverage and heterogeneous read lengths. In tests on four genomes, it generated the longest contigs among all assemblers tested. It exploited the mate constraints provided by paired-end reads from either platform to build larger contigs and scaffolds, which were validated by comparison to a finished reference sequence. A low rate of contig mis-assembly was detected in some CABOG assemblies, but this was reduced in the presence of sufficient mate pair data.  Availability: The software is freely available as open-source from http://wgs-assembler.sf.net under the GNU Public License.  Contact: jmiller@jcvi.org  Supplementary information: Supplementary data are available at Bioinformatics online. 10.1093/bioinformatics/btn548"
369,46,14636,1,Accurate whole human genome sequencing using reversible terminator chemistry.,"DNA sequence information underpins genetic research, enabling discoveries of important biological or medical benefit. Sequencing projects have traditionally used long (400â800 base pair) reads, but the existence of reference sequences for the human and many other genomes makes it possible to develop new, fast approaches to re-sequencing, whereby shorter reads are compared to a reference to identify intraspecies genetic variation. Here we report an approach that generates several billion bases of accurate nucleotide sequence per experiment at low cost. Single molecules of DNA are attached to a flat surface, amplified in situ and used as templates for synthetic sequencing with fluorescent reversible terminator deoxyribonucleotides. Images of the surface are analysed to generate high-quality sequence. We demonstrate application of this approach to human genome sequencing on flow-sorted X chromosomes and then scale the approach to determine the genome sequence of a male Yoruba from Ibadan, Nigeria. We build an accurate consensus sequence from >30times average depth of paired 35-base reads. We characterize four million single-nucleotide polymorphisms and four hundred thousand structural variants, many of which were previously unknown. Our approach is effective for accurate, rapid and economical whole-genome re-sequencing and many other biomedical applications."
370,46,15287,1,"Rare Variants of {IFIH1}, a Gene Implicated in Antiviral Responses, Protect Against Type 1 Diabetes","Genome-wide association studies (GWASs) are regularly used to map genomic regions contributing to common human diseases, but they often do not identify the precise causative genes and sequence variants. To identify causative type 1 diabetes (T1D) variants, we resequenced exons and splice sites of 10 candidate genes in pools of DNA from 480 patients and 480 controls and tested their disease association in over 30,000 participants. We discovered four rare variants that lowered T1D risk independently of each other (odds ratio = 0.51 to 0.74; P = 1.3 x 10(-3) to 2.1 x 10(-16)) in IFIH1 (interferon induced with helicase C domain 1), a gene located in a region previously associated with T1D by GWASs. These variants are predicted to alter the expression and structure of IFIH1 [MDA5 (melanoma differentiation-associated protein 5)], a cytoplasmic helicase that mediates induction of interferon response to viral RNA. This finding firmly establishes the role of IFIH1 in T1D and demonstrates that resequencing studies can pinpoint disease-causing genes in genomic regions initially identified by GWASs."
371,46,15472,1,High-throughput genotyping by whole-genome resequencing,"The next-generation sequencing technology coupled with the growing number of genome sequences opens the opportunity to redesign genotyping strategies for more effective genetic mapping and genome analysis. We have developed a high-throughput method for genotyping recombinant populations utilizing whole-genome resequencing data generated by the Illumina Genome Analyzer. A sliding window approach is designed to collectively examine genome-wide single nucleotide polymorphisms for genotype calling and recombination breakpoint determination. Using this method, we constructed a genetic map for 150 rice recombinant inbred lines with an expected genotype calling accuracy of 99.94{%} and a resolution of recombination breakpoints within an average of 40 kb. In comparison to the genetic map constructed with 287 PCR-based markers for the rice population, the sequencing-based method was â{Ã }Âº20â{Ã³}faster in data collection and 35â{Ã³}more precise in recombination breakpoint determination. Using the sequencing-based genetic map, we located a quantitative trait locus of large effect on plant height in a 100-kb region containing the rice â{Ã}{Ãº}green revolutionâ{Ã}{Ã¹}gene. Through computer simulation, we demonstrate that the method is robust for different types of mapping populations derived from organisms with variable quality of genome sequences and is feasible for organisms with large genome sizes and low polymorphisms. With continuous advances in sequencing technologies, this genome-based method may replace the conventional marker-based genotyping approach to provide a powerful tool for large-scale gene discovery and for addressing a wide range of biological questions."
372,46,15586,1,Genome assembly reborn: recent computational challenges.,"Research into genome assembly algorithms has experienced a resurgence due to new challenges created by the development of next generation sequencing technologies. Several genome assemblers have been published in recent years specifically targeted at the new sequence data; however, the ever-changing technological landscape leads to the need for continued research. In addition, the low cost of next generation sequencing data has led to an increased use of sequencing in new settings. For example, the new field of metagenomics relies on large-scale sequencing of entire microbial communities instead of isolate genomes, leading to new computational challenges. In this article, we outline the major algorithmic approaches for genome assembly and describe recent developments in this domain. 10.1093/bib/bbp026"
373,46,15610,1,SOAP2: an improved ultrafast tool for short read alignment,"Summary: SOAP2 is a significantly improved version of the short oligonucleotide alignment program that both reduces computer memory usage and increases alignment speed at an unprecedented rate. We used a Burrows Wheeler Transformation (BWT) compression index to substitute the seed strategy for indexing the reference sequence in the main memory. We tested it on the whole human genome and found that this new algorithm reduced memory usage from 14.7 to 5.4 GB and improved alignment speed by 20Ã¢ÂÂ30 times. SOAP2 is compatible with both single- and paired-end reads. Additionally, this tool now supports multiple text and compressed file formats. A consensus builder has also been developed for consensus assembly and SNP detection from alignment of short reads on a reference genome.Availability: http://soap.genomics.org.cnContact: soap@genomics.org.cn"
374,46,15755,1,Probabilistic resolution of multi-mapping reads in massively parallel sequencing data using MuMRescueLite,"Summary: Multi-mapping sequence tags are a significant impediment to short-read sequencing platforms. These tags are routinely omitted from further analysis, leading to experimental bias and reduced coverage. Here, we present MuMRescueLite, a low-resource requirement version of the MuMRescue software that has been used by several next generation sequencing projects to probabilistically reincorporate multi-mapping tags into mapped short read data.  Availability and implementation: MuMRescueLite is written in Python; executables and documentation are available from http://genome.gsc.riken.jp/osc/english/software/.  Contact: geoff.faulkner@roslin.ed.ac.uk 10.1093/bioinformatics/btp438"
375,46,15908,1,Parametric Complexity of Sequence Assembly: Theory and Applications to Next Generation Sequencing,"In recent years, a flurry of new DNA sequencing technologies have altered the landscape of genomics, providing a vast amount of sequence information at a fraction of the costs that were previously feasible. The task of assembling these sequences into a genome has, however, still remained an algorithmic challenge that is in practice answered by heuristic solutions. In order to design better assembly algorithms and exploit the characteristics of sequence data from new technologies, we need an improved understanding of the parametric complexity of the assembly problem. In this article, we provide a first theoretical study in this direction, exploring the connections between repeat complexity, read lengths, overlap lengths and coverage in determining the ""hard"" instances of the assembly problem. Our work suggests at least two ways in which existing assemblers can be extended in a rigorous fashion, in addition to delineating directions for future theoretical investigations."
376,46,16065,1,ALLPATHS 2: small genomes assembled accurately and with high continuity from short paired reads,"We demonstrate that genome sequences approaching finished quality can be generated from short paired reads. Using 36 base (fragment) and 26 base (jumping) reads from five microbial genomes of varied GC composition and sizes up to 40 Mb, ALLPATHS2 generated assemblies with long, accurate contigs and scaffolds. Velvet and EULER-SR were less accurate. For example, for Escherichia coli, the fraction of 10-kb stretches that were perfect was 99.8% (ALLPATHS2), 68.7% (Velvet), and 42.1 (EULER-SR)."
377,46,16363,1,Population genetic inference from genomic sequence variation,"10.1101/gr.079509.108 Population genetics has evolved from a theory-driven field with little empirical data into a data-driven discipline in which genome-scale data sets test the limits of available models and computational analysis methods. In humans and a few model organisms, analyses of whole-genome sequence polymorphism data are currently under way. And in light of the falling costs of next-generation sequencing technologies, such studies will soon become common in many other organisms as well. Here, we assess the challenges to analyzing whole-genome sequence polymorphism data, and we discuss the potential of these data to yield new insights concerning population history and the genomic prevalence of natural selection."
378,46,16435,1,Microindel detection in short-read sequence data,"Motivation: Several recent studies have demonstrated the effectiveness of resequencing and single nucleotide variant (SNV) detection by deep short-read sequencing platforms. While several reliable algorithms are available for automated SNV detection, the automated detection of microindels in deep short-read data presents a new bioinformatics challenge.Results: We systematically analyzed how the short-read mapping tools MAQ, Bowtie, Burrows-Wheeler alignment tool (BWA), Novoalign and RazerS perform on simulated datasets that contain indels and evaluated how indels affect error rates in SNV detection. We implemented a simple algorithm to compute the equivalent indel region eir, which can be used to process the alignments produced by the mapping tools in order to perform indel calling. Using simulated data that contains indels, we demonstrate that indel detection works well on short-read data: the detection rate for microindels (<4 bp) is >90%. Our study provides insights into systematic errors in SNV detection that is based on ungapped short sequence read alignments. Gapped alignments of short sequence reads can be used to reduce this error and to detect microindels in simulated short-read data. A comparison with microindels automatically identified on the ABI Sanger and Roche 454 platform indicates that microindel detection from short sequence reads identifies both overlapping and distinct indels.Contact: peter.krawitz@googlemail.com; peter.robinson@charite.deSupplementary information: Supplementary data are available at Bioinformatics online."
379,46,16607,1,"De novo Assembly of a 40 Mb Eukaryotic Genome from Short Sequence Reads: Sordaria macrospora, a Model Organism for Fungal Morphogenesis","Filamentous fungi are of great importance in ecology, agriculture, medicine, and biotechnology. Thus, it is not surprising that genomes for more than 100 filamentous fungi have been sequenced, most of them by Sanger sequencing. While next-generation sequencing techniques have revolutionized genome resequencing, e.g. for strain comparisons, genetic mapping, or transcriptome and ChIP analyses, de novo assembly of eukaryotic genomes still presents significant hurdles, because of their large size and stretches of repetitive sequences. Filamentous fungi contain few repetitive regions in their 30Ã¢â¬â90 Mb genomes and thus are suitable candidates to test de novo genome assembly from short sequence reads. Here, we present a high-quality draft sequence of the Sordaria macrospora genome that was obtained by a combination of Illumina/Solexa and Roche/454 sequencing. Paired-end Solexa sequencing of genomic DNA to 85-fold coverage and an additional 10-fold coverage by single-end 454 sequencing resulted in ~4 Gb of DNA sequence. Reads were assembled to a 40 Mb draft version (N50 of 117 kb) with the Velvet assembler. Comparative analysis with Neurospora genomes increased the N50 to 498 kb. The S. macrospora genome contains even fewer repeat regions than its closest sequenced relative, Neurospora crassa . Comparison with genomes of other fungi showed that S. macrospora , a model organism for morphogenesis and meiosis, harbors duplications of several genes involved in self/nonself-recognition. Furthermore, S. macrospora contains more polyketide biosynthesis genes than N. crassa . Phylogenetic analyses suggest that some of these genes may have been acquired by horizontal gene transfer from a distantly related ascomycete group. Our study shows that, for typical filamentous fungi, de novo assembly of genomes from short sequence reads alone is feasible, that a mixture of Solexa and 454 sequencing substantially improves the assembly, and that the resulting data can be used for comparative studies to address basic questions of fungal biology."
380,46,16611,1,Structural variation analysis with strobe reads,"Motivation: Structural variation including deletions, duplications and rearrangements of DNA sequence are an important contributor to genome variation in many organisms. In human, many structural variants are found in complex and highly repetitive regions of the genome making their identification difficult. A new sequencing technology called strobe sequencing generates strobe reads containing multiple subreads from a single contiguous fragment of DNA. Strobe reads thus generalize the concept of paired reads, or mate pairs, that have been routinely used for structural variant detection. Strobe sequencing holds promise for unraveling complex variants that have been difficult to characterize with current sequencing technologies.  Results: We introduce an algorithm for identification of structural variants using strobe sequencing data. We consider strobe reads from a test genome that have multiple possible alignments to a reference genome due to sequencing errors and/or repetitive sequences in the reference. We formulate the combinatorial optimization problem of finding the minimum number of structural variants in the test genome that are consistent with these alignments. We solve this problem using an integer linear program. Using simulated strobe sequencing data, we show that our algorithm has better sensitivity and specificity than paired read approaches for structural variation identification.  Contact: braphael@brown.edu 10.1093/bioinformatics/btq153"
381,46,16733,1,Gap5Ã¢ÂÂediting the billion fragment sequence assembly,"Motivation: Existing sequence assembly editors struggle with the volumes of data now readily available from the latest generation of DNA sequencing instruments.Results: We describe the Gap5 software along with the data structures and algorithms used that allow it to be scalable. We demonstrate this with an assembly of 1.1 billion sequence fragments and compare the performance with several other programs. We analyse the memory, CPU, I/O usage and file sizes used by Gap5.Availability and Implementation: Gap5 is part of the Staden Package and is available under an Open Source licence from http://staden.sourceforge.net. It is implemented in C and Tcl/Tk. Currently it works on Unix systems only.Contact: jkb@sanger.ac.ukSupplementary information: Supplementary data are available at Bioinformatics online."
382,46,16744,1,Efficient construction of an assembly string graph using the FM-index,"Motivation: Sequence assembly is a difficult problem whose importance has grown again recently as the cost of sequencing has dramatically dropped. Most new sequence assembly software has started by building a de Bruijn graph, avoiding the overlap-based methods used previously because of the computational cost and complexity of these with very large numbers of short reads. Here, we show how to use suffix array-based methods that have formed the basis of recent very fast sequence mapping algorithms to find overlaps and generate assembly string graphs asymptotically faster than previously described algorithms.Results: Standard overlap assembly methods have time complexity O(N2), where N is the sum of the lengths of the reads. We use the FerraginaÃ¢ÂÂManzini index (FM-index) derived from the BurrowsÃ¢ÂÂWheeler transform to find overlaps of length at least ÃÂ among a set of reads. As well as an approach that finds all overlaps then implements transitive reduction to produce a string graph, we show how to output directly only the irreducible overlaps, significantly shrinking memory requirements and reducing compute time to O(N), independent of depth. Overlap-based assembly methods naturally handle mixed length read sets, including capillary reads or long reads promised by the third generation sequencing technologies. The algorithms we present here pave the way for overlap-based assembly approaches to be developed that scale to whole vertebrate genome de novo assembly.Contact: js18@sanger.ac.uk"
383,47,486,1,Community structure in social and biological networks,"10.1073/pnas.122653799 A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well knownâa collaboration network and a food webâand find that it detects significant and informative community divisions in both cases."
384,47,2928,1,GOstat: find statistically overrepresented Gene Ontologies within a group of genes.,"SUMMARY: Modern experimental techniques, as for example DNA microarrays, as a result usually produce a long list of genes, which are potentially interesting in the analyzed process. In order to gain biological understanding from this type of data, it is necessary to analyze the functional annotations of all genes in this list. The Gene-Ontology (GO) database provides a useful tool to annotate and analyze the functions of a large number of genes. Here, we introduce a tool that utilizes this information to obtain an understanding of which annotations are typical for the analyzed list of genes. This program automatically obtains the GO annotations from a database and generates statistics of which annotations are overrepresented in the analyzed list of genes. This results in a list of GO terms sorted by their specificity. AVAILABILITY: Our program GOstat is accessible via the Internet at http://gostat.wehi.edu.au"
385,47,3126,1,Uncovering the overlapping community structure of complex networks in nature and society,"Many complex systems in nature and society can be described in terms of networks capturing the intricate web of connections among the units they are made of1, 2, 3, 4. A key question is how to interpret the global organization of such networks as the coexistence of their structural subunits (communities) associated with more highly interconnected parts. Identifying these a priori unknown building blocks (such as functionally related proteins5, 6, industrial sectors7 and groups of people8, 9) is crucial to the understanding of the structural and functional properties of networks. The existing deterministic methods used for large networks find separated communities, whereas most of the actual networks are made of highly overlapping cohesive groups of nodes. Here we introduce an approach to analysing the main statistical features of the interwoven sets of overlapping communities that makes a step towards uncovering the modular structure of complex systems. After defining a set of new characteristic quantities for the statistics of communities, we apply an efficient technique for exploring overlapping communities on a large scale. We find that overlaps are significant, and the distributions we introduce reveal universal features of networks. Our studies of collaboration, word-association and protein interaction graphs show that the web of communities has non-trivial correlations and specific scaling properties."
386,47,7597,1,Topological structure analysis of the protein-protein interaction network in budding yeast,"Interaction detection methods have led to the discovery of thousands of interactions between proteins, and discerning relevance within large-scale data sets is important to present-day biology. Here, a spectral method derived from graph theory was introduced to uncover hidden topological structures (i.e. quasi-cliques and quasi-bipartites) of complicated protein-protein interaction networks. Our analyses suggest that these hidden topological structures consist of biologically relevant functional groups. This result motivates a new method to predict the function of uncharacterized proteins based on the classification of known proteins within topological structures. Using this spectral analysis method, 48 quasi-cliques and six quasi-bipartites were isolated from a network involving 11 855 interactions among 2617 proteins in budding yeast, and 76 uncharacterized proteins were assigned functions. 10.1093/nar/gkg340"
387,47,10377,1,A faster circular binary segmentation algorithm for the analysis of array CGH data.,"MOTIVATION: Array CGH technologies enable the simultaneous measurement of DNA copy number for thousands of sites on a genome. We developed the circular binary segmentation (CBS) algorithm to divide the genome into regions of equal copy number (Olshen et al ,2004). The algorithm tests for change-points using a maximal t-statistic with a permutation reference distribution to obtain the corresponding p-value. The number of computations required for the maximal test statistic is O(N(2)), where N is the number of markers. This makes the full permutation approach computationally prohibitive for the newer arrays that contain tens of thousands markers and highlights the need for a faster algorithm. RESULTS: We present a hybrid approach to obtain the p-value of the test statistic in linear time. We also introduce a rule for stopping early when there is strong evidence for the presence of a change. We show through simulations that the hybrid approach provides a substantial gain in speed with only a negligible loss in accuracy and that the stopping rule further increases speed. We also present the analyses of array CGH data from breast cancer cell lines to show the impact of the new approaches on the analysis of real data. AVAILABILITY: An R (R Development Core Team, 2006) version of the CBS algorithm has been implemented in the ""DNAcopy"" package of the Bioconductor project (Gentleman et al., 2004). The proposed hybrid method for the p-value is available in version 1.2.1 or higher and the stopping rule for declaring a change early is available in version 1.5.1 or higher."
388,48,1214,1,Conditional random fields: Probabilistic models for segmenting and labeling sequence data,"We present Â£  Â¤  Â¥  Â¦ Â§Â¨Â§ Â¤  Â¥  Â© ï¿½ï¿½ï¿½ Â©  Â¥  Â¦  Â¤ ï¿½ï¿½ ï¿½ ï¿½ï¿½ Â¦  ï¿½ , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data. 1."
389,48,2195,1,Prediction of Complete Gene Structures in Human Genomic DNA,"We introduce a general probabilistic model of the gene structure of human genomic sequences which incorporates descriptions of the basic transcriptional, translational and splicing signals, as well as length distributions and compositional features of exons, introns and intergenic regions. {D}istinct sets of model parameters are derived to account for the many substantial differences in gene density and structure observed in distinct {C} + {G} compositional regions of the human genome. {I}n addition, new models of the donor and acceptor splice signals are described which capture potentially important dependencies between signal positions. {T}he model is applied to the problem of gene identification in a computer program, {GENSCAN}, which identifies complete exon/intron structures of genes in genomic {DNA}. {N}ovel features of the program include the capacity to predict multiple genes in a sequence, to deal with partial as well as complete genes, and to predict consistent sets of genes occurring on either or both {DNA} strands. {GENSCAN} is shown to have substantially higher accuracy than existing methods when tested on standardized sets of human and vertebrate genes, with 75 to 80% of exons identified exactly. {T}he program is also capable of indicating fairly accurately the reliability of each predicted exon. {C}onsistently high levels of accuracy are observed for sequences of differing {C} + {G} content and for distinct groups of vertebrates."
390,48,3442,1,Multiple sequence alignment using partial order graphs,"Motivation: Progressive Multiple Sequence Alignment (MSA) methods depend on reducing an MSA to a linear profile for each alignment step. However, this leads to loss of information needed for accurate alignment, and gap scoring artifacts. Results: We present a graph representation of an MSA that can itself be aligned directly by pairwise dynamic programming, eliminating the need to reduce the MSA to a profile. This enables our algorithm (Partial Order Alignment (POA)) to guarantee that the optimal alignment of each new sequence versus each sequence in the MSA will be considered. Moreover, this algorithm introduces a new edit operator, homologous recombination, important for multidomain sequences. The algorithm has improved speed (linear time complexity) over existing MSA algorithms, enabling construction of massive and complex alignments (e.g. an alignment of 5000 sequences in 4 h on a Pentium II). We demonstrate the utility of this algorithm on a family of multidomain SH2 proteins, and on EST assemblies containing alternative splicing and polymorphism. Availability: The partial order alignment program POA is available at http://www.bioinformatics.ucla.edu/poa. Contact: leec@mbi.ucla.edu"
391,48,5430,1,A statistical approach to machine translation,"In this paper, we present a statistical approach to machine translation. We describe  the application of our approach to translation from French to English and give  preliminary results.  1  A Statistical Approach to Machine Translation 2 1 Introduction  The field of machine translation is almost as old as the modern digital computer. In 1949 Warren Weaver suggested that the problem be attacked with statistical methods and ideas from information theory, an area which he, Claude Shannon and..."
392,48,7466,1,Learning to Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment,"We address the text-to-text generation problem of sentence-level paraphrasing --- a phenomenon distinct from and more difficult than word- or phrase-level paraphrasing. Our approach applies  multiple-sequence alignment  to sentences gathered from unannotated comparable corpora: it learns a set of paraphrasing patterns represented by  word lattice  pairs and automatically determines how to apply these patterns to rewrite new sentences. The results of our evaluation experiments show that the system derives accurate paraphrases, outperforming baseline systems."
393,48,8567,1,Citances: Citation Sentences for Semantic Analysis of Bioscience Text,"We propose the use of the text of the sentences surrounding citations as an important tool for semantic interpretation of bioscience text. We hypothesize several diï¬erent uses of citation sentences (which we call citances), including the creation of training and testing data for semantic analysis (especially for entity and relation recognition), synonym set creation, database curation, document summarization, and information retrieval generally. We illustrate some of these ideas, showing that citations to one document in particular align well with what a hand-built curator extracted. We also show preliminary results on the problem of normalizing the diï¬erent ways that the same concepts are expressed within a set of citances, using and improving on existing techniques in automatic paraphrase generation."
394,48,9241,1,A codon-based model of nucleotide substitution for protein-coding DNA sequences.,"A codon-based model for the evolution of protein-coding DNA sequences is presented for use in phylogenetic estimation. A Markov process is used to describe substitutions between codons. Transition/transversion rate bias and codon usage bias are allowed in the model, and selective restraints at the protein level are accommodated using physicochemical distances between the amino acids coded for by the codons. Analyses of two data sets suggest that the new codon-based model can provide a better fit to data than can nucleotide-based models and can produce more reliable estimates of certain biologically important measures such as the transition/transversion rate ratio and the synonymous/nonsynonymous substitution rate ratio."
395,49,16,1,Network motifs: simple building blocks of complex networks.,"Complex networks are studied across many fields of science. {T}o uncover their structural design principles, we defined ""network motifs,"" patterns of interconnections occurring in complex networks at numbers that are significantly higher than those in randomized networks. {W}e found such motifs in networks from biochemistry, neurobiology, ecology, and engineering. {T}he motifs shared by ecological food webs were distinct from the motifs shared by the genetic networks of {E}scherichia coli and {S}accharomyces cerevisiae or from those found in the {W}orld {W}ide {W}eb. {S}imilar motifs were found in networks that perform information processing, even though they describe elements as different as biomolecules within a cell and synaptic connections between neurons in {C}aenorhabditis elegans. {M}otifs may thus define universal classes of networks. {T}his approach may uncover the basic building blocks of most networks."
396,49,51,1,Genomic analysis of regulatory network dynamics reveals large topological changes,"Network analysis has been applied widely, providing a unifying language to describe disparate systems ranging from social interactions to power grids. It has recently been used in molecular biology, but so far the resulting networks have only been analysed statically1, 2, 3, 4, 5, 6, 7, 8. Here we present the dynamics of a biological network on a genomic scale, by integrating transcriptional regulatory information9, 10, 11 and gene-expression data12, 13, 14, 15, 16 for multiple conditions in Saccharomyces cerevisiae. We develop an approach for the statistical analysis of network dynamics, called SANDY, combining well-known global topological measures, local motifs and newly derived statistics. We uncover large changes in underlying network architecture that are unexpected given current viewpoints and random simulations. In response to diverse stimuli, transcription factors alter their interactions to varying degrees, thereby rewiring the network. A few transcription factors serve as permanent hubs, but most act transiently only during certain conditions. By studying sub-network structures, we show that environmental responses facilitate fast signal propagation (for example, with short regulatory cascades), whereas the cell cycle and sporulation direct temporal progression through multiple stages (for example, with highly inter-connected transcription factors). Indeed, to drive the latter processes forward, phase-specific transcription factors inter-regulate serially, and ubiquitously active transcription factors layer above them in a two-tiered hierarchy. We anticipate that many of the concepts presented hereâparticularly the large-scale topological changes and hub transienceâwill apply to other biological networks, including complex sub-systems in higher eukaryotes."
397,49,206,1,Transcriptional Regulatory Networks in Saccharomyces cerevisiae,"We have determined how most of the transcriptional regulators encoded in the eukaryote {S}accharomyces cerevisiae associate with genes across the genome in living cells. {J}ust as maps of metabolic networks describe the potential pathways that may be used by a cell to accomplish metabolic processes, this network of regulator-gene interactions describes potential pathways yeast cells can use to regulate global gene expression programs. {W}e use this information to identify network motifs, the simplest units of network architecture, and demonstrate that an automated process can use motifs to assemble a transcriptional regulatory network structure. {O}ur results reveal that eukaryotic cellular functions are highly connected through networks of transcriptional regulators that regulate other transcriptional regulators."
398,49,209,1,Gene regulatory network growth by duplication,"We are beginning to elucidate transcriptional regulatory networks on a large scale and to understand some of the structural principles of these networks, but the evolutionary mechanisms that form these networks are still mostly unknown. {H}ere we investigate the role of gene duplication in network evolution. {G}ene duplication is the driving force for creating new genes in genomes: at least 50\% of prokaryotic genes and over 90\% of eukaryotic genes are products of gene duplication. {T}he transcriptional interactions in regulatory networks consist of multiple components, and duplication processes that generate new interactions would need to be more complex. {W}e define possible duplication scenarios and show that they formed the regulatory networks of the prokaryote {E}scherichia coli and the eukaryote {S}accharomyces cerevisiae. {G}ene duplication has had a key role in network evolution: more than one-third of known regulatory interactions were inherited from the ancestral transcription factor or target gene after duplication, and roughly one-half of the interactions were gained during divergence after duplication. {I}n addition, we conclude that evolution has been incremental, rather than making entire regulatory circuits or motifs by duplication with inheritance of interactions."
399,49,762,1,Mfold web server for nucleic acid folding and hybridization prediction,"10.1093/nar/gkg595 The abbreviated name, Ã¢ÂÂmfold web serverÃ¢ÂÂ, describes a number of closely related software applications available on the World Wide Web (WWW) for the prediction of the secondary structure of single stranded nucleic acids. The objective of this web server is to provide easy access to RNA and DNA folding and hybridization software to the scientific community at large. By making use of universally available web GUIs (Graphical User Interfaces), the server circumvents the problem of portability of this software. Detailed output, in the form of structure plots with or without reliability information, single strand frequency plots and Ã¢ÂÂenergy dot plotsÃ¢ÂÂ, are available for the folding of single sequences. A variety of Ã¢ÂÂbulkÃ¢ÂÂ servers give less information, but in a shorter time and for up to hundreds of sequences at once. The portal for the mfold web server is http://www.bioinfo.rpi.edu/applications/mfold. This URL will be referred to as Ã¢ÂÂMFOLDROOTÃ¢ÂÂ."
400,49,1080,1,Aligning Multiple Genomic Sequences With the Threaded Blockset Aligner,"10.1101/gr.1933104 We define a âthreaded blockset,â which is a novel generalization of the classic notion of a multiple alignment. A new computer program called TBA (for âthreaded blockset alignerâ) builds a threaded blockset under the assumption that all matching segments occur in the same order and orientation in the given sequences; inversions and duplications are not addressed. TBA is designed to be appropriate for aligning many, but by no means all, megabase-sized regions of multiple mammalian genomes. The output of TBA can be projected onto any genome chosen as a reference, thus guaranteeing that different projections present consistent predictions of which genomic positions are orthologous. This capability is illustrated using a new visualization tool to view TBA-generated alignments of vertebrate Hox clusters from both the mammalian and fish perspectives. Experimental evaluation of alignment quality, using a program that simulates evolutionary change in genomic sequences, indicates that TBA is more accurate than earlier programs. To perform the dynamic-programming alignment step, TBA runs a stand-alone program called MULTIZ, which can be used to align highly rearranged or incompletely sequenced genomes. We describe our use of MULTIZ to produce the whole-genome multiple alignments at the Santa Cruz Genome Browser."
401,49,1082,1,Ultraconserved elements in the human genome.,"{There are 481 segments longer than 200 base pairs (bp) that are absolutely conserved (100\% identity with no insertions or deletions) between orthologous regions of the human, rat, and mouse genomes. Nearly all of these segments are also conserved in the chicken and dog genomes, with an average of 95 and 99\% identity, respectively. Many are also significantly conserved in fish. These ultraconserved elements of the human genome are most often located either overlapping exons in genes involved in RNA processing or in introns or nearby genes involved in the regulation of transcription and development. Along with more than 5000 sequences of over 100 bp that are absolutely conserved among the three sequenced mammals, these represent a class of genetic elements whose functions and evolutionary origins are yet to be determined, but which are more highly conserved between these species than are proteins and appear to be essential for the ontogeny of mammals and other vertebrates.}"
402,49,1291,1,A memory-efficient dynamic programming algorithm for optimal alignment of a sequence to an RNA secondary structure.,"Abstract Background Covariance models (CMs) are probabilistic models of RNA secondary structure, analogous to profile hidden Markov models of linear sequence. The dynamic programming algorithm for aligning a CM to an RNA sequence of length N is O(N3) in memory. This is only practical for small RNAs. Results I describe a divide and conquer variant of the alignment algorithm that is analogous to memory-efficient Myers/Miller dynamic programming algorithms for linear sequence alignment. The new algorithm has an O(N2 log N) memory complexity, at the expense of a small constant factor in time. Conclusions Optimal ribosomal RNA structural alignments that previously required up to 150 GB of memory now require less than 270 MB."
403,49,1293,1,RNA sequence analysis using covariance models.,We describe a general approach to several RNA sequence analysis problems using probabilistic models that flexibly describe the secondary structure and primary sequence consensus of an RNA sequence family. We call these models covariance models'. A covariance model of tRNA sequences is an extremely sensitive and discriminative tool for searching for additional tRNAs and tRNA-related sequences in sequence databases. A model can be built automatically from an existing sequence alignment. We also describe an algorithm for learning a model and hence a consensus secondary structure from initially unaligned example sequences and no prior structural information. Models trained on unaligned tRNA examples correctly predict tRNA scondary structure and produce high-quality multiple alignments. The approach may be applied to any family of small RNA sequences. 10.1093/nar/22.11.2079
404,49,1309,1,Using an {RNA} secondary structure partition function to determine confidence in base pairs predicted by free energy minimization,"A partition function calculation for RNA secondary structure is presented that uses a current set of nearest neighbor parameters for conformational free energy at 37 degrees C, including coaxial stacking. For a diverse database of RNA sequences, base pairs in the predicted minimum free energy structure that are predicted by the partition function to have high base pairing probability have a significantly higher positive predictive value for known base pairs. For example, the average positive predictive value, 65.8%, is increased to 91.0% when only base pairs with probability of 0.99 or above are considered. The quality of base pair predictions can also be increased by the addition of experimentally determined constraints, including enzymatic cleavage, flavin mono-nucleotide cleavage, and chemical modification. Predicted secondary structures can be color annotated to demonstrate pairs with high probability that are therefore well determined as compared to base pairs with lower probability of pairing."
405,49,1312,1,Noncoding RNA gene detection using comparative sequence analysis.,"BACKGROUND: Noncoding RNA genes produce transcripts that exert their function without ever producing proteins. Noncoding RNA gene sequences do not have strong statistical signals, unlike protein coding genes. A reliable general purpose computational genefinder for noncoding RNA genes has been elusive. RESULTS: We describe a comparative sequence analysis algorithm for detecting novel structural RNA genes. The key idea is to test the pattern of substitutions observed in a pairwise alignment of two homologous sequences. A conserved coding region tends to show a pattern of synonymous substitutions, whereas a conserved structural RNA tends to show a pattern of compensatory mutations consistent with some base-paired secondary structure. We formalize this intuition using three probabilistic ""pair-grammars"": a pair stochastic context free grammar modeling alignments constrained by structural RNA evolution, a pair hidden Markov model modeling alignments constrained by coding sequence evolution, and a pair hidden Markov model modeling a null hypothesis of position-independent evolution. Given an input pairwise sequence alignment (e.g. from a BLASTN comparison of two related genomes) we classify the alignment into the coding, RNA, or null class according to the posterior probability of each class. CONCLUSIONS: We have implemented this approach as a program, QRNA, which we consider to be a prototype structural noncoding RNA genefinder. Tests suggest that this approach detects noncoding RNA genes with a fair degree of reliability."
406,49,1337,1,The language of RNA: a formal grammar that includes pseudoknots,"Motivation: In a previous paper, we presented a polynomial time dynamic programming algorithm for predicting optimal RNA secondary structure including pseudoknots. However, a formal grammatical representation for RNA secondary structure with pseudoknots was still lacking.Results: Here we show a one-to-one correspondence between that algorithm and a formal transformational grammar. This grammar class encompasses the context-free grammars and goes beyond to generate pseudoknotted structures. The pseudoknot grammar avoids the use of general context-sensitive rules by introducing a small number of auxiliary symbols used to reorder the strings generated by an otherwise context-free grammar. This formal representation of the residue correlations in RNA structure is important because it means we can build full probabilistic models of RNA secondary structure, including pseudoknots, and use them to optimally parse sequences in polynomial time.Contact: eddy@genetics.wustl.edu"
407,49,1558,1,Functional organization of the yeast proteome by systematic analysis of protein complexes.,"Most cellular processes are carried out by multiprotein complexes. The identification and analysis of their components provides insight into how the ensemble of expressed proteins (proteome) is organized into functional units. We used tandem-affinity purification (TAP) and mass spectrometry in a large-scale approach to characterize multiprotein complexes in Saccharomyces cerevisiae. We processed 1,739 genes, including 1,143 human orthologues of relevance to human biology, and purified 589 protein assemblies. Bioinformatic analysis of these assemblies defined 232 distinct multiprotein complexes and proposed new cellular roles for 344 proteins, including 231 proteins with no previous functional annotation. Comparison of yeast and human complexes showed that conservation across species extends from single proteins to their molecular environment. Our analysis provides an outline of the eukaryotic proteome as a network of protein complexes at a level of organization beyond binary interactions. This higher-order map contains fundamental biological information and offers the context for a more reasoned and informed approach to drug discovery. [References: 46]"
408,49,1587,1,{A Bayesian networks approach for predicting protein-protein interactions from genomic data},"We have developed an approach using Bayesian networks to predict protein-protein interactions genome-wide in yeast. Our method naturally weights and combines into reliable predictions genomic features only weakly associated with interaction (e.g., messenger RNAcoexpression, coessentiality, and colocalization). In addition to de novo predictions, it can integrate often noisy, experimental interaction data sets. We observe that at given levels of sensitivity, our predictions are more accurate than the existing high-throughput experimental data sets. We validate our predictions with TAP (tandem affinity purification) tagging experiments. Our analysis, which gives a comprehensive view of yeast interactions, is available at genecensus.org/intint. 10.1126/science.1087361"
409,49,1617,1,{Hierarchical organization of modularity in metabolic networks},"Spatially or chemically isolated functional modules composed of several cellular components and carrying discrete functions are considered fundamental building blocks of cellular organization, but their presence in highly integrated biochemical networks lacks quantitative support. Here, we show that the metabolic networks of 43 distinct organisms are organized into many small, highly connected topologic modules that combine in a hierarchical manner into larger, less cohesive units, with their number and degree of clustering following a power law. Within Escherichia coli, the uncovered hierarchical modularity closely overlaps with known metabolic functions. The identified network architecture may be generic to system-level cellular organization."
410,49,1633,1,{Network motifs in the transcriptional regulation network of Escherichia coli},"Little is known about the design principles1, 2, 3, 4, 5, 6, 7, 8, 9, 10 of transcriptional regulation networks that control gene expression in cells. Recent advances in data collection and analysis2, 11, 12, however, are generating unprecedented amounts of information about gene regulation networks. To understand these complex wiring diagrams1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 13, we sought to break down such networks into basic building blocks2. We generalize the notion of motifs, widely used for sequence analysis, to the level of networks. We define 'network motifs' as patterns of interconnections that recur in many different parts of a network at frequencies much higher than those found in randomized networks. We applied new algorithms for systematically detecting network motifs to one of the best-characterized regulation networks, that of direct transcriptional interactions in Escherichia coli3, 6. We find that much of the network is composed of repeated appearances of three highly significant motifs. Each network motif has a specific function in determining gene expression, such as generating temporal expression programs and governing the responses to fluctuating external signals. The motif structure also allows an easily interpretable view of the entire known transcriptional network of the organism. This approach may help define the basic computational elements of other biological networks."
411,49,1686,1,Hidden Markov Models for Detecting Remote Protein Homologies,"MOTIVATION: A new hidden Markov model method (SAM-T98) for finding remote homologs of protein sequences is described and evaluated. The method begins with a single target sequence and iteratively builds a hidden Markov model (HMM) from the sequence and homologs found using the HMM for database search. SAM-T98 is also used to construct model libraries automatically from sequences in structural databases. METHODS: We evaluate the SAM-T98 method with four datasets. Three of the test sets are fold-recognition tests, where the correct answers are determined by structural similarity. The fourth uses a curated database. The method is compared against WU-BLASTP and against DOUBLE-BLAST, a two-step method similar to ISS, but using BLAST instead of FASTA. RESULTS: SAM-T98 had the fewest errors in all tests-dramatically so for the fold-recognition tests. At the minimum-error point on the SCOP (Structural Classification of Proteins)-domains test, SAM-T98 got 880 true positives and 68 false positives, DOUBLE-BLAST got 533 true positives with 71 false positives, and WU-BLASTP got 353 true positives with 24 false positives. The method is optimized to recognize superfamilies, and would require parameter adjustment to be used to find family or fold relationships. One key to the performance of the HMM method is a new score-normalization technique that compares the score to the score with a reversed model rather than to a uniform null model. AVAILABILITY: A World Wide Web server, as well as information on obtaining the Sequence Alignment and Modeling (SAM) software suite, can be found at http://www.cse.ucsc.edu/research/compbi o/ CONTACT: karplus{\char64}cse.ucsc.edu; http://www.cse.ucsc.edu/karplus"
412,49,1804,1,Probabilistic Inference using {M}arkov Chain {M}onte {C}arlo Methods,"Probabilistic inference is an attractive approach to uncertain reasoning and empirical learning in artificial intelligence. Computational difculties arise, however, because probabilistic models with the necessary realism and flexibility lead to complex distributions over high-dimensional spaces. Related problems in other fields have been tackled using Monte Carlo methods based on sampling using Markov chains, providing a rich array of techniques that can be applied to problems in artificial intelligence. The &#034;Metropolis algorithm&#034; has been used to solve difficult problems in statistical physics for over forty years, and, in the last few years, the related method of &#034;Gibbs sampling&#034; has been applied to problems of statistical inference. Concurrently, an alternative method for solving problems in statistical physics by means of dynamical simulation has been developed as well, and has recently been unified with the Metropolis algorithm to produce the &#034;hybrid Monte Carlo&#034; method. In computer science, Markov chain sampling is the basis of the heuristic optimization technique of &#034;simulated annealing&#034;, and has recently been used in randomized algorithms for approximate counting of large sets. In this review, I outline the role of probabilistic inference in artificial intelligence, and present the theory of Markov chains, and describe various Markov chain Monte Carlo algorithms, along with a number of supporting techniques. I try to present a comprehensive picture of the range of methods that have been developed, including techniques from the varied literature that have not yet seen wide application in artificial intelligence, but which appear relevant. As illustrative examples, I use the problems of probabilitic inference in expert systems, discovery of latent classes from data, and Bayesian learning for neural networks."
413,49,2224,1,Bayesian density estimation and inference using mixtures,"We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation, and are exemplified by special cases where data are modelled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general class of normal mixture models. Keywords: Kernel estimation; Mixtures of Dirichlet processes; Multimodality; Normal mixtures; Posterior sampling; Smoothing parameter estimation * Michael D. Escobar is Assistant Professor, Department of Statistics and Department of Preventive Medicine and Biostatistics, University ..."
414,49,2226,1,Learning Probabilistic Relational Models,"A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with &amp;quot;flat &amp;quot; data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning â the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases. 1"
415,49,2629,1,Systematic interpretation of genetic interactions using protein networks,"Genetic interaction analysis,in which two mutations have a combined effect not exhibited by either mutation alone, is a powerful and widespread tool for establishing functional linkages between genes. In the yeast Saccharomyces cerevisiae, ongoing screens have generated >4,800 such genetic interaction data. We demonstrate that by combining these data with information on protein-protein, prote in-DNA or metabolic networks, it is possible to uncover physical mechanisms behind many of the observed genetic effects. Using a probabilistic model, we found that 1,922 genetic interactions are significantly associated with either between- or within-pathway explanations encoded in the physical networks, covering approximately 40% of known genetic interactions. These models predict new functions for 343 proteins and suggest that between-pathway explanations are better than within-pathway explanations at interpreting genetic interactions identified in systematic screens. This study provides a road map for how genetic and physical interactions can be integrated to reveal pathway organization and function."
416,49,3366,1,From signatures to models: understanding cancer using microarrays.,"Genomics has the potential to revolutionize the diagnosis and management of cancer by offering an unprecedented comprehensive view of the molecular underpinnings of pathology. Computational analysis is essential to transform the masses of generated data into a mechanistic understanding of disease. Here we review current research aimed at uncovering the modular organization and function of transcriptional networks and responses in cancer. We first describe how methods that analyze biological processes in terms of higher-level modules can identify robust signatures of disease mechanisms. We then discuss methods that aim to identify the regulatory mechanisms underlying these modules and processes. Finally, we show how comparative analysis, combining human data with model organisms, can lead to more robust findings. We conclude by discussing the challenges of generalizing these methods from cells to tissues and the opportunities they offer to improve cancer diagnosis and management."
417,49,3381,1,Microarray data normalization and transformation,"Underlying every microarray experiment is an experimental question that one would like to address. Finding a useful and satisfactory answer relies on careful experimental design and the use of a variety of data-mining tools to explore the relationships between genes or reveal patterns of expression. While other sections of this issue deal with these lofty issues, this review focuses on the much more mundane but indispensable tasks of 'normalizing' data from individual hybridizations to make meaningful comparisons of expression levels, and of 'transforming' them to select genes for further analysis and data mining."
418,49,3900,1,Hitchhiking under positive Darwinian selection,"Positive selection can be inferred from its effect on linked neutral variation. In the restrictive case when there is no recombination, all linked variation is removed. If recombination is present but rare, both deterministic and stochastic models of positive selection show that linked variation hitchhikes to either low or high frequencies. While the frequency distribution of variation can be influenced by a number of evolutionary processes, an excess of derived variants at high frequency is a unique pattern produced by hitchhiking (derived refers to the nonancestral state as determined from an outgroup). We adopt a statistic, H, to measure an excess of high compared to intermediate frequency variants. Only a few high-frequency variants are needed to detect hitchhiking since not many are expected under neutrality. This is of particular utility in regions of low recombination where there is not much variation and in regions of normal or high recombination, where the hitchhiking effect can be limited to a small (<1 kb) region. Application of the H test to published surveys of Drosophila variation reveals an excess of high frequency variants that are likely to have been influenced by positive selection."
419,49,4125,1,Evolution of Genetic Potential,"Organisms employ a multitude of strategies to cope with the dynamical environments in which they live. Homeostasis and physiological plasticity buffer changes within the lifetime of an organism, while stochastic developmental programs and hypermutability track changes on longer timescales. An alternative long-term mechanism is &#8220;genetic potential&#8221;&#8212;a heightened sensitivity to the effects of mutation that facilitates rapid evolution to novel states. Using a transparent mathematical model, we illustrate the concept of genetic potential and show that as environmental variability decreases, the evolving population reaches three distinct steady state conditions: (1) organismal flexibility, (2) genetic potential, and (3) genetic robustness. As a specific example of this concept we examine fluctuating selection for hydrophobicity in a single amino acid. We see the same three stages, suggesting that environmental fluctuations can produce allele distributions that are distinct not only from those found under constant conditions, but also from the transient allele distributions that arise under isolated selective sweeps."
420,49,4548,1,Spontaneous evolution of modularity and network motifs,"Biological networks have an inherent simplicity: they are modular with a design that can be separated into units that perform almost independently. {F}urthermore, they show reuse of recurring patterns termed network motifs. {L}ittle is known about the evolutionary origin of these properties. {C}urrent models of biological evolution typically produce networks that are highly nonmodular and lack understandable motifs. {H}ere, we suggest a possible explanation for the origin of modularity and network motifs in biology. {W}e use standard evolutionary algorithms to evolve networks. {A} key feature in this study is evolution under an environment (evolutionary goal) that changes in a modular fashion. {T}hat is, we repeatedly switch between several goals, each made of a different combination of subgoals. {W}e find that such ""modularly varying goals"" lead to the spontaneous evolution of modular network structure and network motifs. {T}he resulting networks rapidly evolve to satisfy each of the different goals. {S}uch switching between related goals may represent biological evolution in a changing environment that requires different combinations of a set of basic biological functions. {T}he present study may shed light on the evolutionary forces that promote structural simplicity in biological networks and offers ways to improve the evolutionary design of engineered systems."
421,49,4915,1,Conservation and evolvability in regulatory networks: The evolution of ribosomal regulation in yeast,"Transcriptional modules of coregulated genes play a key role in regulatory networks. Comparative studies show that modules of coexpressed genes are conserved across taxa. However, little is known about the mechanisms underlying the evolution of module regulation. Here, we explore the evolution of cis-regulatory programs associated with conserved modules by integrating expression profiles for two yeast species and sequence data for a total of 17 fungal genomes. We show that although the cis-elements accompanying certain conserved modules are strictly conserved, those of other conserved modules are remarkably diverged. In particular, we infer the evolutionary history of the regulatory program governing ribosomal modules. We show how a cis-element emerged concurrently in dozens of promoters of ribosomal protein genes, followed by the loss of a more ancient cis-element. We suggest that this formation of an intermediate redundant regulatory program allows conserved transcriptional modules to gradually switch from one regulatory mechanism to another while maintaining their functionality. Our work provides a general framework for the study of the dynamics of promoter evolution at the level of transcriptional modules and may help in understanding the evolvability and increased redundancy of transcriptional regulation in higher organisms."
422,49,5210,1,Predicting the in vivo signature of human gene regulatory sequences,"Motivation: In the living cell nucleus, genomic DNA is packaged into chromatin. DNA sequences that regulate transcription and other chromosomal processes are associated with local disruptions, or openings', in chromatin structure caused by the cooperative action of regulatory proteins. Such perturbations are extremely specific for cis-regulatory elements and occur over short stretches of DNA (typically [~]250 bp). They can be detected experimentally as DNaseI hypersensitive sites (HSs) in vivo, though the process is extremely laborious and costly. The ability to discriminate DNaseI HSs computationally would have a major impact on the annotation and utilization of the human genome. Results: We found that a supervised pattern recognition algorithm, trained using a set of 280 DNaseI HS and 737 non-HS control sequences from erythroid cells, was capable of de novo prediction of HSs across the human genome with surprisingly high accuracy determined by prospective in vivo validation. Systematic application of this computational approach will greatly facilitate the discovery and analysis of functional non-coding elements in the human and other complex genomes. Availability: Supplementary data is available at noble.gs.washington.edu/proj/hs Contact: noble@gs.washington.edu; jstam@regulome.com 10.1093/bioinformatics/bti1047"
423,49,5361,1,MONKEY: identifying conserved transcription-factor binding sites in multiple alignments using a binding site-specific evolutionary model,"We introduce a method (MONKEY) to identify conserved transcription-factor binding sites in multispecies alignments. MONKEY employs probabilistic models of factor specificity and binding-site evolution, on which basis we compute the likelihood that putative sites are conserved and assign statistical significance to each hit. Using genomes from the genus Saccharomyces, we illustrate how the significance of real sites increases with evolutionary distance and explore the relationship between conservation and function."
424,49,5468,1,Profile hidden Markov models,The recent literature on profile hidden Markov model (profile HMM) methods and software is reviewed. Profile HMMs turn a multiple sequence alignment into a position-specific scoring system suitable for searching databases for remotely homologous sequences. Profile HMM analyses complement standard pairwise comparison methods for large- scale sequence analysis. Several software implementations and two large libraries of profile HMMs of common protein domains are available. HMM methods performed comparably to threading methods in the CASP2 structure prediction exercise.
425,49,5475,1,{RSEARCH: finding homologs of single structured RNA sequences},"BACKGROUND: For many RNA molecules, secondary structure rather than primary sequence is the evolutionarily conserved feature. No programs have yet been published that allow searching a sequence database for homologs of a single RNA molecule on the basis of secondary structure. RESULTS: We have developed a program, RSEARCH, that takes a single RNA sequence with its secondary structure and utilizes a local alignment algorithm to search a database for homologous RNAs. For this purpose, we have developed a series of base pair and single nucleotide substitution matrices for RNA sequences called RIBOSUM matrices. RSEARCH reports the statistical confidence for each hit as well as the structural alignment of the hit. We show several examples in which RSEARCH outperforms the primary sequence search programs BLAST and SSEARCH. The primary drawback of the program is that it is slow. The C code for RSEARCH is freely available from our lab's website. CONCLUSION: RSEARCH outperforms primary sequence programs in finding homologs of structured RNA sequences."
426,49,5482,1,The Evolution of Controlled Multitasked Gene Networks: The Role of Introns and Other Noncoding {RNAs} in the Development of Complex Organisms,"Eukaryotic phenotypic diversity arises from multitasking of a core proteome of limited size. Multitasking is routine in computers, as well as in other sophisticated information systems, and requires multiple inputs and outputs to control and integrate network activity. Higher eukaryotes have a mosaic gene structure with a dual output, mRNA (protein-coding) sequences and introns, which are released from the pre-mRNA by posttranscriptional processing. Introns have been enormously successful as a class of sequences and comprise up to 95% of the primary transcripts of protein-coding genes in mammals. In addition, many other transcripts (perhaps more than half) do not encode proteins at all, but appear both to be developmentally regulated and to have genetic function. We suggest that these RNAs (eRNAs) have evolved to function as endogenous network control molecules which enable direct gene-gene communication and multitasking of eukaryotic genomes. Analysis of a range of complex genetic phenomena in which RNA is involved or implicated, including co-suppression, transgene silencing, RNA interference, imprinting, methylation, and transvection, suggests that a higher-order regulatory system based on RNA signals operates in the higher eukaryotes and involves chromatin remodeling as well as other RNA-DNA, RNA-RNA, and RNA-protein interactions. The evolution of densely connected gene networks would be expected to result in a relatively stable core proteome due to the multiple reuse of components, implying that cellular differentiation and phenotypic variation in the higher eukaryotes results primarily from variation in the control architecture. Thus, network integration and multitasking using trans-acting RNA molecules produced in parallel with protein-coding sequences may underpin both the evolution of developmentally sophisticated multicellular organisms and the rapid expansion of phenotypic complexity into uncontested environments such as those initiated in the Cambrian radiation and those seen after major extinction events."
427,49,5498,1,{RNA} secondary structures and their prediction,"Abstract&nbsp;&nbsp;This is a review of past and present attempts to predict the secondary structure of ribonucleic acids (RNAs) through mathematical and computer methods. Related areas covering classification, enumeration and graphical representations of structures are also covered. Various general prediction techniques are discussed, especially the use of thermodynamic criteria to construct an optimal structure. The emphasis in this approach is on the use of dynamic programming algorithms to minimize free energy. One such algorithm is introduced which comprises existing ones as special cases."
428,49,5502,1,{Profile analysis: detection of distantly related proteins},"{Profile analysis is a method for detecting distantly related proteins by sequence comparison. The basis for comparison is not only the customary Dayhoff mutational-distance matrix but also the results of structural studies and information implicit in the alignments of the sequences of families of similar proteins. This information is expressed in a position-specific scoring table (profile), which is created from a group of sequences previously aligned by structural or sequence similarity. The similarity of any other sequence (target) to the group of aligned sequences (probe) can be tested by comparing the target to the profile using dynamic programming algorithms. The profile method differs in two major respects from methods of sequence comparison in common use: (i) Any number of known sequences can be used to construct the profile, allowing more information to be used in the testing of the target than is possible with pairwise alignment methods. (ii) The profile includes the penalties for insertion or deletion at each position, which allow one to include the probe secondary structure in the testing scheme. Tests with globin and immunoglobulin sequences show that profile analysis can distinguish all members of these families from all other sequences in a database containing 3800 protein sequences.}"
429,49,5725,1,Global analysis of protein phosphorylation in yeast.,"Protein phosphorylation is estimated to affect 30% of the proteome and is a major regulatory mechanism that controls many basic cellular processes. Until recently, our biochemical understanding of protein phosphorylation on a global scale has been extremely limited; only one half of the yeast kinases have known in vivo substrates and the phosphorylating kinase is known for less than 160 phosphoproteins. Here we describe, with the use of proteome chip technology, the in vitro substrates recognized by most yeast protein kinases: we identified over 4,000 phosphorylation events involving 1,325 different proteins. These substrates represent a broad spectrum of different biochemical functions and cellular roles. Distinct sets of substrates were recognized by each protein kinase, including closely related kinases of the protein kinase A family and four cyclin-dependent kinases that vary only in their cyclin subunits. Although many substrates reside in the same cellular compartment or belong to the same functional category as their phosphorylating kinase, many others do not, indicating possible new roles for several kinases. Furthermore, integration of the phosphorylation results with protein-protein interaction and transcription factor binding data revealed novel regulatory modules. Our phosphorylation results have been assembled into a first-generation phosphorylation map for yeast. Because many yeast proteins and pathways are conserved, these results will provide insights into the mechanisms and roles of protein phosphorylation in many eukaryotes."
430,49,5788,1,The evolution of transcriptional regulation in eukaryotes,"10.1093/molbev/msg140 Gene expression is central to the genotype-phenotype relationship in all organisms, and it is an important component of the genetic basis for evolutionary change in diverse aspects of phenotype. However, the evolution of transcriptional regulation remains understudied and poorly understood. Here we review the evolutionary dynamics of promoter, or cis-regulatory, sequences and the evolutionary mechanisms that shape them. Existing evidence indicates that populations harbor extensive genetic variation in promoter sequences, that a substantial fraction of this variation has consequences for both biochemical and organismal phenotype, and that some of this functional variation is sorted by selection. As with protein-coding sequences, rates and patterns of promoter sequence evolution differ considerably among loci and among clades for reasons that are not well understood. Studying the evolution of transcriptional regulation poses empirical and conceptual challenges beyond those typically encountered in analyses of coding sequence evolution: promoter organization is much less regular than that of coding sequences, and sequences required for the transcription of each locus reside at multiple other loci in the genome. Because of the strong context-dependence of transcriptional regulation, sequence inspection alone provides limited information about promoter function. Understanding the functional consequences of sequence differences among promoters generally requires biochemical and in vivo functional assays. Despite these challenges, important insights have already been gained into the evolution of transcriptional regulation, and the pace of discovery is accelerating."
431,49,6205,1,The origins of eukaryotic gene structure.,"Most of the phenotypic diversity that we perceive in the natural world is directly attributable to the peculiar structure of the eukaryotic gene, which harbors numerous embellishments relative to the situation in prokaryotes. The most profound changes include introns that must be spliced out of precursor mRNAs, transcribed but untranslated leader and trailer sequences (untranslated regions), modular regulatory elements that drive patterns of gene expression, and expansive intergenic regions that harbor additional diffuse control mechanisms. Explaining the origins of these features is difficult because they each impose an intrinsic disadvantage by increasing the genic mutation rate to defective alleles. To address these issues, a general hypothesis for the emergence of eukaryotic gene structure is provided here. Extensive information on absolute population sizes, recombination rates, and mutation rates strongly supports the view that eukaryotes have reduced genetic effective population sizes relative to prokaryotes, with especially extreme reductions being the rule in multicellular lineages. The resultant increase in the power of random genetic drift appears to be sufficient to overwhelm the weak mutational disadvantages associated with most novel aspects of the eukaryotic gene, supporting the idea that most such changes are simple outcomes of semi-neutral processes rather than direct products of natural selection. However, by establishing an essentially permanent change in the population-genetic environment permissive to the genome-wide repatterning of gene structure, the eukaryotic condition also promoted a reliable resource from which natural selection could secondarily build novel forms of organismal complexity. Under this hypothesis, arguments based on molecular, cellular, and/or physiological constraints are insufficient to explain the disparities in gene, genomic, and phenotypic complexity between prokaryotes and eukaryotes."
432,49,6714,1,Genome-Scale Identification of Nucleosome Positions in S. cerevisiae,"The positioning of nucleosomes along chromatin has been implicated in the regulation of gene expression in eukaryotic cells, because packaging DNA into nucleosomes affects sequence accessibility. We developed a tiled microarray approach to identify at high resolution the translational positions of 2278 nucleosomes over 482 kilobases of Saccharomyces cerevisiae DNA, including almost all of chromosome III and 223 additional regulatory regions. The majority of the nucleosomes identified were well-positioned. We found a stereotyped chromatin organization at Pol II promoters consisting of a nucleosome-free region approximately 200 base pairs upstream of the start codon flanked on both sides by positioned nucleosomes. The nucleosome-free sequences were evolutionarily conserved and were enriched in poly-deoxyadenosine or poly-deoxythymidine sequences. Most occupied transcription factor binding motifs were devoid of nucleosomes, strongly suggesting that nucleosome positioning is a global determinant of transcription factor access."
433,49,6716,1,Global nucleosome occupancy in yeast.,"BACKGROUND: Although eukaryotic genomes are generally thought to be entirely chromatin-associated, the activated PHO5 promoter in yeast is largely devoid of nucleosomes. We systematically evaluated nucleosome occupancy in yeast promoters by immunoprecipitating nucleosomal DNA and quantifying enrichment by microarrays. RESULTS: Nucleosome depletion is observed in promoters that regulate active genes and/or contain multiple evolutionarily conserved motifs that recruit transcription factors. The Rap1 consensus was the only binding motif identified in a completely unbiased search of nucleosome-depleted promoters. Nucleosome depletion in the vicinity of Rap1 consensus sites in ribosomal protein gene promoters was also observed by real-time PCR and micrococcal nuclease digestion. Nucleosome occupancy in these regions was increased by the small molecule rapamycin or, in the case of the RPS11B promoter, by removing the Rap1 consensus sites. CONCLUSIONS: The presence of transcription factor-binding motifs is an important determinant of nucleosome depletion. Most motifs are associated with marked depletion only when they appear in combination, consistent with a model in which transcription factors act collaboratively to exclude nucleosomes and gain access to target sites in the DNA. In contrast, Rap1-binding sites cause marked depletion under steady-state conditions. We speculate that nucleosome depletion enables Rap1 to define chromatin domains and alter them in response to environmental cues."
434,49,7190,1,Positive and Negative Selection on the Human Genome,"The distinction between deleterious, neutral, and adaptive mutations is a fundamental problem in the study of molecular evolution. Two significant quantities are the fraction of DNA variation in natural populations that is deleterious and destined to be eliminated and the fraction of fixed differences between species driven by positive Darwinian selection. We estimate these quantities using the large number of human genes for which there are polymorphism and divergence data. The fraction of amino acid mutations that is neutral is estimated to be 0.20 from the ratio of common amino acid (A) to synonymous (S) single nucleotide polymorphisms (SNPs) at frequencies of [&gt;=]15%. Among the 80% of amino acid mutations that are deleterious at least 20% of them are only slightly deleterious and often attain frequencies of 1-10%. We estimate that these slightly deleterious mutations comprise at least 3% of amino acid SNPs in the average individual or at least 300 per diploid genome. This estimate is not sensitive to human population history. The A/S ratio of fixed differences is greater than that of common SNPs and suggests that a large fraction of protein divergence is adaptive and driven by positive Darwinian selection."
435,49,7287,1,An introduction to graphical models,"The following quotation, from the Preface of [Jor99], provides a very concise introduction to graphical models. Graphical models are a marriage between probability theory and graph theory. They provide a natural tool for dealing with two problems that occur throughout applied mathematics and engineering { uncertainty and complexity { and in particular they are playing an increasingly important role in the design and analysis of machine learning algorithms. Fundamental to the idea of a graphical model is the notion of modularity { a complex system is built by combining simpler parts. Probability theory provides the glue whereby the parts are combined, ensuring that the system as a whole is consistent, and providing ways to interface models to data. The graph theoretic side of graphical models provides both an intuitively appealing interface by which humans can model highly-interacting sets of variables as well as a data structure that lends itself naturally to the design of ecient general-purpose algorithms. Many of the classical multivariate probabalistic systems studied in elds such as statistics, systems engineering, information theory, pattern recognition and statistical mechanics are special cases of the general graphical model formalism { examples include mixture models, factor analysis, hidden Markov models, Kalman lters and Ising models. The graphical model framework provides a way to view all of these systems as instances of a common underlying formalism. This view has many advantages { in particular, specialized techniques that have been developed in one eld can be transferred between research communities and exploited more widely. Moreover, the graphical model formalism provides a natural framework for the design of new systems."
436,49,7755,1,Evidence for stabilizing selection in a eukaryotic enhancer element,"Eukaryotic gene expression is mediated by compact cis-regulatory modules, or enhancers, which are bound by specific sets of transcription factors1. The combinatorial interaction of these bound transcription factors determines time- and tissue-specific gene activation or repression. The even-skipped stripe 2 element controls the expression of the second transverse stripe of even-skipped messenger RNA in Drosophila melanogaster embryos, and is one of the best characterized eukaryotic enhancers2, 3, 4. Although even-skipped stripe 2 expression is strongly conserved in Drosophila, the stripe 2 element itself has undergone considerable evolutionary change in its binding-site sequences and the spacing between them. We have investigated this apparent contradiction, and here we show that two chimaeric enhancers, constructed by swapping the 5' and 3' halves of the native stripe 2 elements of two species, no longer drive expression of a reporter gene in the wild-type pattern. Sequence differences between species have functional consequences, therefore, but they are masked by other co-evolved differences. On the basis of these results, we present a model for the evolution of eukaryotic regulatory sequences."
437,49,7956,1,A hidden {Markov} model approach to variation among sites in rate of evolution,"The method of Hidden Markov Models is used to allow for unequal and unknown evolutionary rates at different sites in molecular sequences. Rates of evolution at different sites are assumed to be drawn from a set of possible rates, with a finite number of possibilities. The overall likelihood of phylogeny is calculated as a sum of terms, each term being the probability of the data given a particular assignment of rates to sites, times the prior probability of that particular combination of rates. The probabilities of different rate combinations are specified by a stationary Markov chain that assigns rate categories to sites. While there will be a very large number of possible ways of assigning rates to sites, a simple recursive algorithm allows the contributions to the likelihood from all possible combinations of rates to be summed, in a time proportional to the number of different rates at a single site. Thus with three rates, the effort involved is no greater than three times that for a single rate. This ""Hidden Markov Model"" method allows for rates to differ between sites and for correlations between the rates of neighboring sites. By summing over all possibilities it does not require us to know the rates at individual sites. However, it does not allow for correlation of rates at nonadjacent sites, nor does it allow for a continuous distribution of rates over sites. It is shown how to use the Newton-Raphson method to estimate branch lengths of a phylogeny and to infer from a phylogeny what assignment of rates to sites has the largest posterior probability. An example is given using beta-hemoglobin DNA sequences in eight mammal species; the regions of high and low evolutionary rates are inferred and also the average length of patches of similar rates."
438,49,8278,1,Stochastic mechanisms in gene expression.,"In cellular regulatory networks, genetic activity is controlled by molecular signals that determine when and how often a given gene is transcribed. {I}n genetically controlled pathways, the protein product encoded by one gene often regulates expression of other genes. {T}he time delay, after activation of the first promoter, to reach an effective level to control the next promoter depends on the rate of protein accumulation. {W}e have analyzed the chemical reactions controlling transcript initiation and translation termination in a single such ""genetically coupled"" link as a precursor to modeling networks constructed from many such links. {S}imulation of the processes of gene expression shows that proteins are produced from an activated promoter in short bursts of variable numbers of proteins that occur at random time intervals. {A}s a result, there can be large differences in the time between successive events in regulatory cascades across a cell population. {I}n addition, the random pattern of expression of competitive effectors can produce probabilistic outcomes in switching mechanisms that select between alternative regulatory paths. {T}he result can be a partitioning of the cell population into different phenotypes as the cells follow different paths. {T}here are numerous unexplained examples of phenotypic variations in isogenic populations of both prokaryotic and eukaryotic cells that may be the result of these stochastic gene expression mechanisms."
439,49,8453,1,Dynamic Topic Models,"A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR&#039;ed archives of the journal Science from 1880 through 2000."
440,49,8777,1,A genomic code for nucleosome positioning,"Eukaryotic genomes are packaged into nucleosome particles that occlude the DNA from interacting with most DNA binding proteins. Nucleosomes have higher affinity for particular DNA sequences, reflecting the ability of the sequence to bend sharply, as required by the nucleosome structure. However, it is not known whether these sequence preferences have a significant influence on nucleosome position in vivo, and thus regulate the access of other proteins to DNA. Here we isolated nucleosome-bound sequences at high resolution from yeast and used these sequences in a new computational approach to construct and validate experimentally a nucleosome-DNA interaction model, and to predict the genome-wide organization of nucleosomes. Our results demonstrate that genomes encode an intrinsic nucleosome organization and that this intrinsic organization can explain approximately 50% of the in vivo nucleosome positions. This nucleosome positioning code may facilitate specific chromosome functions including transcription factor binding, transcription initiation, and even remodelling of the nucleosomes themselves."
441,49,9114,1,Computational methods for transcriptional regulation.,"How is the information from a thousand gene-expression arrays, the location of more than two hundred regulatory factors, and nine sequenced genomes to be integrated into a global view of the regulatory network in budding yeast? Computational methods that fit incomplete noisy data provide the outlines of regulatory pathways, but the errors are not quantified. In the fly, embryonic patterning has proved amenable to computational prediction, but only when the DNA-binding preferences of the relevant factors are taken into account. In both these model organisms, simply restricting attention to regulatory sequences that align with related species (i.e. ""conserved"") discards much information regarding what is functional."
442,49,9411,1,Evolution of alternative transcriptional circuits with identical logic.,"Evolution of gene regulation is an important contributor to the variety of life. Here, we analyse the evolution of a combinatorial transcriptional circuit composed of sequence-specific DNA-binding proteins that are conserved among all eukaryotes. This circuit regulates mating in the ascomycete yeast lineage. We first identify a group of mating genes that was transcriptionally regulated by an activator in a fungal ancestor, but is now transcriptionally regulated by a repressor in modern bakers' yeast. Despite this change in regulatory mechanism, the logical output of the overall circuit remains the same. By examining the regulation of mating in modern yeasts that are related to different extents, we deduce specific, sequential changes in both cis- and trans-regulatory elements that constitute the transition from positive to negative regulation. These changes indicate specific mechanisms by which fitness barriers were traversed during the transition."
443,49,9696,1,Comparative ab initio prediction of gene structures using pair HMMs,"We present a novel comparative method for the ab initio prediction of protein coding genes in eukaryotic genomes. The method simultaneously predicts the gene structures of two un-annotated input DNA sequences which are homologous to each other and retrieves the subsequences which are conserved between the two DNA sequences. It is capable of predicting partial, complete and multiple genes and can align pairs of genes which differ by events of exon-fusion or exon-splitting. The method employs a probabilistic pair hidden Markov model. We generate annotations using our model with two different algorithms: the Viterbi algorithm in its linear memory implementation and a new heuristic algorithm, called the stepping stone, for which both memory and time requirements scale linearly with the sequence length. We have implemented the model in a computer program called DOUBLESCAN. In this article, we introduce the method and confirm the validity of the approach on a test set of 80 pairs of orthologous DNA sequences from mouse and human. More information can be found at: http://www.sanger.ac.uk/Software/analysis/doublescan/"
444,49,9842,1,Principles for the Buffering of Genetic Variation,"Most genetic research has used inbred organisms and has not explored the complexity of natural genetic variation present in outbred populations. The translation of genotype to phenotype is complicated by gene interactions observed as epistasis, canalization, robustness, or buffering. Analysis of double mutations in inbred experimental organisms suggests some principles for gene interaction that may apply to natural variation as well. The buffering of variation in one gene is most often due to a small number of other genes that function in the same biochemical process. However, buffering can also result from genes functioning in processes extrinsic to that of the primary gene."
445,49,10339,1,Evolution of Transcription Factor Binding Sites in Mammalian Gene Regulatory Regions: Conservation and Turnover,"Comparisons between human and rodent DNA sequences are widely used for the identification of regulatory regions (phylogenetic footprinting), and the importance of such intergenomic comparisons for promoter annotation is expanding. The efficacy of such comparisons for the identification of functional regulatory elements hinges on the evolutionary dynamics of promoter sequences. Although it is widely appreciated that conservation of sequence motifs may provide a suggestion of function, it is not known as to what proportion of the functional binding sites in humans is conserved in distant species. In this report, we present an analysis of the evolutionary dynamics of transcription factor binding sites whose function had been experimentally verified in promoters of 51 human genes and compare their sequence to homologous sequences in other primate species and rodents. Our results show that there is extensive divergence within the nucleotide sequence of transcription factor binding sites. Using direct experimental data from functional studies in both human and rodents for 20 of the regulatory regions, we estimate that 32%-40% of the human functional sites are not functional in rodents. This is evidence that there is widespread turnover of transcription factor binding sites. These results have important implications for the efficacy of phylogenetic footprinting and the interpretation of the pattern of evolution in regulatory sequences."
446,49,10362,1,Reconstructing dynamic regulatory maps.,"Even simple organisms have the ability to respond to internal and external stimuli. This response is carried out by a dynamic network of protein-DNA interactions that allows the specific regulation of genes needed for the response. We have developed a novel computational method that uses an input-output hidden Markov model to model these regulatory networks while taking into account their dynamic nature. Our method works by identifying bifurcation points, places in the time series where the expression of a subset of genes diverges from the rest of the genes. These points are annotated with the transcription factors regulating these transitions resulting in a unified temporal map. Applying our method to study yeast response to stress, we derive dynamic models that are able to recover many of the known aspects of these responses. Predictions made by our method have been experimentally validated leading to new roles for Ino4 and Gcn4 in controlling yeast response to stress. The temporal cascade of factors reveals common pathways and highlights differences between master and secondary factors in the utilization of network motifs and in condition-specific regulation."
447,49,10978,1,Genetic reconstruction of a functional transcriptional regulatory network,"Although global analyses of transcription factor binding provide one view of potential transcriptional regulatory networks1, 2, regulation also occurs at levels distinct from transcription factor binding3, 4. Here, we use a genetic approach to identify targets of transcription factors in yeast and reconstruct a functional regulatory network. First, we profiled transcriptional responses in S. cerevisiae strains with individual deletions of 263 transcription factors. Then we used directed-weighted graph modeling and regulatory epistasis analysis to identify indirect regulatory relationships between these transcription factors, and from this we reconstructed a functional transcriptional regulatory network. The enrichment of promoter motifs and Gene Ontology annotations provide insight into the biological functions of the transcription factors."
448,49,11704,1,Beyond the sequence: cellular organization of genome function.,"Genomes are more than linear sequences. In vivo they exist as elaborate physical structures, and their functional properties are strongly determined by their cellular organization. I discuss here the functional relevance of spatial and temporal genome organization at three hierarchical levels: the organization of nuclear processes, the higher-order organization of the chromatin fiber, and the spatial arrangement of genomes within the cell nucleus. Recent insights into the cell biology of genomes have overturned long-held dogmas and have led to new models for many essential cellular processes, including gene expression and genome stability."
449,49,11828,1,Principled Hybrids of Generative and Discriminative Models,"When labelled training data is plentiful, discriminative techniques are widely used since they give excellent generalization performance. However, for large-scale applications such as object recognition, hand labelling of data is expensive, and there is much interest in semi-supervised techniques based on generative models in which the majority of the training data is unlabelled. Although the generalization performance of generative models can often be improved by &#145;training them discriminatively&#146;, they can then no longer make use of unlabelled data. In an attempt to gain the benefit of both generative and discriminative approaches, heuristic procedure have been proposed [2, 3] which interpolate between these two extremes by taking a convex combination of the generative and discriminative objective functions. In this paper we adopt a new perspective which says that there is only one correct way to train a given model, and that a &#145;discriminatively trained&#146; generative model is fundamentally a new model [7]. From this viewpoint, generative and discriminative models correspond to specific choices for the prior over parameters. As well as giving a principled interpretation of &#145;discriminative training&#146;, this approach opens door to very general ways of interpolating between generative and discriminative extremes through alternative choices of prior. We illustrate this framework using both synthetic data and a practical example in the domain of multi-class object recognition. Our results show that, when the supply of labelled training data is limited, the optimum performance corresponds to a balance between the purely generative and the purely discriminative."
450,49,12114,1,Current progress in network research: toward reference networks for key model organisms,"The collection of multiple genome-scale datasets is now routine, and the frontier of research in systems biology has shifted accordingly. Rather than clustering a single dataset to produce a static map of functional modules, the focus today is on data integration, network alignment, interactive visualization and ontological markup. Because of the intrinsic noisiness of high-throughput measurements, statistical methods have been central to this effort. In this review, we briefly survey available datasets in functional genomics, review methods for data integration and network alignment, and describe recent work on using network models to guide experimental validation. We explain how the integration and validation steps spring from a Bayesian description of network uncertainty, and conclude by describing an important near-term milestone for systems biology: the construction of a set of rich reference networks for key model organisms. 10.1093/bib/bbm038"
451,49,12135,1,Natural history and evolutionary principles of gene duplication in fungi.,"Gene duplication and loss is a powerful source of functional innovation. However, the general principles that govern this process are still largely unknown. With the growing number of sequenced genomes, it is now possible to examine these events in a comprehensive and unbiased manner. Here, we develop a procedure that resolves the evolutionary history of all genes in a large group of species. We apply our procedure to seventeen fungal genomes to create a genome-wide catalogue of gene trees that determine precise orthology and paralogy relations across these species. We show that gene duplication and loss is highly constrained by the functional properties and interacting partners of genes. In particular, stress-related genes exhibit many duplications and losses, whereas growth-related genes show selection against such changes. Whole-genome duplication circumvents this constraint and relaxes the dichotomy, resulting in an expanded functional scope of gene duplication. By characterizing the functional fate of duplicate genes we show that duplicated genes rarely diverge with respect to biochemical function, but typically diverge with respect to regulatory control. Surprisingly, paralogous modules of genes rarely arise, even after whole-genome duplication. Rather, gene duplication may drive the modularization of functional networks through specialization, thereby disentangling cellular systems."
452,49,12288,1,A high-resolution atlas of nucleosome occupancy in yeast,"We present the first complete high-resolution map of nucleosome occupancy across the whole Saccharomyces cerevisiae genome, identifying over 70,000 positioned nucleosomes occupying 81% of the genome. On a genome-wide scale, the persistent nucleosome-depleted region identified previously in a subset of genes demarcates the transcription start site. Both nucleosome occupancy signatures and overall occupancy correlate with transcript abundance and transcription rate. In addition, functionally related genes can be clustered on the basis of the nucleosome occupancy patterns observed at their promoters. A quantitative model of nucleosome occupancy indicates that DNA structural features may account for much of the global nucleosome occupancy."
453,49,12501,1,Timescales of genetic and epigenetic inheritance.,"According to classical evolutionary theory, phenotypic variation originates from random mutations that are independent of selective pressure. However, recent findings suggest that organisms have evolved mechanisms to influence the timing or genomic location of heritable variability. Hypervariable contingency loci and epigenetic switches increase the variability of specific phenotypes; error-prone DNA replicases produce bursts of variability in times of stress. Interestingly, these mechanisms seem to tune the variability of a given phenotype to match the variability of the acting selective pressure. Although these observations do not undermine Darwin's theory, they suggest that selection and variability are less independent than once thought."
454,49,12977,1,Analyzing time series gene expression data,"Motivation: Time series expression experiments are an increasingly popular method for studying a wide range of biological systems. However, when analyzing these experiments researchers face many new computational challenges. Algorithms that are specifically designed for time series experiments are required so that we can take advantage of their unique features (such as the ability to infer causality from the temporal response pattern) and address the unique problems they raise (e.g. handling the different non-uniform sampling rates).  Results: We present a comprehensive review of the current research in time series expression data analysis. We divide the computational challenges into four analysis levels: experimental design, data analysis, pattern recognition and networks. For each of these levels, we discuss computational and biological problems at that level and point out some of the methods that have been proposed to deal with these issues. Many open problems in all these levels are discussed. This review is intended to serve as both, a point of reference for experimental biologists looking for practical solutions for analyzing their data, and a starting point for computer scientists interested in working on the computational problems related to time series expression analysis. 10.1093/bioinformatics/bth283"
455,49,13110,1,Bioinformatics challenges of new sequencing technology.,"New DNA sequencing technologies can sequence up to one billion bases in a single day at low cost, putting large-scale sequencing within the reach of many scientists. Many researchers are forging ahead with projects to sequence a range of species using the new technologies. However, these new technologies produce read lengths as short as 35â40 nucleotides, posing challenges for genome assembly and annotation. Here we review the challenges and describe some of the bioinformatics systems that are being proposed to solve them. We specifically address issues arising from using these technologies in assembly projects, both de novo and for resequencing purposes, as well as efforts to improve genome annotation in the fragmented assemblies produced by short read lengths."
456,49,13251,1,The MC-Fold and MC-Sym pipeline infers RNA structure from sequence data,"The classical {RNA} secondary structure model considers {A.U} and {G.C} {Watson-Crick} as well as {G.U} wobble base pairs. Here we substitute it for a new one, in which sets of nucleotide cyclic motifs define {RNA} structures. This model allows us to unify all base pairing energetic contributions in an effective scoring function to tackle the problem of {RNA} folding. We show how pipelining two computer algorithms based on nucleotide cyclic motifs, {MC-Fold} and {MC-Sym,} reproduces a series of experimentally determined {RNA} three-dimensional structures from the sequence. This demonstrates how crucial the consideration of all base-pairing interactions is in filling the gap between sequence and structure. We use the pipeline to define rules of precursor {microRNA} folding in double helices, despite the presence of a number of presumed mismatches and bulges, and to propose a new model of the human immunodeficiency virus-1 -1 frame-shifting element."
457,49,13393,1,Evolution of eukaryotic transcription circuits.,"The gradual modification of transcription circuits over evolutionary time scales is an important source of the diversity of life. Over the past decade, studies in animals have shown how seemingly small molecular changes in gene regulation can have large effects on morphology and physiology and how selective pressures can act on these changes. More recently, genome-wide studies, particularly those in single-cell yeasts, have uncovered evidence of extensive transcriptional rewiring, indicating that even closely related organisms regulate their genes using markedly different circuitries. 10.1126/science.1152398"
458,49,13562,1,Hierarchical structure and the prediction of missing links in networks.,"Networks have in recent years emerged as an invaluable tool for describing and quantifying complex systems in many branches of science1, 2, 3. Recent studies suggest that networks often exhibit hierarchical organization, in which vertices divide into groups that further subdivide into groups of groups, and so forth over multiple scales. In many cases the groups are found to correspond to known functional units, such as ecological niches in food webs, modules in biochemical networks (protein interaction networks, metabolic networks or genetic regulatory networks) or communities in social networks4, 5, 6, 7. Here we present a general technique for inferring hierarchical structure from network data and show that the existence of hierarchy can simultaneously explain and quantitatively reproduce many commonly observed topological properties of networks, such as right-skewed degree distributions, high clustering coefficients and short path lengths. We further show that knowledge of hierarchical structure can be used to predict missing connections in partly known networks with high accuracy, and for more general network structures than competing techniques8. Taken together, our results suggest that hierarchy is a central organizing principle of complex networks, capable of offering insight into many network phenomena."
459,49,13969,1,Population genomics of domestic and wild yeasts,"Since the completion of the genome sequence of Saccharomyces cerevisiae in 1996 (refs 1, 2), there has been a large increase in complete genome sequences, accompanied by great advances in our understanding of genome evolution. Although little is known about the natural and life histories of yeasts in the wild, there are an increasing number of studies looking at ecological and geographic distributions3, 4, population structure5, 6, 7, 8 and sexual versus asexual reproduction9, 10. Less well understood at the whole genome level are the evolutionary processes acting within populations and species that lead to adaptation to different environments, phenotypic differences and reproductive isolation. Here we present one- to fourfold or more coverage of the genome sequences of over seventy isolates of the bakerâs yeast S. cerevisiae and its closest relative, Saccharomyces paradoxus. We examine variation in gene content, single nucleotide polymorphisms, nucleotide insertions and deletions, copy numbers and transposable elements. We find that phenotypic variation broadly correlates with global genome-wide phylogenetic relationships. S. paradoxus populations are well delineated along geographic boundaries, whereas the variation among worldwide S. cerevisiae isolates shows less differentiation and is comparable to a single S. paradoxus population. Rather than one or two domestication events leading to the extant bakerâs yeasts, the population structure of S. cerevisiae consists of a few well-defined, geographically isolated lineages and many different mosaics of these lineages, supporting the idea that human influence provided the opportunity for cross-breeding and production of new combinations of pre-existing variations."
460,49,14190,1,Metabolic gene regulation in a dynamically changing environment,"Natural selection dictates that cells constantly adapt to dynamically changing environments in a context-dependent manner. Gene-regulatory networks often mediate the cellular response to perturbation1, 2, 3, and an understanding of cellular adaptation will require experimental approaches aimed at subjecting cells to a dynamic environment that mimics their natural habitat4, 5, 6, 7, 8, 9. Here we monitor the response of Saccharomyces cerevisiae metabolic gene regulation to periodic changes in the external carbon source by using a microfluidic platform that allows precise, dynamic control over environmental conditions. We show that the metabolic system acts as a low-pass filter that reliably responds to a slowly changing environment, while effectively ignoring fast fluctuations. The sensitive low-frequency response was significantly faster than in predictions arising from our computational modelling, and this discrepancy was resolved by the discovery that two key galactose transcripts possess half-lives that depend on the carbon source. Finally, to explore how induction characteristics affect frequency response, we compare two S. cerevisiae strains and show that they have the same frequency response despite having markedly different induction properties. This suggests that although certain characteristics of the complex networks may differ when probed in a static environment, the system has been optimized for a robust response to a dynamically changing environment."
461,49,14596,1,"Diverse RNA-binding proteins interact with functionally related sets of RNAs, suggesting an extensive regulatory system.","RNA-binding proteins (RBPs) have roles in the regulation of many post-transcriptional steps in gene expression, but relatively few RBPs have been systematically studied. We searched for the RNA targets of 40 proteins in the yeast Saccharomyces cerevisiae: a selective sample of the approximately 600 annotated and predicted RBPs, as well as several proteins not annotated as RBPs. At least 33 of these 40 proteins, including three of the four proteins that were not previously known or predicted to be RBPs, were reproducibly associated with specific sets of a few to several hundred RNAs. Remarkably, many of the RBPs we studied bound mRNAs whose protein products share identifiable functional or cytotopic features. We identified specific sequences or predicted structures significantly enriched in target mRNAs of 16 RBPs. These potential RNA-recognition elements were diverse in sequence, structure, and location: some were found predominantly in 3'-untranslated regions, others in 5'-untranslated regions, some in coding sequences, and many in two or more of these features. Although this study only examined a small fraction of the universe of yeast RBPs, 70% of the mRNA transcriptome had significant associations with at least one of these RBPs, and on average, each distinct yeast mRNA interacted with three of the RBPs, suggesting the potential for a rich, multidimensional network of regulation. These results strongly suggest that combinatorial binding of RBPs to specific recognition elements in mRNAs is a pervasive mechanism for multi-dimensional regulation of their post-transcriptional fate."
462,49,14650,1,Genetic mapping in human disease.,"Genetic mapping provides a powerful approach to identify genes and biological processes underlying any trait influenced by inheritance, including human diseases. We discuss the intellectual foundations of genetic mapping of Mendelian and complex traits in humans, examine lessons emerging from linkage analysis of Mendelian diseases and genome-wide association studies of common diseases, and discuss questions and challenges that lie ahead."
463,49,15097,1,Comprehensive polymorphism survey elucidates population structure of Saccharomyces cerevisiae.,"Comprehensive identification of polymorphisms among individuals within a species is essential both for studying the genetic basis of phenotypic differences and for elucidating the evolutionary history of the species. Large-scale polymorphism surveys have recently been reported for human, mouse and Arabidopsis thaliana. Here we report a nucleotide-level survey of genomic variation in a diverse collection of 63 Saccharomyces cerevisiae strains sampled from different ecological niches (beer, bread, vineyards, immunocompromised individuals, various fermentations and nature) and from locations on different continents. We hybridized genomic DNA from each strain to whole-genome tiling microarrays and detected 1.89 million single nucleotide polymorphisms, which were grouped into 101,343 distinct segregating sites. We also identified 3,985 deletion events of length >200 base pairs among the surveyed strains. We analysed the genome-wide patterns of nucleotide polymorphism and deletion variants, and measured the extent of linkage disequilibrium in S. cerevisiae. These results and the polymorphism resource we have generated lay the foundation for genome-wide association studies in yeast. We also examined the population structure of S. cerevisiae, providing support for multiple domestication events as well as insight into the origins of pathogenic strains."
464,49,15502,1,Metatranscriptomics reveals unique microbial small RNAs in the oceanâs water column,"Microbial gene expression in the environment has recently been assessed via pyrosequencing of total RNA extracted directly from natural microbial assemblages. Several such 'metatranscriptomic' studies1, 2 have reported that many complementary DNA sequences shared no significant homology with known peptide sequences, and so might represent transcripts from uncharacterized proteins. Here we report that a large fraction of cDNA sequences detected in microbial metatranscriptomic data sets are comprised of well-known small RNAs (sRNAs)3, as well as new groups of previously unrecognized putative sRNAs (psRNAs). These psRNAs mapped specifically to intergenic regions of microbial genomes recovered from similar habitats, displayed characteristic conserved secondary structures and were frequently flanked by genes that indicated potential regulatory functions. Depth-dependent variation of psRNAs generally reflected known depth distributions of broad taxonomic groups4, but fine-scale differences in the psRNAs within closely related populations indicated potential roles in niche adaptation. Genome-specific mapping of a subset of psRNAs derived from predominant planktonic species such as Pelagibacter revealed recently discovered as well as potentially new regulatory elements. Our analyses show that metatranscriptomic data sets can reveal new information about the diversity, taxonomic distribution and abundance of sRNAs in naturally occurring microbial communities, and indicate their involvement in environmentally relevant processes including carbon metabolism and nutrient acquisition."
465,49,15574,1,Unstable Tandem Repeats in Promoters Confer Transcriptional Evolvability,"Relative to most regions of the genome, tandemly repeated DNA sequences display a greater propensity to mutate. A search for tandem repeats in the Saccharomyces cerevisiae genome revealed that the nucleosome-free region directly upstream of genes (the promoter region) is enriched in repeats. As many as 25% of all gene promoters contain tandem repeat sequences. Genes driven by these repeat-containing promoters show significantly higher rates of transcriptional divergence. Variations in repeat length result in changes in expression and local nucleosome positioning. Tandem repeats are variable elements in promoters that may facilitate evolutionary tuning of gene expression by affecting local chromatin structure. 10.1126/science.1170097"
466,49,15840,1,Architecture and secondary structure of an entire HIV-1 RNA genome,"Single-stranded RNA viruses encompass broad classes of infectious agents and cause the common cold, cancer, AIDS and other serious health threats. Viral replication is regulated at many levels, including the use of conserved genomic RNA structures. Most potential regulatory elements in viral RNA genomes are uncharacterized. Here we report the structure of an entire HIV-1 genome at single nucleotide resolution using SHAPE, a high-throughput RNA analysis technology. The genome encodes protein structure at two levels. In addition to the correspondence between RNA and protein primary sequences, a correlation exists between high levels of RNA structure and sequences that encode inter-domain loops in HIV proteins. This correlation suggests that RNA structure modulates ribosome elongation to promote native protein folding. Some simple genome elements previously shown to be important, including the ribosomal gag-pol frameshift stem-loop, are components of larger RNA motifs. We also identify organizational principles for unstructured RNA regions, including splice site acceptors and hypervariable regions. These results emphasize that the HIV-1 genome and, potentially, many coding RNAs are punctuated by previously unrecognized regulatory motifs and that extensive RNA structure constitutes an important component of the genetic code."
467,49,15900,1,"Applying mass spectrometry-based proteomics to genetics, genomics and network biology"," The systematic and quantitative molecular analysis of mutant organisms that has been pioneered by studies on mutant metabolomes and transcriptomes holds great promise for improving our understanding of how phenotypes emerge. Unfortunately, owing to the limitations of classical biochemical analysis, proteins have previously been excluded from such studies. Here we review how technical advances in mass spectrometry-based proteomics can be applied to measure changes in protein abundance, posttranslational modifications and proteinâprotein interactions in mutants at the scale of the proteome. We finally discuss examples that integrate proteomics data with genomic and phenomic information to build network-centred models, which provide a promising route for understanding how phenotypes emerge."
468,49,16194,1,Synthetic biology: understanding biological design from synthetic circuits,"An important aim of synthetic biology is to uncover the design principles of natural biological systems through the rational design of gene and protein circuits. Here, we highlight how the process of engineering biological systems â from synthetic promoters to the control of cellâcell interactions â has contributed to our understanding of how endogenous systems are put together and function. Synthetic biological devices allow us to grasp intuitively the ranges of behaviour generated by simple biological circuits, such as linear cascades and interlocking feedback loops, as well as to exert control over natural processes, such as gene expression and population dynamics."
469,49,16254,1,Discrete logic modelling as a means to link protein signalling networks with functional analysis of mammalian signal transduction,"Large-scale protein signalling networks are useful for exploring complex biochemical pathways but do not reveal how pathways respond to specific stimuli. Such specificity is critical for understanding disease and designing drugs. Here we describe a computational approachâimplemented in the free CNO softwareâfor turning signalling networks into logical models and calibrating the models against experimental data. When a literature-derived network of 82 proteins covering the immediate-early responses of human cells to seven cytokines was modelled, we found that training against experimental data dramatically increased predictive power, despite the crudeness of Boolean approximations, while significantly reducing the number of interactions. Thus, many interactions in literature-derived networks do not appear to be functional in the liver cells from which we collected our data. At the same time, CNO identified several new interactions that improved the match of model to data. Although missing from the starting network, these interactions have literature support. Our approach, therefore, represents a means to generate predictive, cell-type-specific models of mammalian signalling from generic protein signalling networks."
470,49,16453,1,Variability in gene expression underlies incomplete penetrance,"The phenotypic differences between individual organisms can often be ascribed to underlying genetic and environmental variation. However, even genetically identical organisms in homogeneous environments vary, indicating that randomness in developmental processes such as gene expression may also generate diversity. To examine the consequences of gene expression variability in multicellular organisms, we studied intestinal specification in the nematode Caenorhabditis elegans in which wild-type cell fate is invariant and controlled by a small transcriptional network. Mutations in elements of this network can have indeterminate effects: some mutant embryos fail to develop intestinal cells, whereas others produce intestinal precursors. By counting transcripts of the genes in this network in individual embryos, we show that the expression of an otherwise redundant gene becomes highly variable in the mutants and that this variation is subjected to a threshold, producing an ON/OFF expression pattern of the master regulatory gene of intestinal differentiation. Our results demonstrate that mutations in developmental networks can expose otherwise buffered stochastic variability in gene expression, leading to pronounced phenotypic variation."
471,49,16510,1,Visualizing genomes: techniques and challenges,"As our ability to generate sequencing data continues to increase, data analysis is replacing data generation as the rate-limiting step in genomics studies. Here we provide a guide to genomic data visualization tools that facilitate analysis tasks by enabling researchers to explore, interpret and manipulate their data, and in some cases perform on-the-fly computations. We will discuss graphical methods designed for the analysis of de novo sequencing assemblies and read alignments, genome browsing, and comparative genomics, highlighting the strengths and limitations of these approaches and the challenges ahead."
472,49,16589,1,Transcriptome-wide Identification of RNA-Binding Protein and MicroRNA Target Sites by PAR-CLIP,"RNA transcripts are subject to posttranscriptional gene regulation involving hundreds of RNA-binding proteins (RBPs) and microRNA-containing ribonucleoprotein complexes (miRNPs) expressed inÂ a cell-type dependent fashion. We developed a cell-based crosslinking approach to determine at high resolution and transcriptome-wide the binding sites of cellular RBPs and miRNPs. The crosslinked sites are revealed by thymidine to cytidine transitions inÂ the cDNAs prepared from immunopurified RNPs of 4-thiouridine-treated cells. We determined the binding sites and regulatory consequences for several intensely studied RBPs and miRNPs, includingÂ PUM2, QKI, IGF2BP1-3, AGO/EIF2C1-4 and TNRC6A-C. Our study revealed that these factors bind thousands of sites containing defined sequence motifs and have distinct preferences for exonic versus intronic or coding versus untranslated transcript regions. The precise mapping of binding sites across the transcriptome will be critical to the interpretation of the rapidly emerging data on genetic variation between individuals and how these variations contribute to complex genetic diseases. Âº PAR-CLIP is a transcriptome-wide crosslinking method for RNA-binding proteins (RBP) Âº It is based on incorporation of photoactivatable nucleoside analogs into nascent RNA Âº Characteristic sequence transitions in the prepared cDNA reveal the precise binding site Âº We deduced binding motifs and preferences for 5 different RBP families"
473,49,16701,1,"Most ""Dark Matter"" Transcripts Are Associated With Known Genes","A series of reports over the last few years have indicated that a much larger portion of the mammalian genome is transcribed than can be accounted for by currently annotated genes, but the quantity and nature of these additional transcripts remains unclear. Here, we have used data from single- and paired-end RNA-Seq and tiling arrays to assess the quantity and composition of transcripts in PolyA+ RNA from human and mouse tissues. Relative to tiling arrays, RNA-Seq identifies many fewer transcribed regions (Ã¢â¬ÅseqfragsÃ¢â¬ï¿½) outside known exons and ncRNAs. Most nonexonic seqfrags are in introns, raising the possibility that they are fragments of pre-mRNAs. The chromosomal locations of the majority of intergenic seqfrags in RNA-Seq data are near known genes, consistent with alternative cleavage and polyadenylation site usage, promoter- and terminator-associated transcripts, or new alternative exons; indeed, reads that bridge splice sites identified 4,544 new exons, affecting 3,554 genes. Most of the remaining seqfrags correspond to either single reads that display characteristics of random sampling from a low-level background or several thousand small transcripts (median length = 111 bp) present at higher levels, which also tend to display sequence conservation and originate from regions with open chromatin. We conclude that, while there are bona fide new intergenic transcripts, their number and abundance is generally low in comparison to known exons, and the genome is not as pervasively transcribed as previously reported."
474,50,329,1,A tutorial on support vector regression,"In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for regression and function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization and capacity control from a SV point of view."
475,50,1204,1,{Maximum likelihood from incomplete data via the EM algorithm},"{A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.}"
476,50,1892,1,An introduction to variable and feature selection,"			Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods."
477,50,5794,1,SMOTE: Synthetic minority over-sampling technique,"[Imbalanced dataset, classifiers (C4.5, Ripper, Naive Bayes Classifier), SMOTE, oversampling, undersampling] An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of ""normal"" examples with only a small percentage of ""abnormal"" or ""interesting"" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that the combination of over-sampling and undersampling can achieve better classifier performance than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy."
478,50,5901,1,{B}ayesian Interpolation,"Although Bayesian analysis has been in use since Laplace, the Bayesian method of model--comparison has only recently been developed in depth.  In this paper, the Bayesian approach to regularisation and model--comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other problems.  Regularising constants are set by examining their posterior probability distribution. Alternative regularisers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. `Occam&#039;s razor&#039; is automatically embodied by this framework.  The way in which Bayes infers the values of regularising constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling.  1 Data modelling and Occam&#039;s razor In science, a central task is to develop and compare models to a..."
479,51,69,1,Footprints: History-Rich Tools for Information Foraging,"Inspired by Hill and Hollan's original work [6], we have been developing a theory of interaction history and building tools to apply this theory to navigation in a complex information space. We have built a series of tools --- map, trails, annotations and signposts --- based on a physical-world navigation metaphor. These tools have been in use for over a year. Our user study involved a controlled browse task and showed that users were able to get the same amount of work done with significantly less effort."
480,51,397,1,Queryfree news search,"Many daily activities present information in the form of a stream of text, and often people can benefit from additional information on the topic discussed. TV broadcast news can be treated as one such stream of text; in this paper we discuss finding news articles on the web that are relevant to news currently being broadcast.We evaluated a variety of algorithms for this problem, looking at the impact of inverse document frequency, stemming, compounds, history, and query length on the relevance and coverage of news articles returned in real time during a broadcast. We also evaluated several postprocessing techniques for improving the precision, including reranking using additional terms, reranking by document similarity, and filtering on document similarity. For the best algorithm, 84%-91% of the articles found were relevant, with at least 64% of the articles being on the exact topic of the broadcast. In addition, a relevant article was found for at least 70% of the topics."
481,51,434,1,Tap: a semantic web platform,"Activities such as Web Services and the Semantic Web are working to create a distributed web of machine understandable data. We address three important problems that need to be solved to realize this vision. We discuss the problem of scalable and deployable query systems and present a simple, but general query interface called GetData. We address the issue of creating global agreements on vocabularies and introduce the concept of Semantic Negotiation, a process by which two programs can..."
482,51,1343,1,Measuring Similarity between Ontologies,"Ontologies now play an important role for many knowledge-intensive applications for which they provide a source of precisely defined terms. However, with their wide-spread usage there come problems concerning their proliferation. Ontology engineers or users frequently have a core ontology that they use, e.g., for browsing or querying data, but they need to extend it with, adapt it to, or compare it with the large set of other ontologies. For the task of detecting and retrieving relevant ontologies, one needs means for measuring the similarity between ontologies. We present a set of ontology similarity measures and a multiple-phase empirical evaluation."
483,51,1348,1,Towards the Self-Annotating Web,"â¢ Learning of extraction rules requires a lot of, frequently too                                                                       many, examples for learning the rules.The success of the Semantic Web depends on the availability of on-tologies as well as on the proliferation of web pages annotated withmetadata conforming to these ontologies. Thus, a crucial ques-tion is where to acquire these metadata. In this paper we proposePANKOW (Pattern-based Annotation through Knowledge on theWeb), a method which employs an unsupervised, pattern-based ap-proach to categorize instances with regard to an ontology. The ap-proach is evaluated against the manual annotations of two humansubjects. The approach is implemented in OntoMat, an annotationtool for the Semantic Web and shows very promising results."
484,51,2770,1,Information filtering and information retrieval: two sides of the same coin?,"Information filtering systems are designed for unstructured or semistructured data, as opposed to database applications, which use very structured data. The systems also deal primarily with textual information, but they may also entail images, voice, video or other data types that are part of multimedia information systems. Information filtering systems also involve a large amount of data and streams of incoming data, whether broadcast from a remote source or sent directly by other sources. Filtering is based on descriptions of individual or group information preferences, or profiles, that typically represent long-term interests. Filtering also implies removal of data from an incoming stream rather than finding data in the stream; users see only the data that is extracted. Models of information retrieval and filtering, and lessons for filtering from retrieval research are presented."
485,51,3004,1,The PROMPT suite: Interactive tools for ontology merging and mapping,"Researchers in the ontology-design field have developed the content for ontologies in many domain areas. This distributed nature of ontology development has led to a large number of ontologies covering overlapping domains. In order for these ontologies to be reused, they first need to be merged or aligned to one another. We developed a suite of tools for managing multiple ontologies. These suite provides users with a uniform framework for comparing, aligning, and merging ontologies, maintaining versions, translating between different formalisms. Two of the tools in the suite support semi-automatic ontology merging: IPROMPT is an interactive ontology-merging tool that guides the user through the merging process, presenting him with suggestions for next steps and identifying inconsistencies and potential problems. ANCHORPROMPT uses a graph structure of ontologies to find correlation between concepts and to provide additional information for IPROMPT. (C) 2003 Elsevier Ltd. All rights reserved."
486,51,3599,1,A Historical View of Context,"{A}bstract. {T}his paper examines a number of the approaches, origins and ideals of context- aware systems design, looking particularly at the way that history inuences what we do in our ongoing activity. {A}s a number of sociologists and philosophers have pointed out, past social interaction, as well as past use of the heterogeneous mix of media, tools and artifacts that we use in our everyday activity, inuence our ongoing interaction with the people and media at hand. {W}e suggest that ones experience and history is thus part of ones current context, with patterns of use temporally and subjectively combining and interconnecting dierent media as well as dierent modes of use of those media. {O}ne such mode of use is transparent use, put forward by {W}eiser as ubicomps design ideal. {O}ne theoretical nding is that this design ideal is unachievable or incomplete because transparent and more focused analytical use are inter- dependent, aecting and feeding into each other through ones experience and history. {U}sing these theoretical points, we discuss a number of context-aware system designs that make good use of history in supporting ongoing user activity."
487,51,3915,1,Implicit feedback for inferring user preference: a bibliography,"Relevance feedback has a history in information retrieval that dates back well over thirty years (c.f [SL96]). Relevance feedback is typically used for query expansion during short-term modeling of a user's immediate information need and for user profiling during long-term modeling of a user's persistent interests and preferences. Traditional relevance feedback methods require that users explicitly give feedback by, for example, specifying keywords, selecting and marking documents, or answering questions about their interests. Such relevance feedback methods force users to engage in additional activities beyond their normal searching behavior. Since the cost to the user is high and the benefits are not always apparent, it can be difficult to collect the necessary data and the effectiveness of explicit techniques can be limited. In this paper we consider the use of implicit feedback techniques for query expansion and user profiling in information retrieval tasks. These techniques unobtrusively obtain information about users by watching their natural interactions with the system. Some of the user behaviors that have been most extensively investigated as sources of implicit feedback include reading time, saving, printing and selecting. The primary advantage to using implicit techniques is that such techniques remove the cost to the user of providing feedback. Implicit measures are generally thought to be less accurate than explicit measures [Nic97], but as large quantities of implicit data can be gathered at no extra cost to the user, they are attractive alternatives . Moreover, implicit measures can be combined with explicit ratings to obtain a more accurate representation of user interests. Implicit feedback techniques have been used to retrieve, filter and recommend a variety of items: hyperlinks, Web documents, academic and professional journal articles, email messages, Internet news articles, movies, books, television programs, jobs and stocks . There is a growing body of literature on implicit feedback techniques for information retrieval tasks, and the purpose of this article is to provide a brief overview of this work. Our intention is not to be exhaustive, but rather to be selective, in that we present key papers that cover a range of approaches . We begin by presenting and extending a classification of behaviors for implicit feedback that was previously presented by Oard and Kim [OK01 ], and classifying the selected papers accordingly. A preponderance of the existing work clusters into one area of this classification, and we further examine those papers . We then provide a brief overview of several key papers, and conclude with a discussion of future research directions suggested by our analysis."
488,51,4497,1,"{Semantic Annotation, Indexing, and Retrieval}","The Semantic Web realization depends on the availability of a critical mass of metadata for the web content, associated with the respective formal knowledge about the world. We claim that the Semantic Web, at its current stage of development, is in a state of a critical need of metadata generation and usage schemata that are specific, well-defined and easy to understand. This paper introduces our vision for a holistic architecture for semantic annotation, indexing, and retrieval of documents with regard to extensive semantic repositories. A system (called KIM), implementing this concept, is presented in brief and it is used for the purposes of evaluation and demonstration. A particular schema for semantic annotation with respect to real-world entities is proposed. The underlying philosophy is that a practical semantic annotation is impossible without some particular knowledge modelling commitments. Our understanding is that a system for such semantic annotation should be based upon a simple model of real-world entity classes, complemented with extensive instance knowledge. To ensure the efficiency, ease of sharing, and reusability of the metadata, we introduce an upper-level ontology (of about 250 classes and 100 properties), which starts with some basic philosophical distinctions and then goes down to the most common entity types (people, companies, cities, etc.). Thus it encodes many of the domain-independent commonsense concepts and allows straightforward domain-specific extensions. On the basis of the ontology, a large-scale knowledge base of entity descriptions is bootstrapped, and further extended and maintained. Currently, the knowledge bases usually scales between 10 5  and 10 6  descriptions. Finally, this paper presents a semantically enhanced information extraction system, which provides automatic semantic annotation with references to classes in the ontology and to instances. The system has been running over a continuously growing document collection (currently about 0.5 million news articles), so it has been under constant testing and evaluation for some time now. On the basis of these semantic annotations, we perform semantic based indexing and retrieval where users can mix traditional information retrieval (IR) queries and ontology-based ones. We argue that such large-scale, fully automatic methods are essential for the transformation of the current largely textual web into a Semantic Web."
489,51,4781,1,{Context-Aware Computing Applications},"This paper describes software that examines and reacts to an individual's changing context. Such software  can promote and mediate people's interactions with devices, computers, and other people, and it can help  navigate unfamiliar places. We believe that a limited amount of information covering a person's proximate  environment is most important for this form of computing since the interesting part of the world around  us is what we can see, hear, and touch. In this paper we define context-aware computing, and describe  four categories of context-aware applications: proximate selection, automatic contextual reconfiguration,  contextual information and commands, and context-triggered actions. Instances of these application types  have been prototyped on the PARCTAB, a wireless, palm-sized computer.  1 Introduction  Our investigation focuses on an extended form of mobile computing in which users employ many different mobile, stationary and embedded computers over the course of the day...."
490,51,5212,1,Personalizing search via automated analysis of interests and activities,"We formulate and study search algorithms that consider a user's prior interactions with a wide variety of content to personalize that user's current Web search. Rather than relying on the unrealistic assumption that people will precisely specify their intent when searching, we pursue techniques that leverage implicit information about the user's interests. This information is used to re-rank Web search results within a relevance feedback framework. We explore rich models of user interests, built from both search-related information, such as previously issued queries and previously visited Web pages, and other information about the user such as documents and email the user has read and created. Our research suggests that rich representations of the user and the corpus are important for personalization, but that it is possible to approximate these representations and provide efficient client-side algorithms for personalizing search. We show that such personalization algorithms can significantly improve on current Web search."
491,51,5848,1,Implicit Interest Indicators,"Recommender systems provide personalized suggestions about items that users will find interesting.  Typically, recommender systems require a user interface that can ``intelligently'' determine the interest of a user and use this information to make suggestions.  The common solution, ``explicit ratings'', where users tell the system what they think about a piece of information, is well-understood and fairly precise.  However, having to stop to enter explicit ratings can alter normal patterns of browsing and reading. A more ``intelligent'' method is to use implicit ratings , where a rating is obtained by a method other than obtaining it directly from the user.  These implicit interest indicators have obvious advantages, including removing the cost of the user rating, and that every user interaction with the system can contribute to an implicit rating.   Current recommender systems mostly do not use implicit ratings, nor is the ability of implicit ratings to predict actual user interest well-understood.  This research studies the correlation between various implicit ratings and the explicit rating for a single Web page.  A Web browser was developed to record the user's actions (implicit ratings) and the explicit rating of a page.  Actions included mouse clicks, mouse movement, scrolling and elapsed time. This browser was used by over 80 people that browsed more than 2500 Web pages.   Using the data collected by the browser, the individual implicit ratings and some combinations of implicit ratings were analyzed and compared with the explicit rating.  We found that the time spent on a page, the amount of scrolling on a page and the combination of time and scrolling had a strong correlation with explicit interest, while individual scrolling methods and mouse-clicks were ineffective in predicting explicit interest."
492,51,6002,1,The Pragmatic Roots of Context,"When modelling complex systems one can not include all the causal factors, but one has to settle for partial models. This is alright if the factors left out are either so constant that they can be ignored or one is able to recognise the circumstances when they will be such that the partial model applies. The transference of knowledge from the point of application to the point of learning utilises a combination of recognition and inference â a simple model of the important features is learnt and later situations where inferences can be drawn from the model are recognised. Context is an abstraction of the collection of background features that are later recognised. Different heuristics for recognition and model formulation will be effective for different learning tasks. Each of these will lead to a different type of context. Given this, there two ways of modelling context: one can either attempt to investigate the contexts that arise out of the heuristics that a particular agent actua lly applies or one can attempt to model context using the external source of regularity that the heuristics exploit. There are also two basic methodologies for the investigation of context: a top-down approach where one tries to lay down general, a priori principles and a bottom-up approach where one can try and find what sorts of context arise by experiment and simulation. A simulation is exhibited which is designed to illustrate the practicality of the bottom-up approach in elucidating the sorts of internal context that arise in an artificial agent which is attempting to learn simple models of a complex environment."
493,51,6620,1,Retrieval effectiveness of an ontology-based model for information selection,"Technology in the field of digital media generates huge amounts of nontextual information, audio, video, and images, along with more familiar textual information. The potential for exchange and retrieval of information is vast and daunting. The key problem in achieving efficient and user-friendly retrieval is the development of a search mechanism to guarantee delivery of minimal irrelevant information (high precision) while insuring relevant information is not overlooked (high recall). The traditional solution employs keyword-based search. The only documents retrieved are those containing user-specified keywords. But many documents convey desired semantic information without containing these keywords. This limitation is frequently addressed through query expansion mechanisms based on the statistical co-occurrence of terms. Recall is increased, but at the expense of deteriorating precision. One can overcome this problem by indexing documents according to context and meaning rather than keywords, although this requires a method of converting words to meanings and the creation of a meaning-based index structure. We have solved the problem of an index structure through the design and implementation of a concept-based model using domain-dependent ontologies. An ontology is a collection of concepts and their interrelationships that provide an abstract view of an application domain. With regard to converting words to meaning, the key issue is to identify appropriate concepts that both describe and identify documents as well as language employed in user requests. This paper describes an automatic mechanism for selecting these concepts. An important novelty is a scalable disambiguation algorithm that prunes irrelevant concepts and allows relevant ones to associate with documents and participate in query generation. We also propose an automatic query expansion mechanism that deals with user requests expressed in natural language. This mechanism generates database queries with appropriate and relevant expansion through knowledge encoded in ontology form. Focusing on audio data, we have constructed a demonstration prototype. We have experimentally and analytically shown that our model, compared to keyword search, achieves a significantly higher degree of precision and recall. The techniques employed can be applied to the problem of information selection in all media types."
494,51,6770,1,Fusion via a linear combination of scores,"We present a thorough analysis of the capabilities of the linear combination (LC) model for fusion of information retrieval systems. The LC model combines the results lists of multiple IR systems by scoring each document using a weighted sum of the scores from each of the component systems. We first present both empirical and analytical justification for the hypotheses that such a model should only be used when the systems involved have high performance, a large overlap of relevant documents, and a small overlap of nonrelevant documents. The empirical approach allows us to very accurately predict the performance of a combined system. We also derive a formula for a theoretically optimal weighting scheme for combining 2 systems. We introduce d&mdash;the difference between the average score on relevant documents and the average score on nonrelevant documents&mdash;as a performance measure which not only allows mathematical reasoning about system performance, but also allows the selection of weights which generalize well to new documents. We describe a number of experiments involving large numbers of different IR systems which support these findings."
495,51,7131,1,GiveALink: Mining a Semantic Network of Bookmarks for Web Search and Recommendation,"GiveALink is a public site where users donate their bookmarks to the Web community. Bookmarks are analyzed to build a new generation of Web mining techniques and new ways to search, recommend, surf, personalize and visualize the Web. We present a semantic similarity measure for URLs that takes advantage both of the hierarchical structure of the bookmark files of individual users, and of collaborative filtering across users. We analyze the social bookmark network induced by the similarity measure. A search and recommendation system is built from a number of ranking algorithms based on prestige, generality, and novelty measures extracted from the similarity data."
496,51,8213,1,Retroactive Answering of Search Queries,"Major search engines currently use the history of a user's actions (e.g., queries, clicks) to personalize search results. In this paper, we present a new personalized service,  query-specific web recommendations  (QSRs), that retroactively answers queries from a user's history as new results arise. The QSR system addresses two important subproblems with applications beyond the system itself: (1) Automatic identification of queries in a user's history that represent standing interests and unfulfilled needs. (2) Effective detection of interesting new results to these queries. We develop a variety of heuristics and algorithms to address these problems, and evaluate them through a study of Google history users. Our results strongly motivate the need for automatic detection of standing interests from a user's history, and identifies the algorithms that are most useful in doing so. Our results also identify the algorithms, some which are counter-intuitive, that are most useful in identifying interesting new results for past queries, allowing us to achieve very high precision over our data set."
497,51,8233,1,Cubesvd: A novel approach to personalized web search,"As the competition of Web search market increases, there is a high demand for personalized Web search to conduct retrieval incorporating Web usersâ information needs. This paper focuses on utilizing clickthrough data to improve Web search. Since millions of searches are conducted everyday, a search engine accumulates a large volume of clickthrough data, which records who submits queries and which pages he/she clicks on. The clickthrough data is highly sparse and contains different types of objects (user, query and Web page), and the relationships among these objects are also very complicated. By performing analysis on these data, we attempt to discover Web usersâ interests and the patterns that users locate information. In this paper, a novel approach CubeSVD is proposed to improve Web search. The clickthrough data is represented by a 3-order tensor, on which we perform 3-mode analysis using the higher-order singular value decomposition technique to automatically capture the latent factors that govern the relations among these multi-type objects: users, queries and Web pages. A tensor reconstructed based on the CubeSVD analysis reflects both the observed interactions among these objects and the implicit associations among them. Therefore, Web search activities can be carried out based on CubeSVD analysis. Experimental evaluations using a real-world data set collected from an MSN search engine show that CubeSVD achieves encouraging search results in comparison with some standard methods."
498,51,8345,1,User Modeling for Adaptive News Access,"We present a framework for adaptive news access, based on machine learning techniques specifically designed for this task. First, we focus on the system's general functionality and system architecture. We then describe the interface and design of two deployed news agents that are part of the described architecture. While the first agent provides personalized news through a web-based interface, the second system is geared towards wireless information devices such as PDAs (personal digital assistants) and cell phones. Based on implicit and explicit user feedback, our agents use a machine learning algorithm to induce individual user models. Motivated by general shortcomings of other user modeling systems for Information Retrieval applications, as well as the specific requirements of news classification, we propose the induction of hybrid user models that consist of separate models for short-term and long-term interests. Furthermore, we illustrate how the described algorithm can be used to address an important issue that has thus far received little attention in the Information Retrieval community: a user's information need changes as a direct result of interaction with information. We empirically evaluate the system's performance based on data collected from regular system users. The goal of the evaluation is not only to understand the performance contributions of the algorithm's individual components, but also to assess the overall utility of the proposed user modeling techniques from a user perspective. Our results provide empirical evidence for the utility of the hybrid user model, and suggest that effective personalization can be achieved without requiring any extra effort from the user."
499,51,8512,1,Exploring Social Annotations for the Semantic Web,"In order to obtain a machine understandable semantics for web resources, research on the Semantic Web tries to annotate web resources with concepts and relations from explicitly defined formal ontologies. This kind of formal annotation is usually done manually or semi-automatically. In this paper, we explore a complement approach that focuses on the ""social annotations of the web"" which are annotations manually made by normal web users without a pre-defined formal ontology. Compared to the formal annotations, although social annotations are coarse-grained, informal and vague, they are also more accessible to more people and better reflect the web resources' meaning from the users' point of views during their actual usage of the web resources. Using a social bookmark service as an example, we show how emergent semantics [2] can be statistically derived from the social annotations. Furthermore, we apply the derived emergent semantics to discover and search shared web bookmarks. The initial evaluation on our implementation shows that our method can effectively discover semantically related web bookmarks that current social bookmark service can not discover easily."
500,51,9018,1,What makes a query difficult?,"This work tries to answer the question of what makes a query difficult. It addresses a novel model that captures the main components of a topic and the relationship between those components and topic difficulty. The three components of a topic are the textual expression describing the information need (the query or queries), the set of documents relevant to the topic (the Qrels), and the entire collection of documents. We show experimentally that topic difficulty strongly depends on the distances between these components. In the absence of knowledge about one of the model components, the model is still useful by approximating the missing component based on the other components. We demonstrate the applicability of the difficulty model for several uses such as predicting query difficulty, predicting the number of topic aspects expected to be covered by the search results, and analyzing the findability of a specific domain."
501,51,9346,1,{{C}oncept {B}ased {Q}uery {E}xpansion},"Query expansion methods have been studied for a long time - with debatable success in many instances. In this paper we present a probabilistic query expansion model based on a similarity thesaurus which was constructed automatically. A similarity thesaurus reflects domain knowledge about the particular collection from which it is constructed. We ad-dress the two important issues with query expansion: the selection and the weighting of additional search terms. In contrast to earlier methods, our queries are expanded by adding those terms that are most similar to the concept of the query, rather than selecting terms that are similar to the query terms. Our experiments show that this kind of query expansion results in a notable improvement in the retrieval effectiveness when measured using both recall-precision and usefulness."
502,51,9454,1,IR evaluation methods for retrieving highly relevant documents,"This paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to retrieve highly relevant documents. This is desirable from the user point of view in modern large IR environments. The proposed methods are (1) a novel application of P-R curves and average precision computations based on separate recall bases for documents of different degrees of relevance, and (2) two novel measures computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. We then demonstrate the use of these evaluation methods in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of relevance. The test was run with a best match retrieval system (In-Query 1 ) in a text database consisting of newspaper articles. The results indicate that the tested strong query structures are most effective in retrieving highly relevant documents. The differences between the query types are practically essential and statistically significant. More generally, the novel evaluation methods and the case demonstrate that non-dichotomous relevance assessments are applicable in IR experiments, may reveal interesting phenomena, and allow harder testing of IR methods."
503,51,10033,1,An Ontology-Based Information Retrieval Model,"Abstract. Semantic search has been one of the motivations of the Semantic Web since it was envisioned. We propose a model for the exploitation of ontologybased KBs to improve search over large document repositories. Our approach includes an ontology-based scheme for the semi-automatic annotation of documents, and a retrieval system. The retrieval model is based on an adaptation of the classic vector-space model, including an annotation weighting algorithm, and a ranking algorithm. Semantic search is combined with keyword-based search to achieve tolerance to KB incompleteness. Our proposal is illustrated with sample experiments showing improvements with respect to keyword-based search, and providing ground for further research and discussion. 1"
504,51,10368,1,User modeling via stereotypes,"This paper addresses the problems that must be considered if computers are going to treat their users as individuals with distinct personalities, goals, and so forth. It first outlines the issues, and then proposes stereotypes as a useful mechanism for building models of individual users on the basis of a small amount of information about them. In order to build user models quickly, a large amount of uncertain knowledge must be incorporated into the models. The issue of how to resolve the conflicts that will arise among such inferences is discussed. A system, Grundy, is described that builds models of its users, with the aid of stereotypes, and then exploits those models to guide it in its task, suggesting novels that people may find interesting. If stereotypes are to be useful to Grundy, they must accurately characterize the users of the system. Some techniques to modify stereotypes on the basis of experience are discussed. An analysis of Grundy's performance shows that its user models are effective in guiding its performance."
505,51,10575,1,Personalized search based on user search histories,"User profiles, descriptions of user interests, can be used by search engines to provide personalized search results. Many approaches to creating user profiles collect user information through proxy servers (to capture browsing histories) or desktop bots (to capture activities on a personal computer). Both these techniques require participation of the user to install the proxy server or the bot. In this study, we explore the use of a less-invasive means of gathering user information for personalized search. In particular, we build user profiles based on activity at the search site itself and study the use of these profiles to provide personalized search results. By implementing a wrapper around the Google search engine, we were able to collect information about individual user search activities. In particular, we collected the queries for which at least one search result was examined, and the snippets (titles and summaries) for each examined result. User profiles were created by classifying the collected information (queries or snippets) into concepts in a reference concept hierarchy. These profiles were then used to re-rank the search results and the rank-order of the user-examined results before and after re-ranking were compared. Our study found that user profiles based on queries were as effective as those based on snippets. We also found that our personalized re-ranking resulted in a 34% improvement in the rankorder of the user-selected results."
506,51,10634,1,Modeling context through domain ontologies,"Abstract&nbsp;&nbsp;Traditional information retrieval systems aim at satisfying most users for most of their searches, leaving aside the context in which the search takes place. We propose to model two main aspects of context: The themes of the user's information need and the specific data the user is looking for to achieve the task that has motivated his search. Both aspects are modeled by means of ontologies. Documents are semantically indexed according to the context representation and the user accesses information by browsing the ontologies. The model has been applied to a case study that has shown the added value of such a semantic representation of context."
507,51,10785,1,Personalized Content Retrieval in Context Using Ontological Knowledge,"Personalized content retrieval aims at improving the retrieval process by taking into account the particular interests of individual users. However, not all user preferences are relevant in all situations. It is well known that human preferences are complex, multiple, heterogeneous, changing, even contradictory, and should be understood in context with the user goals and tasks at hand. In this paper, we propose a method to build a dynamic representation of the semantic context of ongoing retrieval tasks, which is used to activate different subsets of user interests at runtime, in a way that out-of-context preferences are discarded. Our approach is based on an ontology-driven representation of the domain of discourse, providing enriched descriptions of the semantics involved in retrieval actions and preferences, and enabling the definition of effective means to relate preferences and context"
508,51,10987,1,Bayesian adaptive user profiling with explicit & implicit feedback,"Research in information retrieval is now moving into a personalized scenario where a retrieval or filtering system maintains a separate user profile for each user. In this framework, information delivered to the user can be automatically personalized and catered to individual user's information needs. However, a practical concern for such a personalized system is the ""cold start problem"": any user new to the system must endure poor initial performance until sufficient feedback from that user is provided.To solve this problem, we use both explicit and implicit feedback to build a user's profile and use Bayesian hierarchical methods to borrow information from existing users. We analyze the usefulness of implicit feedback and the adaptive performance of the model on two data sets gathered from user studies where users' interaction with a document, or implicit feedback , were recorded along with explicit feedback. Our results are two-fold: first, we demonstrate that the Bayesian modeling approach effectively trades off between shared and user-specific information, alleviating poor initial performance for each user. Second, we find that implicit feedback has very limited unstable predictive value by itself and only marginal value when combined with explicit feedback."
509,51,11190,1,Investigating behavioral variability in web search,"Understanding the extent to which people's search behaviors differ in terms of the interaction flow and information targeted is important in designing interfaces to help World Wide Web users search more effectively. In this paper we describe a longitudinal log-based study that investigated variability in people.s interaction behavior when engaged in search-related activities on the Web.allWe analyze the search interactions of more than two thousand volunteer users over a five-month period, with the aim of characterizing differences in their interaction styles.allThe findings of our study suggest that there are dramatic differences in variability in key aspects of the interaction within and between users, and within and between the search queries they submit.allOur findings also suggest two classes of extreme user. navigators and explorers. whose search interaction is highly consistent or highly variable. Lessons learned from these users can inform the design of tools to support effective Web-search interactions for everyone."
510,51,11204,1,Google news personalization: scalable online collaborative filtering,"Several approaches to collaborative filtering have been studied but seldom have the studies been reported for large (several millions of users and items) and dynamic (the underlying item set is continually changing) settings. In this paper we describe our approach to collaborative filtering for generating personalized recommendations for users of Google News. We generate recommendations using three approaches: collaborative filtering using {MinHash} clustering, Probabilistic Latent Semantic Indexing {(PLSI),} and covisitation counts. We combine recommendations from different algorithms using a linear model. Our approach is content agnostic and consequently domain independent, making it easily adaptible for other applications and languages with minimal effort. This paper will describe our algorithms and system setup in detail, and report results of running the recommendations engine on Google News."
511,51,12207,1,Content-Based Recommendation Systems,"This chapter discusses content-based recommendation systems, i.e., systems that recommend an item to a user based upon a description of the item and a profile of the userâs interests. Content-based recommendation systems may be used in a variety of domains ranging from recommending web pages, news articles, restaurants, television programs, and items for sale. Although the details of various systems differ, content-based recommendation systems share in common a means for describing the items that may be recommended, a means for creating a profile of the user that describes the types of items the user likes, and a means of comparing items to the user profile to determine what to re commend. The profile is often created and updated automatically in response to feedback on the desirability of items that have been presented to the user."
512,51,12681,1,Personalized information retrieval based on context and ontological knowledge,"Context modeling has long been acknowledged as a key aspect in a wide variety of problem domains. In this paper we focus on the combination of contextualization and personalization methods to improve the performance of personalized information retrieval. The key aspects in our proposed approach are (1) the explicit distinction between historic user context and live user context, (2) the use of ontology-driven representations of the domain of discourse, as a common, enriched representational ground for content meaning, user interests, and contextual conditions, enabling the definition of effective means to relate the three of them, and (3) the introduction of fuzzy representations as an instrument to properly handle the uncertainty and imprecision involved in the automatic interpretation of meanings, user attention, and user wishes. Based on a formal grounding at the representational level, we propose methods for the automatic extraction of persistent semantic user preferences, and live, ad-hoc user interests, which are combined in order to improve the accuracy and reliability of personalization for retrieval."
513,51,12886,1,Evaluating the accuracy of implicit feedback from clicks and query reformulations in Web search,"This article examines the reliability of implicit feedback generated from clickthrough data and query reformulations in World Wide Web (WWW) search. Analyzing the users' decision process using eyetracking and comparing implicit feedback against manual relevance judgments, we conclude that clicks are informative but biased. While this makes the interpretation of clicks as absolute relevance judgments difficult, we show that relative preferences derived from clicks are reasonably accurate on average. We find that such relative preferences are accurate not only between results from an individual query, but across multiple sets of results within chains of query reformulations."
514,51,13862,1,How Do Users Find Things with PubMed? Towards Automatic Utility Evaluation with User Simulations,"In the context of document retrieval in the biomedical domain, this paper explores the complex relationship between the quality of initial query results and the overall utility of an interactive retrieval system. We demonstrate that a content-similarity browsing tool can compensate for poor retrieval results, and that the relationship between retrieval performance and overall utility is non-linear. Arguments are advanced with user simulations, which characterize the relevance of documents that a user might encounter with different browsing strategies. With broader implications to IR, this work provides a case study of how user simulations can be exploited as a formative tool for automatic utility evaluation. Simulation-based studies provide researchers with an additional evaluation tool to complement interactive and Cranfield-style experiments."
515,51,13954,1,A basis for information retrieval in context,"Information retrieval (IR) models based on vector spaces have been investigated for a long time. Nevertheless, they have recently attracted much research interest. In parallel, context has been rediscovered as a crucial issue in information retrieval. This article presents a principled approach to modeling context and its role in ranking information objects using vector spaces. First, the article outlines how a basis of a vector space naturally represents context, both its properties and factors. Second, a ranking function computes the probability of context in the objects represented in a vector space, namely, the probability that a contextual factor has affected the preparation of an object."
516,51,14633,1,Personalized recommendation in social tagging systems using hierarchical clustering,"Intro i tag permettono una navigazione non legata ad una gerarchia concettuale. sono intuitivi per l'utente, danno senso di comunit{Ã¡}, permettono di connettere persone in base agli interessi, assorbono velocemente i trend e flessibili ai cambiamenti di vocabolario. Il sistema quindi pu{Ã³} monitorare gli interessi degli utenti nelle risorse e il vocabolario che usano per il tagging di queste risorse. La libert{Ã¡} di vocabolario porta a ambiguit{Ã¡}: singolo tag ha diversi significati ridondanza: diversi tag hanno lo stesso significato Quindi recommendation personalizzata deve aiutare l'utente a interagire con il sistema abbiamo 3 dimensioni da gestire con il recommender system: utenti; items (risorse - gruppi; film; bookmarks, etc...); tags navigazione nel sistema attraversando tag->utente->risorsa scelgo un tag; vedo recommendations e utenti correlati al tag; vedo un utente e scelgo di vedere il suo profilo; nel profilo vedo delle risorse dei chicago bulls e le vado a vedere. devo essere libero di navigare attraverso queste tre dimensioni della folksonomia. Devo presentare all'utente una serie di avenues (strade?) correlate ai suoi interessi, che possono essere viste come una serie di recommendations Con il clustering si pu{Ã³} superare l'ambiguit{Ã¡} dei tags. L'algoritmo di personalizzazione clustering-based funziona cos{\\'\\i}: INPUT: profilo utente; insieme di clusters; tag selezionato OUTPUT: risorse suggerite l'utente seleziona un tag e vuole un elenco di recommendations Le folksonomies sono un aiuto per capire i bisogni informativi degli utenti. L'importanza delle risorse deriva dagli utenti (o viceversa???) L'importanza degli utenti deriva dalle risorse ??? (Dataset) Obiettivi&Tecniche Algoritmo di personalizzazione per recommendation in folksonomies che si basa su cluster di tags gerarchici Folksonomy: U utenti R risorse T tags A annotazioni D = <U, R, T, A> le annotazioni sono triple con user, tag, resource A = {< u, r, t >} Una folksonomy pu{Ã³} essere vista come ipegrafo tripartito: NODI: users, tags e resources IPERARCHI: le annotazioni, che collegano <un utente con un tag con una risorsa> Possibili metodi per recommendation: recency, authority, linkage, popularity, vector space models ogni utente {Ã©} un vettore sull'insieme di (tutti i) tag, dove ogni peso rappresenta l'importanza del tag u = <w(t1), w(t2), ... , w( T )> anche le risorse possono essere modellate come vettore sull'insieme di tags come si calcolano i pesi dei tags nei vettori? tag frequency: numero di volte che una risorsa {Ã©} stata annotata con quel tag tf(t, r) = cardinalit{Ã¡} delle annotazioni che hanno <u, t1, r1> con t1 = t e r1= r poi possiamo modificare il TF*IDF per le folksonomies tf*idf(t, r) = tf(t, r) * log(N/nt) N numero totale di risorse nt numero di tutte le risorse che hanno il tag t le query e le risorse possono essere rappresentate come dei vettori sui tags ipotizziamo che un utente cominci la navigazione con una query che seleziona un solo tag l'utente seleziona un tag e vuole un elenco di recommendations Somiglianza tra vettori: Coseno-somiglianza (Jaccard similarity coefficient) siccome la query {Ã©} solo un tag, si semplifica l'equazione. Misuriamo la somiglianza tra tutte le risorse e la query del tag e selezioniamo le top n. C'{Ã©} bisogno critico di personalizzazione, altrimenti avremmo lo stesso elenco di recommendations per tutti gli utenti. tag ambiguity e redundancy (due utenti, uno dei white sox, uno dei red sox, dietro il tag sox si otterrebbe la stessa lista di risultati) Due fasi dell'algoritmo di personalizzazione 1) si ottiene la lista di risorse raccomandate; 2) si personalizzano tenedo conto del profilo utente e dei cluster di tags; Il clustering si effettua prima, in modalit{Ã¡} offline Processo di recommendation in dettaglio: 1) coseno similarit{Ã¡} calcolato tra la query e tutte le risorse r. si ottiene un sottoinsieme di risorse R' che hanno una certa somiglianza con la query 2) calcolare rilevanza delle risorse r di R' per l'utente u i cluster sono necessari, in quanto punto di collegamento che mette in relazione gli utenti con le risorse, permettendo di mostrare risorse che rispecchino gli interessi dell'utente. INPUT: profilo utente; risorse in R' OUTPUT: rilevanza di ogni risorsa di R' per l'utente u 2.1) calcola l'interesse dell'utente in ogni cluster {Ã©} il rapporto tra numero di risorse che ha annotato con tags di quel cluster e numero totale di annotazioni dell'utente [0, 1] 2.2) calcola i cluster pi{Ãº} vicini ad ogni risorsa la relazione tra risorsa e un cluster si calcola rapporto tra volte che la risorsa {Ã©} stata annotata con un tag del cluster e totale numero di volte che la risorsa {Ã©} stata annotata [0, 1] 2.3) inferisci gli interessi dell'utente nei confronti di ogni risorsa misura di rilevanza utente-risorsa si fa sa somma dei prodotti (dell'interesse utente-cluster con relazione risorsa-cluster) (ogni cluster = topic) 3) calcolare il punteggio di rank personalizzato somiglianza personalizzata: (utente, query, risorsa) coseno-similarit{Ã¡} * misura di rilevanza abbiamo questo valore per tutte le risorse (ed erano state ordinate) e sono restituite le prime n risorse NB!!! : i pesi cluster-risorsa saranno indipendenti dagli utenti, mentre i pesi che connettono gli utenti ai cluster dipendono dal profilo utente. Modellazione utenti e risorse: vettori su un insieme di tags gli interessi di un utente si capiscono misurando la rilevanza di un cluster per un utente associo anche le risorse con i cluster di tag per vedere la rilevanza di una risorsa per un topic descritto da un cluster Risultato: utente<--->tag cluster<--->risorse Risultati Le folksonomies con un solo argomento sono obiettivi pi{Ãº} facili per la recommendation rispetto a folksonomies con diversi argomenti scorrelati Se usiamo una strategia di recommendation progettata per ridurre lo spazio del topic, riusciamo a trattare abbastanza bene sia i casi single topic che multi topic, ma {Ã©} meglio nel multi topic. La chiave {Ã©} ridurre lo spazio dell'informazione per focalizzarci sugli interessi dell'utente, usando il context-dependent hierarchical clustering approach. filtro i tag ambigui e ridondanti. Discussione La selezione del topic {Ã©} una strategia importante per la recommendation nelle folksonomies multi-topic Futuro Idee Context dependent cluster selection Riduco lo spazio dei topic e uso la recommendation strategy basandomi sul profilo utente che rappresenta la memoria a breve termine i pesi cluster-risorsa saranno indipendenti dagli utenti, mentre i pesi che connettono gli utenti ai cluster dipendono dal profilo utente. ERGO: per il mio scopo dovrei rendere i cluster dipendenti dal profilo utente ed avere cos{\\'\\i} pesi risorsa-cluster diversi per ogni profilo, altrimenti {Ã©} come se usassi tutti gli utenti per formare una folksonomia da scomporre in cluster, invece voglio che ogni utente abbia la sua folksonomia di riferimento e suddividere quella in cluster."
517,51,14743,1,Integrating tags in a semantic content-based recommender,"Basic content personalization consists in matching up the attributes of a user profile, in which preferences and interests are stored, with the attributes of a content object. The Web 2.0 (r)evolution and the advent of user generated content have changed the game for personalization, since the role of people has evolved from passive consumers of information to that of active contributors. One of the forms of user generated content that has drawn more attention from the research community is folksonomy, a taxonomy generated by users who collaboratively annotate and categorize resources of interests with freely chosen keywords called tags."
518,51,15105,1,Collaborative Annotation for Context-Aware Retrieval,"We discuss how collaborative annotations can be exploited to simplify and improve the management of context and re- sources in the context-aware retrieval ï¬eld. We apply this approach to our Context Aware Browser, a general purpose solution to Web content perusal by means of mobile devices, based on the userâs context. Instead of relying on a pool of experts and on a rigid categorization, as it is usually done in the context-aware ï¬eld, our solution allows the crowd of users to model, control and manage the contextual knowl- edge through collaboration and participation. We propose two models and we outline an example of application."
519,51,15690,1,Evaluating the Effectiveness of Personalized Web Search,"Although personalized search has been under way for many years and many personalization algorithms have been investigated, it is still unclear whether personalization is consistently effective on different queries for different users and under different search contexts. In this paper, we study this problem and provide some findings. We present a large-scale evaluation framework for personalized search based on query logs and then evaluate five personalized search algorithms (including two click-based ones and three topical-interest-based ones) using 12-day query logs of Windows Live Search. By analyzing the results, we reveal that personalized Web search does not work equally well under various situations. It represents a significant improvement over generic Web search for some queries, while it has little effect and even harms query performance under some situations. We propose click entropy as a simple measurement on whether a query should be personalized. We further propose several features to automatically predict when a query will benefit from a specific personalization algorithm. Experimental results show that using a personalization algorithm for queries selected by our prediction model is better than using it simply for all queries."
520,51,15812,1,Context-aware query classification,"Understanding users'search intent expressed through their search queries is crucial to Web search and online advertisement. Web query classification (QC) has been widely studied for this purpose. Most previous QC algorithms classify individual queries without considering their context information. However, as exemplified by the well-known example on query ""jaguar"", many Web queries are short and ambiguous, whose real meanings are uncertain without the context information. In this paper, we incorporate context information into the problem of query classification by using conditional random field (CRF) models. In our approach, we use neighboring queries and their corresponding clicked URLs (Web pages) in search sessions as the context information. We perform extensive experiments on real world search logs and validate the effectiveness and effciency of our approach. We show that we can improve the F1 score by 52% as compared to other state-of-the-art baselines."
521,52,451,1,Location Systems for Ubiquitous Computing,"To serve us well, emerging mobile computing applications will need to know the physical location of things so that they can record them and report them to us: What lab bench was I standing by when I prepared these tissue samples? How should our search-and-rescue team move to quickly locate all the avalanche victims? Can I automatically display this stock devaluation chart on the large screen I am standing next to? Researchers are working to meet these and similar needs by developing systems and technologies that automatically locate people, equipment, and other tangibles. Indeed, many systems over the years have addressed the problem of automatic location sensing. Because each approach solves a slightly different problem or supports different applications, they vary in many parameters, such as the physical phenomena used for location determination, the form factor of the sensing apparatus, power requirements, infrastructure versus portable elements, and resolution in time and space. To make sense of this domain, we have developed ataxonomy to help developers of location-aware applications better evaluate their options when choosing a location-sensing system. The taxonomy may also aid researchers in identifying opportunities for new location-sensing techniques."
522,52,1404,1,The vision of autonomic computing,"A 2001 IBM manifesto observed that a looming software complexity crisis -caused by applications and environments that number into the tens of millions of lines of code - threatened to halt progress in computing. The manifesto noted the almost impossible difficulty of managing current and planned computing systems, which require integrating several heterogeneous environments into corporate-wide computing systems that extend into the Internet. Autonomic computing, perhaps the most attractive approach to solving this problem, creates systems that can manage themselves when given high-level objectives from administrators. Systems manage themselves according to an administrator's goals. New components integrate as effortlessly as a new cell establishes itself in the human body. These ideas are not science fiction, but elements of the grand challenge to create self-managing computing systems."
523,52,3028,1,The WSLA Framework: Specifying and Monitoring Service Level Agreements for Web Services,"We describe a novel framework for specifying and monitoring Service Level Agreements (SLA) for Web Services. SLA monitoring and enforcement become increasingly important in a Web Service environment where enterprise applications and services rely on services that may be subscribed dynamically and on-demand. For economic and practical reasons, we want an automated provisioning process for both the service itself as well as the SLA managment system that measures and monitors the QoS parameters, checks the agreed-upon service levels, and reports violations to the authorized parties involved in the SLA management process. Our approach to these issues is presented in this paper. The Web Service Level Agreement (WSLA) framework is targeted at defining and monitoring SLAs for Web Services. Although WSLA has been designed for a Web Services environment, it is applicable as well to any inter-domain management scenario, such as business process and service management, or the management of networks, systems and applications in general. The WSLA framework consists of a flexible and extensible language based on XML Schema and a runtime architecture comprising several SLA monitoring services, which may be outsourced to third parties to ensure a maximum of objectivity. WSLA enables service customers and providers to unambiguously define a wide variety of SLAs, specify the SLA parameters and the way they are measured, and relate them to managed resource instrumentations. Upon receipt of an SLA specification, the WSLA monitoring services are automatically configured to enforce the SLA. An implementation of the WSLA framework, termed SLA Compliance Monitor, is publicly available as part of the IBM Web Services Toolkit."
524,52,3261,1,Borrowed-virtual-time (BVT) scheduling: supporting latency-sensitive threads in a general-purpose scheduler,"Systems need to run a larger and more diverse set of applications, from real-time to interactive to batch, on uniprocessor and multiprocessor platforms. However, most schedulers either do not address latency requirements or are specialized to complex real-time paradigms, limiting their applicability to general-purpose systems.  In this paper, we present Borrowed-Virtual-Time (BVT) Scheduling, showing that it provides low-latency for realtime and interactive applications yet weighted sharing of the CPU across applications according to system policy, even with thread failure at the real-time level, all with a low-overhead implementation on multiprocessors as well as uniprocessors. It makes minimal demands on application developers, and can be used with a reservation or admission control module for hard real-time applications.  1 Introduction  With modern processor speeds and memory capacities, systems can now run a wide diversity of application tasks, and they need to in order to meet us..."
525,52,4611,1,Autonomic computing: emerging trends and open problems,"The increasing heterogeneity, dynamism and interconnectivity in software applications, services and networks led to complex, unmanageable and insecure systems. Coping with such a complexity necessitates to investigate a new paradigm namely  Autonomic Computing.  Although academic and industry efforts are beginning to proliferate in this research area, there are still a lots of open issues that remain to be solved. This paper proposes a categorization of complexity in I/T systems and presents an overview of autonomic computing research area. The paper also discusses a summary of the major autonomic computing systems that have been already developed both in academia and industry, and finally outlines the underlying research issues and challenges from a practical as well as a theoretical point of view."
526,52,4639,1,The dawning of the autonomic computing era,"This issue of the  IBM Systems Journal  explores a broad set of ideas and approaches to autonomic computing--some first steps in what we see as a journey to create more self-managing computing systems. Autonomic computing represents a collection and integration of technologies that enable the creation of an information technology computing infrastructure for IBM's agenda for the next era of computing--e-business on demand. This paper presents an overview of IBM's autonomic computing initiative. It examines the genesis of autonomic computing, the industry and marketplace drivers, the fundamental characteristics of autonomic systems, a framework for how systems will evolve to become more self-managing, and the key role for open industry standards needed to support autonomic behavior in heterogeneous system environments. Technologies explored in each of the papers presented in this issue are introduced for the reader."
527,52,6798,1,Self-Managing Systems: A Control Theory Foundation,"Summary form only given. The high cost of ownership of computing systems has resulted in a number of industry initiatives to reduce the burden of operations and management by making systems more self-managing. A major challenge in realizing self-managing systems is understanding how automated actions affect system behavior, especially system stability. Other disciplines such as mechanical, electrical, and aeronautical engineering make use of control theory to design feedback systems. The talk uses control theory as a way to identify a number of requirements for and challenges in building self-managing, or autonomic, systems. In essence, the autonomic computing architecture describes feedback control loops for self-managing systems. The talk has three goals: (1) educating systems oriented computer science researchers and practitioners on the concepts and techniques needed to apply control theory to computing systems; (2) describing how control theory can aid in building self-managing systems and identifying the challenges in doing so; (3) describing a deployable testbed for autonomic computing that is intended to foster research that addresses the challenges identified."
528,53,206,1,Transcriptional Regulatory Networks in Saccharomyces cerevisiae,"We have determined how most of the transcriptional regulators encoded in the eukaryote {S}accharomyces cerevisiae associate with genes across the genome in living cells. {J}ust as maps of metabolic networks describe the potential pathways that may be used by a cell to accomplish metabolic processes, this network of regulator-gene interactions describes potential pathways yeast cells can use to regulate global gene expression programs. {W}e use this information to identify network motifs, the simplest units of network architecture, and demonstrate that an automated process can use motifs to assemble a transcriptional regulatory network structure. {O}ur results reveal that eukaryotic cellular functions are highly connected through networks of transcriptional regulators that regulate other transcriptional regulators."
529,53,1022,1,Comprehensive Identification of Cell Cycle-regulated Genes of the Yeast Saccharomyces cerevisiae by Microarray Hybridization,"We sought to create a comprehensive catalog of yeast genes whose transcript levels vary periodically within the cell cycle. To this end, we used DNA microarrays and samples from yeast cultures synchronized by three independent methods: [alpha] factor arrest, elutriation, and arrest of a cdc15 temperature-sensitive mutant. Using periodicity and correlation algorithms, we identified 800 genes that meet an objective minimum criterion for cell cycle regulation. In separate experiments, designed to examine the effects of inducing either the G1 cyclin Cln3p or the B-type cyclin Clb2p, we found that the mRNA levels of more than half of these 800 genes respond to one or both of these cyclins. Furthermore, we analyzed our set of cell cycle-regulated genes for known and new promoter elements and show that several known elements (or variations thereof) contain information predictive of cell cycle regulation. A full description and complete data sets are available at http://cellcycle-www.stanford.edu"
530,53,5327,1,A mutation accumulation assay reveals a broad capacity for rapid evolution of gene expression,"Mutation is the ultimate source of biological diversity because it generates the variation that fuels evolution(1). Gene expression is the first step by which an organism translates genetic information into developmental change. Here we estimate the rate at which mutation produces new variation in gene expression by measuring transcript abundances across the genome during the onset of metamorphosis in 12 initially identical Drosophila melanogaster lines that independently accumulated mutations for 200 generations(2). We find statistically significant mutational variation for 39% of the genome and a wide range of variability across corresponding genes. As genes are upregulated in development their variability decreases, and as they are downregulated it increases, indicating that developmental context affects the evolution of gene expression. A strong correlation between mutational variance and environmental variance shows that there is the potential for widespread canalization(3). By comparing the evolutionary rates that we report here with differences between species(4,5), we conclude that gene expression does not evolve according to strictly neutral models. Although spontaneous mutations have the potential to generate abundant variation in gene expression, natural variation is relatively constrained."
531,53,5914,1,Gene Expression During the Life Cycle of Drosophila melanogaster,"Molecular genetic studies of Drosophila melanogaster have led to profound advances in understanding the regulation of development. Here we report gene expression patterns for nearly one-third of all Drosophila genes during a complete time course of development. Mutations that eliminate eye or germline tissue were used to further analyze tissue-specific gene expression programs. These studies define major characteristics of the transcriptional programs that underlie the life cycle, compare development in males and females, and show that large-scale gene expression data collected from whole animals can be used to identify genes expressed in particular tissues and organs or genes involved in specific biological and biochemical processes."
532,53,10283,1,The Quantitative Genetics of Transcription," Quantitative geneticists have become interested in the heritability of transcription and detection of expression quantitative trait loci (eQTLs). Linkage mapping methods have identified major-effect eQTLs for some transcripts and have shown that regulatory polymorphisms in cis and in trans affect expression. It is also clear that these mapping strategies have little power to detect polygenic factors, and some new statistical approaches are emerging that paint a more complex picture of transcriptional heritability. Several studies imply pervasive non-additivity of transcription, transgressive segregation and epistasis, and future studies will soon document the extent of genotypeâenvironment interaction and population structure at the transcriptional level. The implications of these findings for genotypeâphenotype mapping and modeling the evolution of transcription are discussed."
533,53,11250,1,THE LOCUS OF EVOLUTION: EVO DEVO AND THE GENETICS OF ADAPTATION,"An important tenet of evolutionary developmental biology (""evo devo"") is that adaptive mutations affecting morphology are more likely to occur in the cis-regulatory regions than in the protein-coding regions of genes. This argument rests on two claims: ( 1) the modular nature of cis-regulatory elements largely frees them from deleterious pleiotropic effects, and ( 2) a growing body of empirical evidence appears to support the predominant role of gene regulatory change in adaptation, especially morphological adaptation. Here we discuss and critique these assertions. We first show that there is no theoretical or empirical basis for the evo devo contention that adaptations involving morphology evolve by genetic mechanisms different from those involving physiology and other traits. In addition, some forms of protein evolution can avoid the negative consequences of pleiotropy, most notably via gene duplication. In light of evo devo claims, we then examine the substantial data on the genetic basis of adaptation from both genome-wide surveys and single-locus studies. Genomic studies lend little support to the cis-regulatory theory: many of these have detected adaptation in protein-coding regions, including transcription factors, whereas few have examined regulatory regions. Turning to single-locus studies, we note that the most widely cited examples of adaptive cis-regulatory mutations focus on trait loss rather than gain, and none have yet pinpointed an evolved regulatory site. In contrast, there are many studies that have both identified structural mutations and functionally verified their contribution to adaptation and speciation. Neither the theoretical arguments nor the data from nature, then, support the claim for a predominance of cis- regulatory mutations in evolution. Although this claim may be true, it is at best premature. Adaptation and speciation probably proceed through a combination of cis-regulatory and structural mutations, with a substantial contribution of the latter."
534,53,11838,1,Widely distributed noncoding purifying selection in the human genome,"10.1073/pnas.0705140104 It is widely assumed that human noncoding sequences comprise a substantial reservoir for functional variants impacting gene regulation and other chromosomal processes. Evolutionarily conserved noncoding sequences (CNSs) in the human genome have attracted considerable attention for their potential to simplify the search for functional elements and phenotypically important human alleles. A major outstanding question is whether functionally significant human noncoding variation is concentrated in CNSs or distributed more broadly across the genome. Here, we combine wholegenome sequence data from four nonhuman species (chimp, dog, mouse, and rat) with recently available comprehensive human polymorphism data to analyze selection at single-nucleotide resolution. We show that a substantial fraction of active purifying selection in human noncoding sequences occurs outside of CNSs and is diffusely distributed across the genome. This finding suggests the existence of a large complement of human noncoding variants that may impact gene expression and phenotypic traits, the majority of which will escape detection with current approaches to genome analysis."
535,53,12223,1,Splicing in disease: disruption of the splicing code and the decoding machinery,"Human genes contain a dense array of diverse cis-acting elements that make up a code required for the expression of correctly spliced mRNAs. Alternative splicing generates a highly dynamic human proteome through networks of coordinated splicing events. Cis- and trans-acting mutations that disrupt the splicing code or the machinery required for splicing and its regulation have roles in various diseases, and recent studies have provided new insights into the mechanisms by which these effects occur. An unexpectedly large fraction of exonic mutations exhibit a primary pathogenic effect on splicing. Furthermore, normal genetic variation significantly contributes to disease severity and susceptibility by affecting splicing efficiency."
536,53,13063,1,Independent effects of cis- and trans-regulatory variation on gene expression in Drosophila melanogaster,"Biochemical interactions between cis-regulatory DNA sequences and trans-regulatory gene products suggest that cis- and trans-acting polymorphisms may interact genetically. Here we present a strategy to test this hypothesis by comparing the relative cis-regulatory activity of two alleles in different genetic backgrounds. Of the eight genes surveyed in this study, five were affected by trans-acting variation that altered total transcript levels, two of which were also affected by differences in cis-regulation. The presence of trans-acting variation had no effect on relative cis-regulatory activity, showing that cis-regulatory polymorphisms can function independently of trans-regulatory variation. The frequency of such independent interactions on a genomic scale is yet to be determined. 10.1534/genetics.107.082032"
537,54,466,1,Integrating ethics and science in the International HapMap Project.,"Genomics resources that use samples from identified populations raise scientific, social and ethical issues that are, in many ways, inextricably linked. Scientific decisions about which populations to sample to produce the HapMap, an international genetic variation resource, have raised questions about the relationships between the social identities used to recruit participants and the biological findings of studies that will use the HapMap. The sometimes problematic implications of those complex relationships have led to questions about how to conduct genetic variation research that uses identified populations in an ethical way, including how to involve members of a population in evaluating the risks and benefits posed for everyone who shares that identity. The ways in which these issues are linked is increasingly drawing the scientific and ethical spheres of genomics research closer together."
538,54,1743,1,The plasticity of aging: insights from long-lived mutants.,"Mutations in genes affecting endocrine signaling, stress responses, metabolism, and telomeres can all increase the life spans of model organisms. These mutations have revealed evolutionarily conserved pathways for aging, some of which appear to extend life span in response to sensory cues, caloric restriction, or stress. Many mutations affecting longevity pathways delay age-related disease, and the molecular analysis of these pathways is leading to a mechanistic understanding of how these two processes--aging and disease susceptibility--are linked."
539,54,2803,1,"Genetic variation, classification and 'race'.","New genetic data has enabled scientists to re-examine the relationship between human genetic variation and 'race'. We review the results of genetic analyses that show that human genetic variation is geographically structured, in accord with historical patterns of gene flow and genetic drift. Analysis of many loci now yields reasonably accurate estimates of genetic similarity among individuals, rather than populations. Clustering of individuals is correlated with geographic origin or ancestry. These clusters are also correlated with some traditional concepts of race, but the correlations are imperfect because genetic variation tends to be distributed in a continuous, overlapping fashion among populations. Therefore, ancestry, or even race, may in some cases prove useful in the biomedical setting, but direct assessment of disease-related genetic variation will ultimately yield more accurate and beneficial information."
540,54,3274,1,The Patterns of Natural Variation in Human Genes.,"Currently, more than 10 million DNA sequence variations have been uncovered in the human genome. The most detailed variation discovery efforts have focused on candidate genes involved in cardiovascular disease or in susceptibilities associated with exposure to environmental agents. Here we provide an overviewof natural genetic variation from the literature and in 510 human candidate genes resequenced for variation discovery. The average human gene contains 126 biallelic polymorphisms, 46 of which are common (>/=5% minor allele frequency) and 5 of which are found in coding regions. Using this complete picture of genetic diversity, we explore conservation, signatures of selection, and historical recombination to mine information useful for candidate gene association studies. In general, we find that the patterns of human gene variation suggest that no one approach will be appropriate for genetic association studies across all genes. Therefore, many different approaches may be required to identify the elusive genotypes associated with common human phenotypes. Expected online publication date for the Annual Review of Genomics and Human Genetics Volume 6 is August 30, 2005. Please see http://www.annualreviews.org/catalog/pub_dates.asp for revised estimates."
541,54,5064,1,Intellectual property. Enhanced: intellectual property landscape of the human genome.,"The impact of gene patents on downstream research and innovation are unknown, in part because of a lack of empirical data on the extent and nature of gene patenting. In this Policy Forum, the authors show that 20% of human gene {DNA} sequences are patented and that some genes are patented as many as 20 times. Unsurprisingly, genes associated with health and disease are more patented than the genome at large. The intellectual property rights for some genes can become highly fragmented between many owners, which suggests that downstream innovators may face considerable costs to gain access to gene-oriented technologies."
542,54,5065,1,Genetic association studies.,"We review the rationale behind and discuss methods of design and analysis of genetic association studies. There are similarities between genetic association studies and classic epidemiological studies of environmental risk factors but there are also issues that are specific to studies of genetic risk factors such as the use of particular family-based designs, the need to account for different underlying genetic mechanisms, and the effect of population history. Association differs from linkage (covered elsewhere in this series) in that the alleles of interest will be the same across the whole population. As with other types of genetic epidemiological study, issues of design, statistical analysis, and interpretation are very important."
543,54,7958,1,The Structure of Haplotype Blocks in the Human Genome,"Haplotype-based methods offer a powerful approach to disease gene mapping, based on the association between causal mutations and the ancestral haplotypes on which they arose. As part of The SNP Consortium Allele Frequency Projects, we characterized haplotype patterns across 51 autosomal regions (spanning 13 megabases of the human genome) in samples from Africa, Europe, and Asia. We show that the human genome can be parsed objectively into haplotype blocks: sizable regions over which there is little evidence for historical recombination and within which only a few common haplotypes are observed. The boundaries of blocks and speciÃc haplotypes they contain are highly correlated across populations. We demonstrate that such haplotype frameworks provide substantial statistical power in association studies of common genetic variation across each region. Our results provide a foundation for the construction of a haplotype map of the human genome, facilitating comprehensive genetic association studies of human disease."
544,55,768,1,Basic local alignment search tool.,"{A new approach to rapid sequence comparison, basic local alignment search tool (BLAST), directly approximates alignments that optimize a measure of local similarity, the maximal segment pair (MSP) score. Recent mathematical results on the stochastic properties of MSP scores allow an analysis of the performance of this method as well as the statistical significance of alignments it generates. The basic algorithm is simple and robust; it can be implemented in a number of ways and applied in a variety of contexts including straightforward DNA and protein sequence database searches, motif searches, gene identification searches, and in the analysis of multiple regions of similarity in long DNA sequences. In addition to its flexibility and tractability to mathematical analysis, BLAST is an order of magnitude faster than existing sequence comparison tools of comparable sensitivity.}"
545,55,1200,1,Controlling the false discovery rate: a practical and powerful approach to multiple testing,"The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses - the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples."
546,55,1705,1,{KEGG: Kyoto Encyclopedia of Genes and Genomes},"K{EGG} ({K}yoto {E}ncyclopedia of {G}enes and {G}enomes) is a knowledge base for systematic analysis of gene functions, linking genomic information with higher order functional information. {T}he genomic information is stored in the {GENES} database, which is a collection of gene catalogs for all the completely sequenced genomes and some partial genomes with up-to-date annotation of gene functions. {T}he higher order functional information is stored in the {PATHWAY} database, which contains graphical representations of cellular processes, such as metabolism, membrane transport, signal transduction and cell cycle. {T}he {PATHWAY} database is supplemented by a set of ortholog group tables for the information about conserved subpathways (pathway motifs), which are often encoded by positionally coupled genes on the chromosome and which are especially useful in predicting gene functions. {A} third database in {KEGG} is {LIGAND} for the information about chemical compounds, enzyme molecules and enzymatic reactions. {KEGG} provides {J}ava graphics tools for browsing genome maps, comparing two genome maps and manipulating expression maps, as well as computational tools for sequence comparison, graph comparison and path computation. {T}he {KEGG} databases are daily updated and made freely available (http://www. genome.ad.jp/kegg/)."
547,55,2147,1,Minimum information about a microarray experiment (MIAME)-toward standards for microarray data.,"Microarray analysis has become a widely used tool for the generation of gene expression data on a genomic scale. Although many significant results have been derived from microarray studies, one limitation has been the lack of standards for presenting and exchanging such data. Here we present a proposal, the Minimum Information About a Microarray Experiment (MIAME), that describes the minimum information required to ensure that microarray data can be easily interpreted and that results derived from its analysis can be independently verified. The ultimate goal of this work is to establish a standard for recording and reporting microarray-based gene expression data, which will in turn facilitate the establishment of databases and public repositories and enable the development of data analysis tools. With respect to MIAME, we concentrate on defining the content and structure of the necessary information rather than the technical format for capturing it."
548,55,3089,1,Comparative metagenomics of microbial communities.,The species complexity of microbial communities and challenges in culturing representative isolates make it difficult to obtain assembled genomes. Here we characterize and compare the metabolic capabilities of terrestrial and marine microbial communities using largely unassembled sequence data obtained by shotgun sequencing DNA isolated from the various environments. Quantitative gene content analysis reveals habitat-specific fingerprints that reflect known characteristics of the sampled environments. The identification of environment-specific genes through a gene-centric comparative analysis presents new opportunities for interpreting and diagnosing environments. 10.1126/science.1107851
549,55,3602,1,The Gene Ontology (GO) database and informatics resource.,"{{T}he {G}ene {O}ntology ({GO}) project (http://www. geneontology.org/) provides structured, controlled vocabularies and classifications that cover several domains of molecular and cellular biology and are freely available for community use in the annotation of genes, gene products and sequences. {M}any model organism databases and genome annotation groups use the {GO} and contribute their annotation sets to the {GO} resource. {T}he {GO} database integrates the vocabularies and contributed annotations and provides full access to this information in several formats. {M}embers of the {GO} {C}onsortium continually work collectively, involving outside experts as needed, to expand and update the {GO} vocabularies. {T}he {GO} {W}eb resource also provides access to extensive documentation about the {GO} project and links to applications that use {GO} data for functional analyses.}"
550,55,4841,1,Protein Molecular Function Prediction by Bayesian Phylogenomics,"We present a statistical graphical model to infer specific molecular function for unannotated protein sequences using homology. Based on phylogenomic principles, SIFTER (Statistical Inference of Function Through Evolutionary Relationships) accurately predicts molecular function for members of a protein family given a reconciled phylogeny and available function annotations, even when the data are sparse or noisy. Our method produced specific and consistent molecular function predictions across 100 Pfam families in comparison to the Gene Ontology annotation database, BLAST, GOtcha, and Orthostrapper. We performed a more detailed exploration of functional predictions on the adenosine-5&#8242;-monophosphate/adenosine deaminase family and the lactate/malate dehydrogenase family, in the former case comparing the predictions against a gold standard set of published functional characterizations. Given function annotations for 3&#37; of the proteins in the deaminase family, SIFTER achieves 96&#37; accuracy in predicting molecular function for experimentally characterized proteins as reported in the literature. The accuracy of SIFTER on this dataset is a significant improvement over other currently available methods such as BLAST (75&#37;), GeneQuiz (64&#37;), GOtcha (89&#37;), and Orthostrapper (11&#37;). We also experimentally characterized the adenosine deaminase from Plasmodium falciparum, confirming SIFTER&#39;s prediction. The results illustrate the predictive power of exploiting a statistical model of function evolution in phylogenomic problems. A software implementation of SIFTER is available from the authors."
551,55,5350,1,GOtcha: a new method for prediction of protein function assessed by the annotation of seven genomes,"Background: The function of a novel gene product is typically predicted by transitive assignment of annotation from similar sequences. We describe a novel method, GOtcha, for predicting gene product function by annotation with Gene Ontology ( GO) terms. GOtcha predicts GO term associations with term-specific probability (P-score) measures of confidence. Term-specific probabilities are a novel feature of GOtcha and allow the identification of conflicts or uncertainty in annotation. Results: The GOtcha method was applied to the recently sequenced genome for Plasmodium falciparum and six other genomes. GOtcha was compared quantitatively for retrieval of assigned GO terms against direct transitive assignment from the highest scoring annotated BLAST search hit (TOPBLAST). GOtcha exploits information deep into the `twilight zone' of similarity search matches, making use of much information that is otherwise discarded by more simplistic approaches. At a P-score cutoff of 50%, GOtcha provided 60\\ better recovery of annotation terms and 20% higher selectivity than annotation with TOPBLAST at an E-value cutoff of 10(-4). Conclusions: The GOtcha method is a useful tool for genome annotators. It has identified both errors and omissions in the original Plasmodium falciparum annotation and is being adopted by many other genome sequencing projects."
552,55,6618,1,{Semantic Similarity Based on Corpus Statistics and Lexical Taxonomy},"This paper presents a new approach for measuring semantic similarity/distance between words and concepts. It combines a lexical taxonomy structure with corpus statistical information so that the semantic distance between nodes in the semantic space constructed by the taxonomy can be better quantified with the computational evidence derived from a distributional analysis of corpus data. Specifically, the proposed measure is a combined approach that inherits the edge-based approach of the edge counting scheme, which is then enhanced by the node-based approach of the information content calculation. When tested on a common data set of word pair similarity ratings, the proposed approach outperforms other computational models. It gives the highest correlation value (r = 0.828) with a benchmark based on human similarity judgements, whereas an upper bound (r = 0.885) is observed when human subjects replicate the same task."
553,55,8337,1,FatiGO: a web tool for finding significant associations of Gene Ontology terms with groups of genes.,"Summary: We present a simple but powerful procedure to extract Gene Ontology (GO) terms that are significantly over- or under-represented in sets of genes within the context of a genome-scale experiment (DNA microarray, proteomics, etc.). Said procedure has been implemented as a web application, FatiGO, allowing for easy and interactive querying. FatiGO, which takes the multiple-testing nature of statistical contrast into account, currently includes GO associations for diverse organisms (human, mouse, fly, worm and yeast) and the TrEMBL/Swissprot GOAnnotations@EBI correspondences from the European Bioinformatics Institute. Availability: http://fatigo.bioinfo.cnio.es 10.1093/bioinformatics/btg455"
554,55,8401,1,A new measure for functional similarity of gene products based on Gene Ontology,"BACKGROUND:Gene Ontology (GO) is a standard vocabulary of functional terms and allows for coherent annotation of gene products. These annotations provide a basis for new methods that compare gene products regarding their molecular function and biological role.RESULTS:We present a new method for comparing sets of GO terms and for assessing the functional similarity of gene products. The method relies on two semantic similarity measures; simRel and funSim. One measure (simRel) is applied in the comparison of the biological processes found in different groups of organisms. The other measure (funSim) is used to find functionally related gene products within the same or between different genomes. Results indicate that the method, in addition to being in good agreement with established sequence similarity approaches, also provides a means for the identification of functionally related proteins independent of evolutionary relationships. The method is also applied to estimating functional similarity between all proteins in Saccharomyces cerevisiae and to visualizing the molecular function space of yeast in a map of the functional space. A similar approach is used to visualize the functional relationships between protein families.CONCLUSION:The approach enables the comparison of the underlying molecular biology of different taxonomic groups and provides a new comparative genomics tool identifying functionally related gene products independent of homology. The proposed map of the functional space provides a new global view on the functional relationships between gene products or protein families."
555,55,9244,1,Coexpression Analysis of Human Genes Across Many Microarray Data Sets,"10.1101/gr.1910904 We present a large-scale analysis of mRNA coexpression based on 60 large human data sets containing a total of 3924 microarrays. We sought pairs of genes that were reliably coexpressed (based on the correlation of their expression profiles) in multiple data sets, establishing a high-confidence network of 8805 genes connected by 220,649 âcoexpression linksâ that are observed in at least three data sets. Confirmed positive correlations between genes were much more common than confirmed negative correlations. We show that confirmation of coexpression in multiple data sets is correlated with functional relatedness, and show how cluster analysis of the network can reveal functionally coherent groups of genes. Our findings demonstrate how the large body of accumulated microarray data can be exploited to increase the reliability of inferences about gene function."
556,55,9443,1,"The Connectivity Map: Using Gene-Expression Signatures to Connect Small Molecules, Genes, and Disease","To pursue a systematic approach to the discovery of functional connections among diseases, genetic perturbation, and drug action, we have created the first installment of a reference collection of gene-expression profiles from cultured human cells treated with bioactive small molecules, together with pattern-matching software to mine these data. We demonstrate that this Ã¢ÂÂConnectivity MapÃ¢ÂÂ resource can be used to find connections among small molecules sharing a mechanism of action, chemicals and physiological processes, and diseases and drugs. These results indicate the feasibility of the approach and suggest the value of a large-scale community Connectivity Map project."
557,55,10940,1,From genes to functional classes in the study of biological systems.,"BACKGROUND: With the popularization of high-throughput techniques, the need for procedures that help in the biological interpretation of results has increased enormously. Recently, new procedures inspired in systems biology criteria have started to be developed. RESULTS: Here we present FatiScan, a web-based program which implements a threshold-independent test for the functional interpretation of large-scale experiments that does not depend on the pre-selection of genes based on the multiple application of independent tests to each gene. The test implemented aims to directly test the behaviour of blocks of functionally related genes, instead of focusing on single genes. In addition, the test does not depend on the type of the data used for obtaining significance values, and consequently different types of biologically informative terms (gene ontology, pathways, functional motifs, transcription factor binding sites or regulatory sites from CisRed) can be applied to different classes of genome-scale studies. We exemplify its application in microarray gene expression, evolution and interactomics. CONCLUSION: Methods for gene set enrichment which, in addition, are independent from the original data and experimental design constitute a promising alternative for the functional profiling of genome-scale experiments. A web server that performs the test described and other similar ones can be found at: http://www.babelomics.org."
558,55,11033,1,"Blast2GO: a universal tool for annotation, visualization and analysis in functional genomics research","Summary: We present here Blast2GO (B2G), a research tool designed with the main purpose of enabling Gene Ontology (GO) based data mining on sequence data for which no GO annotation is yet available. B2G joints in one application GO annotation based on similarity searches with statistical analysis and highlighted visualization on directed acyclic graphs. This tool offers a suitable platform for functional genomics research in non-model species. B2G is an intuitive and interactive desktop application that allows monitoring and comprehension of the whole annotation and analysis process. Availability: Blast2GO is freely available via Java Web Start at http://www.blast2go.de Supplementary material: http://www.blast2go.de -> Evaluation Contact: aconesa@ivia.es; stefang@fis.upv.es"
559,55,12138,1,KAAS: an automatic genome annotation and pathway reconstruction server.,"The number of complete and draft genomes is rapidly growing in recent years, and it has become increasingly important to automate the identification of functional properties and biological roles of genes in these genomes. In the KEGG database, genes in complete genomes are annotated with the KEGG orthology (KO) identifiers, or the K numbers, based on the best hit information using SmithâWaterman scores as well as by the manual curation. Each K number represents an ortholog group of genes, and it is directly linked to an object in the KEGG pathway map or the BRITE functional hierarchy. Here, we have developed a web-based server called KAAS (KEGG Automatic Annotation Server: http://www.genome.jp/kegg/kaas/) i.e. an implementation of a rapid method to automatically assign K numbers to genes in the genome, enabling reconstruction of KEGG pathways and BRITE hierarchies. The method is based on sequence similarities, bi-directional best hit information and some heuristics, and has achieved a high degree of accuracy when compared with the manually curated KEGG GENES database."
560,55,13550,1,High-throughput functional annotation and data mining with the Blast2GO suite,"Functional genomics technologies have been widely adopted in the biological research of both model and non-model species. An efficient functional annotation of DNA or protein sequences is a major requirement for the successful application of these approaches as functional information on gene products is often the key to the interpretation of experimental results. Therefore, there is an increasing need for bioinformatics resources which are able to cope with large amount of sequence data, produce valuable annotation results and are easily accessible to laboratories where functional genomics projects are being undertaken. We present the Blast2GO suite as an integrated and biologist-oriented solution for the high-throughput and automatic functional annotation of DNA or protein sequences based on the Gene Ontology vocabulary. The most outstanding Blast2GO features are: (i) the combination of various annotation strategies and tools controlling type and intensity of annotation, (ii) the numerous graphical features such as the interactive GO-graph visualization for gene-set function profiling or descriptive charts, (iii) the general sequence management features and (iv) high-throughput capabilities. We used the Blast2GO framework to carry out a detailed analysis of annotation behaviour through homology transfer and its impact in functional genomics research. Our aim is to offer biologists useful information to take into account when addressing the task of functionally characterizing their sequence data. 10.1093/nar/gkn176"
561,55,14174,1,High-throughput sequencing provides insights into genome variation and evolution in Salmonella Typhi.,"Isolates of Salmonella enterica serovar Typhi (Typhi), a human-restricted bacterial pathogen that causes typhoid, show limited genetic variation. We generated whole-genome sequences for 19 Typhi isolates using 454 (Roche) and Solexa (Illumina) technologies. Isolates, including the previously sequenced CT18 and Ty2 isolates, were selected to represent major nodes in the phylogenetic tree. Comparative analysis showed little evidence of purifying selection, antigenic variation or recombination between isolates. Rather, evolution in the Typhi population seems to be characterized by ongoing loss of gene function, consistent with a small effective population size. The lack of evidence for antigenic variation driven by immune selection is in contrast to strong adaptive selection for mutations conferring antibiotic resistance in Typhi. The observed patterns of genetic isolation and drift are consistent with the proposed key role of asymptomatic carriers of Typhi as the main reservoir of this pathogen, highlighting the need for identification and treatment of carriers."
562,56,706,1,NP-complete Problems and Physical Reality,"Can NP-complete problems be solved efficiently in the physical universe? I survey proposals including soap bubbles, protein folding, quantum computing, quantum advice, quantum adiabatic algorithms, quantum-mechanical nonlinearities, hidden variables, relativistic time dilation, analog computing, Malament-Hogarth spacetimes, quantum gravity, closed timelike curves, and âanthropic computing. â The section on soap bubbles even includes some âexperimental â results. While I do not believe that any of the proposals will let us solve NP-complete problems efficiently, I argue that by studying them, we can learn something not only about computation but also about physics. 1"
563,56,3537,1,Complexity Theory for Simpletons,"In this article, we shall describe some of the most interesting topics in the subject of Complexity Theory for a general audience. Anyone with a solid foundation in high school mathematics (with some calculus) and an elementary understanding of computer programming will be able to follow this article. First, we shall describe the P versus NP problem and its significance. Next, we shall describe two other famous mathematics problems, the Collatz 3n+1 Conjecture and the Riemann Hypothesis, and show how the notion of {""}computational irreducibility{""} is important for understanding why no one has, as of yet, solved these two problems."
564,56,9056,1,Discrete mathematics: methods and challenges,"Combinatorics is a fundamental mathematical discipline as well as an essential component of many mathematical areas, and its study has experienced an impressive growth in recent years. One of the main reasons for this growth is the tight connection between Discrete Mathematics and Theoretical Computer Science, and the rapid development of the latter. While in the past many of the basic combinatorial results were obtained mainly by ingenuity and detailed reasoning, the modern theory has grown out of this early stage, and often relies on deep, well developed tools. This is a survey of two of the main general techniques that played a crucial role in the development of modern combinatorics; algebraic methods and probabilistic methods. Both will be illustrated by examples, focusing on the basic ideas and the connection to other areas."
565,57,2630,1,How users assess Web pages for information seeking,"In this article, we investigate the criteria used by online searchers when assessing the relevance of Web pages for information-seeking tasks. Twenty-four participants were given three tasks each, and they indicated the features of Web pages that they used when deciding about the usefulness of the pages in relation to the tasks. These tasks were presented within the context of a simulated work-task situation. We investigated the relative utility of features identified by participants (Web page content, structure, and quality) and how the importance of these features is affected by the type of information-seeking task performed and the stage of the search. The results of this study provide a set of criteria used by searchers to decide about the utility of Web pages for different types of tasks. Such criteria can have implications for the design of systems that use or recommend Web pages."
566,57,6160,1,SLIM: an alternative Web interface for MEDLINE/PubMed searches - a preliminary study.,"Background  With the rapid growth of medical information and the pervasiveness of the Internet, online search and retrieval systems have become indispensable tools in medicine. The progress of Web technologies can provide expert searching capabilities to non-expert information seekers. The objective of the project is to create an alternative search interface for MEDLINE/PubMed searches using JavaScript slider bars. SLIM, or Slider Interface for MEDLINE/PubMed searches, was developed with PHP and JavaScript. Interactive slider bars in the search form controlled search parameters such as limits, filters and MeSH terminologies. Connections to PubMed were done using the Entrez Programming Utilities (E-Utilities). Custom scripts were created to mimic the automatic term mapping process of Entrez. Page generation times for both local and remote connections were recorded.  Results  Alpha testing by developers showed SLIM to be functionally stable. Page generation times to simulate loading times were recorded the first week of alpha and beta testing. Average page generation times for the index page, previews and searches were 2.94 milliseconds, 0.63 seconds and 3.84 seconds, respectively. Eighteen physicians from the US, Australia and the Philippines participated in the beta testing and provided feedback through an online survey. Most users found the search interface user-friendly and easy to use. Information on MeSH terms and the ability to instantly hide and display abstracts were identified as distinctive features.  Conclusion  SLIM can be an interactive time-saving tool for online medical literature research that improves user control and capability to instantly refine and refocus search strategies. With continued development and by integrating search limits, methodology filters, MeSH terms and levels of evidence, SLIM may be useful in the practice of evidence-based medicine."
567,57,6977,1,A new biology for a new century,"Biology today is at a crossroads. The molecular paradigm, which so successfully guided the discipline throughout most of the 20th century, is no longer a reliable guide. Its vision of biology now realized, the molecular paradigm has run its course. Biology, therefore, has a choice to make, between the comfortable path of continuing to follow molecular biology's lead or the more invigorating one of seeking a new and inspiring vision of the living world, one that addresses the major problems in biology that 20th century biology, molecular biology, could not handle and, so, avoided. The former course, though highly productive, is certain to turn biology into an engineering discipline. The latter holds the promise of making biology an even more fundamental science, one that, along with physics, probes and defines the nature of reality. This is a choice between a biology that solely does society's bidding and a biology that is society's teacher. 10.1128/MMBR.68.2.173-186.2004"
568,57,8786,1,Scholarly work and the shaping of digital access,"Abstract 10.1002/asi.20204.abs In the cycle of scholarly communication, scholars play the role of both consumer and contributor of intellectual works within the stores of recorded knowledge. In the digital environment scholars are seeking and using information in new ways and generating new types of scholarly products, many of which are specialized resources for access to research information. These practices have important implications for the collection and organization of digital access resources. Drawing on a series of qualitative studies investigating the information work of scientists and humanities scholars, specific information seeking activities influenced by the Internet and two general modes of information access evident in research practice are identified in this article. These conceptual modes of access are examined in relation to the digital access resources currently being developed by researchers in the humanities and neuroscience. Scholars' modes of access and their âworkingâ and âimplicitâ assemblages of information represent what researchers actually do when gathering and working with research materials and therefore provide a useful framework for the collection and organization of access resources in research libraries."
569,57,12469,1,Web Service Infrastructure for Chemoinformatics,"{The vast increase of pertinent information available to drug discovery scientists means that there is a strong demand for tools and techniques for organizing and intelligently mining this information for manageable human consumption. At Indiana University, we have developed an infrastructure of chemoinformatics Web services that simplifies the access to this information and the computational techniques that can be applied to it. In this paper, we describe this infrastructure, give some examples of its use, and then discuss our plans to use it as a platform for chemoinformatics application development in the future.}"
570,57,13742,1,"PolySearch: a web-based text mining system for extracting relationships between human diseases, genes, mutations, drugs and metabolites.","A particular challenge in biomedical text mining is to find ways of handling 'comprehensive' or 'associative' queries such as 'Find all genes associated with breast cancer'. Given that many queries in genomics, proteomics or metabolomics involve these kind of comprehensive searches we believe that a web-based tool that could support these searches would be quite useful. In response to this need, we have developed the PolySearch web server. PolySearch supports >50 different classes of queries against nearly a dozen different types of text, scientific abstract or bioinformatic databases. The typical query supported by PolySearch is 'Given X, find all Y's' where X or Y can be diseases, tissues, cell compartments, gene/protein names, SNPs, mutations, drugs and metabolites. PolySearch also exploits a variety of techniques in text mining and information retrieval to identify, highlight and rank informative abstracts, paragraphs or sentences. PolySearch's performance has been assessed in tasks such as gene synonym identification, protein-protein interaction identification and disease gene identification using a variety of manually assembled 'gold standard' text corpuses. Its f-measure on these tasks is 88, 81 and 79%, respectively. These values are between 5 and 50% better than other published tools. The server is freely available at http://wishart.biology.ualberta.ca/polysearch."
571,57,14254,1,A Semantic Web Management Model for Integrative Biomedical Informatics,"BACKGROUND: Data, data everywhere. The diversity and magnitude of the data generated in the Life Sciences defies automated articulation among complementary efforts. The additional need in this field for managing property and access permissions compounds the difficulty very significantly. This is particularly the case when the integration involves multiple domains and disciplines, even more so when it includes clinical and high throughput molecular data. METHODOLOGY/PRINCIPAL FINDINGS: The emergence of Semantic Web technologies brings the promise of meaningful interoperation between data and analysis resources. In this report we identify a core model for biomedical Knowledge Engineering applications and demonstrate how this new technology can be used to weave a management model where multiple intertwined data structures can be hosted and managed by multiple authorities in a distributed management infrastructure. Specifically, the demonstration is performed by linking data sources associated with the Lung Cancer SPORE awarded to The University of Texas MD Anderson Cancer Center at Houston and the Southwestern Medical Center at Dallas. A software prototype, available with open source at www.s3db.org, was developed and its proposed design has been made publicly available as an open source instrument for shared, distributed data management. CONCLUSIONS/SIGNIFICANCE: The Semantic Web technologies have the potential to addresses the need for distributed and evolvable representations that are critical for systems Biology and translational biomedical research. As this technology is incorporated into application development we can expect that both general purpose productivity software and domain specific software installed on our personal computers will become increasingly integrated with the relevant remote resources. In this scenario, the acquisition of a new dataset should automatically trigger the delegation of its analysis."
572,57,14269,1,"Towards a cyberinfrastructure for the biological sciences: progress, visions and challenges","Wiki pages and commentingBiology is an information-driven science. Large-scale data sets from genomics, physiology, population genetics and imaging are driving research at a dizzying rate. Simultaneously, interdisciplinary collaborations among experimental biologists, theorists, statisticians and computer scientists have become the key to making effective use of these data sets. However, too many biologists have trouble accessing and using these electronic data sets and tools effectively. A 'cyberinfrastructure' is a combination of databases, network protocols and computational services that brings people, information and computational tools together to perform science in this information-driven world. This article reviews the components of a biological cyberinfrastructure, discusses current and pending implementations, and notes the many challenges that lie ahead."
573,57,15224,1,Clickstream Data Yields High-Resolution Maps of Science,"<sec> <title>Background</title> <p>Intricate maps of science have been created from citation data to visualize the structure of scientific activity. However, most scientific publications are now accessed online. Scholarly web portals record detailed log data at a scale that exceeds the number of all existing citations combined. Such log data is recorded immediately upon publication and keeps track of the sequences of user requests (clickstreams) that are issued by a variety of users across many different domains. Given these advantages of log datasets over citation data, we investigate whether they can produce high-resolution, more current maps of science.</p> </sec><sec> <title>Methodology</title> <p>Over the course of 2007 and 2008, we collected nearly 1 billion user interactions recorded by the scholarly web portals of some of the most significant publishers, aggregators and institutional consortia. The resulting reference data set covers a significant part of world-wide use of scholarly web portals in 2006, and provides a balanced coverage of the humanities, social sciences, and natural sciences. A journal clickstream model, i.e. a first-order Markov chain, was extracted from the sequences of user interactions in the logs. The clickstream model was validated by comparing it to the Getty Research Institute's Architecture and Art Thesaurus. The resulting model was visualized as a journal network that outlines the relationships between various scientific domains and clarifies the connection of the social sciences and humanities to the natural sciences.</p> </sec><sec> <title>Conclusions</title> <p>Maps of science resulting from large-scale clickstream data provide a detailed, contemporary view of scientific activity and correct the underrepresentation of the social sciences and humanities that is commonly found in citation data.</p> </sec>"
574,57,15263,1,Semantic web for integrated network analysis in biomedicine,"The Semantic Web technology enables integration of heterogeneous data on the World Wide Web by making the semantics of data explicit through formal ontologies. In this article, we survey the feasibility and state of the art of utilizing the Semantic Web technology to represent, integrate and analyze the knowledge in various biomedical networks. We introduce a new conceptual framework, semantic graph mining, to enable researchers to integrate graph mining with ontology reasoning in network data analysis. Through four case studies, we demonstrate how semantic graph mining can be applied to the analysis of disease-causal genes, Gene Ontology category cross-talks, drug efficacy analysis and herb-drug interactions analysis. 10.1093/bib/bbp002"
575,57,15678,1,Data publication: towards a database of everything,"ABSTRACT: The fabric of science is changing, driven by a revolution in digital technologies that facilitate the acquisition and communication of massive amounts of data. This is changing the nature of collaboration and expanding opportunities to participate in science. If digital technologies are the engine of this revolution, digital data are its fuel. But for many scientific disciplines, this fuel is in short supply. The publication of primary data is not a universal or mandatory part of science, and despite policies and proclamations to the contrary, calls to make data publicly available have largely gone unheeded. In this short essay I consider why, and explore some of the challenges that lie ahead, as we work toward a database of everything."
576,57,16058,1,"Gestores de referencias de Ãºltima generaciÃ³n: anÃ¡lisis comparativo de RefWorks, EndNote Web y Zotero","Reference managing tools are one of the most useful devices for researchers and librarians due to their ability to compile, store and format information related to different products, sources and types of records. In recent years a new generation of reference-managing software has appeared. These new tools include applications from the new technological context that have contributed to reinforcing their capacity and potential. An overview of these tools and their applications is offered. We also makes a comparative analysis of the different products with a view to highlighting their strengths as well as the elements that could be improved in each tool."
577,57,16061,1,Web semÃ¡ntica y ontologÃ­as en el procesamiento de la informaciÃ³n documental,"La carencia de un modelo bien definido de representaciÃ³n de la informaciÃ³n en la web ha traÃ­do consigo problemas de cara a diversos aspectos relacionados con su procesamiento. Para intentar solucionarlos, el W3C, organismo encargado de guiar la evoluciÃ³n de la web, ha propuesto su transformaciÃ³n hacia una nueva web denominada web semÃ¡ntica. En este trabajo se presentan las posibilidades que ofrece este nuevo escenario, asÃ­ como las dificultades para su consecuciÃ³n, prestando especial atenciÃ³n a las ontologÃ­as, herramientas de representaciÃ³n del conocimiento fundamentales para la web semÃ¡ntica. Por Ãºltimo, se analiza el papel del profesional de la biblioteconomÃ­a y documentaciÃ³n en este nuevo entorno.The lack of a well defined model of information representation on the web has produced several problems related to processing information. In an effort to resolve these problems, the W3C has proposed the semantic web project. This new scenario offers both possibilities and difficulties for the future. Special attention is given to ontologies, fundamental tools for the representation of knowledge on the semantic web. Finally, the role of library and information professionals is considered in this new context."
578,57,16484,1,<i>CiteULike</i> y <i>Connotea</i>: herramientas 2.0 para el descubrimiento de la informaciÃ³n cientÃ­fica,"Social reference managers automate repetitive and tedious tasks such as literature management, offering an alternative to search engines and traditional databases for social mediation and scientific discovery. In this study we reflect upon the implications of social tagging processes for personal bibliographic management in the 2.0 environment, and we study two of the most famous applications, although still little known and employed in Spain: CiteULike y Connotea."
579,57,16690,1,A study of Web 2.0 applications in library websites,"Web 2.0 represents an emerging suite of applications that hold immense potential in enriching communication, enabling collaboration and fostering innovation. However, little work has been done hitherto to research Web 2.0 applications in library websites. This paper addresses the following three research questions: (a) To what extent are Web 2.0 applications prevalent in libraries?; (b) In what ways have Web 2.0 applications been used in libraries?; and (c) Does the presence of Web 2.0 applications enhance the quality of library websites? Divided equally between public and academic, 120 libraries' websites from North America, Europe and Asia were sampled and analyzed using a three-step content analysis method. The findings suggest that the order of popularity of Web 2.0 applications implemented in libraries is: blogs, RSS, instant messaging, social networking services, wikis, and social tagging applications. Also, libraries have recognized how different Web 2.0 applications can be used complementarily to increase the level of user engagement. Finally, the presence of Web 2.0 applications was found to be associated with the overall quality, and in particular, service quality of library websites. This paper concludes by highlighting implications for both librarians and scholars interested to delve deeper into the implementation of Web 2.0 applications."
580,58,4676,1,Human gaze control during real-world scene perception,"In human vision, acuity and color sensitivity are best at the point of fixation, and the visual-cognitive system exploits this fact by actively controlling gaze to direct fixation towards important and informative scene regions in real time as needed. How gaze control operates over complex real-world scenes has recently become of central concern in several core cognitive science disciplines including cognitive psychology, visual neuroscience, and machine vision. This article reviews current approaches and empirical findings in human gaze control during real-world scene perception."
581,58,8000,1,Gamma (40-100 Hz) oscillation in the hippocampus of the behaving rat,"The cellular generation and spatial distribution of gamma frequency (40-100 Hz) activity was examined in the hippocampus of the awake rat. Field potentials and unit activity were recorded by multiple site silicon probes (5- and 16-site shanks) and wire electrode arrays. Gamma waves were highly coherent along the long axis of the dentate hilus, but average coherence decreased rapidly in the CA3 and CA1 directions. Analysis of short epochs revealed large fluctuations in coherence values between the dentate and CA1 gamma waves. Current source density analysis revealed large sinks and sources in the dentate gyrus with spatial distribution similar to the dipoles evoked by stimulation of the perforant path. The frequency changes of gamma and theta waves positively correlated (40-100 Hz and 5-10 Hz, respectively). Putative interneurons in the dentate gyrus discharged at gamma frequency and were phase-locked to the ascending part of the gamma waves recorded from the hilus. Following bilateral lesion of the entorhinal cortex the power and frequency of hilar gamma activity significantly decreased or disappeared. Instead, a large amplitude but slower gamma pattern (25-50 Hz) emerged in the CA3-CA1 network. We suggest that gamma oscillation emerges from an interaction between intrinsic oscillatory properties of interneurons and the network properties of the dentate gyrus. We also hypothesize that under physiological conditions the hilar gamma oscillation may be entrained by the entorhinal rhythm and that gamma oscillation in the CA3-CA1 circuitry is suppressed by either the hilar region or the entorhinal cortex."
582,58,11667,1,Cross-frequency coupling between neuronal oscillations," Electrophysiological recordings in animals, including humans, are modulated by oscillatory activities in several frequency bands. Little is known about how oscillations in various frequency bands interact. Recent findings from the human neocortex show that the power of fast gamma oscillations (30â150Â Hz) is modulated by the phase of slower theta oscillations (5â8Â Hz). Given that this coupling reflects a specific interplay between large ensembles of neurons, it is likely to have profound implications for neuronal processing."
583,58,12297,1,Adaptive Coevolutionary Networks &#45;&#45; A Review,"10.1098/rsif.2007.1229 Adaptive networks appear in many biological applications. They combine topological evolution of the network with dynamics in the network nodes. Recently, the dynamics of adaptive networks has been investigated in a number of parallel studies from different fields, ranging from genomics to game theory. Here we review these recent developments and show that they can be viewed from a unique angle. We demonstrate that all these studies are characterized by common themes, most prominently: complex dynamics and robust topological self-organization based on simple local rules."
584,58,13181,1,Rapid Neural Coding in the Retina with Relative Spike Latencies,"Natural vision is a highly dynamic process. Frequent body, head, and eye movements constantly bring new images onto the retina for brief periods, challenging our understanding of the neural code for vision. We report that certain retinal ganglion cells encode the spatial structure of a briefly presented image in the relative timing of their first spikes. This code is found to be largely invariant to stimulus contrast and robust to noisy fluctuations in response latencies. Mechanistically, the observed response characteristics result from different kinetics in two retinal pathways (""ON"" and ""OFF"") that converge onto ganglion cells. This mechanism allows the retina to rapidly and reliably transmit new spatial information with the very first spikes emitted by a neural population. 10.1126/science.1149639"
585,58,14090,1,"Reliability, synchrony and noise.","The brain is noisy. Neurons receive tens of thousands of highly fluctuating inputs and generate spike trains that appear highly irregular. Much of this activity is spontaneous - uncoupled to overt stimuli or motor outputs - leading to questions about the functional impact of this noise. Although noise is most often thought of as disrupting patterned activity and interfering with the encoding of stimuli, recent theoretical and experimental work has shown that noise can play a constructive role - leading to increased reliability or regularity of neuronal firing in single neurons and across populations. These results raise fundamental questions about how noise can influence neural function and computation."
586,58,14279,1,Interhemispheric correlations of slow spontaneous neuronal fluctuations revealed in human sensory cortex.,"Animal studies have shown robust electrophysiological activity in the sensory cortex in the absence of stimuli or tasks. Similarly, recent human functional magnetic resonance imaging (fMRI) revealed widespread, spontaneously emerging cortical fluctuations. However, it is unknown what neuronal dynamics underlie this spontaneous activity in the human brain. Here we studied this issue by combining bilateral single-unit, local field potentials (LFPs) and intracranial electrocorticography (ECoG) recordings in individuals undergoing clinical monitoring. We found slow (&lt;0.1 Hz, following 1/f-like profiles) spontaneous fluctuations of neuronal activity with significant interhemispheric correlations. These fluctuations were evident mainly in neuronal firing rates and in gamma (40-100 Hz) LFP power modulations. Notably, the interhemispheric correlations were enhanced during rapid eye movement and stage 2 sleep. Multiple intracranial ECoG recordings revealed clear selectivity for functional networks in the spontaneous gamma LFP power modulations. Our results point to slow spontaneous modulations in firing rate and gamma LFP as the likely correlates of spontaneous fMRI fluctuations in the human sensory cortex."
587,58,15932,1,Generating Coherent Patterns of Activity from Chaotic Neural Networks," Summary Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated."
588,58,16409,1,The Asynchronous State in Cortical Circuits,"Correlated spiking is often observed in cortical circuits, but its functional role is controversial. It is believed that correlations are a consequence of shared inputs between nearby neurons and could severely constrain information decoding. Here we show theoretically that recurrent neural networks can generate an asynchronous state characterized by arbitrarily low mean spiking correlations despite substantial amounts of shared input. In this state, spontaneous fluctuations in the activity of excitatory and inhibitory populations accurately track each other, generating negative correlations in synaptic currents which cancel the effect of shared input. Near-zero mean correlations were seen experimentally in recordings from rodent neocortex in vivo. Our results suggest a reexamination of the sources underlying observed correlations and their functional consequences for information processing. 10.1126/science.1179850"
589,59,930,1,Statistical Modeling: The Two Cultures,"Abstract. There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated bya given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical communityhas been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theoryand practice, has developed rapidlyin fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move awayfrom exclusive dependence on data models and adopt a more diverse set of tools. 1."
590,59,1189,1,Pattern Recognition and Neural Networks,"{This book uses tools from statistical decision theory and computational learning theory to create  a rigorous foundation for the theory of neural networks. On the theoretical side, <I>Pattern Recognition and  Neural Networks</I> emphasizes probability and statistics. Almost all the results have proofs that are often  original. On the application side, the emphasis is on pattern recognition. Most of the examples are from real  world problems. In addition to the more common types of networks, the book has chapters on decision trees  and belief networks from the machine-learning field. This book is intended for use in graduate courses that  teach statistics and engineering. A strong background in statistics is needed to fully appreciate the  theoretical developments and proofs. However, undergraduate-level linear algebra, calculus, and  probability knowledge is sufficient to follow the book.} {Ripley brings together two crucial ideas in pattern recognition: statistical methods and machine learning via neural networks. He brings unifying principles to the fore, and reviews the state of the subject. Ripley also includes many examples to illustrate real problems in pattern recognition and how to overcome them.}"
591,59,4923,1,The earth is round (pâ<â.05),"After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H-sub-0 is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H-sub-0 one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication."
592,59,6638,1,Numerical Recipes in Fortran 77: The Art of Scientific Computing,"This is the greatly revised and greatly expanded Second Edition of the hugely popular Numerical Recipes: The Art of Scientific Computing. The product of a unique collaboration among four leading scientists in academic research and industry Numerical Recipes is a complete text and reference book on scientific computing. In a self-contained manner it proceeds from mathematical and theoretical considerations to actual practical computer routines. With over 100 new routines bringing the total to well over 300, plus upgraded versions of the original routines, this new edition remains the most practical, comprehensive handbook of scientific computing available today. Highlights of the new material include: -A new chapter on integral equations and inverse methods -Multigrid and other methods for solving partial differential equations -Improved random number routines - Wavelet transforms -The statistical bootstrap method -A new chapter on ""less-numerical"" algorithms including compression coding and arbitrary precision arithmetic. The book retains the informal easy-to-read style that made the first edition so popular, while introducing some more advanced topics. It is an ideal textbook for scientists and engineers and an indispensable reference for anyone who works in scientific computing. The Second Edition is availabe in FORTRAN, the traditional language for numerical calculations and in the increasingly popular C language."
593,59,6921,1,Missing Data,"Sooner or later anyone who does statistical analysis runs into problems with missing data in which information for some variables is missing for some cases. Why is this a problem? Because most statistical methods presume that every case has information on all the variables to be included in the analysis. Using numerous examples and practical tips, this book offers a nontechnical explanation of the standard methods for missing data (such as listwise or casewise deletion) as well as two newer (and, better) methods, maximum likelihood and multiple imputation.   Anyone who has been relying on ad-hoc methods that are statistically inefficient or biased will find this book a welcome and accessible solution to their problems with handling missing data."
594,60,1204,1,{Maximum likelihood from incomplete data via the EM algorithm},"{A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.}"
595,60,6731,1,Mapping mendelian factors underlying quantitative traits using RFLP linkage maps,"The advent of complete genetic linkage maps consisting of codominant DNA markers [typically restriction fragment length polymorphisms (RFLPs)] has stimulated interest in the systematic genetic dissection of discrete Mendelian factors underlying quantitative traits in experimental organisms. We describe here a set of analytical methods that modify and extend the classical theory for mapping such quantitative trait loci (QTLs). These include: (i) a method of identifying promising crosses for QTL mapping by exploiting a classical formula of SEWALL WRIGHT; (ii) a method (interval mapping) for exploiting the full power of RFLP linkage maps by adapting the approach of LOD score analysis used in human genetics, to obtain accurate estimates of the genetic location and phenotypic effect of QTLs; and (iii) a method (selective genotyping) that allows a substantial reduction in the number of progeny that need to be scored with the DNA markers. In addition to the exposition of the methods, explicit graphs are provided that allow experimental geneticists to estimate, in any particular case, the number of progeny required to map QTLs underlying a quantitative trait."
596,61,248,1,Data clustering: a review,"Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify  cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval."
597,61,9825,1,Empirical Analysis of Detection Cascades of Boosted Classifiers for Rapid Object Detection,"Recently Viola et al. have introduced a rapid object detection scheme based on a boosted cascade of simple feature classifiers. In this paper we introduce and empirically analysis two extensions to their approach: Firstly, a novel set of rotated haar-like features is introduced. These novel features significantly enrich the simple features of [6] and can also be calculated efficiently. With these new rotated features our sample face detector shows off on average a 10% lower false alarm rate at a given hit rate. Secondly, we present a through analysis of different boosting algorithms (namely Discrete, Real and Gentle Adaboost) and weak classifiers on the detection performance and computational complexity. We will see that Gentle Adaboost with small CART trees as base classifiers outperform Discrete Adaboost and stumps. The complete object detection training and detection system as well as a trained face detector are available in the Open Computer Vision Library at sourceforge.net [8]."
598,61,11004,1,An Extended Set of Haar-like Features for Rapid Object Detection,"Recently Viola et al. [5] have introduced a rapid object detection scheme based on a boosted cascade of simple features. In this paper we introduce a novel set of rotated haar-like features, which significantly enrich this basic set of simple haar-like features and which can also be calculated very efficiently. At a given hit rate our sample face detector shows off on average a 10 % lower false alarm rate by means of using these additional rotated features. We also present a novel post optimization procedure for a given boosted cascade improving on average the false alarm rate further by 12.5%. Using both enhancements the number of false detections is only 24 at a hit rate of 82.3 % on the CMU face set [7]. 1"
599,62,5453,1,Formation of a Motor Memory by Action Observation,"Mirror neurons discharge with both action observation and action execution. It has been proposed that the mirror neuron system is instrumental in motor learning. The human primary motor cortex (M1) displays mirror activity in response to movement observation, is capable of forming motor memories, and is involved in motor learning. However, it is not known whether movement observation can lead directly to the formation of motor memories in the M1, which is considered a likely physiological step in motor learning. Here, we used transcranial magnetic stimulation (TMS) to show that observation of another individual performing simple repetitive thumb movements gives rise to a kinematically specific memory trace of the observed motions in M1. An extended period of observation of thumb movements that were oriented oppositely to the previously determined habitual directional bias increased the probability of TMS-evoked thumb movements to fall within the observed direction. Furthermore, the acceleration of TMS-evoked thumb movements along the principal movement axis and the balance of excitability of muscle representations active in the observed movements were altered in favor of the observed movement direction. These findings support a role for the mirror neuron system in memory formation and possibly human motor learning."
600,62,8205,1,Seeing it differently: visual processing in autism.,"Several recent behavioral and neuroimaging studies have documented an impairment in face processing in individuals with Autism Spectrum Disorder (ASD). It remains unknown, however, what underlying mechanism gives rise to this face processing difficulty. One theory suggests that the difficulty derives from a pervasive problem in social interaction and/or motivation. An alternative view proposes that the face-processing problem is not entirely social in nature and that a visual perceptual impairment might also contribute. The focus of this review is on this latter, perceptual perspective, documenting the psychological and neural alterations that might account for the face processing impairment. The available evidence suggests that perceptual alterations are present in ASD, independent of social function."
601,62,11038,1,Strong Association of De Novo Copy Number Mutations with Autism,"We tested the hypothesis that de novo copy number variation (CNV) is associated with autism spectrum disorders (ASDs). We performed comparative genomic hybridization (CGH) on the genomic DNA of patients and unaffected subjects to detect copy number variants not present in their respective parents. Candidate genomic regions were validated by higher-resolution CGH, paternity testing, cytogenetics, fluorescence in situ hybridization, and microsatellite genotyping. Confirmed de novo CNVs were significantly associated with autism (P = 0.0005). Such CNVs were identified in 12 out of 118 (10%) of patients with sporadic autism, in 2 out of 77 (3%) of patients with an affected first-degree relative, and in 2 out of 196 (1%) of controls. Most de novo CNVs were smaller than microscopic resolution. Affected genomic regions were highly heterogeneous and included mutations of single genes. These findings establish de novo germline mutation as a more significant risk factor for ASD than previously recognized. 10.1126/science.1138659"
602,62,11711,1,Network and intrinsic cellular mechanisms underlying theta phase precession of hippocampal neurons,"Hippocampal `place cells' systematically shift their phase of firing in relation to the theta rhythm as an animal traverses the `place field'. These dynamics imply that the neural ensemble begins each theta cycle at a point in its state-space that might `represent' the current location of the rat, but that the ensemble `looks ahead' during the rest of the cycle. Phase precession could result from intrinsic cellular dynamics involving interference of two oscillators of different frequencies, or from network interactions, similar to Hebb's `phase sequence' concept, involving asymmetric synaptic connections. Both models have difficulties accounting for all of the available experimental data, however. A hybrid model, in which the look-ahead phenomenon implied by phase precession originates in superficial entorhinal cortex by some form of interference mechanism and is enhanced in the hippocampus proper by asymmetric synaptic plasticity during sequence encoding, seems to be consistent with available data, but as yet there is no fully satisfactory theoretical account of this phenomenon. This review is part of the INMED/TINS special issue Physiogenic and pathogenic oscillations: the beauty and the beast, based on presentations at the annual INMED/TINS symposium (http://inmednet.com)."
603,62,13217,1,Neuronal oscillations and visual amplification of speech,"It is widely recognized that viewing a speaker's face enhances vocal communication, although the neural substrates of this phenomenon remain unknown. We propose that the enhancement effect uses the ongoing oscillatory activity of local neuronal ensembles in the primary auditory cortex. Neuronal oscillations reflect rhythmic shifting of neuronal ensembles between high and low excitability states. Our hypothesis holds that oscillations are `predictively' modulated by visual input, so that related auditory input arrives during a high excitability phase and is thus amplified. We discuss the anatomical substrates and key timing parameters that enable and constrain this effect. Our hypothesis makes testable predictions for future studies and emphasizes the idea that `background' oscillatory activity is instrumental to cortical sensory processing."
604,62,13291,1,A central circuit of the mind," The methodologies of cognitive architectures and functional magnetic resonance imaging can mutually inform each other. For example, four modules of the ACT-R (adaptive control of thought â rational) cognitive architecture have been associated with four brain regions that are active in complex tasks. Activity in a lateral inferior prefrontal region reflects retrieval of information in a declarative module; activity in a posterior parietal region reflects changes to problem representations in an imaginal module; activity in the anterior cingulate cortex reflects the updates of control information in a goal module; and activity in the caudate nucleus reflects execution of productions in a procedural module. Differential patterns of activation in such central regions can reveal the time course of different components of complex cognition."
605,63,149,1,Expertise recommender: a flexible recommendation system and architecture,"Locating the expertise necessary to solve difficult problems is a nuanced social and collaborative problem. In organizations, some people assist others in locating expertise by making referrals. People who make referrals fill key organizational roles that have been identified by CSCW and affiliated research. Expertise locating systems are not designed to replace people who fill these key organizational roles. Instead, expertise locating systems attempt to decrease workload and support people who have no other options. Recommendation systems are collaborative software that can be applied to expertise locating. This work describes a general recommendation architecture that is grounded in a field study of expertise locating. Our expertise recommendation system details the work necessary to fit expertise recommendation to a work setting. The architecture and implementation begin to tease apart the technical aspects of providing good recommendations from social and collaborative concerns."
606,63,918,1,Linked: How Everything Is Connected to Everything Else and What It Means,"How is the human brain like the AIDS epidemic? Ask physicist Albert-LÃ¡szlÃ³ BarabÃ¡si and he'll explain them both in terms of networks of individual nodes connected via complex but understandable relationships. _Linked: The New Science of Networks_ is his bright, accessible guide to the fundamentals underlying neurology, epidemiology, Internet traffic, and many other fields united by complexity.  BarabÃ¡si's gift for concrete, nonmathematical explanations and penchant for eccentric humor would make the book thoroughly enjoyable even if the content weren't engaging. But the results of BarabÃ¡si's research into the behavior of networks are deeply compelling. Not all networks are created equal, he says, and he shows how even fairly robust systems like the Internet could be crippled by taking out a few super-connected nodes, or hubs. His mathematical descriptions of this behavior are helping doctors, programmers, and security professionals design systems better suited to their needs. _Linked_ presents the next step in complexity theory--from understanding chaos to practical applications. _--Rob Lightner_"
607,63,2754,1,The explicit economics of knowledge codification and tacitness,"This paper attempts a greater precision and clarity of understanding concerning the nature and economic significance of knowledge and its variegated forms by presenting 'the skeptical economist's guide to 'tacit knowledge''. It critically reconsiders the ways in which the concepts of tacitness and codification have come to be employed by economists and develops a more coherent re-conceptualization of these aspects of knowledge production and distribution activities. It seeks also to show that a proposed alternative framework for the study of knowledge codification activities offers a more useful guide for further research directed to informing public policies for science, technological innovation and long-run economic growth. 10.1093/icc/9.2.211"
608,64,13217,1,Neuronal oscillations and visual amplification of speech,"It is widely recognized that viewing a speaker's face enhances vocal communication, although the neural substrates of this phenomenon remain unknown. We propose that the enhancement effect uses the ongoing oscillatory activity of local neuronal ensembles in the primary auditory cortex. Neuronal oscillations reflect rhythmic shifting of neuronal ensembles between high and low excitability states. Our hypothesis holds that oscillations are `predictively' modulated by visual input, so that related auditory input arrives during a high excitability phase and is thus amplified. We discuss the anatomical substrates and key timing parameters that enable and constrain this effect. Our hypothesis makes testable predictions for future studies and emphasizes the idea that `background' oscillatory activity is instrumental to cortical sensory processing."
609,64,13443,1,Entrainment of Neuronal Oscillations as a Mechanism of Attentional Selection,"Whereas gamma-band neuronal oscillations clearly appear integral to visual attention, the role of lower-frequency oscillations is still being debated. Mounting evidence indicates that a key functional property of these oscillations is the rhythmic shifting of excitability in local neuronal ensembles. Here, we show that when attended stimuli are in a rhythmic stream, delta-band oscillations in the primary visual cortex entrain to the rhythm of the stream, resulting in increased response gain for task-relevant events and decreased reaction times. Because of hierarchical cross-frequency coupling, delta phase also determines momentary power in higher-frequency activity. These instrumental functions of low-frequency oscillations support a conceptual framework that integrates numerous earlier findings. 10.1126/science.1154735"
610,64,15861,1,Experience sampling during fMRI reveals default network and executive system contributions to mind wandering,"10.1073/pnas.0900234106 Although mind wandering occupies a large proportion of our waking life, its neural basis and relation to ongoing behavior remain controversial. We report an fMRI study that used experience sampling to provide an online measure of mind wandering during a concurrent task. Analyses focused on the interval of time immediately preceding experience sampling probes demonstrate activation of default network regions during mind wandering, a finding consistent with theoretical accounts of default network functions. Activation in medial prefrontal default network regions was observed both in association with subjective self-reports of mind wandering and an independent behavioral measure (performance errors on the concurrent task). In addition to default network activation, mind wandering was associated with executive network recruitment, a finding predicted by behavioral theories of off-task thought and its relation to executive resources. Finally, neural recruitment in both default and executive network regions was strongest when subjects were unaware of their own mind wandering, suggesting that mind wandering is most pronounced when it lacks meta-awareness. The observed parallel recruitment of executive and default network regionsÃ¢ÂÂtwo brain systems that so far have been assumed to work in oppositionÃ¢ÂÂsuggests that mind wandering may evoke a unique mental state that may allow otherwise opposing networks to work in cooperation. The ability of this study to reveal a number of crucial aspects of the neural recruitment associated with mind wandering underscores the value of combining subjective self-reports with online measures of brain function for advancing our understanding of the neurophenomenology of subjective experience."
611,65,1768,1,The magical number seven plus or minus two: some limits on our capacity for processing information,"My problem is that I have been persecuted by an integer. For seven years this number has followed me around, has intruded in my most private data, and has assaulted me from the pages of our most public journals. This number assumes a variety of disguises, being sometimes a little larger and sometimes a little smaller than usual, but never changing so much as to be unrecognizable. The persistence with which this number plagues me is far more than a random accident. There is, to quote a famous senator, a design behind it, some pattern governing its appearances. Either there really is something unusual about the number or else I am suffering from delusions of persecution.  I shall begin my case history by telling you about some experiments that tested how accurately people can assign numbers to the magnitudes of various aspects of a stimulus. In the traditional language of psychology these would be called experiments in absolute judgment. Historical accident, however, has decreed that they should have another name. We now call them experiments on the capacity of people to transmit information. Since these experiments would not have been done without the appearance of information theory on the psychological scene, and since the results are analyzed in terms of the concepts of information theory, I shall have to preface my discussion with a few remarks about this theory."
612,65,5685,1,Current practice in measuring usability: Challenges to usability studies and research,"How to measure usability is an important question in HCI research and user interface evaluation. We review current practice in measuring usability by categorizing and discussing usability measures from 180 studies published in core HCI journals and proceedings. The discussion distinguish several problems with the measures, including whether they actually measure usability, if they cover usability broadly, how they are reasoned about, and if they meet recommendations on how to measure usability. In many studies, the choice of and reasoning about usability measures fall short of a valid and reliable account of usability as quality-in-use of the user interface being studied. Based on the review, we discuss challenges for studies of usability and for research into how to measure usability. The challenges are to distinguish and empirically compare subjective and objective measures of usability; to focus on developing and employing measures of learning and retention; to study long-term use and usability; to extend measures of satisfaction beyond post-use questionnaires; to validate and standardize the host of subjective satisfaction questionnaires used; to study correlations between usability measures as a means for validation; and to use both micro and macro tasks and corresponding measures of usability. In conclusion, we argue that increased attention to the problems identified and challenges discussed may strengthen studies of usability and usability research."
613,65,9913,1,Damaged Merchandise? A Review of Experiments That Compare Usability Evaluation Methods,"An interest in the design of interfaces has been a core topic for researchers and practitioners in the field of human-computer interaction (HCI); an interest in the design of experiments has not. To the extent that reliable and valid guidance for the former depends on the results of the latter, it is necessary that researchers and practitioners understand how small features of an experimental design can cast large shadows over the results and conclusions that can be drawn. In this review we examine the design of 5 experiments that compared usability evaluation methods (UEMs). Each has had an important influence on HCI thought and practice. Unfortunately, our examination shows that small problems in the way these experiments were designed and conducted call into serious question what we thought we knew regarding the efficacy of various UEMs. If the influence of these experiments were trivial, then such small problems could be safely ignored. Unfortunately, the outcomes of these experiments have been used to justify advice to practitioners regarding their choice of UEMs. Making such choices based on misleading or erroneous claims can be detrimental--compromising the quality and integrity of the evaluation, incurring unnecessary costs, or undermining the practitioner's credibility within the design team. The experimental method is a potent vehicle that can help inform the choice of a UEM as well as help to address other HCI issues. However, to obtain the desired outcomes, close attention must be paid to experimental design."
614,65,14735,1,How Colorful Was Your Day? Why Questionnaires Cannot Assess Presence in Virtual Environments,"This paper argues that a scientific basis for âpresenceâ as it's usually understood in virtual environments research, can not be established on the basis of postexperience presence questionnaires alone. To illustrate the point, an arbitrary mental attribute called âcolorfulness of the experienceâ is conjured up, and a set of questions administered to 74 respondents with an online questionnaire. The results suggested that colorfulness of yesterday's experiences was associated with the extent to which a person accomplished their tasks, and also associated with yesterday being a âgoodâ, âpleasantâ, but not frustrating day. The meaning lessness of this analysis illustrates that the equivalent methodology used by presence researchers, may, similarly, bring into being the idea of presence in the minds of {VE} participants. However, it is argued that there can be no evidence on this methodological basis that presence played any role in their actual mental activity or behavior at the time of the experience. It is concluded that presence researchers must move away from heavy reliance on questionnaires in order to make any progress in this area."
615,66,122,1,Serendipity and information seeking: an empirical study,"""Serendipity"" has both a classical origin in literature and a more modern manifestation where it is found in the descriptions of the problem solving and knowledge acquisition of humanities and science scholars. Studies of information retrieval and information seeking have also discussed the utility of the notion of serendipity. Some have implied that it may be stimulated, or that certain people may ""encounter"" serendipitous information more than others. All to some extent accept the classical definition of serendipity as a ""fortuitous"" accident. The analysis presented here is part of a larger study concerning the information-seeking behaviour of interdisciplinary scholars. This paper considers the nature of serendipity in information-seeking contexts, and reinterprets the notion of serendipity as a phenomenon arising from both conditions and strategies - as both a purposive and a non-purposive component of information seeking and related knowledge acquisition."
616,66,693,1,Conditional Independence in Statistical Theory,"Some simple heuristic properties of conditional independence are shown to form a conceptual framework for much of the theory of statistical inference. This framework is illustrated by an examination of the role of conditional independence in several diverse areas of the field of statistics. Topics covered include sufficiency and ancillarity, parameter identification, causal inference, prediction sufficiency, data selection mechanisms, invariant statistical models and a subjectivist approach to model-building."
617,66,923,1,"Bayesian Data Analysis, Second Edition (Chapman & Hall/CRC Texts in Statistical Science)","{Incorporating new and updated information, this second edition of THE bestselling text in Bayesian data analysis continues to emphasize practice over theory, describing how to conceptualize, perform, and critique statistical analyses from a Bayesian perspective. Its world-class authors provide guidance on all aspects of Bayesian data analysis and include examples of real statistical analyses, based on their own research, that demonstrate how to solve complicated problems. Changes in the new edition include: &#183;Stronger focus on MCMC&#183;Revision of the computational advice in Part III&#183;New chapters on nonlinear models and decision analysis&#183;Several additional applied examples from the authors' recent research&#183;Additional chapters on current models for Bayesian data analysis such as nonlinear models, generalized linear mixed models, and more&#183;Reorganization of chapters 6 and 7 on model checking and data collectionBayesian computation is currently at a stage where there are many reasonable ways to compute any given posterior distribution. However, the best approach is not always clear ahead of time. Reflecting this, the new edition offers a more pluralistic presentation, giving advice on performing computations from many perspectives while making clear the importance of being aware that there are different ways to implement any given iterative simulation computation. The new approach, additional examples, and updated information make Bayesian Data Analysis an excellent introductory text and a reference that working scientists will use throughout their professional life.}"
618,66,1036,1,Mining the Web: Discovering Knowledge from Hypertext Data,"{Mining the Web: Discovering Knowledge from Hypertext Data is the first book devoted entirely to techniques for producing knowledge from the vast body of unstructured Web data. Building on an initial survey of infrastructural issuesincluding Web crawling and indexingChakrabarti examines low-level machine learning techniques as they relate specifically to the challenges of Web mining. He then devotes the final part of the book to applications that unite infrastructure and analysis to bring machine learning to bear on systematically acquired and stored data. Here the focus is on results: the strengths and weaknesses of these applications, along with their potential as foundations for further progress. From Chakrabarti's workpainstaking, critical, and forward-lookingreaders will gain the theoretical and practical understanding they need to contribute to the Web mining effort.<br><br>* A comprehensive, critical exploration of statistics-based attempts to make sense of Web Mining.<br>* Details the special challenges associated with analyzing unstructured and semi-structured data.<br>* Looks at how classical Information Retrieval techniques have been modified for use with Web data.<br>* Focuses on today's dominant learning methods: clustering and classification, hyperlink analysis, and supervised and semi-supervised learning.<br>* Analyzes current applications for resource discovery and social network analysis.<br>* An excellent way to introduce students to especially vital applications of data mining and machine learning technology.</li></ul>}"
619,66,1442,1,Computability and Logic,"Computability and Logic has become a classic because of its accessibility to students without a mathematical background and because it covers not simply the staple topics of an intermediate logic course, such as Godel's incompleteness theorems, but also a large number of optional topics, from Turing's theory of computability to Ramsey's theorem. Including a selection of exercises, adjusted for this edition, at the end of each chapter, it offers a new and simpler treatment of the representability of recursive functions, a traditional stumbling block for students on the way to the Godel incompleteness theorems."
620,66,1691,1,Principal component analysis,"Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques it continues to be the subject of much research, ranging from new model- based approaches to algorithmic ideas from neural networks. It is extremely versatile with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is Professor of Statistics at the University of Aberdeen. He is author or co-author of over 60 research papers and three other books. His research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years."
621,66,4028,1,Introduction to Machine Learning (Adaptive Computation and Machine Learning),"{The goal of machine learning is to program computers to use example data or past experience to solve a given problem. Many successful applications of machine learning exist already, including systems that analyze past sales data to predict customer behavior, recognize faces or spoken speech, optimize robot behavior so that a task can be completed using minimum resources, and extract knowledge from bioinformatics data. <i>Introduction to Machine Learning</i> is a comprehensive textbook on the subject, covering a broad array of topics not usually included in introductory machine learning texts. It discusses many methods based in different fields, including statistics, pattern recognition, neural networks, artificial intelligence, signal processing, control, and data mining, in order to present a unified treatment of machine learning problems and solutions. All learning algorithms are explained so that the student can easily move from the equations in the book to a computer program. The book can be used by advanced undergraduates and graduate students who have completed courses in computer programming, probability, calculus, and linear algebra. It will also be of interest to engineers in the field who are concerned with the application of machine learning methods.<br /> <br /> After an introduction that defines machine learning and gives examples of machine learning applications, the book covers supervised learning, Bayesian decision theory, parametric methods, multivariate methods, dimensionality reduction, clustering, nonparametric methods, decision trees, linear discrimination, multilayer perceptrons, local models, hidden Markov models, assessing and comparing classification algorithms, combining multiple learners, and reinforcement learning.}"
622,66,4303,1,Probabilistic and Statistical Properties of Words: An Overview,"In the following, an overview is given on statistical and probabilistic properties of words, as occurring in the analysis of biological sequences. Counts of occurrence, counts of clumps, and renewal counts are distinguished, and exact distributions as well as normal approximations, Poisson process approximations, and compound Poisson approximations are derived. Here, a sequence is modelled as a stationary ergodic Markov chain; a test for determining the appropriate order of the Markov chain is described. The convergence results take the error made by estimating the Markovian transition probabilities into account. The main tools involved are moment generating functions, martingales, Stein's method, and the Chen-Stein method. Similar results are given for occurrences of multiple patterns, and, as an example, the problem of unique recoverability of a sequence from SBH chip data is discussed. Special emphasis lies on disentangling the complicated dependence structure between word occurrences, due to self-overlap as well as due to overlap between words. The results can be used to derive approximate, and conservative, confidence intervals for tests."
623,66,5121,1,Ambient Findability: What We Find Changes Who We Become,"{How do you find your way in an age of information overload? How can you filter streams of complex information to pull out only what you want? Why does it matter how information is structured when Google seems to magically bring up the right answer to your questions? What does it mean to be ""findable"" in this day and age?  This eye-opening new book examines the convergence of information and connectivity. Written by Peter Morville, author of the groundbreaking <i>Information Architecture for the World Wide Web</i>, the book defines our current age as a state of unlimited findability. In other words, anyone can find anything at any time. Complete navigability.   <p>  Morville discusses the Internet, GIS, and other network technologies that are coming together to make unlimited findability possible. He explores how the melding of these innovations impacts society, since Web access is now a standard requirement for successful people and businesses. But before he does that, Morville looks back at the history of wayfinding and human evolution, suggesting that our fear of being lost has driven us to create maps, charts, and now, the mobile Internet.</p>  <p>  The book's central thesis is that information literacy, information architecture, and usability are all critical components of this new world order. Hand in hand with that is the contention that only by planning and designing the best possible software, devices, and Internet, will we be able to maintain this connectivity in the future. Morville's book is highlighted with full color illustrations and rich examples that bring his prose to life.</p>  <p>  <i>Ambient Findability</i> doesn't preach or pretend to know all the answers. Instead, it presents research, stories, and examples in support of its novel ideas. Are we truly at a critical point in our evolution where the quality of our digital networks will dictate how we behave as a species? Is findability indeed the primary key to a successful global marketplace in the 21st century and beyond. Peter Morville takes you on a thought-provoking tour of these memes and more -- ideas that will not only fascinate but will stir your creativity in practical ways that you can apply to your work immediately.</p>  <p>  <i>""A lively, enjoyable and informative tour of a topic that's only going to become more important.""</i><br>  --David Weinberger, Author, <i>Small Pieces Loosely Joined</i> and <i>The Cluetrain Manifesto</i></br></p>  <p>  <i>""I envy the young scholar who finds this inventive book, by whatever strange means are necessary. The future isn't just unwritten--it's unsearched.""</i><br>  --Bruce Sterling, Writer, Futurist, and Co-Founder, The Electronic Frontier Foundation</br></p>  <p>  <i>""Search engine marketing is the hottest thing in Internet business, and deservedly so. Ambient Findability puts SEM into a broader context and provides deeper insights into human behavior. This book will help you grow your online business in a world where being found is not at all certain.""</i><br>  --Jakob Nielsen, Ph.D., Author, <i>Designing Web Usability: The Practice of Simplicity</i></br></p>  <p>  <i>""Information that's hard to find will remain information that's hardly found--from one of the fathers of the discipline of information architecture, and one of its most experienced practitioners, come penetrating observations on why findability is elusive and how the act of seeking changes us.""</i><br>  --Steve Papa, Founder and Chairman, Endeca</br></p>  <p>  <i>""Whether it's a fact or a figure, a person or a place, Peter Morville knows how to make it findable. Morville explores the possibilities of a world where everything can always be found--and the challenges in getting there--in this wide-ranging, thought-provoking book.""</i><br>  --Jesse James Garrett, Author, <i>The Elements of User Experience</i></br></p>  <p>  <i>""It is easy to assume that current searching of the World Wide Web is the last word in finding and using information. Peter Morville shows us that search engines are just the beginning. Skillfully weaving together information science research with his own extensive experience, he develops for the reader a feeling for the near future when information is truly findable all around us. There are immense implications, and Morville's lively and humorous writing brings them home.""</i><br>  --Marcia J. Bates, Ph.D., University of California Los Angeles</br></p>  <p>  <i>""I've always known that Peter Morville was smart. After reading Ambient Findability, I now know he's (as we say in Boston) wicked smart. This is a timely book that will have lasting effects on how we create our future.</i><br>  --Jared Spool, Founding Principal, User Interface Engineering</br></p>  <p>  <i>""In Ambient Findability, Peter Morville has put his mind and keyboard on the pulse of the electronic noosphere. With tangible examples and lively writing, he lays out the challenges and wonders of finding our way in cyberspace, and explains the mutually dependent evolution of our changing world and selves. This is a must read for everyone and a practical guide for designers.""</i><br>  --Gary Marchionini, Ph.D., University of North Carolina</br></p>  <p>  <i>""Find this book! Anyone interested in making information easier to find, or understanding how finding and being found is changing, will find this thoroughly researched, engagingly written, literate, insightful and very, very cool book well worth their time. Myriad examples from rich and varied domains and a valuable idea on nearly every page. Fun to read, too!</i><br>  --Joseph Janes, Ph.D., Founder, Internet Public Library</br></p>}"
624,66,6449,1,Scientific Reasoning: The Bayesian Approach,"{In this clearly reasoned defense of Bayes's Theorem &#151; that probability can be used to reasonably justify scientific theories &#151; Colin Howson and Peter Urbach examine the way in which scientists appeal to probability arguments, and demonstrate that the classical approach to statistical inference is full of flaws. Arguing the case for the Bayesian method with little more than basic algebra, the authors show that it avoids the difficulties of the classical system. The book also refutes the major criticisms leveled against Bayesian logic, especially that it is too subjective. This newly updated edition of this classic textbook is also suitable for college courses.}"
625,66,6579,1,A Bayesian Truth Serum for Subjective Data,"Subjective judgments, an essential information source for science and policy, are problematic because there are no public criteria for assessing judgmental truthfulness. I present a scoring method for eliciting truthful subjective data in situations where objective truth is unknowable. The method assigns high scores not to the most common answers but to the answers that are more common than collectively predicted, with predictions drawn from the same population. This simple adjustment in the scoring criterion removes all bias in favor of consensus: Truthful answers maximize expected score even for respondents who believe that their answer represents a minority view. 10.1126/science.1102081"
626,66,9890,1,Factor analysis,"A frequently applied paradigm in analyzing data from multivariate observations is to model the relevant information (represented in a multivariate variable X) as coming from a limited number of latent factors. In a survey on household consumption, for example, the consumption levels, X, of p different goods during one month could be observed. The variations and covariations of the p components of X throughout the survey might in fact be explained by two or three main social behavior factors of the household. For instance, a basic desire of comfort or the willingness to achieve a certain social level or other social latent concepts might explain most of the consumption behavior. These unobserved factors are much more interesting to the social scientist than the observed quantitative measures (X) themselves, because they give a better understanding of the behavior of households. As shown in the examples below, the same kind of factor analysis is of interest in many fields such as psychology, marketing, economics, politic sciences, etc."
627,66,10438,1,The Neural Basis of Loss Aversion in Decision-Making Under Risk,"People typically exhibit greater sensitivity to losses than to equivalent gains when making decisions. We investigated neural correlates of loss aversion while individuals decided whether to accept or reject gambles that offered a 50/50 chance of gaining or losing money. A broad set of areas (including midbrain dopaminergic regions and their targets) showed increasing activity as potential gains increased. Potential losses were represented by decreasing activity in several of these same gain-sensitive areas. Finally, individual differences in behavioral loss aversion were predicted by a measure of neural loss aversion in several regions, including the ventral striatum and prefrontal cortex."
628,66,11707,1,The Neural Basis of Decision Making,"Abstract The study of decision making spans such varied fields as neuroscience, psychology, economics, statistics, political science, and computer science. Despite this diversity of applications, most decisions share common elements including deliberation and commitment. Here we evaluate recent progress in understanding how these basic elements of decision formation are implemented in the brain. We focus on simple decisions that can be studied in the laboratory but emphasize general principles likely to extend to other settings."
629,66,11999,1,Learning the value of information in an uncertain world,"Our decisions are guided by outcomes that are associated with decisions made in the past. However, the amount of influence each past outcome has on our next decision remains unclear. To ensure optimal decision-making, the weight given to decision outcomes should reflect their salience in predicting future outcomes, and this salience should be modulated by the volatility of the reward environment. We show that human subjects assess volatility in an optimal manner and adjust decision-making accordingly. This optimal estimate of volatility is reflected in the fMRI signal in the anterior cingulate cortex (ACC) when each trial outcome is observed. When a new piece of information is witnessed, activity levels reflect its salience for predicting future outcomes. Furthermore, variations in this ACC signal across the population predict variations in subject learning rates. Our results provide a formal account of how we weigh our different experiences in guiding our future actions."
630,66,13155,1,The hippocampus and memory: insights from spatial processing.,"The hippocampus appears to be crucial for long-term episodic memory, yet its precise role remains elusive. Electrophysiological studies in rodents offer a useful starting point for developing models of hippocampal processing in the spatial domain. Here we review one such model that points to an essential role for the hippocampus in the construction of mental images. We explain how this neural-level mechanistic account addresses some of the current controversies in the field, such as the role of the hippocampus in imagery and short-term memory, and discuss its broader implications for the neural bases of episodic memory."
631,66,13352,1,The R Book,"{The high-level language of R is recognized as one of the most powerful and flexible statistical software environments, and is rapidly becoming the standard setting for quantitative analysis, statistics and graphics. R provides free access to unrivalled coverage and cutting-edge applications, enabling the user to apply numerous statistical methods ranging from simple regression to time series or multivariate analysis.   <p>   Building on the success of the authorâs bestselling <i>Statistics: An Introduction using R</i>, <i>The R Book</i> is packed with worked examples, providing an all inclusive guide to R, ideal for novice and more accomplished users alike. The book assumes no background in statistics or computing and introduces the advantages of the R environment, detailing its applications in a wide range of disciplines.    <ul type=""disc"">    <li>Provides the first comprehensive reference manual for the R language, including practical guidance and full coverage of the graphics facilities.    <li>Introduces all the statistical models covered by R, beginning with simple classical tests such as chi-square and t-test.    <li>Proceeds to examine more advance methods, from regression and analysis of variance, through to generalized linear models, generalized mixed models, time series, spatial statistics, multivariate statistics and much more.    </ul>   <p>   <i>The R Book</i> is aimed at undergraduates, postgraduates and professionals in science, engineering and medicine. It is also ideal for students and professionals in statistics, economics, geography and the social sciences.}"
632,67,616,1,Folksonomies - Cooperative Classification and Communication Through Shared Metadata,"This paper examines user-&#8205;generated metadata as implemented and applied in two web services designed to share and organize digital media to better understand grassroots classification. Metadata - data about data - allows systems to collocate related information, and helps users find relevant information. The creation of metadata has generally been approached in two ways: professional creation and author creation. In libraries and other organizations, creating metadata, primarily in the form of catalog records, has traditionally been the domain of dedicated professionals working with complex, detailed rule sets and vocabularies. The primary problem with this approach is scalability and its impracticality for the vast amounts of content being produced and used, especially on the World Wide Web. The apparatus and tools built around professional cataloging systems are generally too complicated for anyone without specialized training and knowledge. A second approach is for metadata to be created by authors. The movement towards creator described documents was heralded by SGML, the WWW, and the Dublin Core Metadata Initiative. There are problems with this approach as well - often due to inadequate or inaccurate description, or outright deception. This paper examines a third approach: user-&#8205;created metadata, where users of the documents and media create metadata for their own individual use that is also shared throughout a community."
633,67,2359,1,Social Bookmarking Tools (II): A Case Study - Connotea,"Connotea [1] is a free online reference management and social bookmarking service for scientists created by Nature Publishing Group [2]. While somewhat experimental in nature, Connotea already has a large and growing number of users, and is a real, fully functioning service [3]. The label 'experimental' is not meant to imply that the service is any way ephemeral or esoteric, rather that the concept of social bookmarking itself and the application of that concept to reference management are both recent developments. Connotea is under active development, and we are still in the process of discovering how people will use it. In addition to Connotea being a free and public service, the core code is freely available under an open source license [4]. Connotea was conceived from the outset as an online, social tool. Seeing the possibilities that del.icio.us [5] was opening up for its users in the area of general web linking, we realised that scholarly reference management was a similar problem space. Connotea was designed and developed late in 2004, and soft-launched at the end of December 2004. Usage has grown over the past several months, to the point where there is now enough data in the system for interesting second-order effects to emerge. This paper will start by giving an overview of Connotea, and will outline the key concepts and describe its main features. We will then take the reader on a brief guided tour, show some of the aforementioned second-order effects, and end with a discussion of Connotea's likely future direction."
634,67,6348,1,Folksonomies: Tidying up Tags?,"1. Introduction A folksonomy is a type of distributed classification system. It is usually created by a group of individuals, typically the resource users. Users add tags to online items, such as images, videos, bookmarks and text. These tags are then shared and sometimes refined. A general review of social bookmarking tools, one popular use area of folksonomies, was given in the April edition of D-Lib [1]. In the article the authors elaborate on the approach taken by social classification systems and the motivators behind tagging. They write, ""...tags are just one kind of metadata and are not a replacement for formal classification systems such as Dublin Core, MODS, etc.... Rather, they are a supplemental means to organise information and order search results."" In this article we look at what makes folksonomies work. We agree with the premise that tags are no replacement for formal systems, but we see this as being the core quality that makes folksonomy tagging so useful. We begin by looking at the issue of ""sloppy tags"", a problem to which critics of folksonomies are keen to allude, and ask if there are ways the folksonomy community could offset such problems and create systems that are conducive to searching, sorting and classifying. We then go on to question this ""tidying up"" approach and its underlying assumptions, highlighting issues surrounding removal of low-quality, redundant or nonsense metadata, and the potential risks of tidying too neatly and thereby losing the very openness that has made folksonomies so popular."
635,67,8706,1,Semantic annotation for knowledge management: Requirements and a survey of the state of the art,"While much of a company's knowledge can be found in text repositories, current content management systems have limited capabilities for structuring and interpreting documents. In the emerging Semantic Web, search, interpretation and aggregation can be addressed by ontology-based semantic mark-up. In this paper, we examine semantic annotation, identify a number of requirements, and review the current generation of semantic annotation systems. This analysis shows that, while there is still some way to go before semantic annotation tools will be able to address fully all the knowledge management needs, research in the area is active and making good progress."
636,67,8929,1,Annotation and Navigation in Semantic Wikis,"Semantic Wikis allow users to semantically annotate their Wiki content. The particular annotations can differ in expressive power, simplicity, and meaning. We present an elaborate conceptual model for semantic annotations, introduce a unique and rich Wiki syntax for these annotations, and discuss how to best formally represent the augmented Wiki content. We improve existing navigation techniques to automat- ically construct faceted browsing for semistructured data. By utilising the Wiki annotations we provide greatly enhanced information retrieval. Further we report on our ongoing development of these techniques in our prototype SemperWiki."
637,67,10972,1,Web 2.0: Werkzeuge fÃ¼r die Wissenschaft,"Um den Begriff Web 2.0 ist ein gigantischer Buzz entstanden. Dabei existieren viele der Methoden, die sich hinter dem Begriff verbergen, schon seit geraumer Zeit. In diesem Artikel, der eine Ausarbeitung des am 23. DV-Treffen der MPG am 15. November 2006 gehaltenen Workshops ist, mÂ¨ochte ich die wichtigsten dieser Methoden vorstellen, um einmal einen Blick hinter den Hype zu werfen. Dabei werde ich auf Begriff wie RSS, AJAX, Mashup, Social Software etc. eingehen und die zugrunde liegenden Werkzeuge vorstellen. Darauf aufbauend wird ein Entwurf einer Peer-2-Peer-Web-2.0-Anwendung vorgestellt, die als Modell fÂ¨ur wissenschaftliche, kollaborative Arbeitsumgebungen dienen kann."
638,67,11217,1,Citeulike: A Researcher's Social Bookmarking Service,"Describes Citeulike, a fusion of Web-based social bookmarking services and traditional bibliographic management tools. The article begins with a discussion of how Citeulike turns the linear 'gather, collect, share' process inherent in academic research into a circular 'gather, collect, share and network' process, enabling the sharing and discovery of academic literature and research papers. The basic functionality of the tool is simple: when a researcher sees a paper on the Web that interests them, they can click a button and have a link to it added to their personal library. Ultimately Citeulike works because it is useful to its users. It automates a repetitive bibliographic management task and it offers a complimentary alternative to search engines and databases of academic literature through socially mediated retrieval and discovery of papers. Adapted from the source document"
639,67,11924,1,Social Software. Formen der Kooperation in computerbasierten Netzwerken,"""Mit Social Software bezeichnet man computernetzwerkgestÃ¼tzte Systeme zur Zusammenarbeit von Teilnehmern. Der Begriff bezieht sich vor allem auf neuere Anwendungen wie Wikis, Weblogs, gemeinsame Fotosammlungen, kollaborativ erstellte Verschlagwortungsseiten und Instant Messaging. Zwei Merkmale sind allen Systemen gemein: zum einen erstellen die Nutzer die Inhalte selbst, zum anderen verfÃ¼gen Social Software Anwendungen Ã¼ber Komponenten zur Herausbildung und StÃ¼tzung von Gemeinschaftsaspekten. In der EinfÃ¼hrung wird ein Ãberblick Ã¼ber Social Software-Anwendungen gegeben. Es werden damit zusammenhÃ¤ngende Herausforderungen an die Medien- und Kommunikationsforschung in diesem Gebiet diskutiert und die einzelnen BeitrÃ¤ge diskutiert."""
640,67,12449,1,Learning with Semantic Wikis,"Abstract. The knowledge society requires life-long learning and flexible learning environments that allow learners to learn whenever they have time, whereever they are, and according to their own needs and background knowledge. In this article, we investigate how Semantic Wikis â a combination of Wiki and Semantic Web technology â can support learners in such flexible learning environments. We first summarise common features of Wikis and Semantic Wikis and then describe different aspects of Semantic Wikis for learning. We also introduce our Semantic Wiki system called IkeWiki and show why it is particularly promising as a learning tool."
641,68,358,1,Tropical Forest Fragments Enhance Pollinator Activity in Nearby Coffee Crops,":&#8194; Crop pollination by wild bees is an ecosystem service of enormous value, but it is under increasing threat from agricultural intensification. As with many ecosystem services, the mechanisms, scales, and species through which crop pollination is provided are too poorly understood to inform land-use decisions. I investigated the role of tropical forest remnants as sources of pollinators to surrounding coffee crops in Costa Rica. In 2001 and 2002 I observed bee activity and pollen deposition rates at coffee flowers along distance gradients from two fragments and one narrow riparian strip of forest. Eleven eusocial species were the most common visitors: 10 species of native meliponines and the introduced honeybee, Apis mellifera (hereafter Apis). Bee richness, overall visitation rate, and pollen deposition rate were all significantly higher in sites within approximately 100 m of forest fragments than in sites farther away (maximum distance of 1.6 km). Apis visitation rates were constant across the distance gradient, however, and Apis accounted for &#62;90% of all floral visits in distant sites. The gradient from the riparian strip showed a similar drop in bee species richness with distance, but visitation rates were uniformly low along the gradient. Throughout the study area, Apis abundances declined sharply from 2001 to 2002, reducing visitation rates by over 50% in distant sites (where Apis was almost the only pollinator). In near sites, however, overall visitation rates dropped only 9% because native species almost entirely compensated for the Apis decline. Forest fragments (more so than the riparian strip) thus provided nearby coffee with a diversity of bees that increased both the amount and stability of pollination services by reducing dependence on a single introduced pollinator. Exploring the economic links between forest preservation and coffee cultivation may help align the goals of conservation and agriculture within many regions of global conservation priority."
642,68,2075,1,Modeling the sustainability of subsistence farming and hunting in the Ituri forest of Zaire,"We used empirical data to simulate the impacts, over the next 40 years, of subsistence-level agricultural clearing and bushmeat consumption on forest resources within the recently established Okapi Wildlife Reserve in northeastern Zaire. Satellite imagery, human population census data, and field measurements were used to calculate Present and projected impacts of agricultural clearing on forest cove: Data on per capita meat consumption and the species captured by hunters were combined with relevant ecological data to estimate ratios of consumption to production and to assess the sustainability of hunting. Even with projected population growth of nearly 300% among local communities over 40 years, sufficient secondary forest is available that agricultural clearing will have minimal effect on mature forest throughout most of the reserve. Impacts on the reserve's fauna will be more dramatic particularly within 15 km of villages where most hunting currently occurs Subsistence exploitation of forest antelopes may be sustainable in much of the reserve (especially if high estimates of game production are used), but as the human Population continues to increase duikers will likely be over- hunted. Primate populations do not appear to be threatened In the near future in those areas where bow hunters exploit monkeys, but an increase in this specialized activity in other legions of the reserve and growing human populations could change this. Although additional surveys of commonly hunted species throughout the Okapi wildlife Reserve are essential to enhancing the precision of the simulation, our results suggest that mitigation efforts should he designed and implemented note if the long-term effects of domestic bushmeat consumption are to be addressed."
643,68,2571,1,"Similarity indices, sample size and diversity","The effect of sample size and species diversity on a variety of similarity indices is explored. Real values of a similarity index must be evaluated relative to the expected maximum value of that index, which is the value obtained for samples randomly drawn from the same universe, with the diversity and sample sizes of the real samples. It is shown that these expected maxima differ from the theoretical maxima, the values obtained for two identical samples, and that the relationship between expected and theoretical maxima depends on sample size and on species diversity in all cases, without exception. In all cases but one (the Morisita index) the expected maxima depend strongly to fairly strongly on sample size and diversity. For some of the more useful indices empirical equations are given to calculate the expected maximum value of the indices to which the observed values can be related at any combination of sample sizes. It is recommended that the Morisita index be used whenever possible to avoid the complex dealings with effects of sample size and diversity; however, when previous logarithmic transformation of the data is required, which often may be the case, the Morisita-Horn or the Renkonen indices are recommended."
644,68,4390,1,"The geoarchaeology of the prehistoric ditched sites of the upper Mae Nam Mun Valley, NE Thailand, III: Late Holocene vegetation history","The upper Mae Nam (River) Mun Valley of northeast Thailand has been occupied at least since the Bronze Age, but is notable for the rapid expansion of intense town-based Iron Age settlement. The area presently forms the seasonally-arid core of mainland southeast Asia, and is presently dominated by increasingly saline soils, low-productivity rice cultivation and regrowth semi-arid scrub. However, the archaeological evidence for this region indicates a highly-productive natural environment within the last two millennia. Pollen sequences from the infill of Iron Age features provide the first palynological evidence for this part of northeast Thailand, detailing Late Holocene vegetational change. The area around the sites was initially dominated by forest, which then underwent two phases of the replacement by mosaics of grassland, probable rice cultivation, arboriculture and scrub, prior to a subsequent phase of forest and woodland regeneration. Spatial patterning of the study area's palaeovegetation appears to have been complex. While a general progress of landscape change is evident, local compositional differences are also clear. Although the region's archaeological and, especially, geomorphological evidence suggests significant climatic change during this period, the pollen record, as in studies further north in the region for the same period, appears to have been dominated by human influences. Of note are the effects of intensified human settlement and thus increased land and natural resource use. At present this Late Holocene pollen sequence yields no evidence for a direct relationship with climatic change."
645,68,5062,1,Responses of ants to selective logging of a central Amazonian forest,"			Summary1. Relatively little information exists on the effects of logging on rain forest organisms, particularly in the Neotropics where logging operations have increased dramatically in recent years. In this study we determined experimentally the effects of selective logging of a central Amazonian forest on ground-living ants.2. The experimental design consisted of three 4-ha replicated plots representing control unlogged forest, forest logged 10 years prior to the start of the study (1987), and forest logged 4 years prior to the start of the study (1993). The logging operation removed 50% of the basal area of trees of commercial value, or about eight trees per hectare. This resulted in a significant decrease in canopy cover, and an increase in understorey vegetation density in logged plots relative to controls.3. Collection and identification of ants from a total of 360 1-m2 samples of leaf-litter revealed 143 ant species, of which 97 were found in the control plots, 97 in the plots logged in 1987, and 106 in those logged in 1993. Species richness, evenness and mean abundance (ants m-2) per plot did not vary among treatments. Most of the species found in the control plots were also present in the logged plots. However, population density of many species changed as a result of logging, an effect that persisted for at least 10 years after logging. Species commonly found in sites that were directly disturbed by logging (gaps and tracks) were rare in the undisturbed forest, as revealed by an additional collection of ants.4. These results suggest that the persistence of ant assemblages typical of undisturbed forest is likely to depend on the amount of structural damage incurred by logging. Thus management techniques that minimize logging impacts on forest structure are likely to help maintain the conservation value of logged forests for ground-dwelling ants. It is particularly important to minimize the extent of logging roads and tracks created by heavy machinery because these areas appear more prone to invasion by non-forest species."
646,68,5669,1,Integrating the statistical analysis of spatial data in ecology,"In many areas of ecology there is an increasing emphasis on spatial relationships. Often ecologists are interested in new ways of analyzing data with the objective of quantifying spatial patterns, and in designing surveys and experiments in light of the recognition that there may be underlying spatial pattern in biotic responses. In doing so, ecologists have adopted a number of widely different techniques and approaches derived from different schools of thought, and from other scientific disciplines. While the adaptation of a diverse array of statistical approaches and methodologies for the analysis of spatial data has yielded considerable insight into various ecological problems, this diversity of approaches has sometimes impeded communication and retarded more rapid progress in this emergent area. Many of these different statistical methods provide similar information about spatial characteristics, but the differences among these methods make it difficult to compare the results of studies that employ contrasting approaches. The papers in this mini-series explore possible areas of agreement and synthesis between a diversity of approaches to spatial analysis in ecology."
647,68,6202,1,"Land use and vegetation fires in Jambi Province, Sumatra, Indonesia","In Indonesia, vegetation fires occur every year in the dry season. To determine where and why fires occur, the natural and cultural landscape features that influence the location of fires were analysed. We investigated the probability of fire occurrence as a function of predisposing conditions and ignition sources, such as land use, land use zoning, accessibility or land cover, to understand the spatial determinants of fires. The study area is the entire province of Jambi, central Sumatra, Indonesia. This province has a diverse setting of actors (small- and large-holders), land cover types and land uses. Fires were extracted for 1992/1993 from National Oceanic Atmospheric Administration&rsquo;s Advanced Very High Resolution Radiometer (NOAA-AVHRR) satellite data. The results of the spatial statistical analysis show that fire occurrence in Jambi Province in 1992/1993 was determined both by predisposing conditions (mostly climate, elevation and suitability for specific tree crops) and human-related causes (presence of transmigration projects and land allocation to specific land uses). National policies are thus a major driving forces of fires through land allocation. Road accessibility is only an important determinant of fires in forests. Few fires seem to be accidental. While logging companies control fire during their exploitation of concessions, logged-over forests and forests allocated to production but not yet under use have many fires. In 1992/1993, large- and small-holders were likely to be both responsible for fire occurrence. These results highlight the large influence of land use and policies on vegetation fires in Indonesia."
648,68,6712,1,Mixed biodiversity benefits of agri-environment schemes in five European countries,"Abstract Agri-environment schemes are an increasingly important tool for the maintenance and restoration of farmland biodiversity in Europe but their ecological effects are poorly known. Scheme design is partly based on non-ecological considerations and poses important restrictions on evaluation studies. We describe a robust approach to evaluate agri-environment schemes and use it to evaluate the biodiversity effects of agri-environment schemes in five European countries. We compared species density of vascular plants, birds, bees, grasshoppers and crickets, and spiders on 202 paired fields, one with an agri-environment scheme, the other conventionally managed. In all countries, agri-environment schemes had marginal to moderately positive effects on biodiversity. However, uncommon species benefited in only two of five countries and species listed in Red Data Books rarely benefited from agri-environment schemes. Scheme objectives may need to differentiate between biodiversity of common species that can be enhanced with relatively simple modifications in farming practices and diversity or abundance of endangered species which require more elaborate conservation measures."
649,68,6907,1,Pseudoreplication and the design of ecological field experiments,"Pseudoreplication is defined as the use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated (though samples may be) or replicates are not statistically independent. In ANOVA terminology, it is the testing for treatment effects with an error term inappropriate to the hypothesis being considered. Scrutiny of 176 experimental studies published between 1960 and the present revealed that pseudoreplication occurred in 27% of them, or 48% of all such studies that applied inferential statistics. The incidence of pseudoreplication is especially high in studies of marine benthos and small mammals. The critical features of controlled experimentation are reviewed. Nondemonic intrusion is defined as the impingement of chance events on an experiment in progress. As a safeguard against both it and preexisting gradients, interspersion of treatments is argued to be an obligatory feature of good design. Especially in small experiments, adequate interspersion can sometimes be assured only by dispensing with strict randomization procedures. Comprehension of this conflict between interspersion and randomization is aided by distinguishing preâlayout (or conventional) and layoutâspecific alpha (probability of type I error). Suggestions are offered to statisticians and editors of ecological journals as to how ecologists' understanding of experimental design and statistics might be improved."
650,68,6913,1,Quantifying biodiversity: procedures and pitfalls in the measurement and comparison of species richness,"Species richness is a fundamental measurement of community and regional diversity, and it underlies many ecological models and conservation strategies. In spite of its importance, ecologists have not always appreciated the effects of abundance and sampling effort on richness measures and comparisons. We survey a series of common pitfalls in quantifying and comparing taxon richness. These pitfalls can be largely avoided by using accumulation and rarefaction curves, which may be based on either individuals or samples. These taxon sampling curves contain the basic information for valid richness comparisons, including category2013subcategory ratios (species-to-genus and species-to-individual ratios). Rarefaction methods 2013 both sample-based and individual-based 2013 allow for meaningful standardization and comparison of datasets. Standardizing data sets by area or sampling effort may produce very different results compared to standardizing by number of individuals collected, and it is not always clear which measure of diversity is more appropriate. Asymptotic richness estimators provide lower-bound estimates for taxon-rich groups such as tropical arthropods, in which observed richness rarely reaches an asymptote, despite intensive sampling. Recent examples of diversity studies of tropical trees, stream invertebrates, and herbaceous plants emphasize the importance of carefully quantifying species richness using taxon sampling curves."
651,68,7567,1,Autocorrelated Rates of Change in Animal Populations and their Relationship to Precipitation,"I examined the prevalence of autocorrelation in mammalian, avian, and precipitation time series, how well autocorrelation in the environment translates into autocorrelation in animal populations, and length of the time series needed to accurately characterize the degree of autocorrelation. These are important questions because more-complex population models are incorporating autocorrelation terms in life-history characteristics and the intrinsic rate of increase. Including inaccurate or nonsignificant autocorrelation can alter the conclusions reached, providing either an unduly rosy or bleak picture of the likelihood of population viability and persistence. Using autocorrelation analysis in 175 vertebrate and 88 precipitation data sets, I found that 17.8% of the mammalian time series, 61.5% of the avian time series, and 97.7% of the precipitation data sets were autocorrelated. Carnivore populations were more likely than herbivore populations to show significant autocorrelation at lags of 2 or more years. I found only two cases of significant cross correlation between rate of population increase and local precipitation. This indicates that, although some environmental variables may be highly autocorrelated, it does not translate into autocorrelation in the resident animal populations. Based on subsampling of the precipitation and vertebrate data, I found that 15 years of data is sufficient to produce an autocorrelation not significantly different from one based on 100 years of data, although the variance continues to decrease with the length of the time series, as expected. My results suggest that, although some populations show temporal autocorrelation, it is not ubiquitous, and that environmental autocorrelation may not be a good predictor of autocorrelation in rates of increase. Population modelers should determine if autocorrelation exists in populations of interest prior to modeling their viability or probability of persistence because not all populations are equally influenced by autocorrelation. Tasas de Cambio en Poblaciones de Animales Autocorrelacionadas y sus Relaciones con la Precipitacion. Se examino el predominio de la autocorrelacion en series de tiempo de mamiferos, aves y precipitacion; que tanto la autocorrelacion ambiental se traduce en autocorrelacion en poblaciones de animales, asi como la longitud de las series de tiempo necesaria para caraterizar con precision el grado de autocorrelacion. Estas son preguntas importantes puesto que los modelos poblacionales mas complejos incorporan la autocorrelacion en las caracteristicas de la historia de vida y la tasa intrinseca de incremento. Aun la inexactitud o autocorrelatcion no significativa puede alterar las conclusiones obtenidas, proveyendo indebidamente una idea prometedora o poco promotedora de la probabilidad de viabilidad y persistencia de una poblacion. Mediante el uso del analisis de autocorrelacion en 175 juegos de datos de vertebrados y 88 de precipitacion, encontre que un 17.8% de las series de tiempo de mamiferos, 61.5% de las series de tiempo de aves y un 97.8% de los datos de precipitacion se encontraban autocorrelacionados. Las poblaciones de carnivoros fueron las mas viables a mostrar autocorrelacion en lapsos de 2 o mas anos que las poblaciones de herbivoros. Encontre solo dos casos de correlacion cruzada significativa entre la tasa de incremento poblacional y la precipitacion local. Esto indica que aunque algunas variables ambientales pueden estar altamente autocorrelacionadas, no se traduce en una autocorrelacion en las poblaciones de animales residentes. Basado en un submuestreo de la precipitacion y datos de vertebrados, encontre que 15 anos de datos es suficiente para producir una autocorrelacion que no es significativamente diferente de aquella basada en 100 anos de datos, aunque la varianza continua disminuyendo con la longitud de las series de tiempo a como es de esperarse. Mis resultados sugieren que aunque algunas poblaciones muestran autocorrelacion temporal, esta no es evidente y la autocorrelacion ambiental podria no ser un buen predictor de autocorrelacion de tasas de incremento. Los modeladores de poblaciones deberan determinar si la autocorrelacion existe en poblaciones de interes antes de modelar su viabilidad o la probabilidad de persistencia puesto que no todas las poblaciones son afectadas por la autocorrelacion de la misma manera."
652,68,8070,1,The ED strategy: how species-level surrogates indicate general biodiversity patterns through an 'environmental diversity' perspective,"Abstract Biodiversity assessment requires that we use surrogate information in practice to indicate more general biodiversity patterns. 'ED' refers to a surrogates framework that can link species data and environmental information based on a robust relationship of compositional dissimilarities to ordinations that indicate underlying environmental variation. In an example analysis of species and environmental data from Panama, the environmental and spatial variables that correlate with an hybrid multi-dimensional scaling ordination were able to explain 83% of the variation in the corresponding Bray Curtis dissimilarities. The assumptions of ED also provide the rationale for its use of p-median optimization criteria to measure biodiversity patterns among sites in a region. M.B. Araujo, P.J. Densham & P.H. Williams (2004, Journal of Biogeography31, 1) have re-named ED as 'AD' in their evaluation of the surrogacy value of ED based on European species data. Because lessons from previous work on ED options consequently may have been neglected, we use a corroboration framework to investigate the evidence and 'background knowledge' presented in their evaluations of ED. Investigations focus on the possibility that their weak corroboration of ED surrogacy (non-significance of target species recovery relative to a null model) may be a consequence of Araujo et al.'s use of particular evidence and randomizations. We illustrate how their use of discrete ED, and not the recommended continuous ED, may have produced unnecessarily poor species recovery values. Further, possible poor optimization of their MDS ordinations, due to small numbers of simulations and/or low resolution of stress values appears to have provided a possible poor basis for ED application and, consequently, may have unnecessarily favoured non-corroboration results. Consideration of Araujo et al.'s randomizations suggests that acknowledged sampling biases in the European data have not only artefactually promoted the non-significance of ED recovery values, but also artefactually elevated the significance of competing species surrogates recovery values. We conclude that little credence should be given to the comparisons of ED and species-based complementarity sets presented in M.B. Araujo, P.J. Densham & P.H. Williams (2004, Journal of Biogeography31, 1), unless the factors outlined here can be analysed for their effects on results. We discuss the lessons concerning surrogates evaluation emerging from our investigations, calling for better provision in such studies of the background information that can allow (i) critical examination of evidence (both at the initial corroboration and re-evaluation stages), and (ii) greater synthesis of lessons about the pitfalls of different forms of evidence in different contexts."
653,68,8539,1,Abundance-occupancy relationships,"1. The abundance and distribution of species tend to be linked, such that species declining in abundance often tend also to show declines in the number of sites they occupy, while species increasing in abundance tend also to be increasing in occupancy. Therefore, intraspecific abundance-occupancy relationships are commonly positive. 2. The intraspecific pattern is mirrored by more general positive interspecific abundance-occupancy relationships: widespread species tend to be abundant, and narrowly distributed species rare. 3. Here, we review recent research on these patterns based on the flora and fauna of the British Isles. We assess their generality, describe what is currently known about their structure, and summarize the results of tests of the several hypotheses proposed to explain their existence. 4. The positive form generally exhibited by abundance-occupancy relationships, intraspecific or interspecific, has consequences for several areas of applied ecology, including conservation, harvesting, biological invasions and biodiversity inventorying. These implications are discussed briefly"
654,68,8549,1,"Positive feedbacks among forest fragmentation, drought, and climate change in the Amazon","The Amazon basin is experiencing rapid forest loss and fragmentation. Fragmented forests are more prone than intact forests to periodic damage from El Nino-Southern Oscillation (ENSO) droughts, which cause elevated tree mortality, increased litterfall, shifts in plant phenology, and other ecological changes, especially near forest edges. Moreover, positive feedbacks among forest loss, fragmentation, fire, and regional climate change appear increasingly likely. Deforestation reduces plant evapotranspiration, which in turn constrains regional rainfall, increasing the vulnerability of forests to fire. Forest fragments are especially vulnerable because they have dry, fire-prone edges, are logged frequently, and often are adjoined by cattle pastures, which are burned regularly. The net result is that there may be a critical ""deforestation threshold"" above which Amazonian rainforests can no longer be sustained, particularly in relatively seasonal areas of the basin. Global warming could exacerbate this problem if it promotes drier climates or stronger ENSO droughts. Synergisms among many simultaneous environmental changes are posing unprecedented threats to Amazonian forests."
655,68,9948,1,Ten years of individual-based modelling in ecology: what have we learned and what could we learn in the future?,"Each modeller who builds and analyses an individual-based model learns of course a great deal, but what has ecology as a whole learned from the individual-based models published during the last decade? Answering this question proves extremely difficult as there is no common motivation behind individual-based models. The distinction is introduced between 'pragmatic' motivation, which uses the individual-based approach as a tool without any reference to the theoretical issues which have emerged from the classical state variable approach and 'paradigmatic' motivation, which explicitly refers to theoretical ecology. A mini-review of 50 individual-based animal population models shows that the majority are driven by pragmatic motivation. Most models are very complex and special techniques to cope with this complexity during their analysis are only occasionally applied. It is suggested that in order to orient individual-based modelling more towards general theoretical issues, we need increased explicit reference to theoretical ecology and an advanced strategy for building and analysing individual-based models. To this end, a heuristic list of rules is presented which may help us to advance the practice of individual-based modelling and to learn more general lessons from individual-based modelling in the future than we have during the last decade. The main ideas behind these rules are as follows: (1) Individual-based models usually make more realistic assumptions than state variable models, but it should not be forgotten that the aim of individual-based modelling is not 'realism' but modelling. (2) The individual- based approach is a bottom-up approach which starts with the 'parts' (i.e. individuals) of a system (i.e. population) and then tries to understand how the system's properties emerge from the interaction among these parts. However, bottom-up approaches alone will never lead to theories at the systems level. State variable or top-down approaches are needed to provide an appropriate integrated view, i.e. the relevant questions at the population level. (C) 1999 Elsevier Science B.V. All rights reserved."
656,68,11379,1,Integrating landscape and metapopulation modeling approaches: viability of the sharp-tailed grouse in a dynamic landscape.,": The lack of management experience at the landscape scale and the limited feasibility of experiments at this scale have increased the use of scenario modeling to analyze the effects of different management actions on focal species. However, current modeling approaches are poorly suited for the analysis of viability in dynamic landscapes. Demographic (e.g., metapopulation) models of species living in these landscapes do not incorporate the variability in spatial patterns of early successional habitats, and landscape models have not been linked to population viability models. We link a landscape model to a metapopulation model and demonstrate the use of this model by analyzing the effect of forest management options on the viability of the Sharp-tailed Grouse ( Tympanuchus phasianellus) in the Pine Barrens region of northwestern Wisconsin (U.S.A.). This approach allows viability analysis based on landscape dynamics brought about by processes such as succession, disturbances, and silviculture. The landscape component of the model (LANDIS) predicts forest landscape dynamics in the form of a time series of raster maps. We combined these maps into a time series of patch structures, which formed the dynamic spatial structure of the metapopulation component (RAMAS). Our results showed that the viability of Sharp-tailed Grouse was sensitive to landscape dynamics and demographic variables such as fecundity and mortality. Ignoring the landscape dynamics gave overly optimistic results, and results based only on landscape dynamics (ignoring demography) lead to a different ranking of the management options than the ranking based on the more realistic model incorporating both landscape and demographic dynamics. Thus, models of species in dynamic landscapes must consider habitat and population dynamics simultaneously."
657,68,11380,1,Agroecology: the science of natural resource management for poor farmers in marginal environments.,"Throughout the developing world, resource-poor farmers (about 1.4 billion people) located in risk-prone, marginal environments, remain untouched by modern agricultural technology. A new approach to natural resource management must be developed so that new management systems can be tailored and adapted in a site-specific way to highly variable and diverse farm conditions typical of resource-poor farmers. Agroecology provides the scientific basis to address the production by a biodiverse agroecosystem able to sponsor its own functioning. The latest advances in agroecological research are reviewed in order to better define elements of a research agenda in natural resource management that is compatible with the needs and aspirations of peasants. Obviously, a relevant research agenda setting should involve the full participation of farmers with other institutions serving a facilitating role. The implementation of the agenda will also imply major institutional and policy changes."
658,68,11387,1,Agent based simulation of a small catchment water management in northern Thailand description of the CATCHSCAPE model.,"Due to mounting human pressure, stakeholders in northern Thailand are facing crucial natural resources management (NRM) issues. Among others, the impact of upstream irrigation management on downstream agricultural viability is a growing source of conflict, which often has both biophysical and social origins. As multiple rural stakeholders are involved, appropriate solutions should only emerge from negotiation. CATCHSCAPE is a Multi-Agent System (MAS) that enables us to simulate the whole catchment features as well as farmer&rsquo;s individual decisions. The biophysical modules simulate the hydrological system with its distributed water balance, irrigation scheme management and crop and vegetation dynamics. The social dynamics are described as a set of resource management processes (water, land, cash, labour force). Water management is described according to the actual different levels of control (individual, scheme and catchment). Moreover, the model&rsquo;s architecture is presented in a way that emphasises the transparency of the rules and methods implemented. Finally, one simulated scenario is described along with its main results, according to different viewpoints (economy, landscape, water management)."
659,68,11389,1,Ecology : From individuals to ecosystems.,"Begon, Townsend, and Harper's Ecology has long been regarded as the definitive textbook on all aspects of ecology. This new edition provides a comprehensive treatment of the subject, from the first principles of ecology to the current state of the field, and aims to improve students' preparedness to address the environmental problems of the new millennium. Thoroughly revised and updated, this fourth edition includes: * three new chapters on applied ecology, reflecting a rigorous, scientific approach to the ecological problems now facing mankind * discussion of over 800 new studies, updating the text throughout * an updated, user-friendly design with margin notes and chapter summaries that serve as study aids * dedicated website at www.blackwellpublishing.com/begon The resulting textbook is easy to use, lucid and up-to-date, and is the essential reference for all students whose degree program includes ecology and for practicing ecologists."
660,68,11395,1,An improved method for the rapid assessment of forest understorey light environments,"1. The high spatial and temporal variability of forest understorey light environments requires lengthy and/or extensive sampling in order to characterize it by direct measurement. As this is often impractical, a number of surrogate measures have been developed that estimate light availability from assessments of forest canopy structure. 2. The subjective crown illumination index developed by Clark & Clark (1992) was compared with Garrison's (1949) moosehorn and two new methods: (i) the crown illumination ellipses method, which compares the size of canopy gaps with a series of standard area ellipses printed on a transparent screen; and (ii) the canopy-scope that, like the moosehorn, uses an array of 25 dots printed on a transparent screen to assess canopy openness, but is more robust and portable, measuring the largest canopy gap visible from the point of measurement rather than canopy openness overhead. 3. The new measures were more highly correlated with canopy openness in the range 0-30%, measured from hemispherical photographs, than the crown illumination index, and showed lower levels of between-observer variability. 4. The canopy-scope has the potential to be widely used for the simple and rapid assessment of forest understorey light environments. It has the advantage of giving ratio scale measurements that can be used in parametric statistics. The crown illumination ellipses can be used to score the illumination of crowns that are above head height."
661,68,11398,1,On the use of surrogate species in conservation biology.,"Conservation biologists have used surrogate species as a shortcut to monitor or solve conservation problems. Indicator species have been used to assess the magnitude of anthropogenic disturbance, to monitor population trends in other species, and to locate areas of high regional biodiversity. Umbrella species have been used to delineate the type of habitat or size of area for protection, and flagship species have been employed to attract public attention. Unfortunately, there has been considerable confusion over these terms, and several have been applied loosely and interchangeably. We attempt to provide some clarification and guidelines for the application of these different terms. For each type of surrogate, we briefly describe the way it has been used in conservation biology and then examine the criteria that managers and researchers use in selecting appropriate surrogate species. By juxtaposing these concepts, it becomes clear that both the goals and selection criteria of different surrogate classes differ substantially, indicating that they should not be conflated. This can be facilitated by first outlining the goals of a conservation study, explicitly stating the criteria involved in selecting a surrogate species, identifying a species according to these criteria, and then performing a pilot study to check whether the choice of species was appropriate before addressing the conservation problem itself. Surrogate species need to be used with greater care if they are to remain useful in conservation biology. BiÃ³logos de la conservaciÃ³n han utilizado especies sucedÃ¡neas como atajos para monitorear o resolver problemas de conservaciÃ³n. Las especies indicadoras han sido utilizadas para evaluar la magnitud de la pertubaciÃ³n antropogÃ©nica, para monitorear tendencias poblacionales en otras especies y para localizar Ã¡reas de alta biodiversidad regional. Las especies sombrilla han sido utilizadas para delinear el tipo de hÃ¡bitat o tamaÃ±o de Ã¡rea para protecciÃ³n y las especies bandera han sido empleadas para atraer la atenciÃ³n del pÃºblico. Desafortunadamente, ha habido una considerable confusiÃ³n sobre estos tÃ©rminos y muchos han sido aplicados de un amanera vaga e intercambiable. Intentamos proveer albunas aclaraciones y lineamientos para la aplicaciÃ³n de estos diferentes tÃ©rminos. Para cada tipo de sustituto describimos brevemente la forma en que ha sido usado en la conservaciÃ³n biolÃ³gica y posteriormente examinamos los criterios que los manejadores e investigadores usan en la selecciÃ³n de las especies sustitutas apropiadas. Al yuxtaponer estos conceptos, se hace claro que tanto las metas como los criterios de selecciÃ³n de diferentes clases de sustituos difieren substancialmente indicando que estos no diberÃ­n ser confundidos. Esto puede ser facilitado primero al subrayar las metas de un estudio de conservaciÃ³n, estableciendo explÃ­citamente los criterios involucrados en la selecciÃ³n de la especie sustituta, identificando a las especies utilizando este criterio y posteriormente llevando a cobo un estudio piloto para checar si la especie seleccionada es la apropiada antes de proceder a abordar el problema de conservaciÃ³n en si. Las especies sucedÃ¡neas necesitan ser utilizadas con mayor cuidado si queremos que sigan siendo Ãºtiles para la biologÃ­a de conservaciÃ³n."
662,68,11403,1,Effects of the past and the present on species distribution: land-use history and demography of wintergreen.,"Summary 1 Past land use can have long-term effects on plant speciesâ distributional patterns if alterations in resources and environmental conditions have persistent effects on population demography (environmental change) and/or if plants are intrinsically limited in their colonization ability (historical factors). 2 We evaluated the role of environmental alteration vs. historical factors in controlling distributional patterns of Gaultheria procumbens, a woody, clonal understorey species with a pronounced restriction to areas that have never been ploughed, and near absence from adjoining areas that were ploughed in the 19th century. The demographic study was conducted in scrub oak and hardwood plant communities on an extensive sand plain, where it was possible to control for the effect of variation in environment prior to land use. 3 The observed demographic effects were contrary to the hypothesis that persistent environmental alteration depressed demographic performance and limited the distribution of G. procumbens. We observed no overall effect of land-use history on stem density, stem recruitment or flower production. In fact, some aspects of performance were enhanced in previously ploughed areas. Populations in previously ploughed areas exhibited less stem mortality in scrub oak transitions, an increase in germination, seedling longevity and proportion of potentially reproductive stems in both plant communities, a trend for slower observed rates of population decline in both plant communities, and a higher projected rate of population growth in the scrub oak transitions. Thus, particularly in scrub oak communities, the lower abundance of G. procumbens in formerly ploughed than in unploughed areas contrasted with its performance. 4 The limited occurrence of G. procumbens in formerly farmed areas was explained instead by its slow intrinsic growth rate, coupled with limited seedling establishment. Lateral population extension occurred exclusively through vegetative growth, allowing a maximum expansion of 43 cm year-1. 5 We conclude that inherent limitations in the colonizing ability of some plant species may present a major obstacle in the restoration or recovery of plant communities on intensively disturbed sites, even in the absence of persistent environmental effects that depress population growth."
663,68,11407,1,Host-plant selection by insects â a theory based on âappropriate/inappropriate landingsâ by pest insects of cruciferous plants.,"Seven hypotheses, including the `Resource Concentration Hypothesis' and the `Enemies Hypothesis', have been put forward to explain why fewer specialist insects are found on host plants growing in diverse backgrounds than on similar plants growing in bare soil. All seven hypotheses are discussed and discounted, primarily because no one has used any of them to produce a general theory of host plant selection, they still remain as hypotheses. However, we have developed a general theory based on detailed observations of insect behaviour. Our theory is based on the fact that during host plant finding the searching insects land indiscriminately on green objects such as the leaves of host plants (appropriate landings) and non-host plants (inappropriate landings), but avoid landing on brown surfaces, such as soil. The complete system of host plant selection involves a three-link chain of events in which the first link is governed by cues from volatile plant chemicals, the central link by visual stimuli, and the final link by cues from non-volatile plant chemicals. The previously `missing' central link, which is based on what we have described as `appropriate/inappropriate landings', is governed by visual stimuli. Our theory explains why attempts to show that olfaction is the crucial component in the central link of host plant selection proved intractable. The `appropriate/inappropriate landings' theory is discussed to indicate the type of work needed in future studies to improve our understanding of how intercropping, undersowing and companion planting can be used to optimum effect in crop protection. The new theory is used also to suggest how insect biotypes could develop and to describe why pest insects do not decimate wild host plants growing in `natural' situations."
664,68,11411,1,Assessing biodiversity at landscape level in northern Thailand and Sumatra (Indonesia): the importance of environmental context.,"{Most biodiversity assessment methods tend to sample isolated areas of land cover such as closed forest or local land use mosaics. Contemporary methods of assessing biodiversity are briefly reviewed and focus on the relative roles of the Linnean species and plant functional types (PFTs). Recent case studies from central Sumatra and northern Thailand indicate how the range distributions of many plant and animal species and functional types frequently extend along regional gradients of light, water and nutrient availability and corresponding land use intensity. We show that extending the sampling context to include a broader array of environmental determinants of biodiversity results in a more interpretable pattern of biodiversity. Our results indicate sampling within a limited environmental context has the potential to generate highly truncated range distributions and thus misleading information for land managers and for conservation. In an intensive, multi-taxa survey in lowland Sumatra, vegetational data were collected along a land use intensity gradient using a proforma specifically designed for rapid survey. Each vegetation sample plot was a focal point for faunal survey. Whereas biodiversity pattern from samples within closed canopy rain forest was difficult to interpret, extending the sample base to include a wider variety of land cover and land use greatly improved interpretation of plant and animal distribution. Apart from providing an improved theoretical and practical basis for forecasting land use impact on biodiversity, results illustrate how specific combinations of plant-based variables might be used to predict impacts on specific animal taxa, functional types and above-ground carbon. Implications for regional assessment and monitoring of biodiversity and in improving understanding of the landscape dynamics are briefly discussed. (C) 2004 Elsevier B.V. All rights reserved.}"
665,68,11416,1,META-X: generic software for metapopulation viability analysis.,"The major tools used to make population viability analyses (PVA) quantitative are stochastic models of population dynamics. Since a specially tailored model cannot be developed for every threatened population, generic models have been designed which can be parameterised and analysed by non-modellers. These generic models compromise on detail so that they can be used for a wide range of species. However, generic models have been criticised because they can be employed without the user being fully aware of the concepts, methods, potentials, and limitations of PVA. Here, we present the conception of a new generic software package for metapopulation viability analysis, META-X. This conception is based on three elements, which take into account the criticism of earlier generic PVA models: (1) comparative simulation experiments; (2) an occupancy-type model structure which ignores details of local population dynamics (these details are integrated in external submodels); and (3) a unifying currency to quantify persistence and viability, the `intrinsic mean time to extinc- tion'. The rationale behind these three elements is explained and demonstrated by exemplary applications of META-X in the three fields for which META-X has been designed: teaching, risk assessment in the field, and planning. The conception of META-X is based on the notion that PVA is a tool to deal with rather than to overcome uncertainty. The purpose of PVA is to produce relative, not absolute, assessments of extinction risk which support, but do not supplant, management decisions."
