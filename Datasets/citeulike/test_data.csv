,user.id,doc.id,rating,raw.title,raw.abstract
0,0,3333,1,Binding MOAD (Mother Of All Databases).,"Binding MOAD (Mother of All Databases) is the largest collection of high-quality, protein-ligand complexes available from the Protein Data Bank. At this time, Binding MOAD contains 5331 protein-ligand complexes comprised of 1780 unique protein families and 2630 unique ligands. We have searched the crystallography papers for all 5000+ structures and compiled binding data for 1375 (26%) of the protein-ligand complexes. The binding-affinity data ranges 13 orders of magnitude. This is the largest collection of binding data reported to date in the literature. We have also addressed the issue of redundancy in the data. To create a nonredundant dataset, one protein from each of the 1780 protein families was chosen as a representative. Representatives were chosen by tightest binding, best resolution, etc. For the 1780 ""best"" complexes that comprise the nonredundant version of Binding MOAD, 475 (27%) have binding data. This significant collection of protein-ligand complexes will be very useful in elucidating the biophysical patterns of molecular recognition and enzymatic regulation. The complexes with binding-affinity data will help in the development of improved scoring functions and structure-based drug discovery techniques. The dataset can be accessed at http://www.BindingMOAD.org. Proteins 2005. (c) 2005 Wiley-Liss, Inc."
1,0,4890,1,Diversity of protein-protein interactions.,"In this review, we discuss the structural and functional diversity of protein-protein interactions (PPIs) based primarily on protein families for which three-dimensional structural data are available. PPIs play diverse roles in biology and differ based on the composition, affinity and whether the association is permanent or transient. In vivo, the protomer's localization, concentration and local environment can affect the interaction between protomers and are vital to control the composition and oligomeric state of protein complexes. Since a change in quaternary state is often coupled with biological function or activity, transient PPIs are important biological regulators. Structural characteristics of different types of PPIs are discussed and related to their physiological function, specificity and evolution."
2,0,7107,1,SCOWLP: a web-based database for detailed characterization and visualization of protein interfaces.,"ABSTRACT: BACKGROUND: Currently there is a strong need for methods that help to obtain an accurate description of protein interfaces in order to be able to understand the principles that govern molecular recognition and protein function. Many of the recent efforts to computationally identify and characterize protein networks extract protein interaction information at atomic resolution from the PDB. However, they pay none or little attention to small protein ligands and solvent. They are key components and mediators of protein interactions and fundamental for a complete description of protein interfaces. Interactome profiling requires the development of computational tools to extract and analyze protein-protein, protein-ligand and detailed solvent interaction information from the PDB in an automatic and comparative fashion. Adding this information to the existing one on protein-protein interactions will allow us to better understand protein interaction networks and protein function. RESULTS: SCOWLP (Structural Characterization Of Water, Ligands and Proteins) is a user-friendly and publicly accessible web-based relational database for detailed characterization and visualization of the PDB protein interfaces. The SCOWLP database includes proteins, peptidic-ligands and interface water molecules as descriptors of protein interfaces. It contains currently 74,907 protein interfaces and 2,093,976 residue-residue interactions formed by 60,664 structural units (protein domains and peptidic-ligands) and their interacting solvent. The SCOWLP web-server allows detailed structural analysis and comparisons of protein interfaces at atomic level by text query of PDB codes and/or by navigating a SCOP-based tree. It includes a visualization tool to interactively display the interfaces and label interacting residues and interface solvent by atomic physicochemical properties. SCOWLP is automatically updated with every SCOP release. CONCLUSIONS: SCOWLP enriches substantially the description of protein interfaces by adding detailed interface information of peptidic-ligands and solvent to the existing protein-protein interaction databases. SCOWLP may be of interest to many structural bioinformaticians. It provides a platform for automatic global mapping of protein interfaces at atomic level, representing a useful tool for classification of protein interfaces, protein binding comparative studies, reconstruction of protein complexes and understanding protein networks. The web-server with the database and its additional summary tables used for our analysis are available at http://www.scowlp.org."
3,0,10509,1,The molecular architecture of protein-protein binding sites.,"The formation of specific protein interactions plays a crucial role in most, if not all, biological processes, including signal transduction, cell regulation, the immune response and others. Recent advances in our understanding of the molecular architecture of protein-protein binding sites, which facilitates such diversity in binding affinity and specificity, are enabling us to address key questions. What is the amino acid composition of binding sites? What are interface hotspots? How are binding sites organized? What are the differences between tight and weak interacting complexes? How does water contribute to binding? Can the knowledge gained be translated into protein design? And does a universal code for binding exist, or is it the architecture and chemistry of the interface that enable diverse but specific binding solutions?"
4,0,12685,1,Determining the architectures of macromolecular assemblies.,"To understand the workings of a living cell, we need to know the architectures of its macromolecular assemblies. Here we show how proteomic data can be used to determine such structures. The process involves the collection of sufficient and diverse high-quality data, translation of these data into spatial restraints, and an optimization that uses the restraints to generate an ensemble of structures consistent with the data. Analysis of the ensemble produces a detailed architectural map of the assembly. We developed our approach on a challenging model system, the nuclear pore complex (NPC). The NPC acts as a dynamic barrier, controlling access to and from the nucleus, and in yeast is a 50 MDa assembly of 456 proteins. The resulting structure, presented in an accompanying paper, reveals the configuration of the proteins in the NPC, providing insights into its evolution and architectural principles. The present approach should be applicable to many other macromolecular assemblies."
5,0,13188,1,A careful disorderliness in the proteome: Sites for interaction and targets for future therapies,"The community of scientists interested in studying intrinsically unstructured (or disordered) proteins has emerged in recent years. What began as a controversial idea has become an established phenomenon. The new, greater focus on proteins that are in some way normally unstructured promises to provide a greater understanding of protein function, particularly with respect to proteinâprotein interactions. These regions also offer new possibilities into how interactions can be targeted by small molecules."
6,0,15192,1,"Protein allostery, signal transmission and dynamics: a classification scheme of allosteric mechanisms.","Allostery has come of age; the number, breadth and functional roles of documented protein allostery cases are rising quickly. Since all dynamic proteins are potentially allosteric and allostery plays crucial roles in all cellular pathways, sorting and classifying allosteric mechanisms in proteins should be extremely useful in understanding and predicting how the signals are regulated and transmitted through the dynamic multi-molecular cellular organizations. Classification organizes the complex information thereby unraveling relationships and patterns in molecular activation and repression. In signaling, current classification schemes consider classes of molecules according to their functions; for example, epinephrine and norepinephrine secreted by the central nervous system are classified as neurotransmitters. Other schemes would account for epinephrine when secreted by the adrenal medulla to be hormone-like. Yet, such classifications account for the global function of the molecule; not for the molecular mechanism of how the signal transmission initiates and how it is transmitted. Here we provide a unified view of allostery and the first classification framework. We expect that a classification scheme would assist in comprehension of allosteric mechanisms, in prediction of signaling on the molecular level, in better comprehension of pathways and regulation of the complex signals, in translating them to the cascading events, and in allosteric drug design. We further provide a range of examples illustrating mechanisms in protein allostery and their classification from the cellular functional standpoint."
7,1,943,1,MedKit: a helper toolkit for automatic mining of MEDLINE/PubMed citations,"Summary: MEDLINE/PubMed is one of the most important information sources for bioinformatics text mining. However, there remain limitations in working with MEDLINE/PubMed citations. For example, PubMed imposes an upper limit of 10 000 for downloading PMID list or citations; and MEDLINE files are too large for most off-the-shelf XML parsers. We developed a Java package, MedKit, to work-around the limitations, as well as provide other useful functionalities, e.g. random sampling. Its four modules (querier, sampler, fetcher and parser) can work independently, or be pipelined in various combinations. It can be used as a stand-alone GUI application, or integrated into other text-mining systems. Text mining researchers and others may download and use the toolkit free for non-commercial purposes.  Availability: http://metnetdb.gdcb.iastate.edu/medkit  Contact: berleant@iastate.edu 10.1093/bioinformatics/bti087"
8,1,6458,1,"Mining MEDLINE: abstracts, sentences, or phrases?","A growing body of works address automated mining of biochemical knowledge from digital repositories of scientific literature, such as MEDLINE. Some of these works use abstracts as the unit of text from which to extract facts. Others use sentences for this purpose, while still others use phrases. Here we compare abstracts, sentences, and phrases in MEDLINE using the standard information retrieval performance measures of recall, precision, and effectiveness, for the task of mining interactions among biochemical terms based on term co-occurrence. Results show statistically significant differences that can impact the choice of text unit."
9,1,13617,1,Comparative analysis of five protein-protein interaction corpora,"BACKGROUND: Growing interest in the application of natural language processing methods to biomedical text has led to an increasing number of corpora and methods targeting protein-protein interaction (PPI) extraction. However, there is no general consensus regarding PPI annotation and consequently resources are largely incompatible and methods are difficult to evaluate. RESULTS: We present the first comparative evaluation of the diverse PPI corpora, performing quantitative evaluation using two separate information extraction methods as well as detailed statistical and qualitative analyses of their properties. For the evaluation, we unify the corpus PPI annotations to a shared level of information, consisting of undirected, untyped binary interactions of non-static types with no identification of the words specifying the interaction, no negations, and no interaction certainty.We find that the F-score performance of a state-of-the-art PPI extraction method varies on average 19 percentage units and in some cases over 30 percentage units between the different evaluated corpora. The differences stemming from the choice of corpus can thus be substantially larger than differences between the performance of PPI extraction methods, which suggests definite limits on the ability to compare methods evaluated on different resources. We analyse a number of potential sources for these differences and identify factors explaining approximately half of the variance. We further suggest ways in which the difficulty of the PPI extraction tasks codified by different corpora can be determined to advance comparability. Our analysis also identifies points of agreement and disagreement in PPI corpus annotation that are rarely explicitly stated by the authors of the corpora. CONCLUSIONS: Our comparative analysis uncovers key similarities and differences between the diverse PPI corpora, thus taking an important step towards standardization. In the course of this study we have created a major practical contribution in converting the corpora into a shared format. The conversion software is freely available at http://mars.cs.utu.fi/PPICorpora."
10,1,15459,1,U-Compare: share and compare text mining tools with UIMA,"Summary: Due to the increasing number of text mining resources (tools and corpora) available to biologists, interoperability issues between these resources are becoming significant obstacles to using them effectively. UIMA, the Unstructured Information Management Architecture, is an open framework designed to aid in the construction of more interoperable tools. U-Compare is built on top of the UIMA framework, and provides both a concrete framework for out-of-the-box text mining and a sophisticated evaluation platform allowing users to run specific tools on any target text, generating both detailed statistics and instance-based visualizations of outputs. U-Compare is a joint project, providing the world's largest, and still growing, collection of UIMA-compatible resources. These resources, originally developed by different groups for a variety of domains, include many famous tools and corpora. U-Compare can be launched straight from the web, without needing to be manually installed. All U-Compare components are provided ready-to-use and can be combined easily via a drag-and-drop interface without any programming. External UIMA components can also simply be mixed with U-Compare components, without distinguishing between locally and remotely deployed resources.  Availability: http://u-compare.org/  Contact: kano@is.s.u-tokyo.ac.jp 10.1093/bioinformatics/btp289"
11,1,16746,1,Complex event extraction at PubMed scale,"Motivation: There has recently been a notable shift in biomedical information extraction (IE) from relation models toward the more expressive event model, facilitated by the maturation of basic tools for biomedical text analysis and the availability of manually annotated resources. The event model allows detailed representation of complex natural language statements and can support a number of advanced text mining applications ranging from semantic search to pathway extraction. A recent collaborative evaluation demonstrated the potential of event extraction systems, yet there have so far been no studies of the generalization ability of the systems nor the feasibility of large-scale extraction.  Results: This study considers event-based IE at PubMed scale. We introduce a system combining publicly available, state-of-the-art methods for domain parsing, named entity recognition and event extraction, and test the system on a representative 1% sample of all PubMed citations. We present the first evaluation of the generalization performance of event extraction systems to this scale and show that despite its computational complexity, event extraction from the entire PubMed is feasible. We further illustrate the value of the extraction approach through a number of analyses of the extracted information.  Availability: The event detection system and extracted data are open source licensed and available at http://bionlp.utu.fi/.  Contact: jari.bjorne@utu.fi 10.1093/bioinformatics/btq180"
12,2,4459,1,Discovering models of software processes from event-based data,"Many software process methods and tools presuppose the existence of a formal model of a process. Unfortunately, developing a formal model for an on-going, complex process can be difficult, costly, and error prone. This presents a practical barrier to the adoption of process technologies, which would be lowered by automated assistance in creating formal models. To this end, we have developed a data analysis technique that we term process discovery. Under this technique, data describing process events are first captured from an on-going process and then used to generate a formal model of the behavior of that process. In this article we describe a Markov method that we developed specifically for process discovery, as well as describe two additional methods that we   adopted from other domains and augmented for our purposes. The three methods range from the purely algorithmic to the purely statistical. We compare the methods and discuss their application in an industrial case study."
13,2,7873,1,{L}ow-{L}evel {C}omponents of {A}nalytic {A}ctivity in {I}nformation {V}isualization,"Existing system-level taxonomies of visualization tasks are geared more towards the design of particular representations than the facilitation of user analytic activity. We present a set of ten low-level analysis tasks that largely capture peopleâs activities while employing information visualization tools for understanding data. To help develop these tasks, we collected nearly 200 sample questions from students about how they would analyze five particular data sets from different domains. The questions, while not being totally comprehensive, illustrated the sheer variety of analytic questions typically posed by users when employing information visualization systems. We hope that the presented set of tasks is useful for information visualization system designers as a kind of common substrate to discuss the relative analytic capabilities of the systems. Further, the tasks may provide a form of checklist for system designers."
14,3,893,1,A cookbook for using the model-view controller user interface paradigm in Smalltalk-80,"This essay describes the Model-View-Controller (MVC) programming paradigm and methodology used in the Smalltalk-80TM programming system. MVC programming is the application of a three-way factoring, whereby objects of different classes take over the operations related to the application domain, the display of the application's state, and the user interaction with the model and the view. We present several extended examples of MVC implementations and of the layout of composite application views. The Appendices provide reference materials for the Smalltalk-80 programmer wishing to understand and use MVC better within the Smalltalk-80 system."
15,4,4036,1,Crystal structure of the ribosome at 5.5 A resolution.,"We describe the crystal structure of the complete Thermus thermophilus 70S ribosome containing bound messenger RNA and transfer RNAs (tRNAs) at 5.5 angstrom resolution. All of the 16S, 23S, and 5S ribosomal RNA (rRNA) chains, the A-, P-, and E-site tRNAs, and most of the ribosomal proteins can be fitted to the electron density map. The core of the interface between the 30S small subunit and the 50S large subunit, where the tRNA substrates are bound, is dominated by RNA, with proteins located mainly at the periphery, consistent with ribosomal function being based on rRNA. In each of the three tRNA binding sites, the ribosome contacts all of the major elements of tRNA, providing an explanation for the conservation of tRNA structure. The tRNAs are closely juxtaposed with the intersubunit bridges, in a way that suggests coupling of the 20 to 50 angstrom movements associated with tRNA translocation with intersubunit movement."
16,5,649,1,The part-time parliament,"Digital Equipment Corporation Recent archaeological discoveries on the island of Paxos reveal that the parliament functioned despite the peripatetic propensity of its part-time legislators. The legislators maintained consistent copies of the parliamentary record, despite their frequent forays from the chamber and the forgetfulness of their messengers. The Paxon parliamentâs protocol provides a new way of implementing the state-machine approach to the design of distributed systems."
17,5,954,1,Highly dynamic Destination-Sequenced Distance-Vector routing (DSDV) for mobile computers,"An  ad-hoc  network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize ad hoc networks. Our modifications address some of the previous  objections  to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks."
18,5,5452,1,MANET simulation studies: the incredibles,"Simulation is the research tool of choice for a majority of the mobile ad hoc network (MANET) community. However, while the use of simulation has increased, the credibility of the simulation results has decreased. To determine the state of MANET simulation studies, we surveyed the 2000-2005 proceedings of the ACM International Symposium on Mobile Ad Hoc Networking and Computing (MobiHoc). From our survey, we found significant shortfalls. We present the results of our survey in this paper. We then summarize common simulation study pitfalls found in our survey. Finally, we discuss the tools available that aid the development of rigorous simulation studies. We offer these results to the community with the hope of improving the credibility of MANET simulation-based studies."
19,5,7343,1,DSR The Dynamic Source Routing Protocol for Multihop Wireless Ad Hoc Networks,"The Dynamic Source Routing protocol (DSR) is a simple and efficient routing protocol designed  specifically for use in multi-hop wireless ad hoc networks of mobile nodes. DSR allows the  network to be completely self-organizing and self-configuring, without the need for any existing  network infrastructure or administration. The protocol is composed of the two mechanisms of  Route Discovery and Route Maintenance, which work together to allow nodes to discover and  maintain source routes to arbitrary destinations in the ad hoc network. The use of source routing  allows packet routing to be trivially loop-free, avoids the need for up-to-date routing information  in the intermediate nodes through which packets are forwarded, and allows nodes forwarding  or overhearing packets to cache the routing information in them for their own future use. All  aspects of the protocol operate entirely on-demand, allowing the routing packet overhead of  DSR to scale automatically to only that needed to react to changes in the routes currently in use. We have"
20,6,7493,1,The basis of anisotropic water diffusion in the nervous system - a technical review,"Anisotropic water diffusion in neural fibres such as nerve, white matter in spinal cord, or white matter in brain forms the basis for the utilization of diffusion tensor imaging (DTI) to track fibre pathways. The fact that water diffusion is sensitive to the underlying tissue microstructure provides a unique method of assessing the orientation and integrity of these neural fibres, which may be useful in assessing a number of neurological disorders. The purpose of this review is to characterize the relationship of nuclear magnetic resonance measurements of water diffusion and its anisotropy (i.e. directional dependence) with the underlying microstructure of neural fibres. The emphasis of the review will be on model neurological systems both in vitro and in vivo. A systematic discussion of the possible sources of anisotropy and their evaluation will be presented followed by an overview of various studies of restricted diffusion and compartmentation as they relate to anisotropy. Pertinent pathological models, developmental studies and theoretical analyses provide further insight into the basis of anisotropic diffusion and its potential utility in the nervous system. Copyright Â© 2002 John Wiley & Sons, Ltd."
21,7,5385,1,Inferring DomainâDomain Interactions From ProteinâProtein Interactions,"10.1101/gr.153002 The interaction between proteins is one of the most important features of protein functions. Behind proteinâprotein interactions there are protein domains interacting physically with one another to perform the necessary functions. Therefore, understanding protein interactions at the domain level gives a global view of the protein interaction network, and possibly of protein functions. Two research groups used yeast two-hybrid assays to generate 5719 interactions between proteins of the yeast . This allows us to study the large-scale conserved patterns of interactions between protein domains. Using evolutionarily conserved domains defined in a proteinâdomain database called PFAM (), we apply a Maximum Likelihood Estimation method to infer interacting domains that are consistent with the observed proteinâprotein interactions. We estimate the probabilities of interactions between every pair of domains and measure the accuracies of our predictions at the protein level. Using the inferred domainâdomain interactions, we predict interactions between proteins. Our predicted proteinâprotein interactions have a significant overlap with the proteinâprotein interactions (MIPS: ) obtained by methods other than the two-hybrid assays. The mean correlation coefficient of the gene expression profiles for our predicted interaction pairs is significantly higher than that for random pairs. Our method has shown robustness in analyzing incomplete data sets and dealing with various experimental errors. We found several novel proteinâprotein interactions such as RPS0A interacting with APG17 and TAF40 interacting with SPT3, which are consistent with the functions of the proteins. [Supplementary material is available online at  and.]"
22,8,2216,1,A Tutorial on Principal Component Analysis,"This tutorial is designed to give the reader an understanding of Principal Components Analysis (PCA). PCA is a useful statistical technique that has found application in fields such as face recognition and image compression, and is a common technique for finding patterns in data of high dimension. Before getting to a description of PCA, this tutorial first introduces mathematical concepts that will be used in PCA. It covers standard deviation, covariance, eigenvec- tors and eigenvalues. This background knowledge is meant to make the PCA section very straightforward, but can be skipped if the concepts are already familiar. There are examples all the way through this tutorial that are meant to illustrate the concepts being discussed. If further information is required, the mathematics textbook âElementary Linear Algebra 5eâ by Howard Anton, Publisher John Wiley & Sons Inc, ISBN 0-471-85223-6 is a good source of information regarding the mathematical back- ground"
23,8,4833,1,The Geometry of Graphs and some of its Algorithmic Applications,"In this paper we explore some implications of viewing graphs as geometric objects. This approach offers a new perspective on a number of graph--theoretic and algorithmic problems. There are several ways to model graphs geometrically and our main concern here is with geometric representations that respect the metric of the (possibly weighted) graph. Given a graph G we map its vertices to a normed space in an attempt to (i) keep down the dimension of the host space, and (ii) guarantee a small distortion, i.e., make sure that distances between vertices in G closely match the distances between their geometric images. In this paper we develop efficient algorithms for embedding graphs low--dimensionally with a small distortion. Further algorithmic applications include:  ffl A simple, unified approach to a number of problems on multicommodity flows, including the Leighton--Rao Theorem [37] and some of its extensions. We solve an open question in this area, showing that the max--flow vs. min--..."
24,8,6365,1,The Price of Stability for Network Design with Fair Cost Allocation,"Network design is a fundamental problem for which it is important to understand the effects of strategic behavior. Given a collection of selfinterested agents who want to form a network connecting certain endpoints, the set of stable solutions --- the Nash equilibria --- may look quite different from the centrally enforced optimum. We study the quality of the best Nash equilibrium, and refer to the ratio of its cost to the optimum network cost as the price of stability. The best Nash equilibrium solution has a natural meaning of stability in this context --- it is the optimal solution that can be proposed from which no user will ""defect"". We consider the price of stability for network design with respect to one of the most widely-studied protocols for network cost allocation, in which the cost of each edge is divided equally between users whose connections make use of it; this fair-division scheme can be derived from the Shapley value, and has a number of basic economic motivations. We show that the price of stability for network design with respect to this fair cost allocation is $O(log k)$, where $k$ is the number of users, and that a good Nash equilibrium can be achieved via best-response dynamics in which users iteratively defect from a starting solution. This establishes that the fair cost allocation protocol is in fact a useful mechanism for inducing strategic behavior to form near-optimal equilibria. We discuss connections to the class of potential games defined by Monderer and Shapley, and extend our results to cases in which users are seeking to balance network design costs with latencies in the constructed network, with stronger results when the network has only delays and no construction costs. We also present bounds on the convergence time of best-response dynamics, and discuss extensions to a weighted game."
25,8,9056,1,Discrete mathematics: methods and challenges,"Combinatorics is a fundamental mathematical discipline as well as an essential component of many mathematical areas, and its study has experienced an impressive growth in recent years. One of the main reasons for this growth is the tight connection between Discrete Mathematics and Theoretical Computer Science, and the rapid development of the latter. While in the past many of the basic combinatorial results were obtained mainly by ingenuity and detailed reasoning, the modern theory has grown out of this early stage, and often relies on deep, well developed tools. This is a survey of two of the main general techniques that played a crucial role in the development of modern combinatorics; algebraic methods and probabilistic methods. Both will be illustrated by examples, focusing on the basic ideas and the connection to other areas."
26,9,2096,1,Basics of qualitative research: Grounded theory procedures and techniques,"Since the `discovery' of grounded theory in 1969, it has become one of the major strategies for qualitative research available to the social scientist and professional. But only now, with the publication of **Basics of Qualitative Research **do Strauss, co-creator of grounded theory, and Corbin present the practical procedures and techniques for doing grounded theory studies at a level easily understood by students in applied disciplines.  Written in accessible language and replete with definitions and examples, **Basics of Grounded Theory **provides a step-by-step approach to doing research from formulation of the initial research question, through various systems of coding and analysis, to the process of writing or speaking on the research topic. A final chapter containing standards for evaluating a grounded theory study will establish the benchmark for this kind of work. This volume is an invaluable tool for the novice researcher and a useful text for courses in qualitative research in social science programmes and for professionals engaged in research."
27,9,7591,1,A review of outsourcing from the resource-based view of the firm,"The phenomenon of outsourcing is becoming increasingly widespread among organizations and is now one of the strategic decisions that attract the greatest interest from professionals and organizational scholars. The primary purpose of the paper is to contribute with a review of the principal works that address outsourcing from the resource-based view of the firm (RBV). The paper begins by setting out the main premises of outsourcing and then presents the different concepts of outsourcing and proposes a concept that is more in line with the theoretical framework used. This is followed by an analysis of the principal differences and similarities of the treatments of outsourcing from the traditional perspective of the transaction costs economics theory (TCE) and from the more strategic and up-to-date RBV. The next section contains a review of the most significant theoretical and empirical works on outsourcing that address outsourcing from the RBV. The contributions are classified into two categories, depending on the objectives: works that study the propensity to outsource and works that study the relationship between the outsourcing decision and organizational performance. Finally, a framework is proposed that is based on the resource and capability view with the aim of contributing to a better understanding of outsourcing and facilitating future empirical works from the RBV that are complementary and examine issues of greater interest that have been less developed in the literature to date."
28,9,10385,1,The Economics of Organization: The Transaction Cost Approach,"The transaction cost approach to the study of economic organization regards the transaction as the basic unit of analysis and holds that an understanding of transaction cost economizing is central to the study of organizations. Applications of this approach require that transactions be dimensionalized and that alternative governance structures be described. Economizing is accomplished by assigning transactions to governance structures in a discriminating way. The approach applies both to the determination of efficient boundaries, as between firms and markets, and to the organization of internal transactions, including the design of employment relations. The approach is compared and contrasted with selected parts of the organization theory literature."
29,10,1127,1,STRIPS: A new approach to the application of theorem proving to problem solving,We describe a new problem solver called STRIPS that attempts to find a sequence of operators in a space of world models to transform a given initial world model in which a given goal formula can be proven to be true. STRIPS represents a world model as an arbitrary collection in first-order predicate calculus formulas and is designed to work with models consisting of large numbers of formula. It employs a resolution theorem prover to answer questions of particular models and uses means-ends analysis to guide it to the desired goal-satisfying model.
30,10,2915,1,The Computer for the 21st Century,"The arcane aura that surrounds personal computers is not just a `user interface' problem. The idea of a `personal' computer itself is misplaced and that the vision of laptop machines, dynabooks and knowledge navigators is only a transitional step toward achieving the real potential of information technology. Such machines cannot truly make computing an integral, invisible part of people's lives. The author and his colleagues are therefore trying to conceive a new way of thinking about computers, one that takes into account the human world and allows the computers themselves to vanish into the background."
31,10,5041,1,Current Solutions for Web Service Composition,"Web service composition lets developers create applications on top of service-oriented computing's native description, discovery, and communication capabilities. Such applications are rapidly deployable and offer developers reuse possibilities and users seamless access to a variety of complex services.  There are many existing approaches to service composition, ranging from abstract methods to those aiming to be industry standards.  The authors describe four key issues for Web service composition."
32,10,11028,1,A Survey of Automated Web Service Composition Methods,"In todayâs Web, Web services are created and updated on the fly. Itâs already beyond the human ability to analysis them and generate the composition plan manually. A number of approaches have been proposed to tackle that problem. Most of them are inspired by the researches in cross-enterprise workflow and AI planning. This paper gives an overview of recent research efforts of automatic Web service composition both from the workflow and AI planning research community."
33,11,1756,1,Intelligence without representation,"Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, with strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outline our approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not into independent information processing units which must interface with each other via representations. Instead, the intelligent system is decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather than interface to each other particularly much. The notions of central and peripheral systems evaporate--everything is both central and peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in standard office environments."
34,11,2769,1,"Charting past, present, and future research in ubiquitous computing","The proliferation of computing into the physical world promises more than the ubiquitous availability of computing infrastructure; it suggest new paradigms of interaction inspired by constant access to information and computational capabilities. For the past decade, application-driven research on abiquitous computing (ubicomp) has pushed three interaction themes: natural interfaces, context-aware applications, and automated capture and access . To chart a course for future research in ubiquitous computing, we review the accomplishments of these efforts and point to remaining research challenges. Research in ubiquitious computing implicitly requires addressing some notion of scale, whether in the number and type of devices, the physical space of    distributed computing, or the number of people using a system. We posit a new area of applications research,  everyday computing,  focussed on scaling interaction with respect to time. Just as pushing the availiability of computing away from the traditional desktop fundamentally changes the relationship between humans and computers, providing  continuous interaction moves computing from a localized tool to a constant companion. Designing for continous interaction requires addressing interruption and reumption of intreaction, representing passages of time and providing associative storage models. Inherent in all of these interaction themes are difficult issues in the  social implications  of ubiquitous computing and the challenges of     evaluating>  ubiquitious computing research. Although cumulative experience points to lessons in privacy, security, visibility, and control, there are no simple guidelines for steering research efforts. Akin to any efforts involving new technologies, evaluation strategies form a spectrum from technology feasibility efforts to long-term use studies&mdash;but a user-centric perspective is always possible and necessary"
35,11,4013,1,Towards a taxonomy of software connectors,"Software systems of today are frequently composed from prefabricated, heterogeneous components that provide complex functionality and engage in complex interactions. Existing research on component-based development has mostly focused on component structure, interfaces, and functionality. Recently, software architecture has emerged as an area that also places significant importance on component interactions, embodied in the notion of software connectors. However, the current level of understanding and support for connectors has been insufficient. This has resulted in their inconsistent treatment and a notable lack of understanding of what the fundamental building blocks of software interaction are and how they can be composed into more complex interactions. This paper attempts to address this problem. It presents a comprehensive classification framework and taxonomy of software connectors. The taxonomy is obtained through an extensive analysis of existing component interactions. The taxonomy is used both to understand existing software connectors and to suggest new, unprecedented connectors. We demonstrate the use of the taxonomy on the architecture of a large, existing system."
36,11,4729,1,Making sense of sensing systems: five questions for designers and researchers,"This paper borrows ideas from social science to inform the design of novel ""sensing"" user-interfaces for computing technology. Specifically, we present five design challenges inspired by analysis of human-human communication that are mundanely addressed by traditional graphical user interface designs (GUIs). Although classic GUI conventions allow us to finesse these questions, recent research into innovative interaction techniques such as 'Ubiquitous Computing' and 'Tangible Interfaces' has begun to expose the interaction challenges and problems they pose. By making them explicit we open a discourse on how an approach similar to that used by social scientists in studying human-human interaction might inform the design of novel interaction mechanisms that can be used to handle human-computer communication accomplishments"
37,11,7319,1,Architecting for Usability; a Survey,"Over the years the software engineering community has increasingly realized the important roles oftware architecture plays in fulfilling the quality requirements of a system. The quality attributes of a software system are, to a large extent determined by the system&#039;s software architecture. In recent years, the software engineering community has developed various tools and techniques that allow for design for quality attributes, such as performance or maintainability, at the software architecture level. We believe this design approach can be applied not only to &#034;traditional&#034; quality attributes such as performance or maintainability but also to usability. This survey explores the feasibility of such a design approach. Current practice is surveyed from the perspective of a software architect. Are there any design methods that allow for design for usability at the architectural level? Are there any evaluation tools that allow assessment of architectures for their support of usability? What is usability? A framework is presented which visualizes these three research questions. Usability should drive design at all stages, but current usability engineering practice fails to fully achieve this goal. Our survey shows that there are no design techniques or assessment tools that allow for design for usability at the architectural level."
38,11,11176,1,{The Architecture of Complexity},"A number of proposals have been advanced in recent years for the development of âgeneral systems theory â that, abstracting from properties peculiar to physical, biological, or social systems, would be applicable to all of them. 1 We might well feel that, while the goal is laudable, systems of such diverse kinds could hardly be expected to have any nontrivial properties in common. Metaphor and analogy can be helpful, or they can be misleading. All depends on whether the similarities the metaphor captures are significant or superficial. It may not be entirely vain, however, to search for common properties among diverse kinds of complex systems. The ideas that go by the name of cybernetics constitute, if not a theory, at least a point of view that has been proving fruitful over a wide range of applications. 2 It has been useful to look at the behavior of adaptive systems in terms of the concepts of feedback and homeostasis, and to analyze adaptiveness in terms of the theory of selective information. 3 The ideas of feedback and information provide a frame of reference for viewing a wide range of situations, just as do the ideas of evolution, of relativism, of axiomatic method, and of"
39,11,14288,1,"A survey of autonomic computing---degrees, models, and applications","Autonomic Computing is a concept that brings together many fields of computing with the purpose of creating computing systems that self-manage. In its early days it was criticised as being a &ldquo;hype topic&rdquo; or a rebadging of some Multi Agent Systems work. In this survey, we hope to show that this was not indeed &lsquo;hype&rsquo; and that, though it draws on much work already carried out by the Computer Science and Control communities, its innovation is strong and lies in its robust application to the specific self-management of computing systems. To this end, we first provide an introduction to the motivation and concepts of autonomic computing and describe some research that has been seen as seminal in influencing a large proportion of early work. Taking the components of an established reference model in turn, we discuss the works that have provided significant contributions to that area. We then look at larger scaled systems that compose autonomic systems illustrating the hierarchical nature of their architectures. Autonomicity is not a well defined subject and as such different systems adhere to different degrees of Autonomicity, therefore we cross-slice the body of work in terms of these degrees. From this we list the key applications of autonomic computing and discuss the research work that is missing and what we believe the community should be considering."
40,12,1539,1,Gene ontology: Tool for the unification of biology,"Genomic sequencing has made it clear that a large fraction of the genes specifying the core biological functions are shared by all eukaryotes. Knowledge of the biological role of such shared proteins in one organism can often be transferred to other organisms. The goal of the Gene Ontology Consortium is to produce a dynamic, controlled vocabulary that can be applied to all eukaryotes even as knowledge of gene and protein roles in cells is accumulating and changing. To this end, three independent ontologies accessible on the World-Wide Web (http://www.geneontology.org) are being constructed: biological process, molecular function and cellular component."
41,12,3628,1,CSCW at play: <i>'there'</i> as a collaborative virtual environment,"Video games are of increasing importance, both as a cultural phenomenon and as an application of collaborative technology. In particular, many recent online games feature persistent collaborative virtual environments (CVEs), with complex social organisation and strong social bonds between players. This paper presents a study of &#60;i>'There'&#60;/i>, one such game, focusing on how &#60;i>There&#60;/i> has been appropriated by its players. In particular we describe how its flexibility has allowed players to develop their own forms of play within the game. Three aspects of &#60;i>There&#60;/i> are discussed: first, how the environment supports a range of social activities around objects. Second, how the chat environment is used to produce overlapping chat and how the game itself provides topics for conversation. Lastly, how the 'place' of &#60;i>There&#60;/i> is a fluid interaction space that supports safe interactions between strangers. The paper concludes by drawing design lessons concerning the importance of supporting shared online activity, interaction between strangers, and the difficulties of designing for play."
42,12,4980,1,Integration of Touch and Sound in Auditory Cortex,"SummaryTo form a coherent percept of the environment, our brain combines information from different senses. Such multisensory integration occurs in higher association cortices; but supposedly, it also occurs in early sensory areas. Confirming the latter hypothesis, we unequivocally demonstrate supra-additive integration of touch and sound stimulation at the second stage of the auditory cortex. Using high-resolution fMRI of the macaque monkey, we quantified the integration of auditory broad-band noise and tactile stimulation of hand and foot in anaesthetized animals. Integration was found posterior to and along the lateral side of the primary auditory cortex in the caudal auditory belt. Integration was stronger for temporally coincident stimuli and obeyed the principle of inverse effectiveness: greater enhancement for less effective stimuli. These findings demonstrates that multisensory integration occurs early and close to primary sensory areas and--because it occurs in anaesthetized animals--suggests that this integration is mediated by preattentive bottom-up mechanisms."
43,12,7871,1,Laws of Software Evolution Revisited,"Data obtained during a 1968 study of the software process [8] led to an investigation of the evolution of OS/360 [13] and and, over a period of twenty years, to formulation of eight Laws of Software Evolution. The FEAST project recently initiated (see sections 4â6 below) is expected to throw additional light on the phenomenology underlying these laws, to increase understanding of them, to explore their finer detail, to expose their wider relevance and implications and to develop means for their beneficial exploitation. This paper is intended to trigger wider interest in the laws and in the FEAST study of feedback and feedback control in the context of the software process and its improvement to ensure beneficial exploitation of their potential."
44,12,9836,1,Structure and tie strengths in mobile communication networks,"10.1073/pnas.0610245104 Electronic databases, from phone to e-mails logs, currently provide detailed records of human communication patterns, offering novel avenues to map and explore the structure of social and communication networks. Here we examine the communication patterns of millions of mobile phone users, allowing us to simultaneously study the local and the global structure of a society-wide communication network. We observe a coupling between interaction strengths and the network's local structure, with the counterintuitive consequence that social networks are robust to the removal of the strong ties but fall apart after a phase transition if the weak ties are removed. We show that this coupling significantly slows the diffusion process, resulting in dynamic trapping of information in communities and find that, when it comes to information diffusion, weak and strong ties are both simultaneously ineffective."
45,12,11492,1,ATM and ATR Substrate Analysis Reveals Extensive Protein Networks Responsive to DNA Damage,"Cellular responses to DNA damage are mediated by a number of protein kinases, including ATM (ataxia telangiectasia mutated) and ATR (ATM and Rad3-related). The outlines of the signal transduction portion of this pathway are known, but little is known about the physiological scope of the DNA damage response (DDR). We performed a large-scale proteomic analysis of proteins phosphorylated in response to DNA damage on consensus sites recognized by ATM and ATR and identified more than 900 regulated phosphorylation sites encompassing over 700 proteins. Functional analysis of a subset of this data set indicated that this list is highly enriched for proteins involved in the DDR. This set of proteins is highly interconnected, and we identified a large number of protein modules and networks not previously linked to the DDR. This database paints a much broader landscape for the DDR than was previously appreciated and opens new avenues of investigation into the responses to DNA damage in mammals. 10.1126/science.1140321"
46,12,12991,1,Maps of random walks on complex networks reveal community structure,"10.1073/pnas.0706851105 To comprehend the multipartite organization of large-scale biological and social systems, we introduce an information theoretic approach that reveals community structure in weighted and directed networks. We use the probability flow of random walks on a network as a proxy for information flows in the real system and decompose the network into modules by compressing a description of the probability flow. The result is a map that both simplifies and highlights the regularities in the structure and their relationships. We illustrate the method by making a map of scientific communication as captured in the citation patterns of >6,000 journals. We discover a multicentric organization with fields that vary dramatically in size and degree of integration into the network of science. Along the backbone of the networkâincluding physics, chemistry, molecular biology, and medicineâinformation flows bidirectionally, but the map reveals a directional pattern of citation from the applied fields to the basic sciences."
47,12,15879,1,"Strategic reading, ontologies, and the future of scientific publishing.","The revolution in scientific publishing that has been promised since the 1980s is about to take place. Scientists have always read strategically, working with many articles simultaneously to search, filter, scan, link, annotate, and analyze fragments of content. An observed recent increase in strategic reading in the online environment will soon be further intensified by two current trends: (i) the widespread use of digital indexing, retrieval, and navigation resources and (ii) the emergence within many scientific disciplines of interoperable ontologies. Accelerated and enhanced by reading tools that take advantage of ontologies, reading practices will become even more rapid and indirect, transforming the ways in which scientists engage the literature and shaping the evolution of scientific publishing. 10.1126/science.1157784"
48,12,16815,1,Online Professionalism and the Mirror of Social Media,"The rise of social mediaâcontent created by Internet users and hosted by popular sites such as Facebook, Twitter, YouTube, and Wikipedia, and blogsâhas brought several new hazards for medical professionalism. First, many physicians may find applying principles for medical professionalism to the online environment challenging in certain contexts. Second, physicians may not consider the potential impact of their online content on their patients and the public. Third, a momentary lapse in judgment by an individual physician to create unprofessional content online can reflect poorly on the entire profession. To overcome these challenges, we encourage individual physicians to realize that as they  tread  through the World Wide Web, they leave behind a  footprint  that may have unintended negative consequences for them and for the profession at large. We also recommend that institutions take a proactive approach to engage users of social media in setting consensus-based standards for  online professionalism.  Finally, given that professionalism encompasses more than the avoidance of negative behaviors, we conclude with examples of more positive applications for this technology. Much like a mirror, social media can reflect the best and worst aspects of the content placed before it for all to see."
49,13,2879,1,Analysis of TCP Performance Over Mobile Ad Hoc Networks,"Mobile ad hoc networks have gained a lot of attention lately as a means of providing continuous network connectivity to mobile computing devices regardless of physical location. Recently, a large amount of research has focused on the routing protocols needed in such an environment. In this paper, we investigate the effects that link breakage due to mobility has on TCP performance. Through simulation, we show that TCP throughput drops significantly when nodes move, due to TCP's inability to..."
50,13,7001,1,Ad hoc On-Demand Distance Vector (AODV) Routing,"The Ad Hoc On-Demand Distance Vector (AODV) routing protocol is intended for use by mobile nodes in an ad hoc network. It offers quick adaptation to dynamic link conditions, low processing and memory overhead, low network utilization, and determines both unicast Perkins, Royer, Das Expires 22 April 2000 [Page i]  Internet Draft AODV 22 October 1999 and multicast routes between sources and destinations. It uses destination sequence numbers to ensure loop freedom at all times (even in the face of anomalous delivery of routing control messages), solving problems (such as &#034;counting to infinity&#034;) associated with classical distance vector protocols. Contents Status of This Memo i Abstract i 1. Introduction 1 2. Overview 2 3. AODV Terminology 4 4. Route Request (RREQ) Message Format 6 5. Route Reply (RREP) Message Format 8 6. Route Error (RERR) Message Format 9 7. Multicast Activation (MACT) Message Format 10 8. Group Hello (GRPH) Message Format 11 9. Node Operation - Unicast 12 9.1. Maintai..."
51,14,7454,1,Communities of practice and social learning systems,"10.1177/135050840072002  This essay argues that the success of organizations depends on their ability to design themselves as social learning systems and also to participate in broader learning systems such as an industry, a region, or a consortium. It explores the structure of these social learning systems. It proposes a social definition of learning and distinguishes between three `modes of belonging' by which we participate in social learning systems. Then it uses this framework to look at three constitutive elements of these systems: communities of practice, boundary processes among these communities, and identities as shaped by our participation in these systems."
52,15,6360,1,Literature mining for the biologist: from information retrieval to biological discovery," For the average biologist, hands-on literature mining currently means a keyword search in PubMed. However, methods for extracting biomedical facts from the scientific literature have improved considerably, and the associated tools will probably soon be used in many laboratories to automatically annotate and analyse the growing number of system-wide experimental data sets. Owing to the increasing body of text and the open-access policies of many journals, literature mining is also becoming useful for both hypothesis generation and biological discovery. However, the latter will require the integration of literature and high-throughput data, which should encourage close collaborations between biologists and computational linguists."
53,16,886,1,Using mixture models for collaborative filtering,"A collaborative filtering system at an e-commerce site or similar service uses data about aggregate user behavior to make recommendations tailored to specific user interests. We develop recommendation algorithms with provable performance guarantees in a probabilistic mixture model for collaborative filtering proposed by Hoffman and Puzicha. We identify certain novel parameters of mixture models that are closely connected with the best achievable performance of a recommendation algorithm; we show that for any system in which these parameters are bounded, it is possible to give recommendations whose quality converges to optimal as the amount of data grows. All our bounds depend on a new measure of independence that can be viewed as an $L_1$-analogue of the smallest singular value of a matrix. Using this, we introduce a technique based on generalized pseudoinverse matrices and linear programming for handling sets of high-dimensional vectors. We also show that standard approaches based on $L_2$ spectral methods are not strong enough to yield comparable results, thereby suggesting some inherent limitations of spectral analysis."
54,16,5604,1,Shilling recommender systems for fun and profit,"Recommender systems have emerged in the past several years as an effective way to help people cope with the problem of information overload. One application in which they have become particularly common is in e-commerce, where recommendation of items can often help a customer find what she is interested in and, therefore can help drive sales. Unscrupulous producers in the never-ending quest for market penetration may find it profitable to shill recommender systems by lying to the systems in order to have their products recommended more often than those of their competitors. This paper explores four open questions that may affect the effectiveness of such shilling attacks: which recommender algorithm is being used, whether the application is producing recommendations or predictions, how detectable the attacks are by the operator of the system, and what the properties are of the items being attacked. The questions are explored experimentally on a large data set of movie ratings. Taken together, the results of the paper suggest that new ways must be used to evaluate and detect shilling attacks on recommender systems."
55,17,442,1,Simultaneous determination of protein structure and dynamics,We present a protocol for the experimental determination of ensembles of protein conformations that represent simultaneously the native structure and its associated dynamics. The procedure combines the strengths of nuclear magnetic resonance spectroscopyâfor obtaining experimental information at the atomic level about the structural and dynamical features of proteinsâwith the ability of molecular dynamics simulations to explore a wide range of protein conformations. We illustrate the method for human ubiquitin in solution and find that there is considerable conformational heterogeneity throughout the protein structure. The interior atoms of the protein are tightly packed in each individual conformation that contributes to the ensemble but their overall behaviour can be described as having a significant degree of liquid-like character. The protocol is completely general and should lead to significant advances in our ability to understand and utilize the structures of native proteins.
56,17,4140,1,Calculating structures and free energies of complex molecules: Combining molecular mechanics and continuum models,"PMID: 11123888 A historical perspective on the application of molecular dynamics (MD) to biological macromolecules is presented. Recent developments combining state-of-the-art force fields with continuum solvation calculations have allowed us to reach the fourth era of MD applications in which one can often derive both accurate structure and accurate relative free energies from molecular dynamics trajectories. We illustrate such applications on nucleic acid duplexes, RNA hairpins, protein folding trajectories, and proteinligand, proteinprotein, and proteinnucleic acid interactions."
57,18,4236,1,Visual receptive field organization,"Increasingly systematic approaches to quantifying receptive fields in primary visual cortex, combined with inspired ideas about functional circuitry, non-linearities, and visual stimuli, are bringing new interest to classical problems. This includes the distinction and hierarchy between simple and complex cells, the mechanisms underlying the receptive field surround, and debates about optimal stimuli for mapping receptive fields. An important new problem arises from recent observations of stimulus-dependent spatial and temporal summation in primary visual cortex. It appears that the receptive field can no longer be considered unique, and we might have to relinquish this cherished notion as the embodiment of neuronal function in primary visual cortex."
58,19,10606,1,Why We Tag: Motivations for Annotation in Mobile and Online Media,"morganya stanford.edu Why do people tag? Users have mostly avoided annotating media such as photos â both in desktop and mobile environments â despite the many potential uses for annotations, including recall and retrieval. We investigate the incentives for annotation in Flickr, a popular web-based photo-sharing system, and ZoneTag, a cameraphone photo capture and annotation tool that uploads images to Flickr. In Flickr, annotation (as textual tags) serves both personal and social purposes, increasing incentives for tagging and resulting in a relatively high number of annotations. ZoneTag, in turn, makes it easier to tag cameraphone photos that are uploaded to Flickr by allowing annotation and suggesting relevant tags immediately after capture. A qualitative study of ZoneTag/Flickr users exposed various tagging patterns and emerging motivations for photo annotation. We offer a taxonomy of motivations for annotation in this system along two dimensions (sociality and function), and explore the various factors that people consider when tagging their photos. Our findings suggest implications for the design of digital photo organization and sharing applications, as well as other applications that incorporate user-based annotation."
59,19,13444,1,Understanding the Efficiency of Social Tagging Systems using Information Theory,"Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software.Understanding the Efficiency of Social Tagging Systems using Information Theory Ed H. Chi Palo Alto Research Center 3333 Coyote Hill Road, Palo Alto, CA echi@parc.com Todd Mytkowicz Dept. of Computer Science University of Colorado, Boulder, CO Todd.Mytkowicz@colorado.edu ABSTRACT Given the rise in popularity of social tagging systems, it seems only natural to ask how ecient is the organically evolved tagging vocabulary in describing underlying docu- ment objects? Does this distributed process really provide a way to circumnavigate the traditional \vocabulary problem"" with ontology? We analyze a social tagging site, namely del.icio.us, with information theory in order to evaluate the eciency of this social tagging site for encoding navigation paths to information sources. We show that information theory provides a natural and interesting way to understand this eciency or the descriptive, encoding power of tags. Our results indicate the eciency of tags appears to be wan- ing. We discuss the implications of our ndings and provide insight into how our methods can be used to design more usable social tagging software."
60,19,14178,1,Social tag prediction,"In this paper, we look at the âsocial tag predictionâ prob- lem. Given a set of objects, and a set of tags applied to those objects by users, can we predict whether a given tag could/should be applied to a particular object? We inves- tigated this question using one of the largest crawls of the social bookmarking system del.icio.us gathered to date. For URLs in del.icio.us, we predicted tags based on page text, anchor text, surrounding hosts, and other tags applied to the URL. We found an entropy-based metric which captures the generality of a particular tag and informs an analysis of how well that tag can be predicted. We also found that tag-based association rules can produce very high-precision predictions as well as giving deeper understanding into the relationships between tags. Our results have implications for both the study of tagging systems as potential information retrieval tools, and for the design of such systems."
61,20,922,1,Foundations of Statistical Natural Language Processing,"{""Statistical natural-language processing is, in my estimation, one of the most fast-moving and exciting areas of computer science these days. Anyone who wants to learn this field would be well advised to get this book. For that matter, the same goes for anyone who is already in the field. I know that it is going to be one of the most well-thumbed books on my bookshelf."" -- Eugene Charniak, Department of Computer Science, Brown University  <P>Statistical approaches to processing natural language text have become dominant in recent years. This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.  <P>More on this book}"
62,20,1182,1,Machine Learning,"Machine Learning is the study of methods for programming computers to learn. Computers are applied to a wide range of tasks, and for most of these it is relatively easy for programmers to design and implement the necessary software. However, there are many tasks for which this is difficult or impossible. These can be divided into four general categories. First, there are problems for which there exist no human experts. For example, in modern automated manufacturing facilities, there is a need to predict machine failures before they occur by analyzing sensor readings. Because the machines are new, there are no human experts who can be interviewed by a programmer to provide the knowledge necessary to build a computer system. A machine learning system can study recorded data and subsequent machine failures and learn prediction rules. Second, there are problems where human experts exist, but where they are unable to explain their expertise. This is the case in many perceptual tasks, such as speech recognition, hand-writing recognition, and natural language understanding. Virtually all humans exhibit expert-level abilities on these tasks, but none of them can describe the detailed steps that they follow as they perform them. Fortunately, humans can provide machines with examples of the inputs and correct outputs for these tasks, so machine learning algorithms can learn to map the inputs to the outputs. Third, there are problems where phenomena are changing rapidly. In finance, for example, people would like to predict the future behavior of the stock market, of consumer purchases, or of exchange rates. These behaviors change frequently, so that even if a programmer could construct a good predictive computer program, it would need to be rewritten frequently. A learning program can relieve the programmer of this burden by constantly modifying and tuning a set of learned prediction rules. Fourth, there are applications that need to be customized for each computer user separately. Consider, for example, a program to filter unwanted electronic mail messages. Different users will need different filters. It is unreasonable to expect each user to program his or her own rules, and it is infeasible to provide every user with a software engineer to keep the rules up-to-date. A machine learning system can learn which mail messages the user rejects and maintain the filtering rules automatically. Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis. Statistics focuses on understanding the phenomena that have generated the data, often with the goal of testing different hypotheses about those phenomena. Data mining seeks to find patterns in the data that are understandable by people. Psychological studies of human learning aspire to understand the mechanisms underlying the various learning behaviors exhibited by people (concept learning, skill acquisition, strategy change, etc.). In contrast, machine learning is primarily concerned with the accuracy and effectiveness of the resulting computer system. To illustrate this, consider the different questions that might be asked about speech data. A machine learning approach focuses on building an accurate and efficient speech recognition system. A statistician might collaborate with a psychologist to test hypotheses about the mechanisms underlying speech recognition. A data mining approach might look for patterns in speech data that could be applied to group speakers according to age, sex, or level of education."
63,20,1626,1,Parallel human genome analysis: microarray-based expression monitoring of 1000 genes.,"Microarrays containing 1046 human cDNAs of unknown sequence were printed on glass with high-speed robotics. These 1.0-cm2 DNA ""chips"" were used to quantitatively monitor differential expression of the cognate human genes using a highly sensitive two-color hybridization assay. Array elements that displayed differential expression patterns under given experimental conditions were characterized by sequencing. The identification of known and novel heat shock and phorbol ester-regulated genes in human T cells demonstrates the sensitivity of the assay. Parallel gene analysis with microarrays provides a rapid and efficient method for large-scale human gene discovery."
64,20,2220,1,Nonlinear Programming,"{This extensive rigorous texbook, developed through instruction at MIT, focuses on nonlinear and other types of optimization: iterative algorithms for constrained and unconstrained optimization, Lagrange multipliers and duality, large scale problems, and the interface between continuous and discrete optimization.   Among its special features, the book:  1) provides extensive coverage of iterative optimization methods within a unifying framework  2) provides a detailed treatment of interior point methods for linear programming  3) covers in depth duality theory from both a variational and a geometrical/convex analysis point of view  4) includes much new material on a number of topics, such as neural network training, discrete-time optimal control, and large-scale optimization  5) includes a large number of examples and exercises detailed solutions of many of which are posted on the internet  Much supplementary/support material can be found at the book's web page  http://www.athenasc.com/nonlinbook.html}"
65,20,3322,1,Evolving Neural Networks through Augmenting Topologies,"An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT) that outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution."
66,20,5109,1,A haplotype map of the human genome,"Inherited genetic variation has a critical but as yet largely uncharacterized role in human disease. Here we report a public database of common variation in the human genome: more than one million single nucleotide polymorphisms (SNPs) for which accurate and complete genotypes have been obtained in 269 DNA samples from four populations, including ten 500-kilobase regions in which essentially all information about common DNA variation has been extracted. These data document the generality of recombination hotspots, a block-like structure of linkage disequilibrium and low haplotype diversity, leading to substantial correlations of SNPs with many of their neighbours. We show how the HapMap resource can guide the design and analysis of genetic association studies, shed light on structural variation and recombination, and identify loci that may have been subject to natural selection during human evolution."
67,20,5624,1,"On Image Filtering, Noise and Morphological Size Intensity Diagrams","In the absence of a pure noise-free image it is hard to define what noise is, in any original noisy image, and as a consequence also where it is, and in what amount. In fact, the definition of noise depends largely on our own aim in the whole image analysis process, and (perhaps more important) in our self-perception of noise. For instance, when we perceive noise as disconnected and small it is normal to use MMAF filters to treat it. There is two evidences of this. First, in many instances..."
68,20,5633,1,"On Ants, Bacteria and Dynamic Environments","Wasps, bees, ants and termites all make effective use of their environment and resources by displaying collective âswarmâ intelligence. Termite colonies - for instance - build nests with a complexity far beyond the comprehension of the individual termite, while ant colonies dynamically allocate labor to various vital tasks such as foraging or defense without any central decision-making ability. Recent research suggests that microbial life can be even richer: highly social, intricately networked, and teeming with interactions, as found in bacteria. What strikes from these observations is that both ant colonies and bacteria have similar natural mechanisms based on Stigmergy and Self-Organization in order to emerge coherent and sophisticated patterns of global behaviour. Keeping in mind the above characteristics we will present a simple model to tackle the collective adaptation of a social swarm based on real ant colony behaviors (SSA algorithm) for tracking extrema in dynamic environments and highly multimodal complex functions described in the well-know De Jong test suite. Then, for the purpose of comparison, a recent model of artificial bacterial foraging (BFOA algorithm) based on similar stigmergic features is described and analyzed. Final results indicate that the SSA collective intelligence is able to cope and quickly adapt to unforeseen situations even when over the same cooperative foraging period, the community is requested to deal with two different and contradictory purposes, while outperforming BFOA in adaptive speed."
69,20,5879,1,Gene expression correlates of clinical prostate cancer behavior,"Prostate tumors are among the most heterogeneous of cancers, both histologically and clinically. Microarray expression analysis was used to determine whether global biological differences underlie common pathological features of prostate cancer and to identify genes that might anticipate the clinical behavior of this disease. While no expression correlates of age, serum prostate specific antigen (PSA), and measures of local invasion were found, a set of genes was identified that strongly correlated with the state of tumor differentiation as measured by Gleason score. Moreover, a model using gene expression data alone accurately predicted patient outcome following prostatectomy. These results support the notion that the clinical behavior of prostate cancer is linked to underlying gene expression differences that are detectable at the time of diagnosis."
70,20,5888,1,Importance of replication in microarray gene expression studies: Statistical methods and evidence from repetitive cDNA hybridizations,"We present statistical methods for analyzing replicated cDNA microarray expression data and report the results of a controlled experiment. The study was conducted to investigate inherent variability in gene expression data and the extent to which replication in an experiment produces more consistent and reliable findings. We introduce a statistical model to describe the probability that mRNA is contained in the target sample tissue, converted to probe, and ultimately detected on the slide. We also introduce a method to analyze the combined data from all replicates. Of the 288 genes considered in this controlled experiment, 32 would be expected to produce strong hybridization signals because of the known presence of repetitive sequences within them. Results based on individual replicates, however, show that there are 55, 36, and 58 highly expressed genes in replicates 1, 2, and 3, respectively. On the other hand, an analysis by using the combined data from all 3 replicates reveals that only 2 of the 288 genes are incorrectly classified as expressed. Our experiment shows that any single microarray output is subject to substantial variability. By pooling data from replicates, we can provide a more reliable analysis of gene expression data. Therefore, we conclude that designing experiments with replications will greatly reduce misclassification rates. We recommend that at least three replicates be used in designing experiments by using cDNA microarrays, particularly when gene expression data from single specimens are being analyzed."
71,20,5897,1,Yeast microarrays for genome wide parallel genetic and gene expression analysis,"{{W}e have developed high-density {DNA} microarrays of yeast {ORF}s. {T}hese microarrays can monitor hybridization to {ORF}s for applications such as quantitative differential gene expression analysis and screening for sequence polymorphisms. {A}utomated scripts retrieved sequence information from public databases to locate predicted {ORF}s and select appropriate primers for amplification. {T}he primers were used to amplify yeast {ORF}s in 96-well plates, and the resulting products were arrayed using an automated micro arraying device. {A}rrays containing up to 2,479 yeast {ORF}s were printed on a single slide. {T}he hybridization of fluorescently labeled samples to the array were detected and quantitated with a laser confocal scanning microscope. {A}pplications of the microarrays are shown for genetic and gene expression analysis at the whole genome level.}"
72,20,7073,1,Background subtraction techniques: a review,"Background subtraction is a widely used approach for detecting moving objects from static cameras. Many different methods have been proposed over the recent years and both the novice and the expert can be confused about their benefits and limitations. In order to overcome this problem, this paper provides a review of the main methods and an original categorisation based on speed, memory requirements and accuracy. Such a review can effectively guide the designer to select the most suitable method for a given application in a principled way. Methods reviewed include parametric and non-parametric background density estimates and spatial correlation approaches."
73,21,21,1,Functional discovery via a compendium of expression profiles.,"Ascertaining the impact of uncharacterized perturbations on the cell is a fundamental problem in biology. Here, we describe how a single assay can be used to monitor hundreds of different cellular functions simultaneously. We constructed a reference database or âcompendiumâ of expression profiles corresponding to 300 diverse mutations and chemical treatments in S. cerevisiae , and we show that the cellular pathways affected can be determined by pattern matching, even among very subtle profiles. The utility of this approach is validated by examining profiles caused by deletions of uncharacterized genes: we identify and experimentally confirm that eight uncharacterized open reading frames encode proteins required for sterol metabolism, cell wall function, mitochondrial respiration, or protein synthesis. We also show that the compendium can be used to characterize pharmacological perturbations by identifying a novel target of the commonly used drug dyclonine."
74,21,1115,1,Adaptation in Natural and Artificial Systems,"Genetic algorithms are playing an increasingly important role in studies of complex adaptive systems, ranging from adaptive agents in economic theory to the use of machine learning techniques in the design of complex devices such as aircraft turbines and integrated circuits. Adaptation in Natural and Artificial Systems is the book that initiated this field of study, presenting the theoretical foundations and exploring applications.  In its most familiar form, adaptation is a biological process, whereby organisms evolve by rearranging genetic material to survive in environments confronting them. In this now classic work, Holland presents a mathematical model that allows for the nonlinearity of such complex interactions. He demonstrates the model's universality by applying it to economics, physiological psychology, game theory, and artificial intelligence and then outlines the way in which this approach modifies the traditional views of mathematical genetics.  Initially applying his concepts to simply defined artificial systems with limited numbers of parameters, Holland goes on to explore their use in the study of a wide range of complex, naturally occuring processes, concentrating on systems having multiple factors that interact in nonlinear ways. Along the way he accounts for major effects of coadaptation and coevolution: the emergence of building blocks, or schemata, that are recombined and passed on to succeeding generations to provide, innovations and improvements."
75,21,1566,1,Molecular classification of cancer: class discovery and class prediction by gene expression monitoring.,"Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge."
76,21,1692,1,The Stanford Microarray Database,"The Stanford Microarray Database (SMD) stores raw and normalized data from microarray experiments, and provides web interfaces for researchers to retrieve, analyze and visualize their data. The two immediate goals for SMD are to serve as a storage site for microarray data from ongoing research at Stanford University, and to facilitate the public dissemination of that data once published, or released by the researcher. Of paramount importance is the connection of microarray data with the biological data that pertains to the DNA deposited on the microarray (genes, clones etc.). SMD makes use of many public resources to connect expression information to the relevant biology, including SGD [Ball,C.A., Dolinski,K., Dwight,S.S., Harris,M.A., Issel-Tarver,L., Kasarskis,A., Scafe,C.R., Sherlock,G., Binkley,G., Jin,H. et al. (2000) Nucleic Acids Res., 28, 77-80], YPD and WormPD [Costanzo,M.C., Hogan,J.D., Cusick,M.E., Davis,B.P., Fancher,A.M., Hodges,P.E., Kondu,P., Lengieza,C., Lew-Smith,J.E., Lingner,C. et al. (2000) Nucleic Acids Res., 28, 73-76], Unigene [Wheeler,D.L., Chappey,C., Lash,A.E., Leipe,D.D., Madden,T.L., Schuler,G.D., Tatusova,T.A. and Rapp,B.A. (2000) Nucleic Acids Res., 28, 10-14], dbEST [Boguski,M.S., Lowe,T.M. and Tolstoshev,C.M. (1993) Nature Genet., 4, 332-333] and SWISS-PROT [Bairoch,A. and Apweiler,R. (2000) Nucleic Acids Res., 28, 45-48] and can be accessed at http://genome-www.stanford.edu/microarray."
77,21,3365,1,Integrative analysis of the cancer transcriptome.,"DNA microarrays have been widely applied to the study of human cancer, delineating myriad molecular subtypes of cancer, many of which are associated with distinct biological underpinnings, disease progression and treatment response. These primary analyses have begun to decipher the molecular heterogeneity of cancer, but integrative analyses that evaluate cancer transcriptome data in the context of other data sources are often capable of extracting deeper biological insight from the data. Here we discuss several such integrative computational and analytical approaches, including meta-analysis, functional enrichment analysis, interactome analysis, transcriptional network analysis and integrative model system analysis."
78,21,5876,1,Data Analysis with Bayesian Networks: {A} Bootstrap Approach,"In recent years there has been significant  progress in algorithms and methods for inducing  Bayesian networks from data. However, in complex  data analysis problems, we need to go beyond  being satisfied with inducing networks with  high scores. We need to provide confidence measures  on features of these networks: Is the existence  of an edge between two nodes warranted?  Is the Markov blanket of a given node robust?  Can we say something about the ordering of the  variables? We should be able to address these  questions, even when the amount of data is not  enough to induce a high scoring network. In this  paper we propose Efron's Bootstrap as a computationally  efficient approach for answering these  questions. In addition, we propose to use these  confidence measures to induce better structures  from the data, and to detect the presence of latent  variables."
79,21,5885,1,Validating Clustering for Gene Expression Data,"MOTIVATION: Many clustering algorithms have been proposed for the analysis of gene expression data, but little guidance is available to help choose among them. We provide a systematic framework for assessing the results of clustering algorithms. Clustering algorithms attempt to partition the genes into groups exhibiting similar patterns of variation in expression level. Our methodology is to apply a clustering algorithm to the data from all but one experimental condition. The remaining condition is used to assess the predictive power of the resulting clusters-meaningful clusters should exhibit less variation in the remaining condition than clusters formed by chance. RESULTS: We successfully applied our methodology to compare six clustering algorithms on four gene expression data sets. We found our quantitative measures of cluster quality to be positively correlated with external standards of cluster quality."
80,21,5894,1,Semi-Supervised Support Vector Machines,"We introduce a semi-supervised support vector machine (S  3  VM)  method. Given a training set of labeled data and a working set  of unlabeled data, S  3  VM constructs a support vector machine using  both the training and working sets. We use S  3  VM to solve  the transduction problem using overall risk minimization (ORM)  posed by Vapnik. The transduction problem is to estimate the  value of a classification function at the given points in the working  set. This contrasts with the standard inductive learning problem  of estimating the classification function at all possible values and  then using the fixed function to deduce the classes of the working  set data. We propose a general S  3  VM model that minimizes both  the misclassification error and the function capacity based on all  the available data. We show how the S  3  VM model for 1-norm linear  support vector machines can be converted to a mixed-integer  program and then solved exactly using integer programming. Results  of S  3  VM and the standard 1-norm support vector machine  approach are compared on eleven data sets. Our computational  results support the statistical learning theory results showing that  incorporating working data improves generalization when insu#-  cient training information is available. In every case, S  3  VM either  improved or showed no significant di#erence in generalization compared  to the traditional approach.  # This paper has been accepted for publication in Proceedings of Neural Information Processing Systems, Denver, 1998.  1"
81,21,5903,1,"Light-directed, spatially addressable parallel chemical synthesis","Solid-phase chemistry, photolabile protecting groups, and photolithography have been combined to achieve light-directed, spatially addressable parallel chemical synthesis to yield a highly diverse set of chemical products. Binary masking, one of many possible combinatorial synthesis strategies, yields 2n compounds in n chemical steps. An array of 1024 peptides was synthesized in ten steps, and its interaction with a monoclonal antibody was assayed by epifluorescence microscopy. High-density arrays formed by light-directed synthesis are potentially rich sources of chemical diversity for discovering new ligands that bind to biological receptors and for elucidating principles governing molecular interactions. The generality of this approach is illustrated by the light-directed synthesis of a dinucleotide. Spatially directed synthesis of complex compounds could also be used for microfabrication of devices. 10.1126/science.1990438"
82,22,431,1,Kademlia: A peer-to-peer information system based on the XOR metric,"We describe a peer-to-peer distributed hash table with provable consistency and performance in a fault-prone environment. Our system routes queries and locates nodes using a novel XOR-based metric topology that simplifies the algorithm and facilitates our proof. The topology has the property that every message exchanged conveys or reinforces useful contact information. The system exploits this information to send parallel, asynchronous query messages that tolerate node failures without imposing timeout delays on users."
83,22,1359,1,Tarzan: A Peer-to-Peer Anonymizing Network Layer,"Tarzan is a peer-to-peer anonymous IP network overlay. Because it provides IP service, Tarzan is general-purpose and transparent to applications. Organized as a decentralized peer-to-peer overlay, Tarzan is fault-tolerant, highly scalable, and easy to manage. Tarzan achieves its anonymity with layered encryption and multi-hop routing, much like a Chaumian mix. A message initiator chooses a path of peers pseudo-randomly through a restricted topology in a way that adversaries cannot easily influence. Cover traffic prevents a global observer from using traffic analysis to identify an initiator. Protocols toward unbiased peer-selection offer new directions for distributing trust among untrusted entities. Tarzan provides anonymity to either clients or servers, without requiring that both participate. In both cases, Tarzan uses a network address translator (NAT) to bridge between Tarzan hosts and oblivious Internet hosts. Measurements show that Tarzan imposes minimal overhead over a corresponding non-anonymous overlay route."
84,22,2339,1,Tapestry: A Resilient Global-scale Overlay for Service Deployment,"We present Tapestry, a peer-to-peer overlay routing infrastructure offering efficient, scalable, location-independent routing of messages directly to nearby copies of an object or service using only localized resources. Tapestry supports a generic decentralized object location and routing applications programming interface using a self-repairing, soft-state-based routing layer. The paper presents the Tapestry architecture, algorithms, and implementation. It explores the behavior of a Tapestry deployment on PlanetLab, a global testbed of approximately 100 machines. Experimental results show that Tapestry exhibits stable behavior and performance as an overlay, despite the instability of the underlying network layers. Several widely distributed applications have been implemented on Tapestry, illustrating its utility as a deployment infrastructure. ER -"
85,22,2984,1,The sybil attack,"Large-scale peer-to-peer systems face security threats from faulty or hostile remote computing elements. To resist these threats, many such systems employ redundancy. However, if a single faulty entity can present multiple identities, it can control a substantial fraction of the system, thereby undermining this redundancy. One approach to preventing these &#034;Sybil attacks&#034; is to have a trusted agency certify identities. This paper shows that, without a logically centralized authority, Sybil attacks are always possible except under extreme and unrealistic assumptions of resource parity and coordination among entities."
86,22,4462,1,Random Walks in Peer-to-Peer Networks,"We quantify the effectiveness of random walks for searching and construction of unstructured peer-to-peer (P2P) networks. We have identified two cases where the use of random walks for searching achieves better results than flooding: a) when the overlay topology is clustered, and h) when a client re-issues the same query while its horizon does not change much. For construction, we argue that an expander can he maintained dynamically with constant operations per addition. The key technical ingredient of our approach is a deep result of stochastic processes indicating that samples taken from consecutive steps of a random walk can achieve statistical properties similar to independent sampling (if the second eigenvalue of the transition matrix is hounded away from 1, which translates to good expansion of the network; such connectivity is desired, and believed to hold, in every reasonable network and network model). This property has been previously used in complexity theory for construction of pseudorandom number generators. We reveal another facet of this theory and translate savings in random bits to savings in processing overhead."
87,22,6920,1,Accessing Nearby Copies of Replicated Objects in a Distributed Environment,"Consider a set of shared objects in a distributed network, where several copies of each object may exist at any given time. To ensure both fast access to the objects as well as efficient utilization of network resources, it is desirable that each access request be satisfied by a copy &#034;close&#034; to the requesting node. Unfortunately, it is not clear how to efficiently achieve this goal in a dynamic, distributed environment in which large numbers of objects are continuously being created, replicated, and destroyed. In this paper, we design a simple randomized algorithm for accessing shared objects that tends to satisfy each access request with a nearby copy. The algorithm is based on a novel mechanism to maintain and distribute information about object locations, and requires only a small amount of additional memory at each node. We analyze our access scheme for a class of cost functions that captures the hierarchical nature of wide-area networks. We show that under the particular cost mode..."
88,22,7438,1,Sybilproof reputation mechanisms,"Due to the open, anonymous nature of many P2P networks, new identities - or sybils - may be created cheaply and in large numbers. Given a reputation system, a peer may attempt to falsely raise its reputation by creating fake links between its sybils. Many existing reputation mechanisms are not resistant to these types of strategies.Using a static graph formulation of reputation, we attempt to formalize the notion of sybilproofness. We show that there is no symmetric sybilproof reputation function. For nonsymmetric reputations, following the notion of reputation propagation along paths, we give a general asymmetric reputation function based on flow and give conditions for sybilproofness."
89,22,11698,1,The state of peer-to-peer simulators and simulations,"In this paper, we discuss the current situation with respect to simulation usage in P2P research, testing the available P2P simulators against a proposed set of requirements, and surveying over 280 papers to discover what simulators are already being used. We found that no simulator currently meets all our requirements, and that simulation results are generally reported in the literature in a fashion that precludes any reproduction of results. We hope that this paper will give rise to further discussion and knowledge sharing among those of the P2P and network simulation research communities, so that a simulator that meets the needs of rigorous P2P research can be developed"
90,23,1581,1,{Discovering regulatory and signalling circuits in molecular interaction networks},"Motivation: In model organisms such as yeast, large databases of  proteinÃ¢ÂÂprotein and protein-DNA interactions have become an extremely  important resource for the study of protein function, evolution, and gene  regulatory dynamics. In this paper we demonstrate that by integrating these  interactions with widely-available mRNA expression data, it is possible to generate concrete hypotheses for the underlying mechanisms governing the  observed changes in gene expression. To perform this integration  systematically and at large scale, we introduce an approach for screening a  molecular interaction network to identify active subnetworks, i.e.,  connected regions of the network that show significant changes in expression over  particular subsets of conditions. The method we present here combines a  rigorous statistical measure for scoring subnetworks with a search algorithm  for identifying subnetworks with high score.Results: We evaluated our procedure on a small network of 332 genes and 362 interactions and a large network of 4160 genes containing all 7462 proteinÃ¢ÂÂprotein and protein-DNA interactions in the yeast public databases.  In the case of the small network, we identified five significant subnetworks  that covered 41 out of 77 (53%) of all significant changes in expression.  Both network analyses returned several top-scoring subnetworks with good  correspondence to known regulatory mechanisms in the literature. These  results demonstrate how large-scale genomic approaches may be used to  uncover signalling and regulatory pathways in a systematic, integrative  fashion.Availability: The methods presented in this paper are implemented  in the Cytoscape software package which is available to the academic community at  http://www.cytoscape.org.Contact: trey@wi.mit.eduKeywords: molecular interactions; gene expression; data integration; simulated annealing; Monte carlo methods."
91,23,3692,1,Whole-genome random sequencing and assembly of Haemophilus influenzae Rd.,"An approach for genome analysis based on sequencing and assembly of unselected pieces of {DNA} from the whole chromosome has been applied to obtain the complete nucleotide sequence (1,830,137 base pairs) of the genome from the bacterium {H}aemophilus influenzae {R}d. {T}his approach eliminates the need for initial mapping efforts and is therefore applicable to the vast array of microbial species for which genome maps are unavailable. {T}he {H}. influenzae {R}d genome sequence ({G}enome {S}equence {D}ata{B}ase accession number {L}42023) represents the only complete genome sequence from a free-living organism."
92,23,4311,1,{Mauve: Multiple Alignment of Conserved Genomic Sequence With Rearrangements},"10.1101/gr.2289704 As genomes evolve, they undergo large-scale evolutionary processes that present a challenge to sequence comparison not posed by short sequences. Recombination causes frequent genome rearrangements, horizontal transfer introduces new sequences into bacterial chromosomes, and deletions remove segments of the genome. Consequently, each genome is a mosaic of unique lineage-specific segments, regions shared with a subset of other genomes and segments conserved among all the genomes under consideration. Furthermore, the linear order of these segments may be shuffled among genomes. We present methods for identification and alignment of conserved genomic DNA in the presence of rearrangements and horizontal transfer. Our methods have been implemented in a software package called Mauve. Mauve has been applied to align nine enterobacterial genomes and to determine global rearrangement structure in three mammalian genomes. We have evaluated the quality of Mauve alignments and drawn comparison to other methods through extensive simulations of genome evolution."
93,23,5165,1,A single determinant dominates the rate of yeast protein evolution.,"A gene's rate of sequence evolution is among the most fundamental evolutionary quantities in common use, but what determines evolutionary rates has remained unclear. Here, we carry out the first combined analysis of seven predictors (gene expression level, dispensability, protein abundance, codon adaptation index, gene length, number of protein-protein interactions, and the gene's centrality in the interaction network) previously reported to have independent influences on protein evolutionary rates. Strikingly, our analysis reveals a single dominant variable linked to the number of translation events which explains 40-fold more variation in evolutionary rate than any other, suggesting that protein evolutionary rate has a single major determinant among the seven predictors. The dominant variable explains nearly half the variation in the rate of synonymous and protein evolution. We show that the two most commonly used methods to disentangle the determinants of evolutionary rate, partial correlation analysis and ordinary multivariate regression, produce misleading or spurious results when applied to noisy biological data. We overcome these difficulties by employing principal component regression, a multivariate regression of evolutionary rate against the principal components of the predictor variables. Our results support the hypothesis that translational selection governs the rate of synonymous and protein sequence evolution in yeast. 10.1093/molbev/msj038"
94,23,5819,1,Improved prediction of signal peptides: SignalP 3.0.,"We describe improvements of the currently most popular method for prediction of classically secreted proteins, SignalP. SignalP consists of two different predictors based on neural network and hidden Markov model algorithms, where both components have been updated. Motivated by the idea that the cleavage site position and the amino acid composition of the signal peptide are correlated, new features have been included as input to the neural network. This addition, combined with a thorough error-correction of a new data set, have improved the performance of the predictor significantly over SignalP version 2. In version 3, correctness of the cleavage site predictions has increased notably for all three organism groups, eukaryotes, Gram-negative and Gram-positive bacteria. The accuracy of cleavage site prediction has increased in the range 6â17\\ over the previous version, whereas the signal peptide discrimination improvement is mainly due to the elimination of false-positive predictions, as well as the introduction of a new discrimination score for the neural network. The new method has been benchmarked against other available methods. Predictions can be made at the publicly available web server http://www.cbs.dtu.dk/services/SignalP/"
95,23,7759,1,Duplicated genes evolve slower than singletons despite the initial rate increase.,"Background: Gene duplication is an important mechanism that can lead to the emergence of new functions during evolution. The impact of duplication on the mode of gene evolution has been the subject of several theoretical and empirical comparative-genomic studies. It has been shown that, shortly after the duplication, genes seem to experience a considerable relaxation of purifying selection. Results: Here we demonstrate two opposite effects of gene duplication on evolutionary rates. Sequence comparisons between paralogs show that, in accord with previous observations, a substantial acceleration in the evolution of paralogs occurs after duplication, presumably due to relaxation of purifying selection. The effect of gene duplication on evolutionary rate was also assessed by sequence comparison between orthologs that have paralogs ( duplicates) and those that do not ( singletons). It is shown that, in eukaryotes, duplicates, on average, evolve significantly slower than singletons. Eukaryotic ortholog evolutionary rates for duplicates are also negatively correlated with the number of paralogs per gene and the strength of selection between paralogs. A tally of annotated gene functions shows that duplicates tend to be enriched for proteins with known functions, particularly those involved in signaling and related cellular processes; by contrast, singletons include an over-abundance of poorly characterized proteins. Conclusions: These results suggest that whether or not a gene duplicate is retained by selection depends critically on the pre-existing functional utility of the protein encoded by the ancestral singleton. Duplicates of genes of a higher biological import, which are subject to strong functional constraints on the sequence, are retained relatively more often. Thus, the evolutionary trajectory of duplicated genes appears to be determined by two opposing trends, namely, the post-duplication rate acceleration and the generally slow evolutionary rate owing to the high level of functional constraints."
96,23,9421,1,Phylogenetic footprinting of transcription factor binding sites in proteobacterial genomes.,"Toward the goal of identifying complete sets of transcription factor (TF)-binding sites in the genomes of several gamma proteobacteria, and hence describing their transcription regulatory networks, we present a phylogenetic footprinting method for identifying these sites. Probable transcription regulatory sites upstream of Escherichia coli genes were identified by cross-species comparison using an extended Gibbs sampling algorithm. Close examination of a study set of 184 genes with documented transcription regulatory sites revealed that when orthologous data were available from at least two other gamma proteobacterial species, 81\% of our predictions corresponded with the documented sites, and 67\% corresponded when data from only one other species were available. That the remaining predictions included bona fide TF-binding sites was proven by affinity purification of a putative transcription factor (YijC) bound to such a site upstream of the fabA gene. Predicted regulatory sites for 2097 E.coli genes are available at http://www.wadsworth.org/resnres/bioinfo/."
97,23,10042,1,Whole-genome re-sequencing.,"{DNA} sequencing can be used to gain important information on genes, genetic variation and gene function for biological and medical studies. The growing collection of publicly available reference genome sequences will underpin a new era of whole genome re-sequencing, but sequencing costs need to fall and throughput needs to rise by several orders of magnitude. Novel technologies are being developed to meet this need by generating massive amounts of sequence that can be aligned to the reference sequence. The challenge is to maintain the high standards of accuracy and completeness that are hallmarks of the previous genome projects. One or more new sequencing technologies are expected to become the mainstay of future research, and to make {DNA} sequencing centre stage as a routine tool in genetic research in the coming years."
98,23,10651,1,"Minimus: a fast, lightweight genome assembler","BACKGROUND: Genome assemblers have grown very large and complex in response to the need for algorithms to handle the challenges of large whole-genome sequencing projects. Many of the most common uses of assemblers, however, are best served by a simpler type of assembler that requires fewer software components, uses less memory, and is far easier to install and run. RESULTS: We have developed the Minimus assembler to address these issues, and tested it on a range of assembly problems. We show that Minimus performs well on several small assembly tasks, including the assembly of viral genomes, individual genes, and BAC clones. In addition, we evaluate Minimus' performance in assembling bacterial genomes in order to assess its suitability as a component of a larger assembly pipeline. We show that, unlike other software currently used for these tasks, Minimus produces significantly fewer assembly errors, at the cost of generating a more fragmented assembly. CONCLUSION: We find that for small genomes and other small assembly tasks, Minimus is faster and far more flexible than existing tools. Due to its small size and modular design Minimus is perfectly suited to be a component of complex assembly pipelines. Minimus is released as an open-source software project and the code is available as part of the AMOS project at Sourceforge."
99,23,10769,1,The Sorcerer II Global Ocean Sampling expedition: northwest Atlantic through eastern tropical Pacific.,"The world's oceans contain a complex mixture of micro-organisms that are for the most part, uncharacterized both genetically and biochemically. We report here a metagenomic study of the marine planktonic microbiota in which surface (mostly marine) water samples were analyzed as part of the Sorcerer II Global Ocean Sampling expedition. These samples, collected across a several-thousand km transect from the North Atlantic through the Panama Canal and ending in the South Pacific yielded an extensive dataset consisting of 7.7 million sequencing reads (6.3 billion bp). Though a few major microbial clades dominate the planktonic marine niche, the dataset contains great diversity with 85% of the assembled sequence and 57% of the unassembled data being unique at a 98% sequence identity cutoff. Using the metadata associated with each sample and sequencing library, we developed new comparative genomic and assembly methods. One comparative genomic method, termed ""fragment recruitment,"" addressed questions of genome structure, evolution, and taxonomic or phylogenetic diversity, as well as the biochemical diversity of genes and gene families. A second method, termed ""extreme assembly,"" made possible the assembly and reconstruction of large segments of abundant but clearly nonclonal organisms. Within all abundant populations analyzed, we found extensive intra-ribotype diversity in several forms: (1) extensive sequence variation within orthologous regions throughout a given genome; despite coverage of individual ribotypes approaching 500-fold, most individual sequencing reads are unique; (2) numerous changes in gene content some with direct adaptive implications; and (3) hypervariable genomic islands that are too variable to assemble. The intra-ribotype diversity is organized into genetically isolated populations that have overlapping but independent distributions, implying distinct environmental preference. We present novel methods for measuring the genomic similarity between metagenomic samples and show how they may be grouped into several community types. Specific functional adaptations can be identified both within individual ribotypes and across the entire community, including proteorhodopsin spectral tuning and the presence or absence of the phosphate-binding gene PstS."
100,23,11583,1,"Locating proteins in the cell using TargetP, SignalP and related tools.","Determining the subcellular localization of a protein is an important first step toward understanding its function. Here, we describe the properties of three well-known N-terminal sequence motifs directing proteins to the secretory pathway, mitochondria and chloroplasts, and sketch a brief history of methods to predict subcellular localization based on these sorting signals and other sequence properties. We then outline how to use a number of internet-accessible tools to arrive at a reliable subcellular localization prediction for eukaryotic and prokaryotic proteins. In particular, we provide detailed step-by-step instructions for the coupled use of the amino-acid sequence-based predictors TargetP, SignalP, ChloroP and TMHMM, which are all hosted at the Center for Biological Sequence Analysis, Technical University of Denmark. In addition, we describe and provide web references to other useful subcellular localization predictors. Finally, we discuss predictive performance measures in general and the performance of TargetP and SignalP in particular."
101,23,12510,1,"SHARCGS, a fast and highly accurate short-read assembly algorithm for de novo genomic sequencing","10.1101/gr.6435207 The latest revolution in the DNA sequencing field has been brought about by the development of automated sequencers that are capable of generating giga base pair data sets quickly and at low cost. Applications of such technologies seem to be limited to resequencing and transcript discovery, due to the shortness of the generated reads. In order to extend the fields of application to de novo sequencing, we developed the SHARCGS algorithm to assemble short-read (25â40-mer) data with high accuracy and speed. The efficiency of SHARCGS was tested on BAC inserts from three eukaryotic species, on two yeast chromosomes, and on two bacterial genomes (Haemophilus influenzae, Escherichia coli). We show that 30-mer-based BAC assemblies have N50 sizes >20 kbp for Drosophila and Arabidopsis and >4 kbp for human in simulations taking missing reads and wrong base calls into account. We assembled 949,974 contigs with length >50 bp, and only one single contig could not be aligned error-free against the reference sequences. We generated 36-mer reads for the genome of Helicobacter acinonychis on the Illumina 1G sequencing instrument and assembled 937 contigs covering 98% of the genome with an N50 size of 3.7 kbp. With the exception of five contigs that differ in 1â4 positions relative to the reference sequence, all contigs matched the genome error-free. Thus, SHARCGS is a suitable tool for fully exploiting novel sequencing technologies by assembling sequence contigs de novo with high confidence and by outperforming existing assembly algorithms in terms of speed and accuracy."
102,23,13093,1,The RAST Server: rapid annotations using subsystems technology.,"ABSTRACT: BACKGROUND: The number of prokaryotic genome sequences becoming available is growing steadily and is growing faster than our ability to accurately annotate them. Description: We describe a fully automated service for annotating bacterial and archaeal genomes. The service identifies protein-encoding, rRNA and tRNA genes, assigns functions to the genes, predicts which subsystems are represented in the genome, uses this information to reconstruct the metabolic network and makes the output easily downloadable for the user. In addition, the annotated genome can be browsed in an environment that supports comparative analysis with the annotated genomes maintained in the SEED environment. The service normally makes the annotated genome available within 12-24 hours of submission, but ultimately the quality of such a service will be judged in terms of accuracy, consistency, and completeness of the produced annotations. We summarize our attempts to address these issues and discuss plans for incrementally enhancing the service. CONCLUSIONS: By providing accurate, rapid annotation freely to the community we have created an important community resource. The service has now been utilized by over 120 external users annotating over 350 distinct genomes."
103,23,13358,1,Toward a census of bacteria in soil.,"For more than a century, microbiologists have sought to determine the species richness of bacteria in soil, but the extreme complexity and unknown structure of soil microbial communities have obscured the answer. We developed a statistical model that makes the problem of estimating richness statistically accessible by evaluating the characteristics of samples drawn from simulated communities with parametric community distributions. We identified simulated communities with rank-abundance distributions that followed a truncated lognormal distribution whose samples resembled the structure of 16S rRNA gene sequence collections made using Alaskan and Minnesotan soils. The simulated communities constructed based on the distribution of 16S rRNA gene sequences sampled from the Alaskan and Minnesotan soils had a richness of 5,000 and 2,000 operational taxonomic units (OTUs), respectively, where an OTU represents a collection of sequences not more than 3% distant from each other. To sample each of these OTUs in the Alaskan 16S rRNA gene library at least twice, 480,000 sequences would be required; however, to estimate the richness of the simulated communities using nonparametric richness estimators would require only 18,000 sequences. Quantifying the richness of complex environments such as soil is an important step in building an ecological framework. We have shown that generating sufficient sequence data to do so requires less sequencing effort than completely sequencing a bacterial genome."
104,23,13901,1,k-means++: the advantages of careful seeding,"The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is &Theta;(log k )-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically."
105,23,14557,1,"Deep sequencing-based expression analysis shows major advances in robustness, resolution and inter-lab portability over five microarray platforms","The hippocampal expression profiles of wild-type mice and mice transgenic for {delta}C-doublecortin-like kinase were compared with Solexa/Illumina deep sequencing technology and five different microarray platforms. With Illumina's digital gene expression assay, we obtained [~]2.4 million sequence tags per sample, their abundance spanning four orders of magnitude. Results were highly reproducible, even across laboratories. With a dedicated Bayesian model, we found differential expression of 3179 transcripts with an estimated false-discovery rate of 8.5%. This is a much higher figure than found for microarrays. The overlap in differentially expressed transcripts found with deep sequencing and microarrays was most significant for Affymetrix. The changes in expression observed by deep sequencing were larger than observed by microarrays or quantitative PCR. Relevant processes such as calmodulin-dependent protein kinase activity and vesicle transport along microtubules were found affected by deep sequencing but not by microarrays. While undetectable by microarrays, antisense transcription was found for 51% of all genes and alternative polyadenylation for 47%. We conclude that deep sequencing provides a major advance in robustness, comparability and richness of expression profiling data and is expected to boost collaborative, comparative and integrative genomics studies. 10.1093/nar/gkn705"
106,23,14730,1,Exploring Microbial Diversity and Taxonomy Using SSU rRNA Hypervariable Tag Sequencing,"Massively parallel pyrosequencing of hypervariable regions from small subunit ribosomal RNA (SSU rRNA) genes can sample a microbial community two or three orders of magnitude more deeply per dollar and per hour than capillary sequencing of full-length SSU rRNA. As with full-length rRNA surveys, each sequence read is a tag surrogate for a single microbe. However, rather than assigning taxonomy by creating gene trees de novo that include all experimental sequences and certain reference taxa, we compare the hypervariable region tags to an extensive database of rRNA sequences and assign taxonomy based on the best match in a Global Alignment for Sequence Taxonomy (GAST) process. The resulting taxonomic census provides information on both composition and diversity of the microbial community. To determine the effectiveness of using only hypervariable region tags for assessing microbial community membership, we compared the taxonomy assigned to the V3 and V6 hypervariable regions with the taxonomy assigned to full-length SSU rRNA sequences isolated from both the human gut and a deep-sea hydrothermal vent. The hypervariable region tags and full-length rRNA sequences provided equivalent taxonomy and measures of relative abundance of microbial communities, even for tags up to 15% divergent from their nearest reference match. The greater sampling depth per dollar afforded by massively parallel pyrosequencing reveals many more members of the Ã¢â¬Årare biosphereÃ¢â¬ï¿½ than does capillary sequencing of the full-length gene. In addition, tag sequencing eliminates cloning bias and the sequences are short enough to be completely sequenced in a single read, maximizing the number of organisms sampled in a run while minimizing chimera formation. This technique allows the cost-effective exploration of changes in microbial community structure, including the rare biosphere, over space and time and can be applied immediately to initiatives, such as the Human Microbiome Project."
107,23,15226,1,A consistency-based consensus algorithm for de novo and reference-guided sequence assembly of short reads,"Motivation: Novel high-throughput sequencing technologies pose new algorithmic challenges in handling massive amounts of shortread, high-coverage data. A robust and versatile consensus tool is of particular interest for such data since a sound multi-read alignment is a prerequisite for variation analyses, accurate genome assemblies and insert sequencing. Results: A multi-read alignment algorithm for de novo or reference-guided genome assembly is presented. The program identifies segments shared by multiple reads and then aligns these segments using a consistency-enhanced alignment graph. On real de novo sequencing data, obtained from the newly established NCBI Short Read Archive, the program performs similarly in quality to other comparable programs. On more challenging simulated data sets for insert sequencing and variation analyses our program outperforms the other tools. Availability: Availability: The consensus program can be downloaded from http://www.seqan.de/projects/consensus.html. It can be used stand-alone or in conjunction with the Celera Assembler. Both application scenarios as well as the usage of the tool are described in the documentation. Contact: rausch{\\char64}inf.fu-berlin.de"
108,23,16135,1,The Universal Protein Resource (UniProt) in 2010,"The primary mission of UniProt is to support biological research by maintaining a stable, comprehensive, fully classified, richly and accurately annotated protein sequence knowledgebase, with extensive cross-references and querying interfaces freely accessible to the scientific community. UniProt is produced by the UniProt Consortium which consists of groups from the European Bioinformatics Institute (EBI), the Swiss Institute of Bioinformatics (SIB) and the Protein Information Resource (PIR). UniProt is comprised of four major components, each optimized for different uses: the UniProt Archive, the UniProt Knowledgebase, the UniProt Reference Clusters and the UniProt Metagenomic and Environmental Sequence Database. UniProt is updated and distributed every 3 weeks and can be accessed online for searches or download at http://www.uniprot.org."
109,23,16744,1,Efficient construction of an assembly string graph using the FM-index,"Motivation: Sequence assembly is a difficult problem whose importance has grown again recently as the cost of sequencing has dramatically dropped. Most new sequence assembly software has started by building a de Bruijn graph, avoiding the overlap-based methods used previously because of the computational cost and complexity of these with very large numbers of short reads. Here, we show how to use suffix array-based methods that have formed the basis of recent very fast sequence mapping algorithms to find overlaps and generate assembly string graphs asymptotically faster than previously described algorithms.Results: Standard overlap assembly methods have time complexity O(N2), where N is the sum of the lengths of the reads. We use the FerraginaÃ¢ÂÂManzini index (FM-index) derived from the BurrowsÃ¢ÂÂWheeler transform to find overlaps of length at least ÃÂ among a set of reads. As well as an approach that finds all overlaps then implements transitive reduction to produce a string graph, we show how to output directly only the irreducible overlaps, significantly shrinking memory requirements and reducing compute time to O(N), independent of depth. Overlap-based assembly methods naturally handle mixed length read sets, including capillary reads or long reads promised by the third generation sequencing technologies. The algorithms we present here pave the way for overlap-based assembly approaches to be developed that scale to whole vertebrate genome de novo assembly.Contact: js18@sanger.ac.uk"
110,24,10920,1,Generalizations of the clustering coefficient to weighted complex networks,"The recent high level of interest in weighted complex networks gives rise to a need to develop new measures and to generalize existing ones to take the weights of links into account. Here we focus on various generalizations of the clustering coefficient, which is one of the central characteristics in the complex network theory. We present a comparative study of the several suggestions introduced in the literature, and point out their advantages and limitations. The concepts are illustrated by simple examples as well as by empirical data of the world trade and weighted coauthorship networks."
111,24,14263,1,The Collective Dynamics of Smoking in a Large Social Network,"Background The prevalence of smoking has decreased substantially in the United States over the past 30 years. We examined the extent of the person-to-person spread of smoking behavior and the extent to which groups of widely connected people quit together.  Methods We studied a densely interconnected social network of 12,067 people assessed repeatedly from 1971 to 2003 as part of the Framingham Heart Study. We used network analytic methods and longitudinal statistical models.  Results Discernible clusters of smokers and nonsmokers were present in the network, and the clusters extended to three degrees of separation. Despite the decrease in smoking in the overall population, the size of the clusters of smokers remained the same across time, suggesting that whole groups of people were quitting in concert. Smokers were also progressively found in the periphery of the social network. Smoking cessation by a spouse decreased a person's chances of smoking by 67% (95% confidence interval [CI], 59 to 73). Smoking cessation by a sibling decreased the chances by 25% (95% CI, 14 to 35). Smoking cessation by a friend decreased the chances by 36% (95% CI, 12 to 55 ). Among persons working in small firms, smoking cessation by a coworker decreased the chances by 34% (95% CI, 5 to 56). Friends with more education influenced one another more than those with less education. These effects were not seen among neighbors in the immediate geographic area.  Conclusions Network phenomena appear to be relevant to smoking cessation. Smoking behavior spreads through close and distant social ties, groups of interconnected people stop smoking in concert, and smokers are increasingly marginalized socially. These findings have implications for clinical and public health interventions to reduce and prevent smoking. 10.1056/NEJMsa0706154"
112,24,15585,1,Brain Anatomical Network and Intelligence,"Intuitively, higher intelligence might be assumed to correspond to more efficient information transfer in the brain, but no direct evidence has been reported from the perspective of brain networks. In this study, we performed extensive analyses to test the hypothesis that individual differences in intelligence are associated with brain structural organization, and in particular that higher scores on intelligence tests are related to greater global efficiency of the brain anatomical network. We constructed binary and weighted brain anatomical networks in each of 79 healthy young adults utilizing diffusion tensor tractography and calculated topological properties of the networks using a graph theoretical method. Based on their IQ test scores, all subjects were divided into general and high intelligence groups and significantly higher global efficiencies were found in the networks of the latter group. Moreover, we showed significant correlations between IQ scores and network properties across all subjects while controlling for age and gender. Specifically, higher intelligence scores corresponded to a shorter characteristic path length and a higher global efficiency of the networks, indicating a more efficient parallel information transfer in the brain. The results were consistently observed not only in the binary but also in the weighted networks, which together provide convergent evidence for our hypothesis. Our findings suggest that the efficiency of brain structural organization may be an important biological basis for intelligence."
113,25,1714,1,Real Analysis,"This course in real analysis is directed at advanced undergraduates andbeginning graduate students in mathematics and related fields. Presupposingonly a modest background in real analysis or advanced calculus, the bookoffers something of value to specialists and nonspecialists alike. The textcovers three major topics: metric and normed linear spaces, function spaces,and Lebesgue measure and integration on the line. In an informal, down-to-earth style, the author gives motivation and overview of new ideas, whilestill supplying full details and complete proofs. He provides a great manyexercises and suggestions for further study."
114,25,5551,1,"{Petri} nets: properties, analysis, and applications","Starts with a brief review of the history and the application areas considered in the literature. The author then proceeds with introductory modeling examples, behavioral and structural properties, three methods of analysis, subclasses of Petri nets and their analysis. In particular, one section is devoted to marked graphs, the concurrent system model most amenable to analysis. Introductory discussions on stochastic nets with their application to performance modeling, and on high-level nets with their application to logic programming, are provided. Also included are recent results on reachability criteria. Suggestions are provided for further reading on many subject areas of Petri nets."
115,26,4407,1,Development and Testing of the OPLS All-Atom Force Field on Conformational Energetics and Properties of Organic Liquids,"Abstract: The parametrization and testing of the OPLS all-atom force field for organic molecules and peptides are described. Parameters for both torsional and nonbonded energetics have been derived, while the bond stretching and angle bending parameters have been adopted mostly from the AMBER all-atom force field. The torsional parameters were determined by fitting to rotational energy profiles obtained from ab initio molecular orbital calculations at the RHF/6-31G*//RHF/6-31G* level for more than 50 organic molecules and ions. The quality of the fits was high with average errors for conformational energies of less than 0.2 kcal/mol. The force-field results for molecular structures are also demonstrated to closely match the ab initio predictions. The nonbonded parameters were developed in conjunction with Monte Carlo statistical mechanics simulations by computing thermodynamic and structural properties for 34 pure organic liquids including alkanes, alkenes, alcohols, ethers, acetals, thiols, sulfides, disulfides, aldehydes, ketones, and amides. Average errors in comparison with experimental data are 2% for heats of vaporization and densities. The Monte Carlo simulations included sampling all internal and intermolecular degrees of freedom. It is found that such non-polar and monofunctional systems do not show significant condensed-phase effects on internal energies in going from the gas phase to the pure liquids."
116,26,9843,1,Simulating microscopic hydrodynamic phenomena with dissipative particle dynamics,"We present a novel method for simulating hydrodynamic phenomena. This particle-based method combines features from molecular dynamics and lattice-gas automata. It is shown theoretically as well as in simulations that a quantitative description of isothermal Navier-Stokes flow is obtained with relatively few particles. Computationally, the method is much faster than molecular dynamics, and the at same time it is much more flexible than lattice-gas automata schemes."
117,27,101,1,The long memory of the efficient market,"For the London Stock Exchange we demonstrate that the signs of orders obey a long-memory process. The autocorrelation function decays roughly as $\tau^{-\alpha}$ with $\alpha \approx 0.6$, corresponding to a Hurst exponent $H \approx 0.7$. This implies that the signs of future orders are quite predictable from the signs of past orders; all else being equal, this would suggest a very strong market inefficiency. We demonstrate, however, that fluctuations in order signs are compensated for by anti-correlated fluctuations in transaction size and liquidity, which are also long-memory processes. This tends to make the returns whiter. We show that some institutions display long-range memory and others don't."
118,27,2692,1,The evolution of cooperation,"Darwin recognized that natural selection could not favor a trait in one species solely for the benefit of another species. The modern, selfish-gene view of the world suggests that cooperation between individuals, whether of the same species or different species, should be especially vulnerable to the evolution of noncooperators. Yet, cooperation is prevalent in nature both within and between species. What special circumstances or mechanisms thus favor cooperation? Currently, evolutionary biology offers a set of disparate explanations, and a general framework for this breadth of models has not emerged. Here, we offer a tripartite structure that links previously disconnected views of cooperation. We distinguish three general models by which cooperation can evolve and be maintained: (i) directed reciprocation â cooperation with individuals who give in return; (ii) shared genes â cooperation with relatives (e.g., kin selection); and (iii) byproduct benefits â cooperation as an incidental consequence of selfish action. Each general model is further subdivided. Several renowned examples of cooperation that have lacked explanation until recently â plant-rhizobium symbioses and bacteria-squid light organs â fit squarely within this framework. Natural systems of cooperation often involve more than one model, and a fruitful direction for future research is to understand how these models interact to maintain cooperation in the long term."
119,27,5942,1,"The General Theory of Employment, Interest and Money","John Maynard Keynes is the great British economist of the twentieth century whose hugely influential work The General Theory of Employment, Interest and Money is undoubtedly the century s most important book on economics--strongly influencing economic theory and practice, particularly with regard to the role of government in stimulating and regulating a nation's economic life. Keynes's work has undergone significant revaluation in recent years, and ""Keynesian"" views which have been widely defended for so long are now perceived as at odds with Keynes's own thinking. Recent scholarship and research has demonstrated considerable rivalry and controversy concerning the proper interpretation of Keynes's works, such that recourse to the original text is all the more important. Although considered by a few critics that the sentence structures of the book are quite incomprehensible and almost unbearable to read, the book is an essential reading for all those who desire a basic education in economics. The key to understanding Keynes is the notion that at particular times in the business cycle, an economy can become over-productive (or under- consumptive) and thus, a vicious spiral is begun that results in massive layoffs and cuts in production as businesses attempt to equilibrate aggregate supply and demand. Thus, full employment is only one of many or multiple macro equilibria. If an economy reaches an underemployment equilibrium, something is necessary to boost or stimulate demand to produce full employment. This something could be business investment but because of the logic and individualist nature of investment decisions, it is unlikely to rapidly restore full employment. Keynes logically seizes upon the public budget and government expenditures as the quickest way to restore full employment. Borrowing the money to finance the deficit from private households and businesses is a quick, direct way to restore full employment while at the same time, redirecting or siphoning"
120,27,6808,1,Allocative Efficiency of Markets with Zero-Intelligence Traders: Market as a Partial Substitute for Individual Rationality,"This paper reports market experiments in which human traders are replaced by 'zero-intelligence' programs that submit random bids and offers. Imposing a budget constraint (i.e., n ot permitting traders to sell below their costs or buy above their valu es) is sufficient to raise the allocative efficiency of these auctions close to 100 percent. Allocative efficiency of a double auction deri ves largely from its structure, independent of traders' motivation, intelligence, or learning. Adam Smith's invisible hand may be more powerful than some may have thought; it can generate aggregate rationality not only from individual rationality but also from individual irrationality."
121,28,227,1,The PageRank Citation Ranking: Bringing Order to the Web,"The importance of a Web page is an inherently subjective matter, which depends on the readers interests, knowledge and attitudes. But there is still much that can be said objectively about the relative importance of Web pages. This paper describes PageRank, a method for rating Web pages objectively and mechanically, effectively measuring the human interest and attention devoted to them. We compare PageRank to an idealized random Web surfer. We show how to efficiently compute PageRank for large numbers of pages. And, we show how to apply PageRank to search and to user navigation. 1"
122,28,7659,1,A statistical interpretation of term specificity and its application in retrieval,"The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing, in particular, that frequently-occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure."
123,29,2610,1,"Introduction to Algorithms, Second Edition","{Aimed at any serious programmer or computer science student,  the new second edition of <I>Introduction to Algorithms</I> builds on  the tradition of the original with a truly magisterial guide to the  world of algorithms. Clearly presented, mathematically rigorous, and  yet approachable even for the math-averse, this title sets a high  standard for a textbook and reference to the best algorithms for  solving a wide range of computing problems.<p>With sample problems and  mathematical proofs demonstrating the correctness of each algorithm,  this book is ideal as a textbook for classroom study, but its reach  doesn't end there. The authors do a fine job of explaining each  algorithm. (Reference sections on basic mathematical notation will help  readers bridge the gap, but it will help to have some math background  to appreciate the full achievement of this handsome hardcover volume.)  Every algorithm is presented in pseudo-code, which can be implemented  in any computer language, including C/C++ and Java. This ecumenical  approach is one of the book's strengths. When it comes to sorting and  common data structures, from basic linked lists to trees (including  binary trees, red-black, and B-trees), this title really shines, with  clear diagrams that show algorithms in operation. Even if you just  glance over the mathematical notation here, you can definitely benefit  from this text in other ways.<p>The book moves forward with more  advanced algorithms that implement strategies for solving more  complicated problems (including dynamic programming techniques, greedy  algorithms, and amortized analysis). Algorithms for graphing problems  (used in such real-world business problems as optimizing flight  schedules or flow through pipelines) come next. In each case, the  authors provide the best from current research in each topic, along  with sample solutions.<p>This text closes with a grab bag of useful  algorithms including matrix operations and linear programming,  evaluating polynomials, and the well-known Fast Fourier Transformation  (FFT) (useful in signal processing and engineering). Final sections on  ""NP-complete"" problems, like the well-known traveling salesman problem,  show off that while not all problems have a demonstrably final and best  answer, algorithms that generate acceptable approximate solutions can  still be used to generate useful, real-world answers.<p>Throughout this  text, the authors anchor their discussion of algorithms with current  examples drawn from molecular biology (like the Human Genome Project),  business, and engineering. Each section ends with short discussions of  related historical material, often discussing original research in each  area of algorithms. On the whole, they argue successfully that  algorithms are a ""technology"" just like hardware and software that can  be used to write better software that does more, with better  performance. Along with classic books on algorithms (like Donald  Knuth's three-volume set, <I>The Art of Computer  Programming</I>), this title sets a new standard for compiling the  best research in algorithms. For any experienced developer, regardless  of their chosen language, this text deserves a close look for extending  the range and performance of real-world software. <I>--Richard  Dragan</I> <p> <B>Topics covered:</B> Overview of algorithms (including algorithms as  a technology); designing and analyzing algorithms; asymptotic notation;  recurrences and recursion; probabilistic analysis and randomized  algorithms; heapsort algorithms; priority queues; quicksort algorithms;  linear time sorting (including radix and bucket sort); medians and  order statistics (including minimum and maximum); introduction to data  structures (stacks, queues, linked lists, and rooted trees); hash  tables (including hash functions); binary search trees; red-black  trees; augmenting data structures for custom applications; dynamic  programming explained (including assembly-line scheduling, matrix-chain  multiplication, and optimal binary search trees); greedy algorithms  (including Huffman codes and task-scheduling problems); amortized  analysis (the accounting and potential methods); advanced data  structures (including B-trees, binomial and Fibonacci heaps,  representing disjoint sets in data structures); graph algorithms  (representing graphs, minimum spanning trees, single-source shortest  paths, all-pairs shortest paths, and maximum flow algorithms); sorting  networks; matrix operations; linear programming (standard and slack  forms); polynomials and the Fast Fourier Transformation (FFT); number  theoretic algorithms (including greatest common divisor, modular  arithmetic, the Chinese remainder theorem, RSA public-key encryption,  primality testing, integer factorization); string matching;  computational geometry (including finding the convex hull);  NP-completeness (including sample real-world NP-complete problems and  their insolvability); approximation algorithms for NP-complete problems  (including the traveling salesman problem); reference sections for  summations and other mathematical notation, sets, relations, functions,  graphs and trees, as well as counting and probability backgrounder  (plus geometric and binomial distributions).}"
124,30,108,1,The systems biology markup language (SBML): a medium for representation and exchange of biochemical network models,"Motivation: Molecular biotechnology now makes it possible to build elaborate systems models, but the systems biology community needs information standards if models are to be shared, evaluated and developed cooperatively.  Results: We summarize the Systems Biology Markup Language (SBML) Level 1, a free, open, XML-based format for representing biochemical reaction networks. SBML is a software-independent language for describing models common to research in many areas of computational biology, including cell signaling pathways, metabolic pathways, gene regulation, and others.  Availability: The specification of SBML Level 1 is freely available from http://www.sbml.org/  Contact: sysbio-team@caltech.edu 10.1093/bioinformatics/btg015"
125,30,2025,1,An expanded genome-scale model of Escherichia coli K-12 (iJR904 GSM/GPR),"BACKGROUND: Diverse datasets, including genomic, transcriptomic, proteomic and metabolomic data, are becoming readily available for specific organisms. There is currently a need to integrate these datasets within an in silico modeling framework. Constraint-based models of Escherichia coli K-12 MG1655 have been developed and used to study the bacterium's metabolism and phenotypic behavior. The most comprehensive E. coli model to date (E. coli iJE660a GSM) accounts for 660 genes and includes 627 unique biochemical reactions. RESULTS: An expanded genome-scale metabolic model of E. coli (iJR904 GSM/GPR) has been reconstructed which includes 904 genes and 931 unique biochemical reactions. The reactions in the expanded model are both elementally and charge balanced. Network gap analysis led to putative assignments for 55 open reading frames (ORFs). Gene to protein to reaction associations (GPR) are now directly included in the model. Comparisons between predictions made by iJR904 and iJE660a models show that they are generally similar but differ under certain circumstances. Analysis of genome-scale proton balancing shows how the flux of protons into and out of the medium is important for maximizing cellular growth. CONCLUSIONS: E. coli iJR904 has improved capabilities over iJE660a. iJR904 is a more complete and chemically accurate description of E. coli metabolism than iJE660a. Perhaps most importantly, iJR904 can be used for analyzing and integrating the diverse datasets. iJR904 will help to outline the genotype-phenotype relationship for E. coli K-12, as it can account for genomic, transcriptomic, proteomic and fluxomic data simultaneously."
126,30,4762,1,Simulated Diffusion of Phosphorylated {C}he{Y} through the Cytoplasm of {E}scherichia coli,"We describe the use of a computational model to study the effects of cellular architecture and macromolecular crowding on signal transduction in Escherichia coli chemotaxis. A newly developed program, Smoldyn, allows the movement and interaction of a large number of individual molecules in a structured environment to be simulated (S. S. Andrews and D. Bray, Phys. Biol., in press). With Smoldyn, we constructed a three-dimensional model of an E. coli cell and examined the diffusion of CheYp from the cluster of receptors to the flagellar motors under control conditions and in response to attractant and repellent stimuli. Our simulations agree well with experimental observations of cell swimming responses and are consistent with the diffusive behavior expected in wild-type and mutant cells. The high resolution available to us in the new program allows us to calculate the loci of individual CheYp molecules in a cell and the distribution of their lifetimes under different cellular conditions. We find that the time delay between stimulus and response differs for flagellar motors located at different positions in the cell. We explore different possible locations for the phosphatase CheZ and show conditions under which a gradient of CheYp exists in the cell. The introduction of inert blocks into the cytoplasm, representing impenetrable structures such as the nucleoid and large protein complexes, produces a fall in the apparent diffusion coefficient of CheYp and enhances the differences between motors. These and other results are left as predictions for future experiments."
127,30,10781,1,S{{TOCHSIM}}: modelling of stochastic biomolecular processes.,"Summary: STOCHSIM is a stochastic simulator for chemical reactions. Molecules are represented as individual software objects that react according to probabilities derived from concentrations and rate constants. Version 1.2 of STOCHSIM provides a novel cross-platform graphical interface written in Perl/Tk. A simple two-dimensional spatial structure has also been implemented, in which nearest-neighbour interactions of molecules in a 2-D lattice can be simulated.  Availability: Various ports of the program can be retrieved at ftp://ftp.cds.caltech.edu/pub/dbray/  Contact: nl223@cus.cam.ac.uk; tss26@cam.ac.uk 10.1093/bioinformatics/17.6.575"
128,31,3735,1,Multiple neural spike train data analysis: state-of-the-art and future challenges,Multiple electrodes are anow a standard tool in neuroscience research that make it possible to study the simultaneous activity of several neurons in a given brain region or across different regions. The data from multi-electrode studies present important analysis challenges that must be resolved for optimal use of these neurophysiological measurements to answer questions about how the brain works. Here we review statistical methods for the analysis of multiple neural spike-train data and discuss future challenges for methodology research.
129,31,14380,1,Unsupervised natural experience rapidly alters invariant object representation in visual cortex.,"Object recognition is challenging because each object produces myriad retinal images. Responses of neurons from the inferior temporal cortex (IT) are selective to different objects, yet tolerant (""invariant"") to changes in object position, scale, and pose. How does the brain construct this neuronal tolerance? We report a form of neuronal learning that suggests the underlying solution. Targeted alteration of the natural temporal contiguity of visual experience caused specific changes in IT position tolerance. This unsupervised temporal slowness learning (UTL) was substantial, increased with experience, and was significant in single IT neurons after just 1 hour. Together with previous theoretical work and human object perception experiments, we speculate that UTL may reflect the mechanism by which the visual stream builds and maintains tolerant object representations. 10.1126/science.1160028"
130,32,9165,1,Harvesting social knowledge from folksonomies,"Collaborative tagging systems, or folksonomies, have the potential of becoming technological infrastructure to support knowledge management activities in an organization or a society. There are many challenges, however. This paper presents designs that enhance collaborative tagging systems to meet some key challenges: community identification, ontology generation, user and document recommendation. Design prototypes, evaluation methodology and selected preliminary results are presented."
131,33,3647,1,Computational cluster validation in post-genomic data analysis,"Motivation: The discovery of novel biological knowledge from the ab initio analysis of post-genomic data relies upon the use of unsupervised processing methods, in particular clustering techniques. Much recent research in bioinformatics has therefore been focused on the transfer of clustering methods introduced in other scientific fields and on the development of novel algorithms specifically designed to tackle the challenges posed by post-genomic data. The partitions returned by a clustering algorithm are commonly validated using visual inspection and concordance with prior biological knowledge--whether the clusters actually correspond to the real structure in the data is somewhat less frequently considered. Suitable computational cluster validation techniques are available in the general data-mining literature, but have been given only a fraction of the same attention in bioinformatics. Results: This review paper aims to familiarize the reader with the battery of techniques available for the validation of clustering results, with a particular focus on their application to post-genomic data analysis. Synthetic and real biological datasets are used to demonstrate the benefits, and also some of the perils, of analytical clustervalidation. Availability: The software used in the experiments is available at http://dbkweb.ch.umist.ac.uk/handl/clustervalidation/ Contact: J.Handl@postgrad.manchester.ac.uk Supplementary information: Enlarged colour plots are provided in the Supplementary Material, which is available at http://dbkweb.ch.umist.ac.uk/handl/clustervalidation/"
132,33,12531,1,Short pyrosequencing reads suffice for accurate microbial community analysis,"Pyrosequencing technology allows us to characterize microbial communities using 16S ribosomal RNA (rRNA) sequences orders of magnitude faster and more cheaply than has previously been possible. However, results from different studies using pyrosequencing and traditional sequencing are often difficult to compare, because amplicons covering different regions of the rRNA might yield different conclusions. We used sequences from over 200 globally dispersed environments to test whether studies that used similar primers clustered together mistakenly, without regard to environment. We then tested whether primer choice affects sequence-based community analyses using UniFrac, our recently-developed method for comparing microbial communities. We performed three tests of primer effects. We tested whether different simulated amplicons generated the same UniFrac clustering results as near-full-length sequences for three recent large-scale studies of microbial communities in the mouse and human gut, and the Guerrero Negro microbial mat. We then repeated this analysis for short sequences (100-, 150-, 200- and 250-base reads) resembling those produced by pyrosequencing. The results show that sequencing effort is best focused on gathering more short sequences rather than fewer longer ones, provided that the primers are chosen wisely, and that community comparison methods such as UniFrac are surprisingly robust to variation in the region sequenced. 10.1093/nar/gkm541"
133,33,14706,1,"The Pervasive Effects of an Antibiotic on the Human Gut Microbiota, as Revealed by Deep 16S rRNA Sequencing","The human intestinal microbiota is essential to the health of the host and plays a role in nutrition, development, metabolism, pathogen resistance, and regulation of immune responses. Antibiotics may disrupt these coevolved interactions, leading to acute or chronic disease in some individuals. Our understanding of antibiotic-associated disturbance of the microbiota has been limited by the poor sensitivity, inadequate resolution, and significant cost of current research methods. The use of pyrosequencing technology to generate large numbers of 16S rDNA sequence tags circumvents these limitations and has been shown to reveal previously unexplored aspects of the Ã¢â¬Årare biosphere.Ã¢â¬ï¿½ We investigated the distal gut bacterial communities of three healthy humans before and after treatment with ciprofloxacin, obtaining more than 7,000 full-length rRNA sequences and over 900,000 pyrosequencing reads from two hypervariable regions of the rRNA gene. A companion paper in PLoS Genetics (see Huse et al., doi: 10.1371/journal.pgen.1000255 ) shows that the taxonomic information obtained with these methods is concordant. Pyrosequencing of the V6 and V3 variable regions identified 3,300Ã¢â¬â5,700 taxa that collectively accounted for over 99% of the variable region sequence tags that could be obtained from these samples. Ciprofloxacin treatment influenced the abundance of about a third of the bacterial taxa in the gut, decreasing the taxonomic richness, diversity, and evenness of the community. However, the magnitude of this effect varied among individuals, and some taxa showed interindividual variation in the response to ciprofloxacin. While differences of community composition between individuals were the largest source of variability between samples, we found that two unrelated individuals shared a surprising degree of community similarity. In all three individuals, the taxonomic composition of the community closely resembled its pretreatment state by 4 weeks after the end of treatment, but several taxa failed to recover within 6 months. These pervasive effects of ciprofloxacin on community composition contrast with the reports by participants of normal intestinal function and with prior assumptions of only modest effects of ciprofloxacin on the intestinal microbiota. These observations support the hypothesis of functional redundancy in the human gut microbiota. The rapid return to the pretreatment community composition is indicative of factors promoting community resilience, the nature of which deserves future investigation."
134,33,16610,1,Detection and characterization of novel sequence insertions using paired-end next-generation sequencing,"Motivation: In the past few years, human genome structural variation discovery has enjoyed increased attention from the genomics research community. Many studies were published to characterize short insertions, deletions, duplications and inversions, and associate copy number variants (CNVs) with disease. Detection of new sequence insertions requires sequence data, however, the Ã¢ÂÂdetectableÃ¢ÂÂ sequence length with read-pair analysis is limited by the insert size. Thus, longer sequence insertions that contribute to our genetic makeup are not extensively researched.Results: We present NovelSeq: a computational framework to discover the content and location of long novel sequence insertions using paired-end sequencing data generated by the next-generation sequencing platforms. Our framework can be built as part of a general sequence analysis pipeline to discover multiple types of genetic variation (SNPs, structural variation, etc.), thus it requires significantly less-computational resources than de novo sequence assembly. We apply our methods to detect novel sequence insertions in the genome of an anonymous donor and validate our results by comparing with the insertions discovered in the same genome using various sources of sequence data.Availability: The implementation of the NovelSeq pipeline is available at http://compbio.cs.sfu.ca/strvar.htmContact:eee@gs.washington.edu; cenk@cs.sfu.ca"
135,34,1204,1,{Maximum likelihood from incomplete data via the EM algorithm},"{A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.}"
136,34,2836,1,Support vector machine active learning for image retrieval,"Relevance feedback is often a critical component when designing image databases. With these databases it is difficult to specify queries directly and explicitly. Relevance feedback interactively determinines a user's desired output or  query concept  by asking the user whether certain proposed images are relevant or not. For a relevance feedback algorithm to be effective, it must grasp a user's query concept accurately and quickly, while also only asking the user to label a small number of images. We propose the use of a  support vector machine active learning  algorithm for conducting effective relevance feedback for image retrieval. The algorithm selects the most informative images to query a user and quickly learns a boundary that separates the images that satisfy the user's query concept from the rest of the dataset. Experimental results show that our algorithm achieves significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback."
137,34,4113,1,An information-maximization approach to blind separation and blind deconvolution,"We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in ""blind"" signal processing. [Journal Article; In English; United States; MEDLINE]"
138,34,7201,1,Finding Structure in Time,"Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by [Jordan, 1986] which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
139,34,9500,1,Putting Objects in Perspective,"Image understanding requires not only individually esti- mating elements of the visual world but also capturing the interplay among them. In this paper, we provide a frame- work for placing local object detection in the context of the overall 3D scene by modeling the interdependence of ob- jects, surface orientations, and camera viewpoint. Most object detection methods consider all scales and locations in the image as equally likely. We show that with probabilistic estimates of 3D geometry, both in terms of surfaces and world coordinates, we can put objects into perspective and model the scale and location variance in the image. Our approach reflects the cyclical nature of the problem by allowing probabilistic object hypotheses to re- fine geometry and vice-versa. Our framework allows pain- less substitution of almost any object detector and is easily extended to include other aspects of image understanding. Our results confirm the benefits of our integrated approach."
140,34,13125,1,Long Short-Term Memory,"Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
141,34,15932,1,Generating Coherent Patterns of Activity from Chaotic Neural Networks," Summary Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated."
142,35,4874,1,The mechanics of trust: a framework for research and design,"With an increasing number of technologies supporting transactions over distance and replacing traditional forms of interaction, designing for trust in mediated interactions has become a key concern for researchers in human computer interaction {(HCI).} While much of this research focuses on increasing users' trust, we present a framework that shifts the perspective towards factors that support trustworthy behavior. In a second step, we analyze how the presence of these factors can be signalled. We argue that it is essential to take a systemic perspective for enabling well-placed trust and trustworthy behavior in the long term. For our analysis we draw on relevant research from sociology, economics, and psychology, as well as {HCI.} We identify contextual properties (motivation based on temporal, social, and institutional embeddedness) and the actor's intrinsic properties (ability, and motivation based on internalized norms and benevolence) that form the basis of trustworthy behavior. Our analysis provides a frame of reference for the design of studies on trust in technology-mediated interactions, as well as a guide for identifying trust requirements in design processes. We demonstrate the application of the framework in three scenarios: call centre interactions, {B2C} e-commerce, and voice-enabled on-line gaming."
143,35,8594,1,A Collaborative Web Browsing System for Multiple Mobile Users,"In mobile computing environments, handheld devices with low functionality restrict the services provided for mobile users. We propose a new concept of collaborative browsing, where mobile users collaboratively browse web pages designed for desktop PC. In collaborative browsing, a web page is divided into multiple components, and each is distributed to a different device. In mobile computing environments, the number of handheld devices, their capabilities, and other conditions can vary widely amongst mobile users who want to browse content. Therefore, we developed a page partitioning method for collaborative browsing, which divides a web page into multiple components. Moreover, we designed and implemented a collaborative web browsing system in which users can search and browse their target information by discussing and watching partial pages displayed on multiple devices."
144,35,11101,1,Wikinomics: How Mass Collaboration Changes Everything,"{<B> In just the last few years, traditional collaboration&#151;in a meeting room, a conference call, even a convention center&#151;has been superseded by collaborations on an astronomical scale.  </B> <P> Today, encyclopedias, jetliners, operating systems, mutual funds, and many other items are being created by teams numbering in the thousands or even millions. While some leaders fear the heaving growth of these massive online communities, <I>Wikinomics</I> proves this fear is folly. Smart firms can harness collective capability and genius to spur innovation, growth, and success. <P> A brilliant guide to one of the most profound changes of our time, <I>Wikinomics</I> challenges our most deeply-rooted assumptions about business and will prove indispensable to anyone who wants to understand competitiveness in the twenty-first century. <P> Based on a $9 million research project led by bestselling author Don Tapscott, <I>Wikinomics</I> shows how masses of people can participate in the economy like never before. They are creating TV news stories, sequencing the human genome, remixing their favorite music, designing software, finding a cure for disease, editing school texts, inventing new cosmetics, or even building motorcycles.  You'll read about: <BR> &#149; Rob McEwen, the Goldcorp, Inc. CEO who used open source tactics and an online competition to save his company and breathe new life into an old-fashioned industry.<BR> &#149; Flickr, Second Life, YouTube, and other thriving online communities that transcend social networking to pioneer a new form of collaborative production.<BR> &#149; Mature companies like Procter & Gamble that cultivate nimble, trust-based relationships with external collaborators to form vibrant business ecosystems. <BR> <BR> An important look into the future, <I>Wikinomics</I> will be your road map for doing business in the twenty-first century.}"
145,36,3231,1,"A Taxonomy of Data Grids for Distributed Data Sharing, Management and Processing","Data Grids have been adopted as the platform for scientific communities that need to share, access, transport, process and manage large data collections distributed worldwide. They combine high-end computing technologies with high-performance networking and wide-area storage management techniques. In this paper, we discuss the key concepts behind Data Grids and compare them with other data sharing and distribution paradigms such as content delivery networks, peer-to-peer networks and distributed databases. We then provide comprehensive taxonomies that cover various aspects of architecture, data transportation, data replication and resource allocation and scheduling. Finally, we map the proposed taxonomy to various Data Grid systems not only to validate the taxonomy but also to identify areas for future exploration. Through this taxonomy, we aim to categorise existing systems to better understand their goals and their methodology. This would help evaluate their applicability for solving similar problems. This taxonomy also provides a \\gap analysis\\ of this area through which researchers can potentially identify new issues for investigation. Finally, we hope that the proposed taxonomy and mapping also helps to provide an easy way for new practitioners to understand this complex area of research."
146,37,136,1,Understanding and Using Context,"Context is a poorly used source of information in our computing environments. As a result, we have an impoverished understanding of what context is and how it can be used. In this paper, we provide an operational definition of context and discuss the different ways that context can be used by context-aware applications. We also present the Context Toolkit, an architecture that supports the building of these context-aware applications. We discuss the features and abstractions in the toolkit that make the task of building applications easier. Finally, we introduce a new abstraction, a situation, which we believe will provide additional support to application designers.  1. Introduction  Humans are quite successful at conveying ideas to each other and reacting appropriately. This is due to many factors: the richness of the language they share, the common understanding of how the world works, and an implicit understanding of everyday situations. When humans talk with humans, they are able..."
147,37,2830,1,Pervasive computing: vision and challenges,"This article discusses the challenges in computer systems research posed by the emerging field of pervasive computing. It first examines the relationship of this new field to its predecessors: distributed systems and mobile computing. It then identifies four new research thrusts: effective use of smart spaces, invisibility, localized scalability, and masking uneven conditioning. Next, it sketches a couple of hypothetical pervasive computing scenarios, and uses them to identify key capabilities missing from today's systems. The article closes with a discussion of the research necessary to develop these capabilities."
148,37,3974,1,Agent-based software engineering,"The technology of intelligent agents and multi-agent systems seems set to radically alter the way in which complex, distributed, open systems are conceptualized and implemented. The purpose of this paper is to consider the problem of building a multi-agent system as a software engineering enterprise. The article focuses on three issues: (i) how agents might be specified; (ii) how these specifications might be refined or otherwise transformed into efficient implementations; and (iii) how implemented agents and multi-agent systems might subsequently be verified, in order to show that they are correct with respect to their specifications. These issues are discussed with reference to a number of casestudies. The article concludes by setting out some issues and open problems for future research. 1 Introduction  Intelligent agents are ninety-nine percent computer science and one percent AI.  Oren Etzioni [12] Over its 40-year history, Artificial Intelligence (AI) has been subject to many and..."
149,37,6549,1,Intention is choice with commitment,"This paper explores principles governing the rational balance among an agent's beliefs, goals, actions, and intentions. Such principles provide specifications for artificial agents, and approximate a theory of human action (as philosophers use the term). By making explicit the conditions under which an agent can drop his goals, i.e., by specifying how the agent is committed to his goals, the formalism captures a number of important properties of intention. Specifically, the formalism provides analyses for Bratman's three characteristic functional roles played by intentions [7, 9], and shows how agents can avoid intending all the foreseen side-effects of what they actually intend. Finally, the analysis shows how intentions can be adopted relative to a background of relevant beliefs and other intentions or goals. By relativizing one agent's intentions in terms of beliefs about another agent's intentions (or beliefs), we derive a preliminary account of interpersonal commitments."
150,38,13280,1,"I tube, you tube, everybody tubes: analyzing the world's largest user generated content video system","User Generated Content (UGC) is re-shaping the way people watch video and TV, with millions of video producers and consumers. In particular, UGC sites are creating new viewing patterns and social interactions, empowering users to be more creative, and developing new business opportunities. To better understand the impact of UGC systems, we have analyzed YouTube, the world's largest UGC VoD system. Based on a large amount of data collected, we provide an in-depth study of YouTube and other similar UGC systems. In particular, we study the popularity life-cycle of videos, the intrinsic statistical properties of requests and their relationship with video age, and the level of content aliasing or of illegal content in the system. We also provide insights on the potential for more efficient UGC VoD systems (e.g. utilizing P2P techniques or making better use of caching). Finally, we discuss the opportunities to leverage the latent demand for niche videos that are not reached today due to information filtering effects or other system scarcity distortions. Overall, we believe that the results presented in this paper are crucial in understanding UGC systems and can provide valuable information to ISPs, site administrators, and content owners with major commercial and technical implications."
151,39,5617,1,Less is More - Genetic Optimisation of Nearest Neighbour Classifiers,"The present paper deals with optimisation of Nearest Neighbour rule Classifiers via Genetic Algorithms. The methodology consists on implement a Genetic Algorithm capable of search the input feature space used by the NNR classifier. Results show that is adequate to perform feature reduction and simultaneous improve the Recognition Rate. Some practical examples prove that is possible to Recognise Portuguese Granites in 100%, with only 3 morphological features (from an original set of 117..."
152,39,5626,1,Intrusion Detection Systems using Adaptive Regression Splines,"Past few years have witnessed a growing recognition of soft computing technologies for the construction of intelligent and reliable intrusion detection systems. Due to increasing incidents of cyber attacks, building effective intrusion detection systems (IDSs) are essential for protecting information systems security, and yet it remains an elusive goal and a great challenge. In this paper, we report a performance analysis between Multivariate Adaptive Regression Splines (MARS), neural networks and support vector machines. The MARS procedure builds flexible regression models by fitting separate splines to distinct intervals of the predictor variables. A brief comparison of different neural network learning algorithms is also given."
153,39,5635,1,Societal Implicit Memory and his Speed on Tracking Extrema over Dynamic Environments using Self-Regulatory Swarms,"In order to overcome difficult dynamic optimization and environment extrema tracking problems, we propose a Self-Regulated Swarm (SRS) algorithm which hybridizes the advantageous characteristics of Swarm Intelligence as the emergence of a societal environmental memory or cognitive map via collective pheromone laying in the landscape (properly balancing the exploration/exploitation nature of the search strategy), with a simple Evolutionary mechanism that trough a direct reproduction procedure linked to local environmental features is able to self-regulate the above exploratory swarm population, speeding it up globally. In order to test his adaptive response and robustness, we have recurred to different dynamic multimodal complex functions as well as to Dynamic Optimization Control (DOC) problems. Measures were made for different dynamic settings and parameters such as, environmental upgrade frequencies, landscape changing speed severity, type of dynamic (linear or circular), and to dramatic changes on the algorithmic search purpose over each test environment (e.g. shifting the extrema). Finally, comparisons were made with traditional Genetic Algorithms (GA), Bacterial Foraging Optimization Algorithms (BFOA), as well as with more recently proposed Co-Evolutionary approaches. SRS, were able to demonstrate quick adaptive responses, while outperforming the results obtained by the other approaches. Additionally, some successful behaviors were found: SRS was able not only to achieve quick adaptive responses, as to maintaining a number of different solutions, while adapting to new unforeseen extrema; the possibility to spontaneously create and maintain different subpopulations on different peaks, emerging different exploratory corridors with intelligent path planning capabilities; the ability to request for new agents over dramatic changing periods, and economizing those foraging resources over periods of stabilization. Finally, results prove that the present SRS collective swarm of bio-inspired agents is able to track about 65% of moving peaks traveling up to ten times faster than the velocity of a single individual composing that precise swarm tracking system. This emerged behavior is probably one of the most interesting ones achieved by the present work."
154,40,3092,1,The Discovery of Grounded Theory: Strategies for Qualitative Research,"One should not be dissuaded from this book by its publication date. The material is as relevant now the day it was published, making it an essential classic now for over 30 years. Readers of this book will be introduced into an entirely new paradigm for doing rigorous research based on a qualitative methodology. Particularly if you are only familiar with the traditional scientific method and experimentalism, this book open up a whole new world. Glaser & Strauss show how theory emerges from the data, as an ever improving understanding of the signifiance of what is discovered. This book really presents a robust way for a researcher in any field, but particularly in the human sciences, to approach day to day research. It is the living method of creativity and innovation, presenting a system for understanding one's discoveries and framing them to producing meaningful knowledge. Everyone is approaching life this way, living out experiences and drawing conclusions from them, including the most mainstream of scientists. But Glaser and Strauss show how that process can be transformed from a willy-nilly gut feel into something that more reliably produces defensible knowledge claims and builds a substantive theoretical network."
155,41,1148,1,Consciousness Explained,"{Consciousness is notoriously difficult to explain. On one hand, there are facts about conscious experience--the way clarinets sound, the way lemonade tastes--that we know subjectively, from the inside. On the other hand, such facts are not readily accommodated in the objective world described by science. How, after all, could the reediness of clarinets or the tartness of lemonade be predicted in advance? Central to Daniel C. Dennett's attempt to resolve this dilemma is the ""heterophenomenological"" method, which treats reports of introspection nontraditionally--not as evidence to be used in explaining consciousness, but as data to be explained. Using this method, Dennett argues against the myth of the Cartesian theater--the idea that consciousness can be precisely located in space or in time. To replace the Cartesian theater, he introduces his own multiple drafts model of consciousness, in which the mind is a bubbling congeries of unsupervised parallel processing. Finally, Dennett tackles the conventional philosophical questions about consciousness, taking issue not only with the traditional answers but also with the traditional methodology by which they were reached.<p>  Dennett's writing, while always serious, is never solemn; who would have thought that combining philosophy, psychology, and neuroscience could be such fun? Not every reader will be convinced that Dennett has succeeded in explaining consciousness; many will feel that his account fails to capture essential features of conscious experience. But none will want to deny that the attempt was well worth making. <I>--Glenn Branch</I>}"
156,41,1760,1,The Modularity of Mind,{This study synthesizes current information from the various fields of cognitive science in support of a new and exciting theory of mind. Most psychologists study horizontal processes like memory and information flow; Fodor postulates a vertical and modular psychological organization underlying biologically coherent behaviors. This view of mental architecture is consistent with the historical tradition of faculty psychology while integrating a computational approach to mental processes. One of the most notable aspects of Fodor's work is that it articulates features not only of speculative cognitive architectures but also of current research in artificial intelligence.<br /> <br /> Jerry A. Fodor is Professor of Psychology and Chairman of the Department of Philosophy at MIT.}
157,41,6449,1,Scientific Reasoning: The Bayesian Approach,"{In this clearly reasoned defense of Bayes's Theorem &#151; that probability can be used to reasonably justify scientific theories &#151; Colin Howson and Peter Urbach examine the way in which scientists appeal to probability arguments, and demonstrate that the classical approach to statistical inference is full of flaws. Arguing the case for the Bayesian method with little more than basic algebra, the authors show that it avoids the difficulties of the classical system. The book also refutes the major criticisms leveled against Bayesian logic, especially that it is too subjective. This newly updated edition of this classic textbook is also suitable for college courses.}"
158,41,10791,1,Visual routines,"This paper examines the processing of visual information beyond the creation of the early representations. A fundamental requirement at this level is the capacity to establish visually abstract shape properties and spatial relations. This capacity plays a major role in object recognition, visually guided manipulation, and more abstract visual thinking. For the human visual system, the perception of spatial properties and relations that are complex from a computational standpoint nevertheless often appears deceivingly immediate and effortless. The proficiency of the human system in analyzing spatial information far surpasses the capacities of current artificial systems. The study of the computations that underlie this competence may therefore lead to the development of new more efficient methods for the spatial analysis of visual information. The perception of abstract shape properties and spatial relations raises fundamental difficulties with major implications for the overall processing of visual information. It will be argued that the computation of spatial relations divides the analysis of visual information into two main stages. The first is the bottom-up creation of certain representations of the visible environment. The second stage involves the application of process called âvisual routinesâ to the representations constructed in the first stage. These routines can establish properties and relations that cannot be represented explicitly in the initial representations. Visual routines are composed of sequences of elemental operations. Routines for different properties and relations share elemental operations. Using a fixed set of basic operations, the visual system can assemble different routines to extract an unbounded variety of shape properties and spatial relations."
159,42,1788,1,"Generative Programming: Methods, Tools, and Applications","{The authors present a grand tour of Generative Programming that is bound to become a classic. They . . . focus on the generally unappreciated connection between Domain Specific Languages and Generative Programming as a motivation for future development. Their wide-ranging and practical methods for Domain Analysis and Domain Engineering describe the first steps that developers can take right now . . . and are valuable both when existing systems are used or in preparation for emerging new generative technologies."" --Charles Simonyi, Chief Architect at Microsoft Research and the inventor of Intentional Programming  ""The book develops strong themes around unifying principles that tie the pieces together, most notably domain engineering and metaprogramming. It is crucial to understand that this book is not just some refreshing diversion, nor just an exposition of some noteworthy niche techniques: It is a harbinger of a broader enlightenment that opens the door to a new age."" --From the Foreword by James Coplien, a Distinguished Member of Technical Staff at Lucent Technologies, Bell Laboratories  <P>Generative Programming (GP) offers great promise to application developers. It makes the idea of moving from one-of-a-kind software systems to the semi-automated manufacture of wide varieties of software quite real. In short, GP is about recognizing the benefits of automation in software development. Generative Programming covers methods and tools that will help you design and implement the right components for a system family and automate component assembly. The methods presented here are applicable for all commercial development--from ""programming in the small,"" at the level of classes and procedures--to ""programming in the large,"" or developing families of large systems.   <P>Generative Programming is your complete guide and reference to this emerging discipline. It provides in-depth treatment of critical technologies and topics including:  Domain Engineering  Feature Modeling  Generic Programming  Aspect-Oriented Programming  Template Metaprogramming in C++  Generators  Microsoft's Intentional Programming  Using this book you will learn how these techniques fit together and, more importantly, how to apply them in practice. The text contains three comprehensive case studies in three different domains: programming domain (container data structures), business domain (banking), and scientific computing (matrix computations).}"
160,42,4252,1,{Variability Issues in Software Product Lines},"Software product lines (or system families) have achieved considerable adoption by the software industry. A software product line captures the commonalities between a set of products while providing for the differences. Differences are managed by delaying design decisions, thereby introducing variation points. The whole of variation points is typically referred to as the variability of the software product line. Variability management is, however, not a trivial activity and several issues exist, both in general as well as specific to individual phases in the lifecycle. This paper identifies and describes several variability issues based on practical experiences and theoretical understanding of the problem domain."
161,42,5390,1,Version models for software configuration management,"After more than 20 years of research and practice in software configuration management (SCM), constructing consistent configurations of versioned software products still remains a challenge. This article focuses on the version models underlying both commercial systems and research prototypes. It provides an overview and classification of different versioning paradigms and defines and relates fundamental concepts such as revisions, variants, configurations, and changes. In particular, we focus on intensional versioning, that is, construction of versions based on configuration rules. Finally, we provide an overview of systems that have had significant impact on the development of the SCM discipline and classify them according to a detailed taxonomy."
162,42,10617,1,Representing concerns in source code,"A software modification task often addresses several concerns . A concern is anything a stakeholder may want to consider as a conceptual unit, including features, nonfunctional requirements, and design idioms. In many cases, the source code implementing a concern is not encapsulated in a single programming language module, and is instead scattered and tangled throughout a system. Inadequate separation of concerns increases the difficulty of evolving software in a correct and cost-effective manner. To make it easier to modify concerns that are not well modularized, we propose an approach in which the implementation of concerns is documented in artifacts, called concern graphs. Concern graphs are abstract models that describe which parts of the source code are relevant to different concerns. We present a formal model for concern graphs and the tool support we developed to enable software developers to create and use concern graphs during software evolution tasks. We report on five empirical studies, providing evidence that concern graphs support views and operations that facilitate the task of modifying the code implementing scattered concerns, are cost-effective to create and use, and robust enough to be used with different versions of a software system."
163,43,250,1,Mining Association Rules between Sets of Items in Large Databases,"We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm."
164,43,1105,1,Elements of Information Theory,"{Following a brief introduction and overview, early chapters cover the basic algebraic relationships of entropy, relative entropy and mutual information, AEP, entropy rates of stochastics processes and data compression, duality of data compression and the growth rate of wealth. Later chapters explore Kolmogorov complexity, channel capacity, differential entropy, the capacity of the fundamental Gaussian channel, the relationship between information theory and statistics, rate distortion and network information theories. The final two chapters examine the stock market and inequalities in information theory. In many cases the authors actually describe the properties of the solutions before the presented problems.}"
165,43,2303,1,A Method for Semi-Automatic Ontology Acquisition from a Corporate Intranet,"This paper describes our actual and ongoing work in supporting semiautomatic  ontology acquisition from a corporate intranet of an insurance company.  A comprehensive architecture and a system for semi-automatic ontology  acquisition supports processing semi-structured information (e.g. contained in  dictionaries) and natural language documents and including existing core ontologies  (e.g. GermaNet, WordNet). We present a method for acquiring a applicationtailored  domain ontology from given..."
166,43,3188,1,Support Vector Machine Active Learning with Applications to Text Classification,". Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a version space. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.  Keywords: Active Learning, Selective Sampling, Support Vector Machines, Classification, Relevance Feedback  Abbreviations: SVM -- Support Vector Machine; TSVM -- Transductive Support Vector Machine 1."
167,43,4198,1,Unsupervised learning of natural languages,"Edited by James L. McClelland, Carnegie Mellon University, Pittsburgh, PA, and approved June 14, 2005 (received for review December 25, 2004)We address the problem, fundamental to linguistics, bioinformatics, and certain other disciplines, of using corpora of raw symbolic sequential data to infer underlying rules that govern their production. Given a corpus of strings (such as text, transcribed speech, chromosome or protein sequence data, sheet music, etc.), our unsupervised algorithm recursively distills from it hierarchically structured patterns. The ADIOS (automatic distillation of structure) algorithm relies on a statistical method for pattern extraction and on structured generalization, two processes that have been implicated in language acquisition. It has been evaluated on artificial context-free grammars with thousands of rules, on natural languages as diverse as English and Chinese, and on protein data correlating sequence with function. This unsupervised algorithm is capable of learning complex syntax, generating grammatical novel sentences, and proving useful in other fields that call for structure discovery from raw data, such as bioinformatics."
168,43,7044,1,Error bounds for convolutional codes and an asymptotically optimum decoding algorithm,"The probability of error in decoding an optimal convolutional code transmitted over a memoryless channel is bounded from above and below as a function of the constraint length of the code. For all but pathological channels the bounds are asymptotically (exponentially) tight for rates above<tex>R_{0}</tex>, the computational cutoff rate of sequential decoding. As a function of constraint length the performance of optimal convolutional codes is shown to be superior to that of block codes of the same length, the relative improvement increasing with rate. The upper bound is obtained for a specific probabilistic nonsequential decoding algorithm which is shown to be asymptotically optimum for rates above<tex>R_{0}</tex>and whose performance bears certain similarities to that of sequential decoding algorithms."
169,43,7732,1,Accurate Unlexicalized Parsing,"We demonstrate that an unlexicalized PCFG can parse much more accurately than previously shown, by making use of simple, linguistically motivated state splits, which break down false independence assumptions latent in a vanilla treebank grammar. Indeed, its performance of 86.36% (LP/LR F 1 ) is better than that of early  lexicalized  PCFG models, and surprisingly close to the current state-of-the-art. This result has potential uses beyond establishing a strong lower bound on the maximum possible accuracy of unlexicalized models: an unlexicalized PCFG is much more compact, easier to replicate, and easier to interpret than more complex lexical models, and the parsing algorithms are simpler, more widely understood, of lower asymptotic complexity, and easier to optimize."
170,43,9326,1,Automatic ontology-based knowledge extraction from Web documents,"To bring the Semantic Web to life and provide advanced knowledge services, we need efficient ways to access and extract knowledge from Web documents. Although Web page annotations could facilitate such knowledge gathering, annotations are rare and will probably never be rich or detailed enough to cover all the knowledge these documents contain. Manual annotation is impractical and unscalable, and automatic annotation tools remain largely undeveloped. Specialized knowledge services therefore require tools that can search and extract specific knowledge directly from unstructured text on the Web, guided by an ontology that details what type of knowledge to harvest. An ontology uses concepts and relations to classify domain knowledge. Other researchers have used ontologies to support knowledge extraction, but few have explored their full potential in this domain. The paper considers the Artequakt project which links a knowledge extraction tool with an ontology to achieve continuous knowledge support and guide information extraction. The extraction tool searches online documents and extracts knowledge that matches the given classification structure. It provides this knowledge in a machine-readable format that will be automatically maintained in a knowledge base (KB). Knowledge extraction is further enhanced using a lexicon-based term expansion mechanism that provides extended ontology terminology."
171,43,10556,1,Open Information Extraction from the Web,"Traditionally, Information Extraction (IE) has fo- cused on satisfying precise, narrow, pre-speciï¬ed requests from small homogeneous corpora (e.g., extract the location and time of seminars from a set of announcements). Shifting to a new domain requires the user to name the target relations and to manually create new extraction rules or hand-tag new training examples. This manual labor scales linearly with the number of target relations. This paper introduces Open IE (OIE), a new ex- traction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input. The paper also introduces T EXT RUNNER, a fully implemented, highly scalable OIE system where the tuples are assigned a probability and indexed to support efï¬cient extraction and explo- ration via user queries. We report on experiments over a 9,000,000 Web page corpus that compare T EXT RUNNER with K NOW I TA LL, a state-of-the-art Web IE system. T EXT RUNNER achieves an error reduction of 33% on a comparable set of extractions. Furthermore, in the amount of time it takes K NOW I TA LL to per- form extraction for a handful of pre-speciï¬ed re- lations, T EXT RUNNER extracts a far broader set of facts reï¬ecting orders of magnitude more rela- tions, discovered on the ï¬y. We report statistics on T EXT RUNNERâs 11,000,000 highest probability tuples, and show that they contain over 1,000,000 concrete facts and over 6,500,000 more abstract as- sertions."
172,43,13594,1,Contextual correlates of semantic similarity,"Investigated the relationship between semantic and contextual similarity for pairs of nouns that vary from high to low semantic similarity in 86 undergraduates in 3 experiments. Semantic similarity was estimated by subjective ratings; contextual similarity was estimated by the method of sorting sentential contexts. Results show an inverse linear relationship between similarity of meaning and the discriminability of contexts. This relation is obtained for 2 separate corpora of sentence contexts. It is concluded that for words in the same language drawn from the same syntactic and semantic categories, the more often 2 words can be substituted into the same contexts the more similar in meaning they are judged to be. ((c) 1997 APA/PsycINFO, all rights reserved)"
173,44,478,1,Emergence of scaling in random networks,"Recently retired as head of the Global Alliance for Vaccines and Immunization (GAVI) secretariat and as a health advisor to leading global entities, Tore Godal is now a Special Advisor to the Norwegian Prime Minister. He is nevertheless continuing to fight for better global health, cogently articulating the needs of the world's poor and disadvantaged. He is a leading leprosy expert, ex-director of the world's premier agency for research and training in tropical diseases, instigator and prime mover of some global innovative public-private health sector partnerships, adept fund mobilizer, and advocate of the `let's get it done' school of leadership. Few individuals are, therefore, more experienced or better suited for such a crucial and much-needed role"
174,44,3420,1,How dynamic is the web?,"Recent experiments and analysis suggest that there are about 800 million publicly-indexable Web pages. However, unlike books in a traditional library, Web pages continue to change even after they are initially published by their authors and indexed by search engines. This paper describes preliminary data on and statistical analysis of the frequency and nature of Web page modifications. Using empirical models and a novel analytic metric of âup-to-dateness', we estimate the rate at which Web search engines must re-index the Web to remain current."
175,44,5274,1,The {W}eb as a Graph,"The pages and hyperlinks of the World-Wide Web may be viewed as nodes and edges in a directed graph. This graph has about a billion nodes today, several billion links, and appears to grow exponentially with time. There are many reasons---mathematical, sociological, and commercial---for studying the evolution of this graph. We first review a set of algorithms that operate on the Web graph, addressing problems from Web search, automatic community discovery, and classification. We then recall a number of measurements and properties of the Web graph. Noting that traditional random graph models do not explain these observations, we propose a new family of random graph models."
176,44,11206,1,Optimizing web search using social annotations,"This paper explores the use of social annotations to improve web search. Nowadays, many services, e.g. del.icio.us, have been developed for web users to organize and share their favorite web pages on line by using social annotations. We observe that the social annotations can benefit web  search in two aspects: 1) the annotations are usually good summaries of corresponding web pages; 2) the count of annotations indicates the popularity of web pages. Two novel algorithms are proposed to incorporate the above information into page ranking: 1) SocialSimRank (SSR) calculates the similarity between social annotations and web queries; 2) SocialPageRank (SPR) captures the popularity of web pages. Preliminary experimental results show that SSR can find the latent semantic association between queries and annotations, while SPR successfully measures the quality (popularity) of a web page from the web usersâ perspective. We further evaluate the proposed methods empirically with 50 manually constructed queries and 3000 auto-generated queries on a  dataset crawled from del.icio.us. Experiments show that both SSR and SPR benefit web search significantly"
177,45,1885,1,Multimeric threading-based prediction of protein-protein interactions on a genomic scale: application to the Saccharomyces cerevisiae proteome.,"MULTIPROSPECTOR, a multimeric threading algorithm for the prediction of protein-protein interactions, is applied to the genome of Saccharomyces cerevisiae. Each possible pairwise interaction among more than 6000 encoded proteins is evaluated against a dimer database of 768 complex structures by using a confidence estimate of the fold assignment and the magnitude of the statistical interfacial potentials. In total, 7321 interactions between pairs of different proteins are predicted, based on 304 complex structures. Quality estimation based on the coincidence of subcellular localizations and biological functions of the predicted interactors shows that our approach ranks third when compared with all other large-scale methods. Unlike other in silico methods, MULTIPROSPECTOR is able to identify the residues that participate directly in the interaction. Three hundred seventy-four of our predictions can be found by at least one of the other studies, which is compatible with the overlap between two different other methods. From the analysis of the mRNA abundance data, our method does not bias towards proteins with high abundance. Finally, several relevant predictions involved in various functions are presented. In summary, we provide a novel approach to predict protein-protein interactions on a genomic scale that is a useful complement to experimental methods."
178,45,3759,1,Protein function prediction using local 3D templates.,"The prediction of a protein's function from its 3D structure is becoming more and more important as the worldwide structural genomics initiatives gather pace and continue to solve 3D structures, many of which are of proteins of unknown function. Here, we present a methodology for predicting function from structure that shows great promise. It is based on 3D templates that are defined as specific 3D conformations of small numbers of residues. We use four types of template, covering enzyme active sites, ligand-binding residues, DNA-binding residues and reverse templates. The latter are templates generated from the target structure itself and scanned against a representative subset of all known protein structures. Together, the templates provide a fairly thorough coverage of the known structures and ensure that if there is a match to a known structure it is unlikely to be missed. A new scoring scheme provides a highly sensitive means of discriminating between true positive and false positive template matches. In all, the methodology provides a powerful new tool for function prediction to complement those already in use. (c) 2005 Elsevier Ltd. All rights reserved."
179,46,1632,1,Cytoscape: a software environment for integrated models of biomolecular interaction networks.,"Cytoscape is an open source software project for integrating biomolecular interaction networks with high-throughput expression data and other molecular states into a unified conceptual framework. Although applicable to any system of molecular components and interactions, Cytoscape is most powerful when used in conjunction with large databases of protein-protein, protein-DNA, and genetic interactions that are increasingly available for humans and model organisms. Cytoscape's software Core provides basic functionality to layout and query the network; to visually integrate the network with expression profiles, phenotypes, and other molecular states; and to link the network to databases of functional annotations. The Core is extensible through a straightforward plug-in architecture, allowing rapid development of additional computational analyses and features. Several case studies of Cytoscape plug-ins are surveyed, including a search for interaction pathways correlating with changes in gene expression, a study of protein complexes involved in cellular recovery to DNA damage, inference of a combined physical/functional interaction network for Halobacterium, and an interface to detailed stochastic/kinetic gene regulatory models."
180,46,8303,1,ARACNE: an algorithm for the reconstruction of gene regulatory networks in a mammalian cellular context.,"BACKGROUND: Elucidating gene regulatory networks is crucial for understanding normal cell physiology and complex pathologic phenotypes. Existing computational methods for the genome-wide ""reverse engineering"" of such networks have been successful only for lower eukaryotes with simple genomes. Here we present ARACNE, a novel algorithm, using microarray expression profiles, specifically designed to scale up to the complexity of regulatory networks in mammalian cells, yet general enough to address a wider range of network deconvolution problems. This method uses an information theoretic approach to eliminate the majority of indirect interactions inferred by co-expression methods. RESULTS: We prove that ARACNE reconstructs the network exactly (asymptotically) if the effect of loops in the network topology is negligible, and we show that the algorithm works well in practice, even in the presence of numerous loops and complex topologies. We assess ARACNE's ability to reconstruct transcriptional regulatory networks using both a realistic synthetic dataset and a microarray dataset from human B cells. On synthetic datasets ARACNE achieves very low error rates and outperforms established methods, such as Relevance Networks and Bayesian Networks. Application to the deconvolution of genetic networks in human B cells demonstrates ARACNE's ability to infer validated transcriptional targets of the cMYC proto-oncogene. We also study the effects of misestimation of mutual information on network reconstruction, and show that algorithms based on mutual information ranking are more resilient to estimation errors. CONCLUSION: ARACNE shows promise in identifying direct transcriptional interactions in mammalian cellular networks, a problem that has challenged existing reverse engineering algorithms. This approach should enhance our ability to use microarray data to elucidate functional mechanisms that underlie cellular processes and to identify molecular targets of pharmacological compounds in mammalian cellular networks."
181,46,12155,1,The population genetics of structural variation,"Population genetics is central to our understanding of human variation, and by linking medical and evolutionary themes, it enables us to understand the origins and impacts of our genomic differences. Despite current limitations in our knowledge of the locations, sizes and mutational origins of structural variants, our characterization of their population genetics is developing apace, bringing new insights into recent human adaptation, genome biology and disease. We summarize recent dramatic advances, describe the diverse mutational origins of chromosomal rearrangements and argue that their complexity necessitates a re-evaluation of existing population genetic methods."
182,46,13354,1,Predicting biological networks from genomic data.,"Continuing improvements in DNA sequencing technologies are providing us with vast amounts of genomic data from an ever-widening range of organisms. The resulting challenge for bioinformatics is to interpret this deluge of data and place it back into its biological context. Biological networks provide a conceptual framework with which we can describe part of this context, namely the different interactions that occur between the molecular components of a cell. Here, we review the computational methods available to predict biological networks from genomic sequence data and discuss how they relate to high-throughput experimental methods."
183,46,14063,1,Consensus generation and variant detection by Celera Assembler,"Motivation: We present an algorithm to identify allelic variation given a Whole Genome Shotgun (WGS) assembly of haploid sequences, and to produce a set of haploid consensus sequences rather than a single consensus sequence. Existing WGS assemblers take a column-by-column approach to consensus generation, and produce a single consensus sequence which can be inconsistent with the underlying haploid alleles, and inconsistent with any of the aligned sequence reads. Our new algorithm uses a dynamic windowing approach. It detects alleles by simultaneously processing the portions of aligned reads spanning a region of sequence variation, assigns reads to their respective alleles, phases adjacent variant alleles and generates a consensus sequence corresponding to each confirmed allele. This algorithm was used to produce the first diploid genome sequence of an individual human. It can also be applied to assemblies of multiple diploid individuals and hybrid assemblies of multiple haploid organisms.  Results: Being applied to the individual human genome assembly, the new algorithm detects exactly two confirmed alleles and reports two consensus sequences in 98.98% of the total number 2 033 311 detected regions of sequence variation. In 33 269 out of 460 373 detected regions of size >1 bp, it fixes the constructed errors of a mosaic haploid representation of a diploid locus as produced by the original Celera Assembler consensus algorithm. Using an optimized procedure calibrated against 1 506 344 known SNPs, it detects 438 814 new heterozygous SNPs with false positive rate 12%.  Availability: The open source code is available at: http://wgs-assembler.cvs.sourceforge.net/wgs-assembler/  Contact: gdenisov@jcvi.org 10.1093/bioinformatics/btn074"
184,46,14636,1,Accurate whole human genome sequencing using reversible terminator chemistry.,"DNA sequence information underpins genetic research, enabling discoveries of important biological or medical benefit. Sequencing projects have traditionally used long (400â800 base pair) reads, but the existence of reference sequences for the human and many other genomes makes it possible to develop new, fast approaches to re-sequencing, whereby shorter reads are compared to a reference to identify intraspecies genetic variation. Here we report an approach that generates several billion bases of accurate nucleotide sequence per experiment at low cost. Single molecules of DNA are attached to a flat surface, amplified in situ and used as templates for synthetic sequencing with fluorescent reversible terminator deoxyribonucleotides. Images of the surface are analysed to generate high-quality sequence. We demonstrate application of this approach to human genome sequencing on flow-sorted X chromosomes and then scale the approach to determine the genome sequence of a male Yoruba from Ibadan, Nigeria. We build an accurate consensus sequence from >30times average depth of paired 35-base reads. We characterize four million single-nucleotide polymorphisms and four hundred thousand structural variants, many of which were previously unknown. Our approach is effective for accurate, rapid and economical whole-genome re-sequencing and many other biomedical applications."
185,46,15472,1,High-throughput genotyping by whole-genome resequencing,"The next-generation sequencing technology coupled with the growing number of genome sequences opens the opportunity to redesign genotyping strategies for more effective genetic mapping and genome analysis. We have developed a high-throughput method for genotyping recombinant populations utilizing whole-genome resequencing data generated by the Illumina Genome Analyzer. A sliding window approach is designed to collectively examine genome-wide single nucleotide polymorphisms for genotype calling and recombination breakpoint determination. Using this method, we constructed a genetic map for 150 rice recombinant inbred lines with an expected genotype calling accuracy of 99.94{%} and a resolution of recombination breakpoints within an average of 40 kb. In comparison to the genetic map constructed with 287 PCR-based markers for the rice population, the sequencing-based method was â{Ã }Âº20â{Ã³}faster in data collection and 35â{Ã³}more precise in recombination breakpoint determination. Using the sequencing-based genetic map, we located a quantitative trait locus of large effect on plant height in a 100-kb region containing the rice â{Ã}{Ãº}green revolutionâ{Ã}{Ã¹}gene. Through computer simulation, we demonstrate that the method is robust for different types of mapping populations derived from organisms with variable quality of genome sequences and is feasible for organisms with large genome sizes and low polymorphisms. With continuous advances in sequencing technologies, this genome-based method may replace the conventional marker-based genotyping approach to provide a powerful tool for large-scale gene discovery and for addressing a wide range of biological questions."
186,46,15610,1,SOAP2: an improved ultrafast tool for short read alignment,"Summary: SOAP2 is a significantly improved version of the short oligonucleotide alignment program that both reduces computer memory usage and increases alignment speed at an unprecedented rate. We used a Burrows Wheeler Transformation (BWT) compression index to substitute the seed strategy for indexing the reference sequence in the main memory. We tested it on the whole human genome and found that this new algorithm reduced memory usage from 14.7 to 5.4 GB and improved alignment speed by 20Ã¢ÂÂ30 times. SOAP2 is compatible with both single- and paired-end reads. Additionally, this tool now supports multiple text and compressed file formats. A consensus builder has also been developed for consensus assembly and SNP detection from alignment of short reads on a reference genome.Availability: http://soap.genomics.org.cnContact: soap@genomics.org.cn"
187,46,15908,1,Parametric Complexity of Sequence Assembly: Theory and Applications to Next Generation Sequencing,"In recent years, a flurry of new DNA sequencing technologies have altered the landscape of genomics, providing a vast amount of sequence information at a fraction of the costs that were previously feasible. The task of assembling these sequences into a genome has, however, still remained an algorithmic challenge that is in practice answered by heuristic solutions. In order to design better assembly algorithms and exploit the characteristics of sequence data from new technologies, we need an improved understanding of the parametric complexity of the assembly problem. In this article, we provide a first theoretical study in this direction, exploring the connections between repeat complexity, read lengths, overlap lengths and coverage in determining the ""hard"" instances of the assembly problem. Our work suggests at least two ways in which existing assemblers can be extended in a rigorous fashion, in addition to delineating directions for future theoretical investigations."
188,46,16363,1,Population genetic inference from genomic sequence variation,"10.1101/gr.079509.108 Population genetics has evolved from a theory-driven field with little empirical data into a data-driven discipline in which genome-scale data sets test the limits of available models and computational analysis methods. In humans and a few model organisms, analyses of whole-genome sequence polymorphism data are currently under way. And in light of the falling costs of next-generation sequencing technologies, such studies will soon become common in many other organisms as well. Here, we assess the challenges to analyzing whole-genome sequence polymorphism data, and we discuss the potential of these data to yield new insights concerning population history and the genomic prevalence of natural selection."
189,46,16607,1,"De novo Assembly of a 40 Mb Eukaryotic Genome from Short Sequence Reads: Sordaria macrospora, a Model Organism for Fungal Morphogenesis","Filamentous fungi are of great importance in ecology, agriculture, medicine, and biotechnology. Thus, it is not surprising that genomes for more than 100 filamentous fungi have been sequenced, most of them by Sanger sequencing. While next-generation sequencing techniques have revolutionized genome resequencing, e.g. for strain comparisons, genetic mapping, or transcriptome and ChIP analyses, de novo assembly of eukaryotic genomes still presents significant hurdles, because of their large size and stretches of repetitive sequences. Filamentous fungi contain few repetitive regions in their 30Ã¢â¬â90 Mb genomes and thus are suitable candidates to test de novo genome assembly from short sequence reads. Here, we present a high-quality draft sequence of the Sordaria macrospora genome that was obtained by a combination of Illumina/Solexa and Roche/454 sequencing. Paired-end Solexa sequencing of genomic DNA to 85-fold coverage and an additional 10-fold coverage by single-end 454 sequencing resulted in ~4 Gb of DNA sequence. Reads were assembled to a 40 Mb draft version (N50 of 117 kb) with the Velvet assembler. Comparative analysis with Neurospora genomes increased the N50 to 498 kb. The S. macrospora genome contains even fewer repeat regions than its closest sequenced relative, Neurospora crassa . Comparison with genomes of other fungi showed that S. macrospora , a model organism for morphogenesis and meiosis, harbors duplications of several genes involved in self/nonself-recognition. Furthermore, S. macrospora contains more polyketide biosynthesis genes than N. crassa . Phylogenetic analyses suggest that some of these genes may have been acquired by horizontal gene transfer from a distantly related ascomycete group. Our study shows that, for typical filamentous fungi, de novo assembly of genomes from short sequence reads alone is feasible, that a mixture of Solexa and 454 sequencing substantially improves the assembly, and that the resulting data can be used for comparative studies to address basic questions of fungal biology."
190,46,16733,1,Gap5Ã¢ÂÂediting the billion fragment sequence assembly,"Motivation: Existing sequence assembly editors struggle with the volumes of data now readily available from the latest generation of DNA sequencing instruments.Results: We describe the Gap5 software along with the data structures and algorithms used that allow it to be scalable. We demonstrate this with an assembly of 1.1 billion sequence fragments and compare the performance with several other programs. We analyse the memory, CPU, I/O usage and file sizes used by Gap5.Availability and Implementation: Gap5 is part of the Staden Package and is available under an Open Source licence from http://staden.sourceforge.net. It is implemented in C and Tcl/Tk. Currently it works on Unix systems only.Contact: jkb@sanger.ac.ukSupplementary information: Supplementary data are available at Bioinformatics online."
191,47,486,1,Community structure in social and biological networks,"10.1073/pnas.122653799 A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well knownâa collaboration network and a food webâand find that it detects significant and informative community divisions in both cases."
192,47,3126,1,Uncovering the overlapping community structure of complex networks in nature and society,"Many complex systems in nature and society can be described in terms of networks capturing the intricate web of connections among the units they are made of1, 2, 3, 4. A key question is how to interpret the global organization of such networks as the coexistence of their structural subunits (communities) associated with more highly interconnected parts. Identifying these a priori unknown building blocks (such as functionally related proteins5, 6, industrial sectors7 and groups of people8, 9) is crucial to the understanding of the structural and functional properties of networks. The existing deterministic methods used for large networks find separated communities, whereas most of the actual networks are made of highly overlapping cohesive groups of nodes. Here we introduce an approach to analysing the main statistical features of the interwoven sets of overlapping communities that makes a step towards uncovering the modular structure of complex systems. After defining a set of new characteristic quantities for the statistics of communities, we apply an efficient technique for exploring overlapping communities on a large scale. We find that overlaps are significant, and the distributions we introduce reveal universal features of networks. Our studies of collaboration, word-association and protein interaction graphs show that the web of communities has non-trivial correlations and specific scaling properties."
193,47,10377,1,A faster circular binary segmentation algorithm for the analysis of array CGH data.,"MOTIVATION: Array CGH technologies enable the simultaneous measurement of DNA copy number for thousands of sites on a genome. We developed the circular binary segmentation (CBS) algorithm to divide the genome into regions of equal copy number (Olshen et al ,2004). The algorithm tests for change-points using a maximal t-statistic with a permutation reference distribution to obtain the corresponding p-value. The number of computations required for the maximal test statistic is O(N(2)), where N is the number of markers. This makes the full permutation approach computationally prohibitive for the newer arrays that contain tens of thousands markers and highlights the need for a faster algorithm. RESULTS: We present a hybrid approach to obtain the p-value of the test statistic in linear time. We also introduce a rule for stopping early when there is strong evidence for the presence of a change. We show through simulations that the hybrid approach provides a substantial gain in speed with only a negligible loss in accuracy and that the stopping rule further increases speed. We also present the analyses of array CGH data from breast cancer cell lines to show the impact of the new approaches on the analysis of real data. AVAILABILITY: An R (R Development Core Team, 2006) version of the CBS algorithm has been implemented in the ""DNAcopy"" package of the Bioconductor project (Gentleman et al., 2004). The proposed hybrid method for the p-value is available in version 1.2.1 or higher and the stopping rule for declaring a change early is available in version 1.5.1 or higher."
194,48,2195,1,Prediction of Complete Gene Structures in Human Genomic DNA,"We introduce a general probabilistic model of the gene structure of human genomic sequences which incorporates descriptions of the basic transcriptional, translational and splicing signals, as well as length distributions and compositional features of exons, introns and intergenic regions. {D}istinct sets of model parameters are derived to account for the many substantial differences in gene density and structure observed in distinct {C} + {G} compositional regions of the human genome. {I}n addition, new models of the donor and acceptor splice signals are described which capture potentially important dependencies between signal positions. {T}he model is applied to the problem of gene identification in a computer program, {GENSCAN}, which identifies complete exon/intron structures of genes in genomic {DNA}. {N}ovel features of the program include the capacity to predict multiple genes in a sequence, to deal with partial as well as complete genes, and to predict consistent sets of genes occurring on either or both {DNA} strands. {GENSCAN} is shown to have substantially higher accuracy than existing methods when tested on standardized sets of human and vertebrate genes, with 75 to 80% of exons identified exactly. {T}he program is also capable of indicating fairly accurately the reliability of each predicted exon. {C}onsistently high levels of accuracy are observed for sequences of differing {C} + {G} content and for distinct groups of vertebrates."
195,48,5430,1,A statistical approach to machine translation,"In this paper, we present a statistical approach to machine translation. We describe  the application of our approach to translation from French to English and give  preliminary results.  1  A Statistical Approach to Machine Translation 2 1 Introduction  The field of machine translation is almost as old as the modern digital computer. In 1949 Warren Weaver suggested that the problem be attacked with statistical methods and ideas from information theory, an area which he, Claude Shannon and..."
196,48,8567,1,Citances: Citation Sentences for Semantic Analysis of Bioscience Text,"We propose the use of the text of the sentences surrounding citations as an important tool for semantic interpretation of bioscience text. We hypothesize several diï¬erent uses of citation sentences (which we call citances), including the creation of training and testing data for semantic analysis (especially for entity and relation recognition), synonym set creation, database curation, document summarization, and information retrieval generally. We illustrate some of these ideas, showing that citations to one document in particular align well with what a hand-built curator extracted. We also show preliminary results on the problem of normalizing the diï¬erent ways that the same concepts are expressed within a set of citances, using and improving on existing techniques in automatic paraphrase generation."
197,49,16,1,Network motifs: simple building blocks of complex networks.,"Complex networks are studied across many fields of science. {T}o uncover their structural design principles, we defined ""network motifs,"" patterns of interconnections occurring in complex networks at numbers that are significantly higher than those in randomized networks. {W}e found such motifs in networks from biochemistry, neurobiology, ecology, and engineering. {T}he motifs shared by ecological food webs were distinct from the motifs shared by the genetic networks of {E}scherichia coli and {S}accharomyces cerevisiae or from those found in the {W}orld {W}ide {W}eb. {S}imilar motifs were found in networks that perform information processing, even though they describe elements as different as biomolecules within a cell and synaptic connections between neurons in {C}aenorhabditis elegans. {M}otifs may thus define universal classes of networks. {T}his approach may uncover the basic building blocks of most networks."
198,49,206,1,Transcriptional Regulatory Networks in Saccharomyces cerevisiae,"We have determined how most of the transcriptional regulators encoded in the eukaryote {S}accharomyces cerevisiae associate with genes across the genome in living cells. {J}ust as maps of metabolic networks describe the potential pathways that may be used by a cell to accomplish metabolic processes, this network of regulator-gene interactions describes potential pathways yeast cells can use to regulate global gene expression programs. {W}e use this information to identify network motifs, the simplest units of network architecture, and demonstrate that an automated process can use motifs to assemble a transcriptional regulatory network structure. {O}ur results reveal that eukaryotic cellular functions are highly connected through networks of transcriptional regulators that regulate other transcriptional regulators."
199,49,762,1,Mfold web server for nucleic acid folding and hybridization prediction,"10.1093/nar/gkg595 The abbreviated name, Ã¢ÂÂmfold web serverÃ¢ÂÂ, describes a number of closely related software applications available on the World Wide Web (WWW) for the prediction of the secondary structure of single stranded nucleic acids. The objective of this web server is to provide easy access to RNA and DNA folding and hybridization software to the scientific community at large. By making use of universally available web GUIs (Graphical User Interfaces), the server circumvents the problem of portability of this software. Detailed output, in the form of structure plots with or without reliability information, single strand frequency plots and Ã¢ÂÂenergy dot plotsÃ¢ÂÂ, are available for the folding of single sequences. A variety of Ã¢ÂÂbulkÃ¢ÂÂ servers give less information, but in a shorter time and for up to hundreds of sequences at once. The portal for the mfold web server is http://www.bioinfo.rpi.edu/applications/mfold. This URL will be referred to as Ã¢ÂÂMFOLDROOTÃ¢ÂÂ."
200,49,1082,1,Ultraconserved elements in the human genome.,"{There are 481 segments longer than 200 base pairs (bp) that are absolutely conserved (100\% identity with no insertions or deletions) between orthologous regions of the human, rat, and mouse genomes. Nearly all of these segments are also conserved in the chicken and dog genomes, with an average of 95 and 99\% identity, respectively. Many are also significantly conserved in fish. These ultraconserved elements of the human genome are most often located either overlapping exons in genes involved in RNA processing or in introns or nearby genes involved in the regulation of transcription and development. Along with more than 5000 sequences of over 100 bp that are absolutely conserved among the three sequenced mammals, these represent a class of genetic elements whose functions and evolutionary origins are yet to be determined, but which are more highly conserved between these species than are proteins and appear to be essential for the ontogeny of mammals and other vertebrates.}"
201,49,1293,1,RNA sequence analysis using covariance models.,We describe a general approach to several RNA sequence analysis problems using probabilistic models that flexibly describe the secondary structure and primary sequence consensus of an RNA sequence family. We call these models covariance models'. A covariance model of tRNA sequences is an extremely sensitive and discriminative tool for searching for additional tRNAs and tRNA-related sequences in sequence databases. A model can be built automatically from an existing sequence alignment. We also describe an algorithm for learning a model and hence a consensus secondary structure from initially unaligned example sequences and no prior structural information. Models trained on unaligned tRNA examples correctly predict tRNA scondary structure and produce high-quality multiple alignments. The approach may be applied to any family of small RNA sequences. 10.1093/nar/22.11.2079
202,49,1312,1,Noncoding RNA gene detection using comparative sequence analysis.,"BACKGROUND: Noncoding RNA genes produce transcripts that exert their function without ever producing proteins. Noncoding RNA gene sequences do not have strong statistical signals, unlike protein coding genes. A reliable general purpose computational genefinder for noncoding RNA genes has been elusive. RESULTS: We describe a comparative sequence analysis algorithm for detecting novel structural RNA genes. The key idea is to test the pattern of substitutions observed in a pairwise alignment of two homologous sequences. A conserved coding region tends to show a pattern of synonymous substitutions, whereas a conserved structural RNA tends to show a pattern of compensatory mutations consistent with some base-paired secondary structure. We formalize this intuition using three probabilistic ""pair-grammars"": a pair stochastic context free grammar modeling alignments constrained by structural RNA evolution, a pair hidden Markov model modeling alignments constrained by coding sequence evolution, and a pair hidden Markov model modeling a null hypothesis of position-independent evolution. Given an input pairwise sequence alignment (e.g. from a BLASTN comparison of two related genomes) we classify the alignment into the coding, RNA, or null class according to the posterior probability of each class. CONCLUSIONS: We have implemented this approach as a program, QRNA, which we consider to be a prototype structural noncoding RNA genefinder. Tests suggest that this approach detects noncoding RNA genes with a fair degree of reliability."
203,49,1558,1,Functional organization of the yeast proteome by systematic analysis of protein complexes.,"Most cellular processes are carried out by multiprotein complexes. The identification and analysis of their components provides insight into how the ensemble of expressed proteins (proteome) is organized into functional units. We used tandem-affinity purification (TAP) and mass spectrometry in a large-scale approach to characterize multiprotein complexes in Saccharomyces cerevisiae. We processed 1,739 genes, including 1,143 human orthologues of relevance to human biology, and purified 589 protein assemblies. Bioinformatic analysis of these assemblies defined 232 distinct multiprotein complexes and proposed new cellular roles for 344 proteins, including 231 proteins with no previous functional annotation. Comparison of yeast and human complexes showed that conservation across species extends from single proteins to their molecular environment. Our analysis provides an outline of the eukaryotic proteome as a network of protein complexes at a level of organization beyond binary interactions. This higher-order map contains fundamental biological information and offers the context for a more reasoned and informed approach to drug discovery. [References: 46]"
204,49,1617,1,{Hierarchical organization of modularity in metabolic networks},"Spatially or chemically isolated functional modules composed of several cellular components and carrying discrete functions are considered fundamental building blocks of cellular organization, but their presence in highly integrated biochemical networks lacks quantitative support. Here, we show that the metabolic networks of 43 distinct organisms are organized into many small, highly connected topologic modules that combine in a hierarchical manner into larger, less cohesive units, with their number and degree of clustering following a power law. Within Escherichia coli, the uncovered hierarchical modularity closely overlaps with known metabolic functions. The identified network architecture may be generic to system-level cellular organization."
205,49,1686,1,Hidden Markov Models for Detecting Remote Protein Homologies,"MOTIVATION: A new hidden Markov model method (SAM-T98) for finding remote homologs of protein sequences is described and evaluated. The method begins with a single target sequence and iteratively builds a hidden Markov model (HMM) from the sequence and homologs found using the HMM for database search. SAM-T98 is also used to construct model libraries automatically from sequences in structural databases. METHODS: We evaluate the SAM-T98 method with four datasets. Three of the test sets are fold-recognition tests, where the correct answers are determined by structural similarity. The fourth uses a curated database. The method is compared against WU-BLASTP and against DOUBLE-BLAST, a two-step method similar to ISS, but using BLAST instead of FASTA. RESULTS: SAM-T98 had the fewest errors in all tests-dramatically so for the fold-recognition tests. At the minimum-error point on the SCOP (Structural Classification of Proteins)-domains test, SAM-T98 got 880 true positives and 68 false positives, DOUBLE-BLAST got 533 true positives with 71 false positives, and WU-BLASTP got 353 true positives with 24 false positives. The method is optimized to recognize superfamilies, and would require parameter adjustment to be used to find family or fold relationships. One key to the performance of the HMM method is a new score-normalization technique that compares the score to the score with a reversed model rather than to a uniform null model. AVAILABILITY: A World Wide Web server, as well as information on obtaining the Sequence Alignment and Modeling (SAM) software suite, can be found at http://www.cse.ucsc.edu/research/compbi o/ CONTACT: karplus{\char64}cse.ucsc.edu; http://www.cse.ucsc.edu/karplus"
206,49,2224,1,Bayesian density estimation and inference using mixtures,"We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation, and are exemplified by special cases where data are modelled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general class of normal mixture models. Keywords: Kernel estimation; Mixtures of Dirichlet processes; Multimodality; Normal mixtures; Posterior sampling; Smoothing parameter estimation * Michael D. Escobar is Assistant Professor, Department of Statistics and Department of Preventive Medicine and Biostatistics, University ..."
207,49,2629,1,Systematic interpretation of genetic interactions using protein networks,"Genetic interaction analysis,in which two mutations have a combined effect not exhibited by either mutation alone, is a powerful and widespread tool for establishing functional linkages between genes. In the yeast Saccharomyces cerevisiae, ongoing screens have generated >4,800 such genetic interaction data. We demonstrate that by combining these data with information on protein-protein, prote in-DNA or metabolic networks, it is possible to uncover physical mechanisms behind many of the observed genetic effects. Using a probabilistic model, we found that 1,922 genetic interactions are significantly associated with either between- or within-pathway explanations encoded in the physical networks, covering approximately 40% of known genetic interactions. These models predict new functions for 343 proteins and suggest that between-pathway explanations are better than within-pathway explanations at interpreting genetic interactions identified in systematic screens. This study provides a road map for how genetic and physical interactions can be integrated to reveal pathway organization and function."
208,49,3381,1,Microarray data normalization and transformation,"Underlying every microarray experiment is an experimental question that one would like to address. Finding a useful and satisfactory answer relies on careful experimental design and the use of a variety of data-mining tools to explore the relationships between genes or reveal patterns of expression. While other sections of this issue deal with these lofty issues, this review focuses on the much more mundane but indispensable tasks of 'normalizing' data from individual hybridizations to make meaningful comparisons of expression levels, and of 'transforming' them to select genes for further analysis and data mining."
209,49,4125,1,Evolution of Genetic Potential,"Organisms employ a multitude of strategies to cope with the dynamical environments in which they live. Homeostasis and physiological plasticity buffer changes within the lifetime of an organism, while stochastic developmental programs and hypermutability track changes on longer timescales. An alternative long-term mechanism is &#8220;genetic potential&#8221;&#8212;a heightened sensitivity to the effects of mutation that facilitates rapid evolution to novel states. Using a transparent mathematical model, we illustrate the concept of genetic potential and show that as environmental variability decreases, the evolving population reaches three distinct steady state conditions: (1) organismal flexibility, (2) genetic potential, and (3) genetic robustness. As a specific example of this concept we examine fluctuating selection for hydrophobicity in a single amino acid. We see the same three stages, suggesting that environmental fluctuations can produce allele distributions that are distinct not only from those found under constant conditions, but also from the transient allele distributions that arise under isolated selective sweeps."
210,49,4915,1,Conservation and evolvability in regulatory networks: The evolution of ribosomal regulation in yeast,"Transcriptional modules of coregulated genes play a key role in regulatory networks. Comparative studies show that modules of coexpressed genes are conserved across taxa. However, little is known about the mechanisms underlying the evolution of module regulation. Here, we explore the evolution of cis-regulatory programs associated with conserved modules by integrating expression profiles for two yeast species and sequence data for a total of 17 fungal genomes. We show that although the cis-elements accompanying certain conserved modules are strictly conserved, those of other conserved modules are remarkably diverged. In particular, we infer the evolutionary history of the regulatory program governing ribosomal modules. We show how a cis-element emerged concurrently in dozens of promoters of ribosomal protein genes, followed by the loss of a more ancient cis-element. We suggest that this formation of an intermediate redundant regulatory program allows conserved transcriptional modules to gradually switch from one regulatory mechanism to another while maintaining their functionality. Our work provides a general framework for the study of the dynamics of promoter evolution at the level of transcriptional modules and may help in understanding the evolvability and increased redundancy of transcriptional regulation in higher organisms."
211,49,5361,1,MONKEY: identifying conserved transcription-factor binding sites in multiple alignments using a binding site-specific evolutionary model,"We introduce a method (MONKEY) to identify conserved transcription-factor binding sites in multispecies alignments. MONKEY employs probabilistic models of factor specificity and binding-site evolution, on which basis we compute the likelihood that putative sites are conserved and assign statistical significance to each hit. Using genomes from the genus Saccharomyces, we illustrate how the significance of real sites increases with evolutionary distance and explore the relationship between conservation and function."
212,49,5475,1,{RSEARCH: finding homologs of single structured RNA sequences},"BACKGROUND: For many RNA molecules, secondary structure rather than primary sequence is the evolutionarily conserved feature. No programs have yet been published that allow searching a sequence database for homologs of a single RNA molecule on the basis of secondary structure. RESULTS: We have developed a program, RSEARCH, that takes a single RNA sequence with its secondary structure and utilizes a local alignment algorithm to search a database for homologous RNAs. For this purpose, we have developed a series of base pair and single nucleotide substitution matrices for RNA sequences called RIBOSUM matrices. RSEARCH reports the statistical confidence for each hit as well as the structural alignment of the hit. We show several examples in which RSEARCH outperforms the primary sequence search programs BLAST and SSEARCH. The primary drawback of the program is that it is slow. The C code for RSEARCH is freely available from our lab's website. CONCLUSION: RSEARCH outperforms primary sequence programs in finding homologs of structured RNA sequences."
213,49,5498,1,{RNA} secondary structures and their prediction,"Abstract&nbsp;&nbsp;This is a review of past and present attempts to predict the secondary structure of ribonucleic acids (RNAs) through mathematical and computer methods. Related areas covering classification, enumeration and graphical representations of structures are also covered. Various general prediction techniques are discussed, especially the use of thermodynamic criteria to construct an optimal structure. The emphasis in this approach is on the use of dynamic programming algorithms to minimize free energy. One such algorithm is introduced which comprises existing ones as special cases."
214,49,5725,1,Global analysis of protein phosphorylation in yeast.,"Protein phosphorylation is estimated to affect 30% of the proteome and is a major regulatory mechanism that controls many basic cellular processes. Until recently, our biochemical understanding of protein phosphorylation on a global scale has been extremely limited; only one half of the yeast kinases have known in vivo substrates and the phosphorylating kinase is known for less than 160 phosphoproteins. Here we describe, with the use of proteome chip technology, the in vitro substrates recognized by most yeast protein kinases: we identified over 4,000 phosphorylation events involving 1,325 different proteins. These substrates represent a broad spectrum of different biochemical functions and cellular roles. Distinct sets of substrates were recognized by each protein kinase, including closely related kinases of the protein kinase A family and four cyclin-dependent kinases that vary only in their cyclin subunits. Although many substrates reside in the same cellular compartment or belong to the same functional category as their phosphorylating kinase, many others do not, indicating possible new roles for several kinases. Furthermore, integration of the phosphorylation results with protein-protein interaction and transcription factor binding data revealed novel regulatory modules. Our phosphorylation results have been assembled into a first-generation phosphorylation map for yeast. Because many yeast proteins and pathways are conserved, these results will provide insights into the mechanisms and roles of protein phosphorylation in many eukaryotes."
215,49,6205,1,The origins of eukaryotic gene structure.,"Most of the phenotypic diversity that we perceive in the natural world is directly attributable to the peculiar structure of the eukaryotic gene, which harbors numerous embellishments relative to the situation in prokaryotes. The most profound changes include introns that must be spliced out of precursor mRNAs, transcribed but untranslated leader and trailer sequences (untranslated regions), modular regulatory elements that drive patterns of gene expression, and expansive intergenic regions that harbor additional diffuse control mechanisms. Explaining the origins of these features is difficult because they each impose an intrinsic disadvantage by increasing the genic mutation rate to defective alleles. To address these issues, a general hypothesis for the emergence of eukaryotic gene structure is provided here. Extensive information on absolute population sizes, recombination rates, and mutation rates strongly supports the view that eukaryotes have reduced genetic effective population sizes relative to prokaryotes, with especially extreme reductions being the rule in multicellular lineages. The resultant increase in the power of random genetic drift appears to be sufficient to overwhelm the weak mutational disadvantages associated with most novel aspects of the eukaryotic gene, supporting the idea that most such changes are simple outcomes of semi-neutral processes rather than direct products of natural selection. However, by establishing an essentially permanent change in the population-genetic environment permissive to the genome-wide repatterning of gene structure, the eukaryotic condition also promoted a reliable resource from which natural selection could secondarily build novel forms of organismal complexity. Under this hypothesis, arguments based on molecular, cellular, and/or physiological constraints are insufficient to explain the disparities in gene, genomic, and phenotypic complexity between prokaryotes and eukaryotes."
216,49,6716,1,Global nucleosome occupancy in yeast.,"BACKGROUND: Although eukaryotic genomes are generally thought to be entirely chromatin-associated, the activated PHO5 promoter in yeast is largely devoid of nucleosomes. We systematically evaluated nucleosome occupancy in yeast promoters by immunoprecipitating nucleosomal DNA and quantifying enrichment by microarrays. RESULTS: Nucleosome depletion is observed in promoters that regulate active genes and/or contain multiple evolutionarily conserved motifs that recruit transcription factors. The Rap1 consensus was the only binding motif identified in a completely unbiased search of nucleosome-depleted promoters. Nucleosome depletion in the vicinity of Rap1 consensus sites in ribosomal protein gene promoters was also observed by real-time PCR and micrococcal nuclease digestion. Nucleosome occupancy in these regions was increased by the small molecule rapamycin or, in the case of the RPS11B promoter, by removing the Rap1 consensus sites. CONCLUSIONS: The presence of transcription factor-binding motifs is an important determinant of nucleosome depletion. Most motifs are associated with marked depletion only when they appear in combination, consistent with a model in which transcription factors act collaboratively to exclude nucleosomes and gain access to target sites in the DNA. In contrast, Rap1-binding sites cause marked depletion under steady-state conditions. We speculate that nucleosome depletion enables Rap1 to define chromatin domains and alter them in response to environmental cues."
217,49,7287,1,An introduction to graphical models,"The following quotation, from the Preface of [Jor99], provides a very concise introduction to graphical models. Graphical models are a marriage between probability theory and graph theory. They provide a natural tool for dealing with two problems that occur throughout applied mathematics and engineering { uncertainty and complexity { and in particular they are playing an increasingly important role in the design and analysis of machine learning algorithms. Fundamental to the idea of a graphical model is the notion of modularity { a complex system is built by combining simpler parts. Probability theory provides the glue whereby the parts are combined, ensuring that the system as a whole is consistent, and providing ways to interface models to data. The graph theoretic side of graphical models provides both an intuitively appealing interface by which humans can model highly-interacting sets of variables as well as a data structure that lends itself naturally to the design of ecient general-purpose algorithms. Many of the classical multivariate probabalistic systems studied in elds such as statistics, systems engineering, information theory, pattern recognition and statistical mechanics are special cases of the general graphical model formalism { examples include mixture models, factor analysis, hidden Markov models, Kalman lters and Ising models. The graphical model framework provides a way to view all of these systems as instances of a common underlying formalism. This view has many advantages { in particular, specialized techniques that have been developed in one eld can be transferred between research communities and exploited more widely. Moreover, the graphical model formalism provides a natural framework for the design of new systems."
218,49,7956,1,A hidden {Markov} model approach to variation among sites in rate of evolution,"The method of Hidden Markov Models is used to allow for unequal and unknown evolutionary rates at different sites in molecular sequences. Rates of evolution at different sites are assumed to be drawn from a set of possible rates, with a finite number of possibilities. The overall likelihood of phylogeny is calculated as a sum of terms, each term being the probability of the data given a particular assignment of rates to sites, times the prior probability of that particular combination of rates. The probabilities of different rate combinations are specified by a stationary Markov chain that assigns rate categories to sites. While there will be a very large number of possible ways of assigning rates to sites, a simple recursive algorithm allows the contributions to the likelihood from all possible combinations of rates to be summed, in a time proportional to the number of different rates at a single site. Thus with three rates, the effort involved is no greater than three times that for a single rate. This ""Hidden Markov Model"" method allows for rates to differ between sites and for correlations between the rates of neighboring sites. By summing over all possibilities it does not require us to know the rates at individual sites. However, it does not allow for correlation of rates at nonadjacent sites, nor does it allow for a continuous distribution of rates over sites. It is shown how to use the Newton-Raphson method to estimate branch lengths of a phylogeny and to infer from a phylogeny what assignment of rates to sites has the largest posterior probability. An example is given using beta-hemoglobin DNA sequences in eight mammal species; the regions of high and low evolutionary rates are inferred and also the average length of patches of similar rates."
219,49,8453,1,Dynamic Topic Models,"A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR&#039;ed archives of the journal Science from 1880 through 2000."
220,49,9114,1,Computational methods for transcriptional regulation.,"How is the information from a thousand gene-expression arrays, the location of more than two hundred regulatory factors, and nine sequenced genomes to be integrated into a global view of the regulatory network in budding yeast? Computational methods that fit incomplete noisy data provide the outlines of regulatory pathways, but the errors are not quantified. In the fly, embryonic patterning has proved amenable to computational prediction, but only when the DNA-binding preferences of the relevant factors are taken into account. In both these model organisms, simply restricting attention to regulatory sequences that align with related species (i.e. ""conserved"") discards much information regarding what is functional."
221,49,9696,1,Comparative ab initio prediction of gene structures using pair HMMs,"We present a novel comparative method for the ab initio prediction of protein coding genes in eukaryotic genomes. The method simultaneously predicts the gene structures of two un-annotated input DNA sequences which are homologous to each other and retrieves the subsequences which are conserved between the two DNA sequences. It is capable of predicting partial, complete and multiple genes and can align pairs of genes which differ by events of exon-fusion or exon-splitting. The method employs a probabilistic pair hidden Markov model. We generate annotations using our model with two different algorithms: the Viterbi algorithm in its linear memory implementation and a new heuristic algorithm, called the stepping stone, for which both memory and time requirements scale linearly with the sequence length. We have implemented the model in a computer program called DOUBLESCAN. In this article, we introduce the method and confirm the validity of the approach on a test set of 80 pairs of orthologous DNA sequences from mouse and human. More information can be found at: http://www.sanger.ac.uk/Software/analysis/doublescan/"
222,49,10339,1,Evolution of Transcription Factor Binding Sites in Mammalian Gene Regulatory Regions: Conservation and Turnover,"Comparisons between human and rodent DNA sequences are widely used for the identification of regulatory regions (phylogenetic footprinting), and the importance of such intergenomic comparisons for promoter annotation is expanding. The efficacy of such comparisons for the identification of functional regulatory elements hinges on the evolutionary dynamics of promoter sequences. Although it is widely appreciated that conservation of sequence motifs may provide a suggestion of function, it is not known as to what proportion of the functional binding sites in humans is conserved in distant species. In this report, we present an analysis of the evolutionary dynamics of transcription factor binding sites whose function had been experimentally verified in promoters of 51 human genes and compare their sequence to homologous sequences in other primate species and rodents. Our results show that there is extensive divergence within the nucleotide sequence of transcription factor binding sites. Using direct experimental data from functional studies in both human and rodents for 20 of the regulatory regions, we estimate that 32%-40% of the human functional sites are not functional in rodents. This is evidence that there is widespread turnover of transcription factor binding sites. These results have important implications for the efficacy of phylogenetic footprinting and the interpretation of the pattern of evolution in regulatory sequences."
223,49,10978,1,Genetic reconstruction of a functional transcriptional regulatory network,"Although global analyses of transcription factor binding provide one view of potential transcriptional regulatory networks1, 2, regulation also occurs at levels distinct from transcription factor binding3, 4. Here, we use a genetic approach to identify targets of transcription factors in yeast and reconstruct a functional regulatory network. First, we profiled transcriptional responses in S. cerevisiae strains with individual deletions of 263 transcription factors. Then we used directed-weighted graph modeling and regulatory epistasis analysis to identify indirect regulatory relationships between these transcription factors, and from this we reconstructed a functional transcriptional regulatory network. The enrichment of promoter motifs and Gene Ontology annotations provide insight into the biological functions of the transcription factors."
224,49,11828,1,Principled Hybrids of Generative and Discriminative Models,"When labelled training data is plentiful, discriminative techniques are widely used since they give excellent generalization performance. However, for large-scale applications such as object recognition, hand labelling of data is expensive, and there is much interest in semi-supervised techniques based on generative models in which the majority of the training data is unlabelled. Although the generalization performance of generative models can often be improved by &#145;training them discriminatively&#146;, they can then no longer make use of unlabelled data. In an attempt to gain the benefit of both generative and discriminative approaches, heuristic procedure have been proposed [2, 3] which interpolate between these two extremes by taking a convex combination of the generative and discriminative objective functions. In this paper we adopt a new perspective which says that there is only one correct way to train a given model, and that a &#145;discriminatively trained&#146; generative model is fundamentally a new model [7]. From this viewpoint, generative and discriminative models correspond to specific choices for the prior over parameters. As well as giving a principled interpretation of &#145;discriminative training&#146;, this approach opens door to very general ways of interpolating between generative and discriminative extremes through alternative choices of prior. We illustrate this framework using both synthetic data and a practical example in the domain of multi-class object recognition. Our results show that, when the supply of labelled training data is limited, the optimum performance corresponds to a balance between the purely generative and the purely discriminative."
225,49,12135,1,Natural history and evolutionary principles of gene duplication in fungi.,"Gene duplication and loss is a powerful source of functional innovation. However, the general principles that govern this process are still largely unknown. With the growing number of sequenced genomes, it is now possible to examine these events in a comprehensive and unbiased manner. Here, we develop a procedure that resolves the evolutionary history of all genes in a large group of species. We apply our procedure to seventeen fungal genomes to create a genome-wide catalogue of gene trees that determine precise orthology and paralogy relations across these species. We show that gene duplication and loss is highly constrained by the functional properties and interacting partners of genes. In particular, stress-related genes exhibit many duplications and losses, whereas growth-related genes show selection against such changes. Whole-genome duplication circumvents this constraint and relaxes the dichotomy, resulting in an expanded functional scope of gene duplication. By characterizing the functional fate of duplicate genes we show that duplicated genes rarely diverge with respect to biochemical function, but typically diverge with respect to regulatory control. Surprisingly, paralogous modules of genes rarely arise, even after whole-genome duplication. Rather, gene duplication may drive the modularization of functional networks through specialization, thereby disentangling cellular systems."
226,49,12501,1,Timescales of genetic and epigenetic inheritance.,"According to classical evolutionary theory, phenotypic variation originates from random mutations that are independent of selective pressure. However, recent findings suggest that organisms have evolved mechanisms to influence the timing or genomic location of heritable variability. Hypervariable contingency loci and epigenetic switches increase the variability of specific phenotypes; error-prone DNA replicases produce bursts of variability in times of stress. Interestingly, these mechanisms seem to tune the variability of a given phenotype to match the variability of the acting selective pressure. Although these observations do not undermine Darwin's theory, they suggest that selection and variability are less independent than once thought."
227,49,13110,1,Bioinformatics challenges of new sequencing technology.,"New DNA sequencing technologies can sequence up to one billion bases in a single day at low cost, putting large-scale sequencing within the reach of many scientists. Many researchers are forging ahead with projects to sequence a range of species using the new technologies. However, these new technologies produce read lengths as short as 35â40 nucleotides, posing challenges for genome assembly and annotation. Here we review the challenges and describe some of the bioinformatics systems that are being proposed to solve them. We specifically address issues arising from using these technologies in assembly projects, both de novo and for resequencing purposes, as well as efforts to improve genome annotation in the fragmented assemblies produced by short read lengths."
228,49,13393,1,Evolution of eukaryotic transcription circuits.,"The gradual modification of transcription circuits over evolutionary time scales is an important source of the diversity of life. Over the past decade, studies in animals have shown how seemingly small molecular changes in gene regulation can have large effects on morphology and physiology and how selective pressures can act on these changes. More recently, genome-wide studies, particularly those in single-cell yeasts, have uncovered evidence of extensive transcriptional rewiring, indicating that even closely related organisms regulate their genes using markedly different circuitries. 10.1126/science.1152398"
229,49,13969,1,Population genomics of domestic and wild yeasts,"Since the completion of the genome sequence of Saccharomyces cerevisiae in 1996 (refs 1, 2), there has been a large increase in complete genome sequences, accompanied by great advances in our understanding of genome evolution. Although little is known about the natural and life histories of yeasts in the wild, there are an increasing number of studies looking at ecological and geographic distributions3, 4, population structure5, 6, 7, 8 and sexual versus asexual reproduction9, 10. Less well understood at the whole genome level are the evolutionary processes acting within populations and species that lead to adaptation to different environments, phenotypic differences and reproductive isolation. Here we present one- to fourfold or more coverage of the genome sequences of over seventy isolates of the bakerâs yeast S. cerevisiae and its closest relative, Saccharomyces paradoxus. We examine variation in gene content, single nucleotide polymorphisms, nucleotide insertions and deletions, copy numbers and transposable elements. We find that phenotypic variation broadly correlates with global genome-wide phylogenetic relationships. S. paradoxus populations are well delineated along geographic boundaries, whereas the variation among worldwide S. cerevisiae isolates shows less differentiation and is comparable to a single S. paradoxus population. Rather than one or two domestication events leading to the extant bakerâs yeasts, the population structure of S. cerevisiae consists of a few well-defined, geographically isolated lineages and many different mosaics of these lineages, supporting the idea that human influence provided the opportunity for cross-breeding and production of new combinations of pre-existing variations."
230,49,14596,1,"Diverse RNA-binding proteins interact with functionally related sets of RNAs, suggesting an extensive regulatory system.","RNA-binding proteins (RBPs) have roles in the regulation of many post-transcriptional steps in gene expression, but relatively few RBPs have been systematically studied. We searched for the RNA targets of 40 proteins in the yeast Saccharomyces cerevisiae: a selective sample of the approximately 600 annotated and predicted RBPs, as well as several proteins not annotated as RBPs. At least 33 of these 40 proteins, including three of the four proteins that were not previously known or predicted to be RBPs, were reproducibly associated with specific sets of a few to several hundred RNAs. Remarkably, many of the RBPs we studied bound mRNAs whose protein products share identifiable functional or cytotopic features. We identified specific sequences or predicted structures significantly enriched in target mRNAs of 16 RBPs. These potential RNA-recognition elements were diverse in sequence, structure, and location: some were found predominantly in 3'-untranslated regions, others in 5'-untranslated regions, some in coding sequences, and many in two or more of these features. Although this study only examined a small fraction of the universe of yeast RBPs, 70% of the mRNA transcriptome had significant associations with at least one of these RBPs, and on average, each distinct yeast mRNA interacted with three of the RBPs, suggesting the potential for a rich, multidimensional network of regulation. These results strongly suggest that combinatorial binding of RBPs to specific recognition elements in mRNAs is a pervasive mechanism for multi-dimensional regulation of their post-transcriptional fate."
231,49,15097,1,Comprehensive polymorphism survey elucidates population structure of Saccharomyces cerevisiae.,"Comprehensive identification of polymorphisms among individuals within a species is essential both for studying the genetic basis of phenotypic differences and for elucidating the evolutionary history of the species. Large-scale polymorphism surveys have recently been reported for human, mouse and Arabidopsis thaliana. Here we report a nucleotide-level survey of genomic variation in a diverse collection of 63 Saccharomyces cerevisiae strains sampled from different ecological niches (beer, bread, vineyards, immunocompromised individuals, various fermentations and nature) and from locations on different continents. We hybridized genomic DNA from each strain to whole-genome tiling microarrays and detected 1.89 million single nucleotide polymorphisms, which were grouped into 101,343 distinct segregating sites. We also identified 3,985 deletion events of length >200 base pairs among the surveyed strains. We analysed the genome-wide patterns of nucleotide polymorphism and deletion variants, and measured the extent of linkage disequilibrium in S. cerevisiae. These results and the polymorphism resource we have generated lay the foundation for genome-wide association studies in yeast. We also examined the population structure of S. cerevisiae, providing support for multiple domestication events as well as insight into the origins of pathogenic strains."
232,49,15574,1,Unstable Tandem Repeats in Promoters Confer Transcriptional Evolvability,"Relative to most regions of the genome, tandemly repeated DNA sequences display a greater propensity to mutate. A search for tandem repeats in the Saccharomyces cerevisiae genome revealed that the nucleosome-free region directly upstream of genes (the promoter region) is enriched in repeats. As many as 25% of all gene promoters contain tandem repeat sequences. Genes driven by these repeat-containing promoters show significantly higher rates of transcriptional divergence. Variations in repeat length result in changes in expression and local nucleosome positioning. Tandem repeats are variable elements in promoters that may facilitate evolutionary tuning of gene expression by affecting local chromatin structure. 10.1126/science.1170097"
233,49,15900,1,"Applying mass spectrometry-based proteomics to genetics, genomics and network biology"," The systematic and quantitative molecular analysis of mutant organisms that has been pioneered by studies on mutant metabolomes and transcriptomes holds great promise for improving our understanding of how phenotypes emerge. Unfortunately, owing to the limitations of classical biochemical analysis, proteins have previously been excluded from such studies. Here we review how technical advances in mass spectrometry-based proteomics can be applied to measure changes in protein abundance, posttranslational modifications and proteinâprotein interactions in mutants at the scale of the proteome. We finally discuss examples that integrate proteomics data with genomic and phenomic information to build network-centred models, which provide a promising route for understanding how phenotypes emerge."
234,49,16254,1,Discrete logic modelling as a means to link protein signalling networks with functional analysis of mammalian signal transduction,"Large-scale protein signalling networks are useful for exploring complex biochemical pathways but do not reveal how pathways respond to specific stimuli. Such specificity is critical for understanding disease and designing drugs. Here we describe a computational approachâimplemented in the free CNO softwareâfor turning signalling networks into logical models and calibrating the models against experimental data. When a literature-derived network of 82 proteins covering the immediate-early responses of human cells to seven cytokines was modelled, we found that training against experimental data dramatically increased predictive power, despite the crudeness of Boolean approximations, while significantly reducing the number of interactions. Thus, many interactions in literature-derived networks do not appear to be functional in the liver cells from which we collected our data. At the same time, CNO identified several new interactions that improved the match of model to data. Although missing from the starting network, these interactions have literature support. Our approach, therefore, represents a means to generate predictive, cell-type-specific models of mammalian signalling from generic protein signalling networks."
235,49,16510,1,Visualizing genomes: techniques and challenges,"As our ability to generate sequencing data continues to increase, data analysis is replacing data generation as the rate-limiting step in genomics studies. Here we provide a guide to genomic data visualization tools that facilitate analysis tasks by enabling researchers to explore, interpret and manipulate their data, and in some cases perform on-the-fly computations. We will discuss graphical methods designed for the analysis of de novo sequencing assemblies and read alignments, genome browsing, and comparative genomics, highlighting the strengths and limitations of these approaches and the challenges ahead."
236,49,16701,1,"Most ""Dark Matter"" Transcripts Are Associated With Known Genes","A series of reports over the last few years have indicated that a much larger portion of the mammalian genome is transcribed than can be accounted for by currently annotated genes, but the quantity and nature of these additional transcripts remains unclear. Here, we have used data from single- and paired-end RNA-Seq and tiling arrays to assess the quantity and composition of transcripts in PolyA+ RNA from human and mouse tissues. Relative to tiling arrays, RNA-Seq identifies many fewer transcribed regions (Ã¢â¬ÅseqfragsÃ¢â¬ï¿½) outside known exons and ncRNAs. Most nonexonic seqfrags are in introns, raising the possibility that they are fragments of pre-mRNAs. The chromosomal locations of the majority of intergenic seqfrags in RNA-Seq data are near known genes, consistent with alternative cleavage and polyadenylation site usage, promoter- and terminator-associated transcripts, or new alternative exons; indeed, reads that bridge splice sites identified 4,544 new exons, affecting 3,554 genes. Most of the remaining seqfrags correspond to either single reads that display characteristics of random sampling from a low-level background or several thousand small transcripts (median length = 111 bp) present at higher levels, which also tend to display sequence conservation and originate from regions with open chromatin. We conclude that, while there are bona fide new intergenic transcripts, their number and abundance is generally low in comparison to known exons, and the genome is not as pervasively transcribed as previously reported."
237,50,1204,1,{Maximum likelihood from incomplete data via the EM algorithm},"{A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.}"
238,50,5794,1,SMOTE: Synthetic minority over-sampling technique,"[Imbalanced dataset, classifiers (C4.5, Ripper, Naive Bayes Classifier), SMOTE, oversampling, undersampling] An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of ""normal"" examples with only a small percentage of ""abnormal"" or ""interesting"" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that the combination of over-sampling and undersampling can achieve better classifier performance than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy."
239,51,69,1,Footprints: History-Rich Tools for Information Foraging,"Inspired by Hill and Hollan's original work [6], we have been developing a theory of interaction history and building tools to apply this theory to navigation in a complex information space. We have built a series of tools --- map, trails, annotations and signposts --- based on a physical-world navigation metaphor. These tools have been in use for over a year. Our user study involved a controlled browse task and showed that users were able to get the same amount of work done with significantly less effort."
240,51,434,1,Tap: a semantic web platform,"Activities such as Web Services and the Semantic Web are working to create a distributed web of machine understandable data. We address three important problems that need to be solved to realize this vision. We discuss the problem of scalable and deployable query systems and present a simple, but general query interface called GetData. We address the issue of creating global agreements on vocabularies and introduce the concept of Semantic Negotiation, a process by which two programs can..."
241,51,1348,1,Towards the Self-Annotating Web,"â¢ Learning of extraction rules requires a lot of, frequently too                                                                       many, examples for learning the rules.The success of the Semantic Web depends on the availability of on-tologies as well as on the proliferation of web pages annotated withmetadata conforming to these ontologies. Thus, a crucial ques-tion is where to acquire these metadata. In this paper we proposePANKOW (Pattern-based Annotation through Knowledge on theWeb), a method which employs an unsupervised, pattern-based ap-proach to categorize instances with regard to an ontology. The ap-proach is evaluated against the manual annotations of two humansubjects. The approach is implemented in OntoMat, an annotationtool for the Semantic Web and shows very promising results."
242,51,3004,1,The PROMPT suite: Interactive tools for ontology merging and mapping,"Researchers in the ontology-design field have developed the content for ontologies in many domain areas. This distributed nature of ontology development has led to a large number of ontologies covering overlapping domains. In order for these ontologies to be reused, they first need to be merged or aligned to one another. We developed a suite of tools for managing multiple ontologies. These suite provides users with a uniform framework for comparing, aligning, and merging ontologies, maintaining versions, translating between different formalisms. Two of the tools in the suite support semi-automatic ontology merging: IPROMPT is an interactive ontology-merging tool that guides the user through the merging process, presenting him with suggestions for next steps and identifying inconsistencies and potential problems. ANCHORPROMPT uses a graph structure of ontologies to find correlation between concepts and to provide additional information for IPROMPT. (C) 2003 Elsevier Ltd. All rights reserved."
243,51,3915,1,Implicit feedback for inferring user preference: a bibliography,"Relevance feedback has a history in information retrieval that dates back well over thirty years (c.f [SL96]). Relevance feedback is typically used for query expansion during short-term modeling of a user's immediate information need and for user profiling during long-term modeling of a user's persistent interests and preferences. Traditional relevance feedback methods require that users explicitly give feedback by, for example, specifying keywords, selecting and marking documents, or answering questions about their interests. Such relevance feedback methods force users to engage in additional activities beyond their normal searching behavior. Since the cost to the user is high and the benefits are not always apparent, it can be difficult to collect the necessary data and the effectiveness of explicit techniques can be limited. In this paper we consider the use of implicit feedback techniques for query expansion and user profiling in information retrieval tasks. These techniques unobtrusively obtain information about users by watching their natural interactions with the system. Some of the user behaviors that have been most extensively investigated as sources of implicit feedback include reading time, saving, printing and selecting. The primary advantage to using implicit techniques is that such techniques remove the cost to the user of providing feedback. Implicit measures are generally thought to be less accurate than explicit measures [Nic97], but as large quantities of implicit data can be gathered at no extra cost to the user, they are attractive alternatives . Moreover, implicit measures can be combined with explicit ratings to obtain a more accurate representation of user interests. Implicit feedback techniques have been used to retrieve, filter and recommend a variety of items: hyperlinks, Web documents, academic and professional journal articles, email messages, Internet news articles, movies, books, television programs, jobs and stocks . There is a growing body of literature on implicit feedback techniques for information retrieval tasks, and the purpose of this article is to provide a brief overview of this work. Our intention is not to be exhaustive, but rather to be selective, in that we present key papers that cover a range of approaches . We begin by presenting and extending a classification of behaviors for implicit feedback that was previously presented by Oard and Kim [OK01 ], and classifying the selected papers accordingly. A preponderance of the existing work clusters into one area of this classification, and we further examine those papers . We then provide a brief overview of several key papers, and conclude with a discussion of future research directions suggested by our analysis."
244,51,4781,1,{Context-Aware Computing Applications},"This paper describes software that examines and reacts to an individual's changing context. Such software  can promote and mediate people's interactions with devices, computers, and other people, and it can help  navigate unfamiliar places. We believe that a limited amount of information covering a person's proximate  environment is most important for this form of computing since the interesting part of the world around  us is what we can see, hear, and touch. In this paper we define context-aware computing, and describe  four categories of context-aware applications: proximate selection, automatic contextual reconfiguration,  contextual information and commands, and context-triggered actions. Instances of these application types  have been prototyped on the PARCTAB, a wireless, palm-sized computer.  1 Introduction  Our investigation focuses on an extended form of mobile computing in which users employ many different mobile, stationary and embedded computers over the course of the day...."
245,51,5848,1,Implicit Interest Indicators,"Recommender systems provide personalized suggestions about items that users will find interesting.  Typically, recommender systems require a user interface that can ``intelligently'' determine the interest of a user and use this information to make suggestions.  The common solution, ``explicit ratings'', where users tell the system what they think about a piece of information, is well-understood and fairly precise.  However, having to stop to enter explicit ratings can alter normal patterns of browsing and reading. A more ``intelligent'' method is to use implicit ratings , where a rating is obtained by a method other than obtaining it directly from the user.  These implicit interest indicators have obvious advantages, including removing the cost of the user rating, and that every user interaction with the system can contribute to an implicit rating.   Current recommender systems mostly do not use implicit ratings, nor is the ability of implicit ratings to predict actual user interest well-understood.  This research studies the correlation between various implicit ratings and the explicit rating for a single Web page.  A Web browser was developed to record the user's actions (implicit ratings) and the explicit rating of a page.  Actions included mouse clicks, mouse movement, scrolling and elapsed time. This browser was used by over 80 people that browsed more than 2500 Web pages.   Using the data collected by the browser, the individual implicit ratings and some combinations of implicit ratings were analyzed and compared with the explicit rating.  We found that the time spent on a page, the amount of scrolling on a page and the combination of time and scrolling had a strong correlation with explicit interest, while individual scrolling methods and mouse-clicks were ineffective in predicting explicit interest."
246,51,6620,1,Retrieval effectiveness of an ontology-based model for information selection,"Technology in the field of digital media generates huge amounts of nontextual information, audio, video, and images, along with more familiar textual information. The potential for exchange and retrieval of information is vast and daunting. The key problem in achieving efficient and user-friendly retrieval is the development of a search mechanism to guarantee delivery of minimal irrelevant information (high precision) while insuring relevant information is not overlooked (high recall). The traditional solution employs keyword-based search. The only documents retrieved are those containing user-specified keywords. But many documents convey desired semantic information without containing these keywords. This limitation is frequently addressed through query expansion mechanisms based on the statistical co-occurrence of terms. Recall is increased, but at the expense of deteriorating precision. One can overcome this problem by indexing documents according to context and meaning rather than keywords, although this requires a method of converting words to meanings and the creation of a meaning-based index structure. We have solved the problem of an index structure through the design and implementation of a concept-based model using domain-dependent ontologies. An ontology is a collection of concepts and their interrelationships that provide an abstract view of an application domain. With regard to converting words to meaning, the key issue is to identify appropriate concepts that both describe and identify documents as well as language employed in user requests. This paper describes an automatic mechanism for selecting these concepts. An important novelty is a scalable disambiguation algorithm that prunes irrelevant concepts and allows relevant ones to associate with documents and participate in query generation. We also propose an automatic query expansion mechanism that deals with user requests expressed in natural language. This mechanism generates database queries with appropriate and relevant expansion through knowledge encoded in ontology form. Focusing on audio data, we have constructed a demonstration prototype. We have experimentally and analytically shown that our model, compared to keyword search, achieves a significantly higher degree of precision and recall. The techniques employed can be applied to the problem of information selection in all media types."
247,51,7131,1,GiveALink: Mining a Semantic Network of Bookmarks for Web Search and Recommendation,"GiveALink is a public site where users donate their bookmarks to the Web community. Bookmarks are analyzed to build a new generation of Web mining techniques and new ways to search, recommend, surf, personalize and visualize the Web. We present a semantic similarity measure for URLs that takes advantage both of the hierarchical structure of the bookmark files of individual users, and of collaborative filtering across users. We analyze the social bookmark network induced by the similarity measure. A search and recommendation system is built from a number of ranking algorithms based on prestige, generality, and novelty measures extracted from the similarity data."
248,51,8233,1,Cubesvd: A novel approach to personalized web search,"As the competition of Web search market increases, there is a high demand for personalized Web search to conduct retrieval incorporating Web usersâ information needs. This paper focuses on utilizing clickthrough data to improve Web search. Since millions of searches are conducted everyday, a search engine accumulates a large volume of clickthrough data, which records who submits queries and which pages he/she clicks on. The clickthrough data is highly sparse and contains different types of objects (user, query and Web page), and the relationships among these objects are also very complicated. By performing analysis on these data, we attempt to discover Web usersâ interests and the patterns that users locate information. In this paper, a novel approach CubeSVD is proposed to improve Web search. The clickthrough data is represented by a 3-order tensor, on which we perform 3-mode analysis using the higher-order singular value decomposition technique to automatically capture the latent factors that govern the relations among these multi-type objects: users, queries and Web pages. A tensor reconstructed based on the CubeSVD analysis reflects both the observed interactions among these objects and the implicit associations among them. Therefore, Web search activities can be carried out based on CubeSVD analysis. Experimental evaluations using a real-world data set collected from an MSN search engine show that CubeSVD achieves encouraging search results in comparison with some standard methods."
249,51,8512,1,Exploring Social Annotations for the Semantic Web,"In order to obtain a machine understandable semantics for web resources, research on the Semantic Web tries to annotate web resources with concepts and relations from explicitly defined formal ontologies. This kind of formal annotation is usually done manually or semi-automatically. In this paper, we explore a complement approach that focuses on the ""social annotations of the web"" which are annotations manually made by normal web users without a pre-defined formal ontology. Compared to the formal annotations, although social annotations are coarse-grained, informal and vague, they are also more accessible to more people and better reflect the web resources' meaning from the users' point of views during their actual usage of the web resources. Using a social bookmark service as an example, we show how emergent semantics [2] can be statistically derived from the social annotations. Furthermore, we apply the derived emergent semantics to discover and search shared web bookmarks. The initial evaluation on our implementation shows that our method can effectively discover semantically related web bookmarks that current social bookmark service can not discover easily."
250,51,9346,1,{{C}oncept {B}ased {Q}uery {E}xpansion},"Query expansion methods have been studied for a long time - with debatable success in many instances. In this paper we present a probabilistic query expansion model based on a similarity thesaurus which was constructed automatically. A similarity thesaurus reflects domain knowledge about the particular collection from which it is constructed. We ad-dress the two important issues with query expansion: the selection and the weighting of additional search terms. In contrast to earlier methods, our queries are expanded by adding those terms that are most similar to the concept of the query, rather than selecting terms that are similar to the query terms. Our experiments show that this kind of query expansion results in a notable improvement in the retrieval effectiveness when measured using both recall-precision and usefulness."
251,51,10033,1,An Ontology-Based Information Retrieval Model,"Abstract. Semantic search has been one of the motivations of the Semantic Web since it was envisioned. We propose a model for the exploitation of ontologybased KBs to improve search over large document repositories. Our approach includes an ontology-based scheme for the semi-automatic annotation of documents, and a retrieval system. The retrieval model is based on an adaptation of the classic vector-space model, including an annotation weighting algorithm, and a ranking algorithm. Semantic search is combined with keyword-based search to achieve tolerance to KB incompleteness. Our proposal is illustrated with sample experiments showing improvements with respect to keyword-based search, and providing ground for further research and discussion. 1"
252,51,10575,1,Personalized search based on user search histories,"User profiles, descriptions of user interests, can be used by search engines to provide personalized search results. Many approaches to creating user profiles collect user information through proxy servers (to capture browsing histories) or desktop bots (to capture activities on a personal computer). Both these techniques require participation of the user to install the proxy server or the bot. In this study, we explore the use of a less-invasive means of gathering user information for personalized search. In particular, we build user profiles based on activity at the search site itself and study the use of these profiles to provide personalized search results. By implementing a wrapper around the Google search engine, we were able to collect information about individual user search activities. In particular, we collected the queries for which at least one search result was examined, and the snippets (titles and summaries) for each examined result. User profiles were created by classifying the collected information (queries or snippets) into concepts in a reference concept hierarchy. These profiles were then used to re-rank the search results and the rank-order of the user-examined results before and after re-ranking were compared. Our study found that user profiles based on queries were as effective as those based on snippets. We also found that our personalized re-ranking resulted in a 34% improvement in the rankorder of the user-selected results."
253,51,10785,1,Personalized Content Retrieval in Context Using Ontological Knowledge,"Personalized content retrieval aims at improving the retrieval process by taking into account the particular interests of individual users. However, not all user preferences are relevant in all situations. It is well known that human preferences are complex, multiple, heterogeneous, changing, even contradictory, and should be understood in context with the user goals and tasks at hand. In this paper, we propose a method to build a dynamic representation of the semantic context of ongoing retrieval tasks, which is used to activate different subsets of user interests at runtime, in a way that out-of-context preferences are discarded. Our approach is based on an ontology-driven representation of the domain of discourse, providing enriched descriptions of the semantics involved in retrieval actions and preferences, and enabling the definition of effective means to relate preferences and context"
254,51,11190,1,Investigating behavioral variability in web search,"Understanding the extent to which people's search behaviors differ in terms of the interaction flow and information targeted is important in designing interfaces to help World Wide Web users search more effectively. In this paper we describe a longitudinal log-based study that investigated variability in people.s interaction behavior when engaged in search-related activities on the Web.allWe analyze the search interactions of more than two thousand volunteer users over a five-month period, with the aim of characterizing differences in their interaction styles.allThe findings of our study suggest that there are dramatic differences in variability in key aspects of the interaction within and between users, and within and between the search queries they submit.allOur findings also suggest two classes of extreme user. navigators and explorers. whose search interaction is highly consistent or highly variable. Lessons learned from these users can inform the design of tools to support effective Web-search interactions for everyone."
255,51,12207,1,Content-Based Recommendation Systems,"This chapter discusses content-based recommendation systems, i.e., systems that recommend an item to a user based upon a description of the item and a profile of the userâs interests. Content-based recommendation systems may be used in a variety of domains ranging from recommending web pages, news articles, restaurants, television programs, and items for sale. Although the details of various systems differ, content-based recommendation systems share in common a means for describing the items that may be recommended, a means for creating a profile of the user that describes the types of items the user likes, and a means of comparing items to the user profile to determine what to re commend. The profile is often created and updated automatically in response to feedback on the desirability of items that have been presented to the user."
256,51,12886,1,Evaluating the accuracy of implicit feedback from clicks and query reformulations in Web search,"This article examines the reliability of implicit feedback generated from clickthrough data and query reformulations in World Wide Web (WWW) search. Analyzing the users' decision process using eyetracking and comparing implicit feedback against manual relevance judgments, we conclude that clicks are informative but biased. While this makes the interpretation of clicks as absolute relevance judgments difficult, we show that relative preferences derived from clicks are reasonably accurate on average. We find that such relative preferences are accurate not only between results from an individual query, but across multiple sets of results within chains of query reformulations."
257,51,13954,1,A basis for information retrieval in context,"Information retrieval (IR) models based on vector spaces have been investigated for a long time. Nevertheless, they have recently attracted much research interest. In parallel, context has been rediscovered as a crucial issue in information retrieval. This article presents a principled approach to modeling context and its role in ranking information objects using vector spaces. First, the article outlines how a basis of a vector space naturally represents context, both its properties and factors. Second, a ranking function computes the probability of context in the objects represented in a vector space, namely, the probability that a contextual factor has affected the preparation of an object."
258,51,14743,1,Integrating tags in a semantic content-based recommender,"Basic content personalization consists in matching up the attributes of a user profile, in which preferences and interests are stored, with the attributes of a content object. The Web 2.0 (r)evolution and the advent of user generated content have changed the game for personalization, since the role of people has evolved from passive consumers of information to that of active contributors. One of the forms of user generated content that has drawn more attention from the research community is folksonomy, a taxonomy generated by users who collaboratively annotate and categorize resources of interests with freely chosen keywords called tags."
259,51,15690,1,Evaluating the Effectiveness of Personalized Web Search,"Although personalized search has been under way for many years and many personalization algorithms have been investigated, it is still unclear whether personalization is consistently effective on different queries for different users and under different search contexts. In this paper, we study this problem and provide some findings. We present a large-scale evaluation framework for personalized search based on query logs and then evaluate five personalized search algorithms (including two click-based ones and three topical-interest-based ones) using 12-day query logs of Windows Live Search. By analyzing the results, we reveal that personalized Web search does not work equally well under various situations. It represents a significant improvement over generic Web search for some queries, while it has little effect and even harms query performance under some situations. We propose click entropy as a simple measurement on whether a query should be personalized. We further propose several features to automatically predict when a query will benefit from a specific personalization algorithm. Experimental results show that using a personalization algorithm for queries selected by our prediction model is better than using it simply for all queries."
260,52,451,1,Location Systems for Ubiquitous Computing,"To serve us well, emerging mobile computing applications will need to know the physical location of things so that they can record them and report them to us: What lab bench was I standing by when I prepared these tissue samples? How should our search-and-rescue team move to quickly locate all the avalanche victims? Can I automatically display this stock devaluation chart on the large screen I am standing next to? Researchers are working to meet these and similar needs by developing systems and technologies that automatically locate people, equipment, and other tangibles. Indeed, many systems over the years have addressed the problem of automatic location sensing. Because each approach solves a slightly different problem or supports different applications, they vary in many parameters, such as the physical phenomena used for location determination, the form factor of the sensing apparatus, power requirements, infrastructure versus portable elements, and resolution in time and space. To make sense of this domain, we have developed ataxonomy to help developers of location-aware applications better evaluate their options when choosing a location-sensing system. The taxonomy may also aid researchers in identifying opportunities for new location-sensing techniques."
261,52,3028,1,The WSLA Framework: Specifying and Monitoring Service Level Agreements for Web Services,"We describe a novel framework for specifying and monitoring Service Level Agreements (SLA) for Web Services. SLA monitoring and enforcement become increasingly important in a Web Service environment where enterprise applications and services rely on services that may be subscribed dynamically and on-demand. For economic and practical reasons, we want an automated provisioning process for both the service itself as well as the SLA managment system that measures and monitors the QoS parameters, checks the agreed-upon service levels, and reports violations to the authorized parties involved in the SLA management process. Our approach to these issues is presented in this paper. The Web Service Level Agreement (WSLA) framework is targeted at defining and monitoring SLAs for Web Services. Although WSLA has been designed for a Web Services environment, it is applicable as well to any inter-domain management scenario, such as business process and service management, or the management of networks, systems and applications in general. The WSLA framework consists of a flexible and extensible language based on XML Schema and a runtime architecture comprising several SLA monitoring services, which may be outsourced to third parties to ensure a maximum of objectivity. WSLA enables service customers and providers to unambiguously define a wide variety of SLAs, specify the SLA parameters and the way they are measured, and relate them to managed resource instrumentations. Upon receipt of an SLA specification, the WSLA monitoring services are automatically configured to enforce the SLA. An implementation of the WSLA framework, termed SLA Compliance Monitor, is publicly available as part of the IBM Web Services Toolkit."
262,52,4611,1,Autonomic computing: emerging trends and open problems,"The increasing heterogeneity, dynamism and interconnectivity in software applications, services and networks led to complex, unmanageable and insecure systems. Coping with such a complexity necessitates to investigate a new paradigm namely  Autonomic Computing.  Although academic and industry efforts are beginning to proliferate in this research area, there are still a lots of open issues that remain to be solved. This paper proposes a categorization of complexity in I/T systems and presents an overview of autonomic computing research area. The paper also discusses a summary of the major autonomic computing systems that have been already developed both in academia and industry, and finally outlines the underlying research issues and challenges from a practical as well as a theoretical point of view."
263,52,6798,1,Self-Managing Systems: A Control Theory Foundation,"Summary form only given. The high cost of ownership of computing systems has resulted in a number of industry initiatives to reduce the burden of operations and management by making systems more self-managing. A major challenge in realizing self-managing systems is understanding how automated actions affect system behavior, especially system stability. Other disciplines such as mechanical, electrical, and aeronautical engineering make use of control theory to design feedback systems. The talk uses control theory as a way to identify a number of requirements for and challenges in building self-managing, or autonomic, systems. In essence, the autonomic computing architecture describes feedback control loops for self-managing systems. The talk has three goals: (1) educating systems oriented computer science researchers and practitioners on the concepts and techniques needed to apply control theory to computing systems; (2) describing how control theory can aid in building self-managing systems and identifying the challenges in doing so; (3) describing a deployable testbed for autonomic computing that is intended to foster research that addresses the challenges identified."
264,53,1022,1,Comprehensive Identification of Cell Cycle-regulated Genes of the Yeast Saccharomyces cerevisiae by Microarray Hybridization,"We sought to create a comprehensive catalog of yeast genes whose transcript levels vary periodically within the cell cycle. To this end, we used DNA microarrays and samples from yeast cultures synchronized by three independent methods: [alpha] factor arrest, elutriation, and arrest of a cdc15 temperature-sensitive mutant. Using periodicity and correlation algorithms, we identified 800 genes that meet an objective minimum criterion for cell cycle regulation. In separate experiments, designed to examine the effects of inducing either the G1 cyclin Cln3p or the B-type cyclin Clb2p, we found that the mRNA levels of more than half of these 800 genes respond to one or both of these cyclins. Furthermore, we analyzed our set of cell cycle-regulated genes for known and new promoter elements and show that several known elements (or variations thereof) contain information predictive of cell cycle regulation. A full description and complete data sets are available at http://cellcycle-www.stanford.edu"
265,53,5914,1,Gene Expression During the Life Cycle of Drosophila melanogaster,"Molecular genetic studies of Drosophila melanogaster have led to profound advances in understanding the regulation of development. Here we report gene expression patterns for nearly one-third of all Drosophila genes during a complete time course of development. Mutations that eliminate eye or germline tissue were used to further analyze tissue-specific gene expression programs. These studies define major characteristics of the transcriptional programs that underlie the life cycle, compare development in males and females, and show that large-scale gene expression data collected from whole animals can be used to identify genes expressed in particular tissues and organs or genes involved in specific biological and biochemical processes."
266,53,11250,1,THE LOCUS OF EVOLUTION: EVO DEVO AND THE GENETICS OF ADAPTATION,"An important tenet of evolutionary developmental biology (""evo devo"") is that adaptive mutations affecting morphology are more likely to occur in the cis-regulatory regions than in the protein-coding regions of genes. This argument rests on two claims: ( 1) the modular nature of cis-regulatory elements largely frees them from deleterious pleiotropic effects, and ( 2) a growing body of empirical evidence appears to support the predominant role of gene regulatory change in adaptation, especially morphological adaptation. Here we discuss and critique these assertions. We first show that there is no theoretical or empirical basis for the evo devo contention that adaptations involving morphology evolve by genetic mechanisms different from those involving physiology and other traits. In addition, some forms of protein evolution can avoid the negative consequences of pleiotropy, most notably via gene duplication. In light of evo devo claims, we then examine the substantial data on the genetic basis of adaptation from both genome-wide surveys and single-locus studies. Genomic studies lend little support to the cis-regulatory theory: many of these have detected adaptation in protein-coding regions, including transcription factors, whereas few have examined regulatory regions. Turning to single-locus studies, we note that the most widely cited examples of adaptive cis-regulatory mutations focus on trait loss rather than gain, and none have yet pinpointed an evolved regulatory site. In contrast, there are many studies that have both identified structural mutations and functionally verified their contribution to adaptation and speciation. Neither the theoretical arguments nor the data from nature, then, support the claim for a predominance of cis- regulatory mutations in evolution. Although this claim may be true, it is at best premature. Adaptation and speciation probably proceed through a combination of cis-regulatory and structural mutations, with a substantial contribution of the latter."
267,53,12223,1,Splicing in disease: disruption of the splicing code and the decoding machinery,"Human genes contain a dense array of diverse cis-acting elements that make up a code required for the expression of correctly spliced mRNAs. Alternative splicing generates a highly dynamic human proteome through networks of coordinated splicing events. Cis- and trans-acting mutations that disrupt the splicing code or the machinery required for splicing and its regulation have roles in various diseases, and recent studies have provided new insights into the mechanisms by which these effects occur. An unexpectedly large fraction of exonic mutations exhibit a primary pathogenic effect on splicing. Furthermore, normal genetic variation significantly contributes to disease severity and susceptibility by affecting splicing efficiency."
268,54,466,1,Integrating ethics and science in the International HapMap Project.,"Genomics resources that use samples from identified populations raise scientific, social and ethical issues that are, in many ways, inextricably linked. Scientific decisions about which populations to sample to produce the HapMap, an international genetic variation resource, have raised questions about the relationships between the social identities used to recruit participants and the biological findings of studies that will use the HapMap. The sometimes problematic implications of those complex relationships have led to questions about how to conduct genetic variation research that uses identified populations in an ethical way, including how to involve members of a population in evaluating the risks and benefits posed for everyone who shares that identity. The ways in which these issues are linked is increasingly drawing the scientific and ethical spheres of genomics research closer together."
269,54,2803,1,"Genetic variation, classification and 'race'.","New genetic data has enabled scientists to re-examine the relationship between human genetic variation and 'race'. We review the results of genetic analyses that show that human genetic variation is geographically structured, in accord with historical patterns of gene flow and genetic drift. Analysis of many loci now yields reasonably accurate estimates of genetic similarity among individuals, rather than populations. Clustering of individuals is correlated with geographic origin or ancestry. These clusters are also correlated with some traditional concepts of race, but the correlations are imperfect because genetic variation tends to be distributed in a continuous, overlapping fashion among populations. Therefore, ancestry, or even race, may in some cases prove useful in the biomedical setting, but direct assessment of disease-related genetic variation will ultimately yield more accurate and beneficial information."
270,54,5064,1,Intellectual property. Enhanced: intellectual property landscape of the human genome.,"The impact of gene patents on downstream research and innovation are unknown, in part because of a lack of empirical data on the extent and nature of gene patenting. In this Policy Forum, the authors show that 20% of human gene {DNA} sequences are patented and that some genes are patented as many as 20 times. Unsurprisingly, genes associated with health and disease are more patented than the genome at large. The intellectual property rights for some genes can become highly fragmented between many owners, which suggests that downstream innovators may face considerable costs to gain access to gene-oriented technologies."
271,54,7958,1,The Structure of Haplotype Blocks in the Human Genome,"Haplotype-based methods offer a powerful approach to disease gene mapping, based on the association between causal mutations and the ancestral haplotypes on which they arose. As part of The SNP Consortium Allele Frequency Projects, we characterized haplotype patterns across 51 autosomal regions (spanning 13 megabases of the human genome) in samples from Africa, Europe, and Asia. We show that the human genome can be parsed objectively into haplotype blocks: sizable regions over which there is little evidence for historical recombination and within which only a few common haplotypes are observed. The boundaries of blocks and speciÃc haplotypes they contain are highly correlated across populations. We demonstrate that such haplotype frameworks provide substantial statistical power in association studies of common genetic variation across each region. Our results provide a foundation for the construction of a haplotype map of the human genome, facilitating comprehensive genetic association studies of human disease."
272,55,1200,1,Controlling the false discovery rate: a practical and powerful approach to multiple testing,"The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses - the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples."
273,55,2147,1,Minimum information about a microarray experiment (MIAME)-toward standards for microarray data.,"Microarray analysis has become a widely used tool for the generation of gene expression data on a genomic scale. Although many significant results have been derived from microarray studies, one limitation has been the lack of standards for presenting and exchanging such data. Here we present a proposal, the Minimum Information About a Microarray Experiment (MIAME), that describes the minimum information required to ensure that microarray data can be easily interpreted and that results derived from its analysis can be independently verified. The ultimate goal of this work is to establish a standard for recording and reporting microarray-based gene expression data, which will in turn facilitate the establishment of databases and public repositories and enable the development of data analysis tools. With respect to MIAME, we concentrate on defining the content and structure of the necessary information rather than the technical format for capturing it."
274,55,3602,1,The Gene Ontology (GO) database and informatics resource.,"{{T}he {G}ene {O}ntology ({GO}) project (http://www. geneontology.org/) provides structured, controlled vocabularies and classifications that cover several domains of molecular and cellular biology and are freely available for community use in the annotation of genes, gene products and sequences. {M}any model organism databases and genome annotation groups use the {GO} and contribute their annotation sets to the {GO} resource. {T}he {GO} database integrates the vocabularies and contributed annotations and provides full access to this information in several formats. {M}embers of the {GO} {C}onsortium continually work collectively, involving outside experts as needed, to expand and update the {GO} vocabularies. {T}he {GO} {W}eb resource also provides access to extensive documentation about the {GO} project and links to applications that use {GO} data for functional analyses.}"
275,55,5350,1,GOtcha: a new method for prediction of protein function assessed by the annotation of seven genomes,"Background: The function of a novel gene product is typically predicted by transitive assignment of annotation from similar sequences. We describe a novel method, GOtcha, for predicting gene product function by annotation with Gene Ontology ( GO) terms. GOtcha predicts GO term associations with term-specific probability (P-score) measures of confidence. Term-specific probabilities are a novel feature of GOtcha and allow the identification of conflicts or uncertainty in annotation. Results: The GOtcha method was applied to the recently sequenced genome for Plasmodium falciparum and six other genomes. GOtcha was compared quantitatively for retrieval of assigned GO terms against direct transitive assignment from the highest scoring annotated BLAST search hit (TOPBLAST). GOtcha exploits information deep into the `twilight zone' of similarity search matches, making use of much information that is otherwise discarded by more simplistic approaches. At a P-score cutoff of 50%, GOtcha provided 60\\ better recovery of annotation terms and 20% higher selectivity than annotation with TOPBLAST at an E-value cutoff of 10(-4). Conclusions: The GOtcha method is a useful tool for genome annotators. It has identified both errors and omissions in the original Plasmodium falciparum annotation and is being adopted by many other genome sequencing projects."
276,55,8337,1,FatiGO: a web tool for finding significant associations of Gene Ontology terms with groups of genes.,"Summary: We present a simple but powerful procedure to extract Gene Ontology (GO) terms that are significantly over- or under-represented in sets of genes within the context of a genome-scale experiment (DNA microarray, proteomics, etc.). Said procedure has been implemented as a web application, FatiGO, allowing for easy and interactive querying. FatiGO, which takes the multiple-testing nature of statistical contrast into account, currently includes GO associations for diverse organisms (human, mouse, fly, worm and yeast) and the TrEMBL/Swissprot GOAnnotations@EBI correspondences from the European Bioinformatics Institute. Availability: http://fatigo.bioinfo.cnio.es 10.1093/bioinformatics/btg455"
277,55,9244,1,Coexpression Analysis of Human Genes Across Many Microarray Data Sets,"10.1101/gr.1910904 We present a large-scale analysis of mRNA coexpression based on 60 large human data sets containing a total of 3924 microarrays. We sought pairs of genes that were reliably coexpressed (based on the correlation of their expression profiles) in multiple data sets, establishing a high-confidence network of 8805 genes connected by 220,649 âcoexpression linksâ that are observed in at least three data sets. Confirmed positive correlations between genes were much more common than confirmed negative correlations. We show that confirmation of coexpression in multiple data sets is correlated with functional relatedness, and show how cluster analysis of the network can reveal functionally coherent groups of genes. Our findings demonstrate how the large body of accumulated microarray data can be exploited to increase the reliability of inferences about gene function."
278,55,10940,1,From genes to functional classes in the study of biological systems.,"BACKGROUND: With the popularization of high-throughput techniques, the need for procedures that help in the biological interpretation of results has increased enormously. Recently, new procedures inspired in systems biology criteria have started to be developed. RESULTS: Here we present FatiScan, a web-based program which implements a threshold-independent test for the functional interpretation of large-scale experiments that does not depend on the pre-selection of genes based on the multiple application of independent tests to each gene. The test implemented aims to directly test the behaviour of blocks of functionally related genes, instead of focusing on single genes. In addition, the test does not depend on the type of the data used for obtaining significance values, and consequently different types of biologically informative terms (gene ontology, pathways, functional motifs, transcription factor binding sites or regulatory sites from CisRed) can be applied to different classes of genome-scale studies. We exemplify its application in microarray gene expression, evolution and interactomics. CONCLUSION: Methods for gene set enrichment which, in addition, are independent from the original data and experimental design constitute a promising alternative for the functional profiling of genome-scale experiments. A web server that performs the test described and other similar ones can be found at: http://www.babelomics.org."
279,55,12138,1,KAAS: an automatic genome annotation and pathway reconstruction server.,"The number of complete and draft genomes is rapidly growing in recent years, and it has become increasingly important to automate the identification of functional properties and biological roles of genes in these genomes. In the KEGG database, genes in complete genomes are annotated with the KEGG orthology (KO) identifiers, or the K numbers, based on the best hit information using SmithâWaterman scores as well as by the manual curation. Each K number represents an ortholog group of genes, and it is directly linked to an object in the KEGG pathway map or the BRITE functional hierarchy. Here, we have developed a web-based server called KAAS (KEGG Automatic Annotation Server: http://www.genome.jp/kegg/kaas/) i.e. an implementation of a rapid method to automatically assign K numbers to genes in the genome, enabling reconstruction of KEGG pathways and BRITE hierarchies. The method is based on sequence similarities, bi-directional best hit information and some heuristics, and has achieved a high degree of accuracy when compared with the manually curated KEGG GENES database."
280,55,14174,1,High-throughput sequencing provides insights into genome variation and evolution in Salmonella Typhi.,"Isolates of Salmonella enterica serovar Typhi (Typhi), a human-restricted bacterial pathogen that causes typhoid, show limited genetic variation. We generated whole-genome sequences for 19 Typhi isolates using 454 (Roche) and Solexa (Illumina) technologies. Isolates, including the previously sequenced CT18 and Ty2 isolates, were selected to represent major nodes in the phylogenetic tree. Comparative analysis showed little evidence of purifying selection, antigenic variation or recombination between isolates. Rather, evolution in the Typhi population seems to be characterized by ongoing loss of gene function, consistent with a small effective population size. The lack of evidence for antigenic variation driven by immune selection is in contrast to strong adaptive selection for mutations conferring antibiotic resistance in Typhi. The observed patterns of genetic isolation and drift are consistent with the proposed key role of asymptomatic carriers of Typhi as the main reservoir of this pathogen, highlighting the need for identification and treatment of carriers."
281,56,3537,1,Complexity Theory for Simpletons,"In this article, we shall describe some of the most interesting topics in the subject of Complexity Theory for a general audience. Anyone with a solid foundation in high school mathematics (with some calculus) and an elementary understanding of computer programming will be able to follow this article. First, we shall describe the P versus NP problem and its significance. Next, we shall describe two other famous mathematics problems, the Collatz 3n+1 Conjecture and the Riemann Hypothesis, and show how the notion of {""}computational irreducibility{""} is important for understanding why no one has, as of yet, solved these two problems."
282,57,2630,1,How users assess Web pages for information seeking,"In this article, we investigate the criteria used by online searchers when assessing the relevance of Web pages for information-seeking tasks. Twenty-four participants were given three tasks each, and they indicated the features of Web pages that they used when deciding about the usefulness of the pages in relation to the tasks. These tasks were presented within the context of a simulated work-task situation. We investigated the relative utility of features identified by participants (Web page content, structure, and quality) and how the importance of these features is affected by the type of information-seeking task performed and the stage of the search. The results of this study provide a set of criteria used by searchers to decide about the utility of Web pages for different types of tasks. Such criteria can have implications for the design of systems that use or recommend Web pages."
283,57,6977,1,A new biology for a new century,"Biology today is at a crossroads. The molecular paradigm, which so successfully guided the discipline throughout most of the 20th century, is no longer a reliable guide. Its vision of biology now realized, the molecular paradigm has run its course. Biology, therefore, has a choice to make, between the comfortable path of continuing to follow molecular biology's lead or the more invigorating one of seeking a new and inspiring vision of the living world, one that addresses the major problems in biology that 20th century biology, molecular biology, could not handle and, so, avoided. The former course, though highly productive, is certain to turn biology into an engineering discipline. The latter holds the promise of making biology an even more fundamental science, one that, along with physics, probes and defines the nature of reality. This is a choice between a biology that solely does society's bidding and a biology that is society's teacher. 10.1128/MMBR.68.2.173-186.2004"
284,57,12469,1,Web Service Infrastructure for Chemoinformatics,"{The vast increase of pertinent information available to drug discovery scientists means that there is a strong demand for tools and techniques for organizing and intelligently mining this information for manageable human consumption. At Indiana University, we have developed an infrastructure of chemoinformatics Web services that simplifies the access to this information and the computational techniques that can be applied to it. In this paper, we describe this infrastructure, give some examples of its use, and then discuss our plans to use it as a platform for chemoinformatics application development in the future.}"
285,57,14254,1,A Semantic Web Management Model for Integrative Biomedical Informatics,"BACKGROUND: Data, data everywhere. The diversity and magnitude of the data generated in the Life Sciences defies automated articulation among complementary efforts. The additional need in this field for managing property and access permissions compounds the difficulty very significantly. This is particularly the case when the integration involves multiple domains and disciplines, even more so when it includes clinical and high throughput molecular data. METHODOLOGY/PRINCIPAL FINDINGS: The emergence of Semantic Web technologies brings the promise of meaningful interoperation between data and analysis resources. In this report we identify a core model for biomedical Knowledge Engineering applications and demonstrate how this new technology can be used to weave a management model where multiple intertwined data structures can be hosted and managed by multiple authorities in a distributed management infrastructure. Specifically, the demonstration is performed by linking data sources associated with the Lung Cancer SPORE awarded to The University of Texas MD Anderson Cancer Center at Houston and the Southwestern Medical Center at Dallas. A software prototype, available with open source at www.s3db.org, was developed and its proposed design has been made publicly available as an open source instrument for shared, distributed data management. CONCLUSIONS/SIGNIFICANCE: The Semantic Web technologies have the potential to addresses the need for distributed and evolvable representations that are critical for systems Biology and translational biomedical research. As this technology is incorporated into application development we can expect that both general purpose productivity software and domain specific software installed on our personal computers will become increasingly integrated with the relevant remote resources. In this scenario, the acquisition of a new dataset should automatically trigger the delegation of its analysis."
286,57,15224,1,Clickstream Data Yields High-Resolution Maps of Science,"<sec> <title>Background</title> <p>Intricate maps of science have been created from citation data to visualize the structure of scientific activity. However, most scientific publications are now accessed online. Scholarly web portals record detailed log data at a scale that exceeds the number of all existing citations combined. Such log data is recorded immediately upon publication and keeps track of the sequences of user requests (clickstreams) that are issued by a variety of users across many different domains. Given these advantages of log datasets over citation data, we investigate whether they can produce high-resolution, more current maps of science.</p> </sec><sec> <title>Methodology</title> <p>Over the course of 2007 and 2008, we collected nearly 1 billion user interactions recorded by the scholarly web portals of some of the most significant publishers, aggregators and institutional consortia. The resulting reference data set covers a significant part of world-wide use of scholarly web portals in 2006, and provides a balanced coverage of the humanities, social sciences, and natural sciences. A journal clickstream model, i.e. a first-order Markov chain, was extracted from the sequences of user interactions in the logs. The clickstream model was validated by comparing it to the Getty Research Institute's Architecture and Art Thesaurus. The resulting model was visualized as a journal network that outlines the relationships between various scientific domains and clarifies the connection of the social sciences and humanities to the natural sciences.</p> </sec><sec> <title>Conclusions</title> <p>Maps of science resulting from large-scale clickstream data provide a detailed, contemporary view of scientific activity and correct the underrepresentation of the social sciences and humanities that is commonly found in citation data.</p> </sec>"
287,57,15678,1,Data publication: towards a database of everything,"ABSTRACT: The fabric of science is changing, driven by a revolution in digital technologies that facilitate the acquisition and communication of massive amounts of data. This is changing the nature of collaboration and expanding opportunities to participate in science. If digital technologies are the engine of this revolution, digital data are its fuel. But for many scientific disciplines, this fuel is in short supply. The publication of primary data is not a universal or mandatory part of science, and despite policies and proclamations to the contrary, calls to make data publicly available have largely gone unheeded. In this short essay I consider why, and explore some of the challenges that lie ahead, as we work toward a database of everything."
288,57,16061,1,Web semÃ¡ntica y ontologÃ­as en el procesamiento de la informaciÃ³n documental,"La carencia de un modelo bien definido de representaciÃ³n de la informaciÃ³n en la web ha traÃ­do consigo problemas de cara a diversos aspectos relacionados con su procesamiento. Para intentar solucionarlos, el W3C, organismo encargado de guiar la evoluciÃ³n de la web, ha propuesto su transformaciÃ³n hacia una nueva web denominada web semÃ¡ntica. En este trabajo se presentan las posibilidades que ofrece este nuevo escenario, asÃ­ como las dificultades para su consecuciÃ³n, prestando especial atenciÃ³n a las ontologÃ­as, herramientas de representaciÃ³n del conocimiento fundamentales para la web semÃ¡ntica. Por Ãºltimo, se analiza el papel del profesional de la biblioteconomÃ­a y documentaciÃ³n en este nuevo entorno.The lack of a well defined model of information representation on the web has produced several problems related to processing information. In an effort to resolve these problems, the W3C has proposed the semantic web project. This new scenario offers both possibilities and difficulties for the future. Special attention is given to ontologies, fundamental tools for the representation of knowledge on the semantic web. Finally, the role of library and information professionals is considered in this new context."
289,57,16690,1,A study of Web 2.0 applications in library websites,"Web 2.0 represents an emerging suite of applications that hold immense potential in enriching communication, enabling collaboration and fostering innovation. However, little work has been done hitherto to research Web 2.0 applications in library websites. This paper addresses the following three research questions: (a) To what extent are Web 2.0 applications prevalent in libraries?; (b) In what ways have Web 2.0 applications been used in libraries?; and (c) Does the presence of Web 2.0 applications enhance the quality of library websites? Divided equally between public and academic, 120 libraries' websites from North America, Europe and Asia were sampled and analyzed using a three-step content analysis method. The findings suggest that the order of popularity of Web 2.0 applications implemented in libraries is: blogs, RSS, instant messaging, social networking services, wikis, and social tagging applications. Also, libraries have recognized how different Web 2.0 applications can be used complementarily to increase the level of user engagement. Finally, the presence of Web 2.0 applications was found to be associated with the overall quality, and in particular, service quality of library websites. This paper concludes by highlighting implications for both librarians and scholars interested to delve deeper into the implementation of Web 2.0 applications."
290,58,8000,1,Gamma (40-100 Hz) oscillation in the hippocampus of the behaving rat,"The cellular generation and spatial distribution of gamma frequency (40-100 Hz) activity was examined in the hippocampus of the awake rat. Field potentials and unit activity were recorded by multiple site silicon probes (5- and 16-site shanks) and wire electrode arrays. Gamma waves were highly coherent along the long axis of the dentate hilus, but average coherence decreased rapidly in the CA3 and CA1 directions. Analysis of short epochs revealed large fluctuations in coherence values between the dentate and CA1 gamma waves. Current source density analysis revealed large sinks and sources in the dentate gyrus with spatial distribution similar to the dipoles evoked by stimulation of the perforant path. The frequency changes of gamma and theta waves positively correlated (40-100 Hz and 5-10 Hz, respectively). Putative interneurons in the dentate gyrus discharged at gamma frequency and were phase-locked to the ascending part of the gamma waves recorded from the hilus. Following bilateral lesion of the entorhinal cortex the power and frequency of hilar gamma activity significantly decreased or disappeared. Instead, a large amplitude but slower gamma pattern (25-50 Hz) emerged in the CA3-CA1 network. We suggest that gamma oscillation emerges from an interaction between intrinsic oscillatory properties of interneurons and the network properties of the dentate gyrus. We also hypothesize that under physiological conditions the hilar gamma oscillation may be entrained by the entorhinal rhythm and that gamma oscillation in the CA3-CA1 circuitry is suppressed by either the hilar region or the entorhinal cortex."
291,58,12297,1,Adaptive Coevolutionary Networks &#45;&#45; A Review,"10.1098/rsif.2007.1229 Adaptive networks appear in many biological applications. They combine topological evolution of the network with dynamics in the network nodes. Recently, the dynamics of adaptive networks has been investigated in a number of parallel studies from different fields, ranging from genomics to game theory. Here we review these recent developments and show that they can be viewed from a unique angle. We demonstrate that all these studies are characterized by common themes, most prominently: complex dynamics and robust topological self-organization based on simple local rules."
292,58,14090,1,"Reliability, synchrony and noise.","The brain is noisy. Neurons receive tens of thousands of highly fluctuating inputs and generate spike trains that appear highly irregular. Much of this activity is spontaneous - uncoupled to overt stimuli or motor outputs - leading to questions about the functional impact of this noise. Although noise is most often thought of as disrupting patterned activity and interfering with the encoding of stimuli, recent theoretical and experimental work has shown that noise can play a constructive role - leading to increased reliability or regularity of neuronal firing in single neurons and across populations. These results raise fundamental questions about how noise can influence neural function and computation."
293,58,15932,1,Generating Coherent Patterns of Activity from Chaotic Neural Networks," Summary Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated."
294,59,930,1,Statistical Modeling: The Two Cultures,"Abstract. There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated bya given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical communityhas been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theoryand practice, has developed rapidlyin fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move awayfrom exclusive dependence on data models and adopt a more diverse set of tools. 1."
295,59,4923,1,The earth is round (pâ<â.05),"After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H-sub-0 is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H-sub-0 one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication."
296,59,6921,1,Missing Data,"Sooner or later anyone who does statistical analysis runs into problems with missing data in which information for some variables is missing for some cases. Why is this a problem? Because most statistical methods presume that every case has information on all the variables to be included in the analysis. Using numerous examples and practical tips, this book offers a nontechnical explanation of the standard methods for missing data (such as listwise or casewise deletion) as well as two newer (and, better) methods, maximum likelihood and multiple imputation.   Anyone who has been relying on ad-hoc methods that are statistically inefficient or biased will find this book a welcome and accessible solution to their problems with handling missing data."
297,60,6731,1,Mapping mendelian factors underlying quantitative traits using RFLP linkage maps,"The advent of complete genetic linkage maps consisting of codominant DNA markers [typically restriction fragment length polymorphisms (RFLPs)] has stimulated interest in the systematic genetic dissection of discrete Mendelian factors underlying quantitative traits in experimental organisms. We describe here a set of analytical methods that modify and extend the classical theory for mapping such quantitative trait loci (QTLs). These include: (i) a method of identifying promising crosses for QTL mapping by exploiting a classical formula of SEWALL WRIGHT; (ii) a method (interval mapping) for exploiting the full power of RFLP linkage maps by adapting the approach of LOD score analysis used in human genetics, to obtain accurate estimates of the genetic location and phenotypic effect of QTLs; and (iii) a method (selective genotyping) that allows a substantial reduction in the number of progeny that need to be scored with the DNA markers. In addition to the exposition of the methods, explicit graphs are provided that allow experimental geneticists to estimate, in any particular case, the number of progeny required to map QTLs underlying a quantitative trait."
298,61,9825,1,Empirical Analysis of Detection Cascades of Boosted Classifiers for Rapid Object Detection,"Recently Viola et al. have introduced a rapid object detection scheme based on a boosted cascade of simple feature classifiers. In this paper we introduce and empirically analysis two extensions to their approach: Firstly, a novel set of rotated haar-like features is introduced. These novel features significantly enrich the simple features of [6] and can also be calculated efficiently. With these new rotated features our sample face detector shows off on average a 10% lower false alarm rate at a given hit rate. Secondly, we present a through analysis of different boosting algorithms (namely Discrete, Real and Gentle Adaboost) and weak classifiers on the detection performance and computational complexity. We will see that Gentle Adaboost with small CART trees as base classifiers outperform Discrete Adaboost and stumps. The complete object detection training and detection system as well as a trained face detector are available in the Open Computer Vision Library at sourceforge.net [8]."
299,62,5453,1,Formation of a Motor Memory by Action Observation,"Mirror neurons discharge with both action observation and action execution. It has been proposed that the mirror neuron system is instrumental in motor learning. The human primary motor cortex (M1) displays mirror activity in response to movement observation, is capable of forming motor memories, and is involved in motor learning. However, it is not known whether movement observation can lead directly to the formation of motor memories in the M1, which is considered a likely physiological step in motor learning. Here, we used transcranial magnetic stimulation (TMS) to show that observation of another individual performing simple repetitive thumb movements gives rise to a kinematically specific memory trace of the observed motions in M1. An extended period of observation of thumb movements that were oriented oppositely to the previously determined habitual directional bias increased the probability of TMS-evoked thumb movements to fall within the observed direction. Furthermore, the acceleration of TMS-evoked thumb movements along the principal movement axis and the balance of excitability of muscle representations active in the observed movements were altered in favor of the observed movement direction. These findings support a role for the mirror neuron system in memory formation and possibly human motor learning."
300,62,11038,1,Strong Association of De Novo Copy Number Mutations with Autism,"We tested the hypothesis that de novo copy number variation (CNV) is associated with autism spectrum disorders (ASDs). We performed comparative genomic hybridization (CGH) on the genomic DNA of patients and unaffected subjects to detect copy number variants not present in their respective parents. Candidate genomic regions were validated by higher-resolution CGH, paternity testing, cytogenetics, fluorescence in situ hybridization, and microsatellite genotyping. Confirmed de novo CNVs were significantly associated with autism (P = 0.0005). Such CNVs were identified in 12 out of 118 (10%) of patients with sporadic autism, in 2 out of 77 (3%) of patients with an affected first-degree relative, and in 2 out of 196 (1%) of controls. Most de novo CNVs were smaller than microscopic resolution. Affected genomic regions were highly heterogeneous and included mutations of single genes. These findings establish de novo germline mutation as a more significant risk factor for ASD than previously recognized. 10.1126/science.1138659"
301,62,13217,1,Neuronal oscillations and visual amplification of speech,"It is widely recognized that viewing a speaker's face enhances vocal communication, although the neural substrates of this phenomenon remain unknown. We propose that the enhancement effect uses the ongoing oscillatory activity of local neuronal ensembles in the primary auditory cortex. Neuronal oscillations reflect rhythmic shifting of neuronal ensembles between high and low excitability states. Our hypothesis holds that oscillations are `predictively' modulated by visual input, so that related auditory input arrives during a high excitability phase and is thus amplified. We discuss the anatomical substrates and key timing parameters that enable and constrain this effect. Our hypothesis makes testable predictions for future studies and emphasizes the idea that `background' oscillatory activity is instrumental to cortical sensory processing."
302,63,149,1,Expertise recommender: a flexible recommendation system and architecture,"Locating the expertise necessary to solve difficult problems is a nuanced social and collaborative problem. In organizations, some people assist others in locating expertise by making referrals. People who make referrals fill key organizational roles that have been identified by CSCW and affiliated research. Expertise locating systems are not designed to replace people who fill these key organizational roles. Instead, expertise locating systems attempt to decrease workload and support people who have no other options. Recommendation systems are collaborative software that can be applied to expertise locating. This work describes a general recommendation architecture that is grounded in a field study of expertise locating. Our expertise recommendation system details the work necessary to fit expertise recommendation to a work setting. The architecture and implementation begin to tease apart the technical aspects of providing good recommendations from social and collaborative concerns."
303,63,2754,1,The explicit economics of knowledge codification and tacitness,"This paper attempts a greater precision and clarity of understanding concerning the nature and economic significance of knowledge and its variegated forms by presenting 'the skeptical economist's guide to 'tacit knowledge''. It critically reconsiders the ways in which the concepts of tacitness and codification have come to be employed by economists and develops a more coherent re-conceptualization of these aspects of knowledge production and distribution activities. It seeks also to show that a proposed alternative framework for the study of knowledge codification activities offers a more useful guide for further research directed to informing public policies for science, technological innovation and long-run economic growth. 10.1093/icc/9.2.211"
304,64,13443,1,Entrainment of Neuronal Oscillations as a Mechanism of Attentional Selection,"Whereas gamma-band neuronal oscillations clearly appear integral to visual attention, the role of lower-frequency oscillations is still being debated. Mounting evidence indicates that a key functional property of these oscillations is the rhythmic shifting of excitability in local neuronal ensembles. Here, we show that when attended stimuli are in a rhythmic stream, delta-band oscillations in the primary visual cortex entrain to the rhythm of the stream, resulting in increased response gain for task-relevant events and decreased reaction times. Because of hierarchical cross-frequency coupling, delta phase also determines momentary power in higher-frequency activity. These instrumental functions of low-frequency oscillations support a conceptual framework that integrates numerous earlier findings. 10.1126/science.1154735"
305,65,1768,1,The magical number seven plus or minus two: some limits on our capacity for processing information,"My problem is that I have been persecuted by an integer. For seven years this number has followed me around, has intruded in my most private data, and has assaulted me from the pages of our most public journals. This number assumes a variety of disguises, being sometimes a little larger and sometimes a little smaller than usual, but never changing so much as to be unrecognizable. The persistence with which this number plagues me is far more than a random accident. There is, to quote a famous senator, a design behind it, some pattern governing its appearances. Either there really is something unusual about the number or else I am suffering from delusions of persecution.  I shall begin my case history by telling you about some experiments that tested how accurately people can assign numbers to the magnitudes of various aspects of a stimulus. In the traditional language of psychology these would be called experiments in absolute judgment. Historical accident, however, has decreed that they should have another name. We now call them experiments on the capacity of people to transmit information. Since these experiments would not have been done without the appearance of information theory on the psychological scene, and since the results are analyzed in terms of the concepts of information theory, I shall have to preface my discussion with a few remarks about this theory."
306,65,9913,1,Damaged Merchandise? A Review of Experiments That Compare Usability Evaluation Methods,"An interest in the design of interfaces has been a core topic for researchers and practitioners in the field of human-computer interaction (HCI); an interest in the design of experiments has not. To the extent that reliable and valid guidance for the former depends on the results of the latter, it is necessary that researchers and practitioners understand how small features of an experimental design can cast large shadows over the results and conclusions that can be drawn. In this review we examine the design of 5 experiments that compared usability evaluation methods (UEMs). Each has had an important influence on HCI thought and practice. Unfortunately, our examination shows that small problems in the way these experiments were designed and conducted call into serious question what we thought we knew regarding the efficacy of various UEMs. If the influence of these experiments were trivial, then such small problems could be safely ignored. Unfortunately, the outcomes of these experiments have been used to justify advice to practitioners regarding their choice of UEMs. Making such choices based on misleading or erroneous claims can be detrimental--compromising the quality and integrity of the evaluation, incurring unnecessary costs, or undermining the practitioner's credibility within the design team. The experimental method is a potent vehicle that can help inform the choice of a UEM as well as help to address other HCI issues. However, to obtain the desired outcomes, close attention must be paid to experimental design."
307,66,122,1,Serendipity and information seeking: an empirical study,"""Serendipity"" has both a classical origin in literature and a more modern manifestation where it is found in the descriptions of the problem solving and knowledge acquisition of humanities and science scholars. Studies of information retrieval and information seeking have also discussed the utility of the notion of serendipity. Some have implied that it may be stimulated, or that certain people may ""encounter"" serendipitous information more than others. All to some extent accept the classical definition of serendipity as a ""fortuitous"" accident. The analysis presented here is part of a larger study concerning the information-seeking behaviour of interdisciplinary scholars. This paper considers the nature of serendipity in information-seeking contexts, and reinterprets the notion of serendipity as a phenomenon arising from both conditions and strategies - as both a purposive and a non-purposive component of information seeking and related knowledge acquisition."
308,66,923,1,"Bayesian Data Analysis, Second Edition (Chapman & Hall/CRC Texts in Statistical Science)","{Incorporating new and updated information, this second edition of THE bestselling text in Bayesian data analysis continues to emphasize practice over theory, describing how to conceptualize, perform, and critique statistical analyses from a Bayesian perspective. Its world-class authors provide guidance on all aspects of Bayesian data analysis and include examples of real statistical analyses, based on their own research, that demonstrate how to solve complicated problems. Changes in the new edition include: &#183;Stronger focus on MCMC&#183;Revision of the computational advice in Part III&#183;New chapters on nonlinear models and decision analysis&#183;Several additional applied examples from the authors' recent research&#183;Additional chapters on current models for Bayesian data analysis such as nonlinear models, generalized linear mixed models, and more&#183;Reorganization of chapters 6 and 7 on model checking and data collectionBayesian computation is currently at a stage where there are many reasonable ways to compute any given posterior distribution. However, the best approach is not always clear ahead of time. Reflecting this, the new edition offers a more pluralistic presentation, giving advice on performing computations from many perspectives while making clear the importance of being aware that there are different ways to implement any given iterative simulation computation. The new approach, additional examples, and updated information make Bayesian Data Analysis an excellent introductory text and a reference that working scientists will use throughout their professional life.}"
309,66,1442,1,Computability and Logic,"Computability and Logic has become a classic because of its accessibility to students without a mathematical background and because it covers not simply the staple topics of an intermediate logic course, such as Godel's incompleteness theorems, but also a large number of optional topics, from Turing's theory of computability to Ramsey's theorem. Including a selection of exercises, adjusted for this edition, at the end of each chapter, it offers a new and simpler treatment of the representability of recursive functions, a traditional stumbling block for students on the way to the Godel incompleteness theorems."
310,66,4028,1,Introduction to Machine Learning (Adaptive Computation and Machine Learning),"{The goal of machine learning is to program computers to use example data or past experience to solve a given problem. Many successful applications of machine learning exist already, including systems that analyze past sales data to predict customer behavior, recognize faces or spoken speech, optimize robot behavior so that a task can be completed using minimum resources, and extract knowledge from bioinformatics data. <i>Introduction to Machine Learning</i> is a comprehensive textbook on the subject, covering a broad array of topics not usually included in introductory machine learning texts. It discusses many methods based in different fields, including statistics, pattern recognition, neural networks, artificial intelligence, signal processing, control, and data mining, in order to present a unified treatment of machine learning problems and solutions. All learning algorithms are explained so that the student can easily move from the equations in the book to a computer program. The book can be used by advanced undergraduates and graduate students who have completed courses in computer programming, probability, calculus, and linear algebra. It will also be of interest to engineers in the field who are concerned with the application of machine learning methods.<br /> <br /> After an introduction that defines machine learning and gives examples of machine learning applications, the book covers supervised learning, Bayesian decision theory, parametric methods, multivariate methods, dimensionality reduction, clustering, nonparametric methods, decision trees, linear discrimination, multilayer perceptrons, local models, hidden Markov models, assessing and comparing classification algorithms, combining multiple learners, and reinforcement learning.}"
311,66,5121,1,Ambient Findability: What We Find Changes Who We Become,"{How do you find your way in an age of information overload? How can you filter streams of complex information to pull out only what you want? Why does it matter how information is structured when Google seems to magically bring up the right answer to your questions? What does it mean to be ""findable"" in this day and age?  This eye-opening new book examines the convergence of information and connectivity. Written by Peter Morville, author of the groundbreaking <i>Information Architecture for the World Wide Web</i>, the book defines our current age as a state of unlimited findability. In other words, anyone can find anything at any time. Complete navigability.   <p>  Morville discusses the Internet, GIS, and other network technologies that are coming together to make unlimited findability possible. He explores how the melding of these innovations impacts society, since Web access is now a standard requirement for successful people and businesses. But before he does that, Morville looks back at the history of wayfinding and human evolution, suggesting that our fear of being lost has driven us to create maps, charts, and now, the mobile Internet.</p>  <p>  The book's central thesis is that information literacy, information architecture, and usability are all critical components of this new world order. Hand in hand with that is the contention that only by planning and designing the best possible software, devices, and Internet, will we be able to maintain this connectivity in the future. Morville's book is highlighted with full color illustrations and rich examples that bring his prose to life.</p>  <p>  <i>Ambient Findability</i> doesn't preach or pretend to know all the answers. Instead, it presents research, stories, and examples in support of its novel ideas. Are we truly at a critical point in our evolution where the quality of our digital networks will dictate how we behave as a species? Is findability indeed the primary key to a successful global marketplace in the 21st century and beyond. Peter Morville takes you on a thought-provoking tour of these memes and more -- ideas that will not only fascinate but will stir your creativity in practical ways that you can apply to your work immediately.</p>  <p>  <i>""A lively, enjoyable and informative tour of a topic that's only going to become more important.""</i><br>  --David Weinberger, Author, <i>Small Pieces Loosely Joined</i> and <i>The Cluetrain Manifesto</i></br></p>  <p>  <i>""I envy the young scholar who finds this inventive book, by whatever strange means are necessary. The future isn't just unwritten--it's unsearched.""</i><br>  --Bruce Sterling, Writer, Futurist, and Co-Founder, The Electronic Frontier Foundation</br></p>  <p>  <i>""Search engine marketing is the hottest thing in Internet business, and deservedly so. Ambient Findability puts SEM into a broader context and provides deeper insights into human behavior. This book will help you grow your online business in a world where being found is not at all certain.""</i><br>  --Jakob Nielsen, Ph.D., Author, <i>Designing Web Usability: The Practice of Simplicity</i></br></p>  <p>  <i>""Information that's hard to find will remain information that's hardly found--from one of the fathers of the discipline of information architecture, and one of its most experienced practitioners, come penetrating observations on why findability is elusive and how the act of seeking changes us.""</i><br>  --Steve Papa, Founder and Chairman, Endeca</br></p>  <p>  <i>""Whether it's a fact or a figure, a person or a place, Peter Morville knows how to make it findable. Morville explores the possibilities of a world where everything can always be found--and the challenges in getting there--in this wide-ranging, thought-provoking book.""</i><br>  --Jesse James Garrett, Author, <i>The Elements of User Experience</i></br></p>  <p>  <i>""It is easy to assume that current searching of the World Wide Web is the last word in finding and using information. Peter Morville shows us that search engines are just the beginning. Skillfully weaving together information science research with his own extensive experience, he develops for the reader a feeling for the near future when information is truly findable all around us. There are immense implications, and Morville's lively and humorous writing brings them home.""</i><br>  --Marcia J. Bates, Ph.D., University of California Los Angeles</br></p>  <p>  <i>""I've always known that Peter Morville was smart. After reading Ambient Findability, I now know he's (as we say in Boston) wicked smart. This is a timely book that will have lasting effects on how we create our future.</i><br>  --Jared Spool, Founding Principal, User Interface Engineering</br></p>  <p>  <i>""In Ambient Findability, Peter Morville has put his mind and keyboard on the pulse of the electronic noosphere. With tangible examples and lively writing, he lays out the challenges and wonders of finding our way in cyberspace, and explains the mutually dependent evolution of our changing world and selves. This is a must read for everyone and a practical guide for designers.""</i><br>  --Gary Marchionini, Ph.D., University of North Carolina</br></p>  <p>  <i>""Find this book! Anyone interested in making information easier to find, or understanding how finding and being found is changing, will find this thoroughly researched, engagingly written, literate, insightful and very, very cool book well worth their time. Myriad examples from rich and varied domains and a valuable idea on nearly every page. Fun to read, too!</i><br>  --Joseph Janes, Ph.D., Founder, Internet Public Library</br></p>}"
312,66,6579,1,A Bayesian Truth Serum for Subjective Data,"Subjective judgments, an essential information source for science and policy, are problematic because there are no public criteria for assessing judgmental truthfulness. I present a scoring method for eliciting truthful subjective data in situations where objective truth is unknowable. The method assigns high scores not to the most common answers but to the answers that are more common than collectively predicted, with predictions drawn from the same population. This simple adjustment in the scoring criterion removes all bias in favor of consensus: Truthful answers maximize expected score even for respondents who believe that their answer represents a minority view. 10.1126/science.1102081"
313,66,10438,1,The Neural Basis of Loss Aversion in Decision-Making Under Risk,"People typically exhibit greater sensitivity to losses than to equivalent gains when making decisions. We investigated neural correlates of loss aversion while individuals decided whether to accept or reject gambles that offered a 50/50 chance of gaining or losing money. A broad set of areas (including midbrain dopaminergic regions and their targets) showed increasing activity as potential gains increased. Potential losses were represented by decreasing activity in several of these same gain-sensitive areas. Finally, individual differences in behavioral loss aversion were predicted by a measure of neural loss aversion in several regions, including the ventral striatum and prefrontal cortex."
314,66,11999,1,Learning the value of information in an uncertain world,"Our decisions are guided by outcomes that are associated with decisions made in the past. However, the amount of influence each past outcome has on our next decision remains unclear. To ensure optimal decision-making, the weight given to decision outcomes should reflect their salience in predicting future outcomes, and this salience should be modulated by the volatility of the reward environment. We show that human subjects assess volatility in an optimal manner and adjust decision-making accordingly. This optimal estimate of volatility is reflected in the fMRI signal in the anterior cingulate cortex (ACC) when each trial outcome is observed. When a new piece of information is witnessed, activity levels reflect its salience for predicting future outcomes. Furthermore, variations in this ACC signal across the population predict variations in subject learning rates. Our results provide a formal account of how we weigh our different experiences in guiding our future actions."
315,66,13352,1,The R Book,"{The high-level language of R is recognized as one of the most powerful and flexible statistical software environments, and is rapidly becoming the standard setting for quantitative analysis, statistics and graphics. R provides free access to unrivalled coverage and cutting-edge applications, enabling the user to apply numerous statistical methods ranging from simple regression to time series or multivariate analysis.   <p>   Building on the success of the authorâs bestselling <i>Statistics: An Introduction using R</i>, <i>The R Book</i> is packed with worked examples, providing an all inclusive guide to R, ideal for novice and more accomplished users alike. The book assumes no background in statistics or computing and introduces the advantages of the R environment, detailing its applications in a wide range of disciplines.    <ul type=""disc"">    <li>Provides the first comprehensive reference manual for the R language, including practical guidance and full coverage of the graphics facilities.    <li>Introduces all the statistical models covered by R, beginning with simple classical tests such as chi-square and t-test.    <li>Proceeds to examine more advance methods, from regression and analysis of variance, through to generalized linear models, generalized mixed models, time series, spatial statistics, multivariate statistics and much more.    </ul>   <p>   <i>The R Book</i> is aimed at undergraduates, postgraduates and professionals in science, engineering and medicine. It is also ideal for students and professionals in statistics, economics, geography and the social sciences.}"
316,67,2359,1,Social Bookmarking Tools (II): A Case Study - Connotea,"Connotea [1] is a free online reference management and social bookmarking service for scientists created by Nature Publishing Group [2]. While somewhat experimental in nature, Connotea already has a large and growing number of users, and is a real, fully functioning service [3]. The label 'experimental' is not meant to imply that the service is any way ephemeral or esoteric, rather that the concept of social bookmarking itself and the application of that concept to reference management are both recent developments. Connotea is under active development, and we are still in the process of discovering how people will use it. In addition to Connotea being a free and public service, the core code is freely available under an open source license [4]. Connotea was conceived from the outset as an online, social tool. Seeing the possibilities that del.icio.us [5] was opening up for its users in the area of general web linking, we realised that scholarly reference management was a similar problem space. Connotea was designed and developed late in 2004, and soft-launched at the end of December 2004. Usage has grown over the past several months, to the point where there is now enough data in the system for interesting second-order effects to emerge. This paper will start by giving an overview of Connotea, and will outline the key concepts and describe its main features. We will then take the reader on a brief guided tour, show some of the aforementioned second-order effects, and end with a discussion of Connotea's likely future direction."
317,67,8706,1,Semantic annotation for knowledge management: Requirements and a survey of the state of the art,"While much of a company's knowledge can be found in text repositories, current content management systems have limited capabilities for structuring and interpreting documents. In the emerging Semantic Web, search, interpretation and aggregation can be addressed by ontology-based semantic mark-up. In this paper, we examine semantic annotation, identify a number of requirements, and review the current generation of semantic annotation systems. This analysis shows that, while there is still some way to go before semantic annotation tools will be able to address fully all the knowledge management needs, research in the area is active and making good progress."
318,67,10972,1,Web 2.0: Werkzeuge fÃ¼r die Wissenschaft,"Um den Begriff Web 2.0 ist ein gigantischer Buzz entstanden. Dabei existieren viele der Methoden, die sich hinter dem Begriff verbergen, schon seit geraumer Zeit. In diesem Artikel, der eine Ausarbeitung des am 23. DV-Treffen der MPG am 15. November 2006 gehaltenen Workshops ist, mÂ¨ochte ich die wichtigsten dieser Methoden vorstellen, um einmal einen Blick hinter den Hype zu werfen. Dabei werde ich auf Begriff wie RSS, AJAX, Mashup, Social Software etc. eingehen und die zugrunde liegenden Werkzeuge vorstellen. Darauf aufbauend wird ein Entwurf einer Peer-2-Peer-Web-2.0-Anwendung vorgestellt, die als Modell fÂ¨ur wissenschaftliche, kollaborative Arbeitsumgebungen dienen kann."
319,67,11924,1,Social Software. Formen der Kooperation in computerbasierten Netzwerken,"""Mit Social Software bezeichnet man computernetzwerkgestÃ¼tzte Systeme zur Zusammenarbeit von Teilnehmern. Der Begriff bezieht sich vor allem auf neuere Anwendungen wie Wikis, Weblogs, gemeinsame Fotosammlungen, kollaborativ erstellte Verschlagwortungsseiten und Instant Messaging. Zwei Merkmale sind allen Systemen gemein: zum einen erstellen die Nutzer die Inhalte selbst, zum anderen verfÃ¼gen Social Software Anwendungen Ã¼ber Komponenten zur Herausbildung und StÃ¼tzung von Gemeinschaftsaspekten. In der EinfÃ¼hrung wird ein Ãberblick Ã¼ber Social Software-Anwendungen gegeben. Es werden damit zusammenhÃ¤ngende Herausforderungen an die Medien- und Kommunikationsforschung in diesem Gebiet diskutiert und die einzelnen BeitrÃ¤ge diskutiert."""
320,68,358,1,Tropical Forest Fragments Enhance Pollinator Activity in Nearby Coffee Crops,":&#8194; Crop pollination by wild bees is an ecosystem service of enormous value, but it is under increasing threat from agricultural intensification. As with many ecosystem services, the mechanisms, scales, and species through which crop pollination is provided are too poorly understood to inform land-use decisions. I investigated the role of tropical forest remnants as sources of pollinators to surrounding coffee crops in Costa Rica. In 2001 and 2002 I observed bee activity and pollen deposition rates at coffee flowers along distance gradients from two fragments and one narrow riparian strip of forest. Eleven eusocial species were the most common visitors: 10 species of native meliponines and the introduced honeybee, Apis mellifera (hereafter Apis). Bee richness, overall visitation rate, and pollen deposition rate were all significantly higher in sites within approximately 100 m of forest fragments than in sites farther away (maximum distance of 1.6 km). Apis visitation rates were constant across the distance gradient, however, and Apis accounted for &#62;90% of all floral visits in distant sites. The gradient from the riparian strip showed a similar drop in bee species richness with distance, but visitation rates were uniformly low along the gradient. Throughout the study area, Apis abundances declined sharply from 2001 to 2002, reducing visitation rates by over 50% in distant sites (where Apis was almost the only pollinator). In near sites, however, overall visitation rates dropped only 9% because native species almost entirely compensated for the Apis decline. Forest fragments (more so than the riparian strip) thus provided nearby coffee with a diversity of bees that increased both the amount and stability of pollination services by reducing dependence on a single introduced pollinator. Exploring the economic links between forest preservation and coffee cultivation may help align the goals of conservation and agriculture within many regions of global conservation priority."
321,68,2571,1,"Similarity indices, sample size and diversity","The effect of sample size and species diversity on a variety of similarity indices is explored. Real values of a similarity index must be evaluated relative to the expected maximum value of that index, which is the value obtained for samples randomly drawn from the same universe, with the diversity and sample sizes of the real samples. It is shown that these expected maxima differ from the theoretical maxima, the values obtained for two identical samples, and that the relationship between expected and theoretical maxima depends on sample size and on species diversity in all cases, without exception. In all cases but one (the Morisita index) the expected maxima depend strongly to fairly strongly on sample size and diversity. For some of the more useful indices empirical equations are given to calculate the expected maximum value of the indices to which the observed values can be related at any combination of sample sizes. It is recommended that the Morisita index be used whenever possible to avoid the complex dealings with effects of sample size and diversity; however, when previous logarithmic transformation of the data is required, which often may be the case, the Morisita-Horn or the Renkonen indices are recommended."
322,68,5062,1,Responses of ants to selective logging of a central Amazonian forest,"			Summary1. Relatively little information exists on the effects of logging on rain forest organisms, particularly in the Neotropics where logging operations have increased dramatically in recent years. In this study we determined experimentally the effects of selective logging of a central Amazonian forest on ground-living ants.2. The experimental design consisted of three 4-ha replicated plots representing control unlogged forest, forest logged 10 years prior to the start of the study (1987), and forest logged 4 years prior to the start of the study (1993). The logging operation removed 50% of the basal area of trees of commercial value, or about eight trees per hectare. This resulted in a significant decrease in canopy cover, and an increase in understorey vegetation density in logged plots relative to controls.3. Collection and identification of ants from a total of 360 1-m2 samples of leaf-litter revealed 143 ant species, of which 97 were found in the control plots, 97 in the plots logged in 1987, and 106 in those logged in 1993. Species richness, evenness and mean abundance (ants m-2) per plot did not vary among treatments. Most of the species found in the control plots were also present in the logged plots. However, population density of many species changed as a result of logging, an effect that persisted for at least 10 years after logging. Species commonly found in sites that were directly disturbed by logging (gaps and tracks) were rare in the undisturbed forest, as revealed by an additional collection of ants.4. These results suggest that the persistence of ant assemblages typical of undisturbed forest is likely to depend on the amount of structural damage incurred by logging. Thus management techniques that minimize logging impacts on forest structure are likely to help maintain the conservation value of logged forests for ground-dwelling ants. It is particularly important to minimize the extent of logging roads and tracks created by heavy machinery because these areas appear more prone to invasion by non-forest species."
323,68,6202,1,"Land use and vegetation fires in Jambi Province, Sumatra, Indonesia","In Indonesia, vegetation fires occur every year in the dry season. To determine where and why fires occur, the natural and cultural landscape features that influence the location of fires were analysed. We investigated the probability of fire occurrence as a function of predisposing conditions and ignition sources, such as land use, land use zoning, accessibility or land cover, to understand the spatial determinants of fires. The study area is the entire province of Jambi, central Sumatra, Indonesia. This province has a diverse setting of actors (small- and large-holders), land cover types and land uses. Fires were extracted for 1992/1993 from National Oceanic Atmospheric Administration&rsquo;s Advanced Very High Resolution Radiometer (NOAA-AVHRR) satellite data. The results of the spatial statistical analysis show that fire occurrence in Jambi Province in 1992/1993 was determined both by predisposing conditions (mostly climate, elevation and suitability for specific tree crops) and human-related causes (presence of transmigration projects and land allocation to specific land uses). National policies are thus a major driving forces of fires through land allocation. Road accessibility is only an important determinant of fires in forests. Few fires seem to be accidental. While logging companies control fire during their exploitation of concessions, logged-over forests and forests allocated to production but not yet under use have many fires. In 1992/1993, large- and small-holders were likely to be both responsible for fire occurrence. These results highlight the large influence of land use and policies on vegetation fires in Indonesia."
324,68,6907,1,Pseudoreplication and the design of ecological field experiments,"Pseudoreplication is defined as the use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated (though samples may be) or replicates are not statistically independent. In ANOVA terminology, it is the testing for treatment effects with an error term inappropriate to the hypothesis being considered. Scrutiny of 176 experimental studies published between 1960 and the present revealed that pseudoreplication occurred in 27% of them, or 48% of all such studies that applied inferential statistics. The incidence of pseudoreplication is especially high in studies of marine benthos and small mammals. The critical features of controlled experimentation are reviewed. Nondemonic intrusion is defined as the impingement of chance events on an experiment in progress. As a safeguard against both it and preexisting gradients, interspersion of treatments is argued to be an obligatory feature of good design. Especially in small experiments, adequate interspersion can sometimes be assured only by dispensing with strict randomization procedures. Comprehension of this conflict between interspersion and randomization is aided by distinguishing preâlayout (or conventional) and layoutâspecific alpha (probability of type I error). Suggestions are offered to statisticians and editors of ecological journals as to how ecologists' understanding of experimental design and statistics might be improved."
325,68,7567,1,Autocorrelated Rates of Change in Animal Populations and their Relationship to Precipitation,"I examined the prevalence of autocorrelation in mammalian, avian, and precipitation time series, how well autocorrelation in the environment translates into autocorrelation in animal populations, and length of the time series needed to accurately characterize the degree of autocorrelation. These are important questions because more-complex population models are incorporating autocorrelation terms in life-history characteristics and the intrinsic rate of increase. Including inaccurate or nonsignificant autocorrelation can alter the conclusions reached, providing either an unduly rosy or bleak picture of the likelihood of population viability and persistence. Using autocorrelation analysis in 175 vertebrate and 88 precipitation data sets, I found that 17.8% of the mammalian time series, 61.5% of the avian time series, and 97.7% of the precipitation data sets were autocorrelated. Carnivore populations were more likely than herbivore populations to show significant autocorrelation at lags of 2 or more years. I found only two cases of significant cross correlation between rate of population increase and local precipitation. This indicates that, although some environmental variables may be highly autocorrelated, it does not translate into autocorrelation in the resident animal populations. Based on subsampling of the precipitation and vertebrate data, I found that 15 years of data is sufficient to produce an autocorrelation not significantly different from one based on 100 years of data, although the variance continues to decrease with the length of the time series, as expected. My results suggest that, although some populations show temporal autocorrelation, it is not ubiquitous, and that environmental autocorrelation may not be a good predictor of autocorrelation in rates of increase. Population modelers should determine if autocorrelation exists in populations of interest prior to modeling their viability or probability of persistence because not all populations are equally influenced by autocorrelation. Tasas de Cambio en Poblaciones de Animales Autocorrelacionadas y sus Relaciones con la Precipitacion. Se examino el predominio de la autocorrelacion en series de tiempo de mamiferos, aves y precipitacion; que tanto la autocorrelacion ambiental se traduce en autocorrelacion en poblaciones de animales, asi como la longitud de las series de tiempo necesaria para caraterizar con precision el grado de autocorrelacion. Estas son preguntas importantes puesto que los modelos poblacionales mas complejos incorporan la autocorrelacion en las caracteristicas de la historia de vida y la tasa intrinseca de incremento. Aun la inexactitud o autocorrelatcion no significativa puede alterar las conclusiones obtenidas, proveyendo indebidamente una idea prometedora o poco promotedora de la probabilidad de viabilidad y persistencia de una poblacion. Mediante el uso del analisis de autocorrelacion en 175 juegos de datos de vertebrados y 88 de precipitacion, encontre que un 17.8% de las series de tiempo de mamiferos, 61.5% de las series de tiempo de aves y un 97.8% de los datos de precipitacion se encontraban autocorrelacionados. Las poblaciones de carnivoros fueron las mas viables a mostrar autocorrelacion en lapsos de 2 o mas anos que las poblaciones de herbivoros. Encontre solo dos casos de correlacion cruzada significativa entre la tasa de incremento poblacional y la precipitacion local. Esto indica que aunque algunas variables ambientales pueden estar altamente autocorrelacionadas, no se traduce en una autocorrelacion en las poblaciones de animales residentes. Basado en un submuestreo de la precipitacion y datos de vertebrados, encontre que 15 anos de datos es suficiente para producir una autocorrelacion que no es significativamente diferente de aquella basada en 100 anos de datos, aunque la varianza continua disminuyendo con la longitud de las series de tiempo a como es de esperarse. Mis resultados sugieren que aunque algunas poblaciones muestran autocorrelacion temporal, esta no es evidente y la autocorrelacion ambiental podria no ser un buen predictor de autocorrelacion de tasas de incremento. Los modeladores de poblaciones deberan determinar si la autocorrelacion existe en poblaciones de interes antes de modelar su viabilidad o la probabilidad de persistencia puesto que no todas las poblaciones son afectadas por la autocorrelacion de la misma manera."
326,68,8539,1,Abundance-occupancy relationships,"1. The abundance and distribution of species tend to be linked, such that species declining in abundance often tend also to show declines in the number of sites they occupy, while species increasing in abundance tend also to be increasing in occupancy. Therefore, intraspecific abundance-occupancy relationships are commonly positive. 2. The intraspecific pattern is mirrored by more general positive interspecific abundance-occupancy relationships: widespread species tend to be abundant, and narrowly distributed species rare. 3. Here, we review recent research on these patterns based on the flora and fauna of the British Isles. We assess their generality, describe what is currently known about their structure, and summarize the results of tests of the several hypotheses proposed to explain their existence. 4. The positive form generally exhibited by abundance-occupancy relationships, intraspecific or interspecific, has consequences for several areas of applied ecology, including conservation, harvesting, biological invasions and biodiversity inventorying. These implications are discussed briefly"
327,68,9948,1,Ten years of individual-based modelling in ecology: what have we learned and what could we learn in the future?,"Each modeller who builds and analyses an individual-based model learns of course a great deal, but what has ecology as a whole learned from the individual-based models published during the last decade? Answering this question proves extremely difficult as there is no common motivation behind individual-based models. The distinction is introduced between 'pragmatic' motivation, which uses the individual-based approach as a tool without any reference to the theoretical issues which have emerged from the classical state variable approach and 'paradigmatic' motivation, which explicitly refers to theoretical ecology. A mini-review of 50 individual-based animal population models shows that the majority are driven by pragmatic motivation. Most models are very complex and special techniques to cope with this complexity during their analysis are only occasionally applied. It is suggested that in order to orient individual-based modelling more towards general theoretical issues, we need increased explicit reference to theoretical ecology and an advanced strategy for building and analysing individual-based models. To this end, a heuristic list of rules is presented which may help us to advance the practice of individual-based modelling and to learn more general lessons from individual-based modelling in the future than we have during the last decade. The main ideas behind these rules are as follows: (1) Individual-based models usually make more realistic assumptions than state variable models, but it should not be forgotten that the aim of individual-based modelling is not 'realism' but modelling. (2) The individual- based approach is a bottom-up approach which starts with the 'parts' (i.e. individuals) of a system (i.e. population) and then tries to understand how the system's properties emerge from the interaction among these parts. However, bottom-up approaches alone will never lead to theories at the systems level. State variable or top-down approaches are needed to provide an appropriate integrated view, i.e. the relevant questions at the population level. (C) 1999 Elsevier Science B.V. All rights reserved."
328,68,11380,1,Agroecology: the science of natural resource management for poor farmers in marginal environments.,"Throughout the developing world, resource-poor farmers (about 1.4 billion people) located in risk-prone, marginal environments, remain untouched by modern agricultural technology. A new approach to natural resource management must be developed so that new management systems can be tailored and adapted in a site-specific way to highly variable and diverse farm conditions typical of resource-poor farmers. Agroecology provides the scientific basis to address the production by a biodiverse agroecosystem able to sponsor its own functioning. The latest advances in agroecological research are reviewed in order to better define elements of a research agenda in natural resource management that is compatible with the needs and aspirations of peasants. Obviously, a relevant research agenda setting should involve the full participation of farmers with other institutions serving a facilitating role. The implementation of the agenda will also imply major institutional and policy changes."
329,68,11389,1,Ecology : From individuals to ecosystems.,"Begon, Townsend, and Harper's Ecology has long been regarded as the definitive textbook on all aspects of ecology. This new edition provides a comprehensive treatment of the subject, from the first principles of ecology to the current state of the field, and aims to improve students' preparedness to address the environmental problems of the new millennium. Thoroughly revised and updated, this fourth edition includes: * three new chapters on applied ecology, reflecting a rigorous, scientific approach to the ecological problems now facing mankind * discussion of over 800 new studies, updating the text throughout * an updated, user-friendly design with margin notes and chapter summaries that serve as study aids * dedicated website at www.blackwellpublishing.com/begon The resulting textbook is easy to use, lucid and up-to-date, and is the essential reference for all students whose degree program includes ecology and for practicing ecologists."
330,68,11398,1,On the use of surrogate species in conservation biology.,"Conservation biologists have used surrogate species as a shortcut to monitor or solve conservation problems. Indicator species have been used to assess the magnitude of anthropogenic disturbance, to monitor population trends in other species, and to locate areas of high regional biodiversity. Umbrella species have been used to delineate the type of habitat or size of area for protection, and flagship species have been employed to attract public attention. Unfortunately, there has been considerable confusion over these terms, and several have been applied loosely and interchangeably. We attempt to provide some clarification and guidelines for the application of these different terms. For each type of surrogate, we briefly describe the way it has been used in conservation biology and then examine the criteria that managers and researchers use in selecting appropriate surrogate species. By juxtaposing these concepts, it becomes clear that both the goals and selection criteria of different surrogate classes differ substantially, indicating that they should not be conflated. This can be facilitated by first outlining the goals of a conservation study, explicitly stating the criteria involved in selecting a surrogate species, identifying a species according to these criteria, and then performing a pilot study to check whether the choice of species was appropriate before addressing the conservation problem itself. Surrogate species need to be used with greater care if they are to remain useful in conservation biology. BiÃ³logos de la conservaciÃ³n han utilizado especies sucedÃ¡neas como atajos para monitorear o resolver problemas de conservaciÃ³n. Las especies indicadoras han sido utilizadas para evaluar la magnitud de la pertubaciÃ³n antropogÃ©nica, para monitorear tendencias poblacionales en otras especies y para localizar Ã¡reas de alta biodiversidad regional. Las especies sombrilla han sido utilizadas para delinear el tipo de hÃ¡bitat o tamaÃ±o de Ã¡rea para protecciÃ³n y las especies bandera han sido empleadas para atraer la atenciÃ³n del pÃºblico. Desafortunadamente, ha habido una considerable confusiÃ³n sobre estos tÃ©rminos y muchos han sido aplicados de un amanera vaga e intercambiable. Intentamos proveer albunas aclaraciones y lineamientos para la aplicaciÃ³n de estos diferentes tÃ©rminos. Para cada tipo de sustituto describimos brevemente la forma en que ha sido usado en la conservaciÃ³n biolÃ³gica y posteriormente examinamos los criterios que los manejadores e investigadores usan en la selecciÃ³n de las especies sustitutas apropiadas. Al yuxtaponer estos conceptos, se hace claro que tanto las metas como los criterios de selecciÃ³n de diferentes clases de sustituos difieren substancialmente indicando que estos no diberÃ­n ser confundidos. Esto puede ser facilitado primero al subrayar las metas de un estudio de conservaciÃ³n, estableciendo explÃ­citamente los criterios involucrados en la selecciÃ³n de la especie sustituta, identificando a las especies utilizando este criterio y posteriormente llevando a cobo un estudio piloto para checar si la especie seleccionada es la apropiada antes de proceder a abordar el problema de conservaciÃ³n en si. Las especies sucedÃ¡neas necesitan ser utilizadas con mayor cuidado si queremos que sigan siendo Ãºtiles para la biologÃ­a de conservaciÃ³n."
331,68,11407,1,Host-plant selection by insects â a theory based on âappropriate/inappropriate landingsâ by pest insects of cruciferous plants.,"Seven hypotheses, including the `Resource Concentration Hypothesis' and the `Enemies Hypothesis', have been put forward to explain why fewer specialist insects are found on host plants growing in diverse backgrounds than on similar plants growing in bare soil. All seven hypotheses are discussed and discounted, primarily because no one has used any of them to produce a general theory of host plant selection, they still remain as hypotheses. However, we have developed a general theory based on detailed observations of insect behaviour. Our theory is based on the fact that during host plant finding the searching insects land indiscriminately on green objects such as the leaves of host plants (appropriate landings) and non-host plants (inappropriate landings), but avoid landing on brown surfaces, such as soil. The complete system of host plant selection involves a three-link chain of events in which the first link is governed by cues from volatile plant chemicals, the central link by visual stimuli, and the final link by cues from non-volatile plant chemicals. The previously `missing' central link, which is based on what we have described as `appropriate/inappropriate landings', is governed by visual stimuli. Our theory explains why attempts to show that olfaction is the crucial component in the central link of host plant selection proved intractable. The `appropriate/inappropriate landings' theory is discussed to indicate the type of work needed in future studies to improve our understanding of how intercropping, undersowing and companion planting can be used to optimum effect in crop protection. The new theory is used also to suggest how insect biotypes could develop and to describe why pest insects do not decimate wild host plants growing in `natural' situations."
332,68,11416,1,META-X: generic software for metapopulation viability analysis.,"The major tools used to make population viability analyses (PVA) quantitative are stochastic models of population dynamics. Since a specially tailored model cannot be developed for every threatened population, generic models have been designed which can be parameterised and analysed by non-modellers. These generic models compromise on detail so that they can be used for a wide range of species. However, generic models have been criticised because they can be employed without the user being fully aware of the concepts, methods, potentials, and limitations of PVA. Here, we present the conception of a new generic software package for metapopulation viability analysis, META-X. This conception is based on three elements, which take into account the criticism of earlier generic PVA models: (1) comparative simulation experiments; (2) an occupancy-type model structure which ignores details of local population dynamics (these details are integrated in external submodels); and (3) a unifying currency to quantify persistence and viability, the `intrinsic mean time to extinc- tion'. The rationale behind these three elements is explained and demonstrated by exemplary applications of META-X in the three fields for which META-X has been designed: teaching, risk assessment in the field, and planning. The conception of META-X is based on the notion that PVA is a tool to deal with rather than to overcome uncertainty. The purpose of PVA is to produce relative, not absolute, assessments of extinction risk which support, but do not supplant, management decisions."
333,68,11425,1,History of vegetation and habitat change in the Austral-Asian region,"Over 1000 marine and terrestrial pollen diagrams and some hundreds of vertebrate faunal sequences have been studied in the Austral-Asian region bisected by the PEPII transect, from the Russian arctic extending south through east Asia, Indochina, southern Asia, insular Southeast Asia (Sunda), Melanesia, Australasia (Sahul) and the western south Pacific. The majority of these records are Holocene but sufficient data exist to allow the reconstruction of the changing biomes over at least the past 200,000 years. The PEPII transect is free of the effects of large northern ice caps yet exhibits vegetational change in glacial cycles of a similar scale to North America. Major processes that can be discerned are the response of tropical forests in both lowlands and uplands to glacial cycles, the expansion of humid vegetation at the PleistoceneâHolocene transition and the change in faunal and vegetational controls as humans occupy the region. There is evidence for major changes in the intensity of monsoon and El Nino-Southern oscillation variability both on glacialâinterglacial and longer time scales with much of the region experiencing a long-term trend towards more variable and/or drier climatic conditions. Temperature variation is most marked in high latitudes and high altitudes with precipitation providing the major climate control in lower latitude, lowland areas. At least some boundary shifts may be the response of vegetation to changing CO2 levels in the atmosphere. Numerous questions of detail remain, however, and current resolution is too coarse to examine the degree of synchroneity of millennial scale change along the transect."
334,68,11434,1,"Grazing intensity and the diversity of grasshoppers, butterflies, and trap-nesting bees and wasps.","The maintenance of grasslands as distinct habitats depends on regular management, usually through grazing or mowing, but their species diversity is known to decline with increasing management intensity. The reduction of management intensity can be a useful tool for the long-term conservation of the biological diversity of grasslands. We analyzed floral and faunal diversity on intensively and extensively (unintensively) grazed pastures and on 5- to 10-year-old ungrazed grasslands in northern Germany. Each of the three grassland habitats differing in grazing intensity was replicated six times. We related diverse taxa such as grasshoppers, butterfly adults and lepidopteran larvae, and trap-nesting solitary bees and wasps to vegetation structure. There was an increase of species richness and abundance from pastures to ungrazed grasslands. The percentage of parasitism of the most abundant trap-nesting species, the digger-wasp (Â Â Â Trypoxylon figulus), was also higher on ungrazed grasslands. Decreased grazing on pastures enhanced species richness for adult butterflies only, whereas the abundance of adult butterflies, solitary bees and wasps, and their natural enemies increased. Although the differences in insect diversity between pastures and ungrazed grassland could be attributed to a greater vegetation height and heterogeneity (Â bottom-up effects) on ungrazed areas, the differences between intensively and extensively grazed pastures could not be explained by changes in vegetation characteristics. Hence, intensive grazing appeared to affect the insect communities through the disruption of plant-insect interactions. A mosaic of extensively grazed grassland and grassland left ungrazed for a few years may be a good means by which to maintain biodiversity and the strength of trophic interactions. El mantenimiento de pastizales como hÃ¡bitats distintos depende del manejo regular, generalmente, por medio de pastoreo o segado, pero se sabe que la diversidad de especies declina con el incremento de intensidad de manejo. La reducciÃ³n de la intensidad de manejo puede ser una herramienta Ãºtil para la conservaciÃ³n a largo plazo de la biodiversidad de pastizales. Analizamos la diversidad florÃ­stica y faunÃ­stica en pastizales pastoreados intensiva y extensivamente (no intensivos) y en pastizales de 5 a 10 aÃ±os no pastoreados en el norte de Alemania. Cada uno de los tres hÃ¡bitats de pastizal diferentes en el grado de pastoreo fue replicado seis veces. Relacionamos diversos taxones como chapulines, mariposas adultas, larvas de lepidÃ³pteros y abejas y avispas solitarias con la estructura de la vegetaciÃ³n. Hubo un incremento en la riqueza y abundancia de especies de pastizales pastoreados a no pastoreados. El porcentaje de parasitismo de la especie de avispa mÃ¡s abundante (Â Trypoxylon figulus) tambiÃ©n fue mayor en pastizales no pastoreados. La reducciÃ³n del pastoreo incrementÃ³ la riqueza de especies de mariposas adultas solamente, mientras que incrementÃ³ la abundancia de mariposas adultas, abejas y avispas solitarias y sus enemigos naturales. Aunque las diferencias en la diversidad de insectos entre pastizales pastoreados y no pastoreados pudiera atribuirse a la mayor altura de la vegetaciÃ³n y a la heterogeneidad (efectos abajo-arriba) en Ã¡reas no pastoreadas, las diferencias entre pastizales pastoreados intensiva y extensivamente no podrÃ­a explicarse por cambios en las caracterÃ­sticas de la vegetaciÃ³n. Por consiguiente, el pastoreo intensivo aparentemente afectÃ³ a las comunidades de insectos por la disrupciÃ³n de las interacciones planta-animal. Un mosaico de pastizales pastoreados extensivamente y pastizales sin pastoreo por varios aÃ±os puede ser una buena estrategia para mantener la biodiversidad y la vigencia de las interacciones trÃ³ficas."
335,68,11443,1,Reserve Selection Using Nonlinear Species Distribution Models,"Reserve design is concerned with optimal selection of sites for new conservation areas. Spatial reserve design explicitly considers the spatial pattern of the proposed reserve network and the effects of that pattern on reserve cost and/or ability to maintain species there. The vast majority of reserve selection formulations have assumed a linear problem structure, which effectively means that the biological value of a potential reserve site does not depend on the pattern of selected cells. However, spatial population dynamics and autocorrelation cause the biological values of neighboring sites to be interdependent. Habitat degradation may have indirect negative effects on biodiversity in areas neighboring the degraded site as a result of, for example, negative edge effects or lower permeability for animal movement. In this study, I present a formulation and a spatial optimization algorithm for nonlinear reserve selection problems in grid-based landscapes that accounts for interdependent site values. The method is demonstrated using habitat maps and nonlinear habitat models for threatened birds in the Netherlands, and it is shown that near-optimal solutions are found for regions consisting of up to hundreds of thousands grid cells, a landscape size much larger than those commonly attempted even with linear reserve selection formulations."
336,68,11452,1,From Individual Behavior to Metapopulation Dynamics: Unifying the Patchy Population and Classic Metapopulation Models.,"Spatially structured populations in patchy habitats show much variation in migration rate, from patchy populations in which individuals move repeatedly among habitat patches to classic metapopulations with infrequent migration among discrete populations. To establish a common framework for population dynamics in patchy habitats, we describe an individual-based model (IBM) involving a diffusion approximation of correlated random walk of individual movements. As an example, we apply the model to the Glanville fritillary butterfly (Melitaea cinxia) inhabiting a highly fragmented landscape. We derive stochastic patch occupancy model (SPOM) approximations for the IBMs assuming pure demographic stochasticity, uncorrelated environmental stochasticity, or completely correlated environmental stochasticity in local dynamics. Using realistic parameter values for the Glanville fritillary, we show that the SPOMs mimic the behavior of the IBMs well. The SPOMs derived from IBMs have parameters that relate directly to the life history and behavior of individuals, which is an advantage for model interpretation and parameter estimation. The modeling approach that we describe here provides a unified framework for patchy populations with much movements among habitat patches and classic metapopulations with infrequent movements."
337,68,11461,1,A Multispecies Approach to Ecological Valuation and Conservation.,"Abstract: The conservation of ecosystems focuses on evaluating individual sites or landscapes based on their component species. To produce a map of conservation values, we developed a method to weight habitat-suitability maps for individual species by species-specific extinction risks. The value of a particular site reflects the importance and magnitude of the threats facing the component species of the ecological community. We applied this approach to a set of species from the California Gap Analysis Project. The resulting map of multispecies conservation values identified the areas with the best habitat for the species most vulnerable to extinction. These methods are flexible and can accommodate the quantity and quality of data available for each individual species in both the development of the habitat-suitability maps and the estimation of the extinction risks. Additionally, the multispecies conservation value can accommodate specific conservation goals, such as preservation of local endemics, making it useful for prioritizing conservation and management actions. This approach provides an estimate of the ecological worth of a site based on habitat characteristics and quantitative models in terms of all the ecological components of a site, rather than a single threatened or endangered species. Una Aproximacion Multiespecifica para la Valoracion Ecologica y la Conservacion Resumen: La conservacion de los ecosistemas se enfoca en la evaluacion de sitios individuales o paisajes en base a las especies que lo componen. Para producir un mapa de valores para la conservacion, desarrollamos un metodo que valora mapas de aptitud del habitat para especies a nivel individual en base a los riesgos de extincion especie-especificos. El valor de un sitio en particular refleja la importancia y la magnitud de las amenazas que enfrentan las especies que componen la comunidad ecologica. Aplicamos esta metodologia a un grupo de especies del Proyecto de Analisis de Aberturas de California. El mapa de valores de conservacion para multiples especies resultante identifico las areas con el mejor habitat para las especies mas vulnerables a la extincion. Estos metodos son flexibles y pueden abarcar la cantidad y calidad de los datos disponibles para cada especie individual tanto para el desarrollo de mapas de aptitud del habitat, como para la eliminacion de los riesgos de extincion. Ademas, los valores de conservacion multi-especie pueden abarcar metas especificas de conservacion, como lo es la preservacion de endemias locales, haciendolos utiles para priorizar las acciones de conservacion y manejo. Esta metodologia provee una estimacion del merito ecologico de un sitio en base a las caracteristicas y modelos cuantitativos en terminos de todos los componentes ecologicos de un sitio, y no en una sola especie amenazada o en peligro."
338,68,11470,1,Evaluating great smoky mountains national park as a population source for the wood thrush.,": The prevailing fragmentation paradigm predicts that large, intact forests are acting as population sources for Neotropical migrant landbirds. We used the Wood Thrush (&#8201;&#8201;Hylocichla mustelina) as a model for evaluating the role Great Smoky Mountains National Park (the largest national park in the eastern United States) may play in maintaining regional songbird populations. We estimated the annual productivity of Wood Thrushes in the park by combining observations on the birds' distribution, abundance, and productivity with estimates of habitat availability. We estimated a breeding population of approximately 10,000 nesting pairs using habitat models developed from over 2500 point-count censuses conducted across the park. Data from 426 nests monitored from 1992 to 1997 produced a daily nest survival rate of 0.96. We estimated an annual fecundity of 2.76 fledglings per breeding pair, based on a model that incorporated the re-nesting behavior of Wood Thrushes. Results indicate that the park is producing approximately 3000 surplus female young each year. Daily nest survival rates were below those reported in other studies of Wood Thrushes in large forest tracts. The relatively high productivity of 3.31 nestlings per successful nest suggests that, in the absence of predation, the park provides high-quality nesting habitat for Wood Thrushes, but that it may also support a more diverse and abundant predator community than more disturbed or less contiguous sites. The difficulties of estimating the size of continental breeding bird populations make assessing the significance of the park within a regional landscape context problematic, but our estimates suggest that, although the park is functioning as a substantial population source on a local scale, its potential to sustain regional or continental Wood Thrush populations is limited. Our findings suggest that species such as the Wood Thrush are capable of moderate levels of surplus productivity in high-quality habitat, but that extensive areas of suitable habitat outside protected areas and other public lands will be required to sustain continental breeding populations."
339,68,11479,1,Choice of species-area function affects identification of hotspots.,"I tested the reliability of species-area curves for use in identifying hotspots, political or geographical regions of high species richness. On a species-area plot, hotspots are points (regions) that appear above the curve to a greater extent than other points. Because several different curves can be fit to species-area data, identification of hotspots may differ depending on the curve-fitting function used. I tested this hypothesis by comparing hotspots identified by the power function, the extreme value function, a linear function, and the exponential function. I examined several species-area data sets varying in size and in the presence of endemics. I defined hotspots as the highest 25% for small data sets and highest 10% and 25% for large data sets of standardized residuals from each function fitted to each data set. For some data sets, the functions agreed in identification of hotspots in that they identified 75% or more of the same hotspots. The extreme value function tended to identify hotspots not identified by the other three functions. For most data sets, the functions did not agree completely in identifying hotspots. Therefore, species-area curves should not be used as the sole means of identifying hotspots of species richness, although they can be used to examine the effect hotspot area has on richness for hotspots identified by other methods. DeterminÃ© la confiabilidad de curvas de especie-Ã¡rea a ser usadas en la identificaciÃ³n de zonas clave, regiones polÃ­ticas o geogrÃ¡ficas de alta riqueza de especies. En grÃ¡ficas de especie-Ã¡rea las zonas clave son puntos (regiones) que aparecen por arriba de la curva con una extenciÃ³n relativamente mayor que los otros puntos. Debido a que hay diferentes curvas que pueden ajustarse a datos de especie-Ã¡rea, la identificaciÃ³n de zonas clave puede diferir dependiendo de la funciÃ³n de ajuste de curva utilizada. ExaminÃ© esta hipÃ³tesis al comparar zonas clave identificadas por una funciÃ³n de potencia, la funciÃ³n de valor extremo, la funciÃ³n lineal y la funciÃ³n exponencial. ExaminÃ© series de datos de especie-Ã¡rea que variaron en tamaÃ±o y en presencia de especies endÃ©micas. DefinÃ­ las zonas clave como aquellas con porcentajes de residuos estandarizados de cada funciÃ³n ajustada en cada base de datos de 25% mayor para juegos de datos pequeÃ±os y 10% y 25% mayor para juegos de datos grandes. Para algunos juegos de datos, las funciones coincidieron en la identificaciÃ³n de zonas clave en las que las funciones identificaron 75% o mas de ellas. La funciÃ³n de valores extremos tendiÃ³ a identificar zonas clave que no fueron identificadas por las otras tres funciones. Para la mayorÃ­a de los juegos de datos las funciones no coincidieron completamente en la identificaciÃ³n de zonas clave. Por lo tanto, las curvas de especie-Ã¡rea no deberÃ­an ser usadas como medio Ãºnico para la identificaciÃ³n de zonas clave de riqueza de especies, aunque pueden ser utilizadas para examinar los efectos que las zonas clave tienen sobre la riqueza para zonas clave identificadas por otros mÃ©todos."
340,69,12466,1,{{I}ntroduction to {C}osmology},"**** _Introduction to Cosmology_ provides a rare combination of a solid foundation of the core physical concepts of cosmology and the most recent astronomical observations. The book is designed for advanced undergraduates or beginning graduate students and assumes no prior knowledge of general relativity. An emphasis is placed on developing the readers' physical insight rather than losing them with complex math. An approachable writing style and wealth of fresh and imaginative analogies from ""everyday"" physics are used to make the concepts of cosmology more accessible. **** The book is unique in that it not only includes recent major developments in cosmology, like the cosmological constant and accelerating universe, but also anticipates key developments expected in the next few years, such as detailed results on the cosmic microwave background. **** For anyone interested in cosmology or astronomy."
341,69,16211,1,Seeing Science,"The ability to represent scientific data and concepts visually is becomingincreasingly important due to the unprecedented exponential growth ofcomputational power during the present digital age. The data sets andsimulations scientists in all fields can now create are literally thousands oftimes as large as those created just 20 years ago. Historically successfulmethods for data visualization can, and should, be applied to today's huge datasets, but new approaches, also enabled by technology, are needed as well.Increasingly, ""modular craftsmanship"" will be applied, as relevantfunctionality from the graphically and technically best tools for a job arecombined as-needed, without low-level programming."
342,70,1231,1,Handbook on Ontologies (International Handbooks on Information Systems),"{An ontology is a description (like a formal specification of a program) of concepts and relationships that can exist for an agent or a community of agents. The concept is important for the purpose of enabling knowledge sharing and reuse. The Handbook on Ontologies&nbsp;provides a comprehensive overview of the current status and future prospectives of the field of ontologies. The handbook demonstrates standards that have been created recently, it surveys methods that have been developed and it shows how to bring both into practice of ontology infrastructures and applications that are the best of their kind.&nbsp;}"
343,70,3712,1,Grounding Spatial Named Entities for Information Extraction and Question Answering,"The task of named entity annotation of unseen text has recently been successfully automated with near-human performance. But the full task involves more than annotation, i.e. identifying the scope of each (continuous) text span and its class (such as place name). It also involves grounding the named entity (i.e. establishing its denotation with respect to the world or a model). The latter aspect has so far been neglected. In this paper, we show how geo-spatial named entities can be grounded using geographic coordinates, and how the results can be visualized using off-the-shelf software. We use this to compare a ?textual surrogate? of a newspaper story, with a ?visual surrogate? based on geographic coordinates."
344,70,4347,1,Relational Learning of Pattern-Match Rules for Information Extraction,"Information extraction is a form of shallow text processing that locates a specified set of relevant items in a natural-language document. Systems for this task require significant domain-specific knowledge and are time-consuming and difficult to build by hand, making them a good application for machine learning. We present an algorithm, RAPIER, that uses pairs of sample documents and filled templates to induce pattern-match rules that directly extract fillers for the slots in the template. RAPIER is a bottom-up learning algorithm that incorporates techniques from several inductive logic programming systems. We have implemented the algorithm in a system that allows patterns to have constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text. We present encouraging experimental results on two domains."
345,71,4102,1,Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling,"Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks."
346,71,8095,1,Automatically Generating Extraction Patterns from Untagged Text,"Many corpus-based natural language processing systems rely on text corpora that have been manually annotated with syntactic or semantic tags. In particular, all previous dictionary construction systems for information extraction have used an annotated training corpus or some form of annotated input. We have developed a system called AutoSlog-TS that creates dictionaries of extraction patterns using only untagged text. AutoSlog-TS is based on the AutoSlog system, which generated extraction patterns using annotated text and a set of heuristic rules. By adapting AutoSlog and combining it with statistical techniques, we eliminated its dependency on tagged text. In experiments with the MUC-4 terrorism domain, AutoSlogTS created a dictionary of extraction patterns that performed comparably to a dictionary created by AutoSlog, using only preclassified texts as input. Motivation The vast amount of text becoming available on-line offers new possibilities for conquering the knowledgeengineerin..."
347,72,15,1,Collective dynamics of 'small-world' networks.,"Networks of coupled dynamical systems have been used to model biological oscillators1, 2, 3, 4, Josephson junction arrays5,6, excitable media7, neural networks8, 9, 10, spatial games11, genetic control networks12 and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks 'rewired' to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them 'small-world' networks, by analogy with the small-world phenomenon13,14 (popularly known as six degrees of separation15). The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices."
348,72,9579,1,Nonequilibrium phase transition in the coevolution of networks and opinions,"Models of the convergence of opinion in social systems have been the subject of a considerable amount of recent attention in the physics literature. These models divide into two classes, those in which individuals form their beliefs based on the opinions of their neighbors in a social network of personal acquaintances, and those in which, conversely, network connections form between individuals of similar beliefs. While both of these processes can give rise to realistic levels of agreement between acquaintances, practical experience suggests that opinion formation in the real world is not a result of one process or the other, but a combination of the two. Here we present a simple model of this combination, with a single parameter controlling the balance of the two processes. We find that the model undergoes a continuous phase transition as this parameter is varied, from a regime in which opinions are arbitrarily diverse to one in which most individuals hold the same opinion. We characterize the static and dynamical properties of this transition."
349,73,7997,1,Design guidelines for landmarks to support navigation in virtual environments,"Unfamiliar, large-scale virtual environments are difficult to navigate. This paper presents design guidelines to ease navigation in such virtual environments. The guidelines presented here focus on the design and placement of landmarks in virtual environments. Moreover, the guidelines are based primarily on the extensive empirical literature on navigation in the real world. A rationale for this approach is provided by the similarities between navigational behavior in real and virtual environments."
350,74,2146,1,Real-time RT-PCR normalisation; strategies and considerations,"Real-time RT-PCR has become a common technique, no longer limited to specialist core facilities. It is in many cases the only method for measuring mRNA levels of vivo low copy number targets of interest for which alternative assays either do not exist or lack the required sensitivity. Benefits of this procedure over conventional methods for measuring RNA include its sensitivity, large dynamic range, the potential for high throughout as well as accurate quantification. To achieve this, however, appropriate normalisation strategies are required to control for experimental error introduced during the multistage process required to extract and process the RNA. There are many strategies that can be chosen; these include normalisation to sample size, total RNA and the popular practice of measuring an internal reference or housekeeping gene. However, these methods are frequently applied without appropriate validation. In this review we discuss the relative merits of different normalisation strategies and suggest a method of validation that will enable the measurement of biologically meaningful results."
351,74,10732,1,Identification of plant promoter constituents by analysis of local distribution of short sequences,"BACKGROUND:Plant promoter architecture is important for understanding regulation and evolution of the promoters, but our current knowledge about plant promoter structure, especially with respect to the core promoter, is insufficient. Several promoter elements including TATA box, and several types of transcriptional regulatory elements have been found to show local distribution within promoters, and this feature has been successfully utilized for extraction of promoter constituents from human genome.RESULTS:LDSS (Local Distribution of Short Sequences) profiles of short sequences along the plant promoter have been analyzed in silico, and hundreds of hexamer and octamer sequences have been identified as having localized distributions within promoters of Arabidopsis thaliana and rice. Based on their localization patterns, the identified sequences could be classified into three groups, pyrimidine patch (Y Patch), TATA box, and REG (Regulatory Element Group). Sequences of the TATA box group are consistent with the ones reported in previous studies. The REG group includes more than 200 sequences, and half of them correspond to known cis-elements. The other REG subgroups, together with about a hundred uncategorized sequences, are suggested to be novel cis-regulatory elements. Comparison of LDSS-positive sequences between Arabidopsis and rice has revealed moderate conservation of elements and common promoter architecture. In addition, a dimer motif named the YR Rule (C/T A/G) has been identified at the transcription start site (-1/+1). This rule also fits both Arabidopsis and rice promoters.CONCLUSION:LDSS was successfully applied to plant genomes and hundreds of putative promoter elements have been extracted as LDSS-positive octamers. Identified promoter architecture of monocot and dicot are well conserved, but there are moderate variations in the utilized sequences."
352,74,11625,1,Origin of phenotypes: Genes and transcripts,"10.1101/gr.6525007 While the concept of a gene has been helpful in defining the relationship of a portion of a genome to a phenotype, this traditional term may not be as useful as it once was. Currently, âgeneâ has come to refer principally to a genomic region producing a polyadenylated mRNA that encodes a protein. However, the recent emergence of a large collection of unannotated transcripts with apparently little protein coding capacity, collectively called transcripts of unknown function (TUFs), has begun to blur the physical boundaries and genomic organization of genic regions with noncoding transcripts often overlapping protein-coding genes on the same (sense) and opposite strand (antisense). Moreover, they are often located in intergenic regions, making the genic portions of the human genome an interleaved network of both annotated polyadenylated and nonpolyadenylated transcripts, including splice variants with novel 5â² ends extending hundreds of kilobases. This complex transcriptional organization and other recently observed features of genomes argue for the reconsideration of the term âgeneâ and suggests that transcripts may be used to define the operational unit of a genome."
353,74,12050,1,How gene order is influenced by the biophysics of transcription regulation,"Edited by Philip P. Green, University of Washington School of Medicine, Seattle, WA, and approved July 2, 2007 (received for review January 24, 2007)What are the forces that shape the structure of prokaryotic genomes: the order of genes, their proximity, and their orientation? Coregulation and coordinated horizontal gene transfer are believed to promote the proximity of functionally related genes and the formation of operons. However, forces that influence the structure of the genome beyond the level of a single operon remain unknown. Here, we show that the biophysical mechanism by which regulatory proteins search for their sites on DNA can impose constraints on genome structure. Using simulations, we demonstrate that rapid and reliable gene regulation requires that the transcription factor (TF) gene be close to the site on DNA the TF has to bind, thus promoting the colocalization of TF genes and their targets on the genome. We use parameters that have been measured in recent experiments to estimate the relevant length and times scales of this process and demonstrate that the search for a cognate site may be prohibitively slow if a TF has a low copy number and is not colocalized. We also analyze TFs and their sites in a number of bacterial genomes, confirm that they are colocalized significantly more often than expected, and show that this observation cannot be attributed to the pressure for coregulation or formation of selfish gene clusters, thus supporting the role of the biophysical constraint in shaping the structure of prokaryotic genomes. Our results demonstrate how spatial organization can influence timing and noise in gene expression. 10.1073/pnas.0700672104"
354,74,13569,1,The Transcriptional Landscape of the Yeast Genome Defined by RNA Sequencing,"The identification of untranslated regions, introns, and coding regions within an organism remains challenging. We developed a quantitative sequencing- based method called RNA- Seq for mapping transcribed regions, in which complementary DNA fragments are subjected to high- throughput sequencing and mapped to the genome. We applied RNA- Seq to generate a high- resolution transcriptome map of the yeast genome and demonstrated that most ( 74.5%) of the nonrepetitive sequence of the yeast genome is transcribed. We confirmed many known and predicted introns and demonstrated that others are not actively used. Alternative initiation codons and upstream open reading frames also were identified for many yeast genes. We also found unexpected 3'- end heterogeneity and the presence of many overlapping genes. These results indicate that the yeast transcriptome is more complex than previously appreciated."
355,74,16059,1,Lost in translation: an assessment and perspective for computational microRNA target identification.,MicroRNAs (miRNAs) are a class of short endogenously expressed RNA molecules that regulate gene expression by binding directly to the messenger RNA of protein coding genes. They have been found to confer a novel layer of genetic regulation in a wide range of biological processes. Computational miRNA target prediction remains one of the key means used to decipher the role of miRNAs in development and disease. Here we introduce the basic idea behind the experimental identification of miRNA targets and present some of the most widely used computational miRNA target identification programs. The review includes an assessment of the prediction quality of these programs and their combinations. Supplementary information: Supplementary data are available at Bioinformatics online.
356,74,16834,1,"More Than 1,001 Problems with Protein Domain Databases: Transmembrane Regions, Signal Peptides and the Issue of Sequence Homology","Large-scale genome sequencing gained general importance for life science because functional annotation of otherwise experimentally uncharacterized sequences is made possible by the theory of biomolecular sequence homology. Historically, the paradigm of similarity of protein sequences implying common structure, function and ancestry was generalized based on studies of globular domains. Having the same fold imposes strict conditions over the packing in the hydrophobic core requiring similarity of hydrophobic patterns. The implications of sequence similarity among non-globular protein segments have not been studied to the same extent; nevertheless, homology considerations are silently extended for them. This appears especially detrimental in the case of transmembrane helices (TMs) and signal peptides (SPs) where sequence similarity is necessarily a consequence of physical requirements rather than common ancestry. Thus, matching of SPs/TMs creates the illusion of matching hydrophobic cores. Therefore, inclusion of SPs/TMs into domain models can give rise to wrong annotations. More than 1001 domains among the 10,340 models of Pfam release 23 and 18 domains of SMART version 6 (out of 809) contain SP/TM regions. As expected, fragment-mode HMM searches generate promiscuous hits limited to solely the SP/TM part among clearly unrelated proteins. More worryingly, we show explicit examples that the scores of clearly false-positive hits, even in global-mode searches, can be elevated into the significance range just by matching the hydrophobic runs. In the PIR iProClass database v3.74 using conservative criteria, we find that at least between 2.1% and 13.6% of its annotated Pfam hits appear unjustified for a set of validated domain models. Thus, false-positive domain hits enforced by SP/TM regions can lead to dramatic annotation errors where the hit has nothing in common with the problematic domain model except the SP/TM region itself. We suggest a workflow of flagging problematic hits arising from SP/TM-containing models for critical reconsideration by annotation users."
357,75,1580,1,{A new approach to decoding life: systems biology},"Systems biology studies biological systems by systematically perturbing them (biologically, genetically, or chemically); monitoring the gene, protein, and informational pathway responses; integrating these data; and ultimately, formulating mathematical models that describe the structure of the system and its response to individual perturbations. The emergence of systems biology is described, as are several examples of specific systems approaches."
358,75,3149,1,Automatic clustering of orthologs and in-paralogs from pairwise species comparisons.,"Orthologs are genes in different species that originate from a single gene in the last common ancestor of these species. Such genes have often retained identical biological roles in the present-day organisms. It is hence important to identify orthologs for transferring functional information between genes in different organisms with a high degree of reliability. For example, orthologs of human proteins are often functionally characterized in model organisms. Unfortunately, orthology analysis between human and e.g. invertebrates is often complex because of large numbers of paralogs within protein families. Paralogs that predate the species split, which we call out-paralogs, can easily be confused with true orthologs. Paralogs that arose after the species split, which we call in-paralogs, however, are bona fide orthologs by definition. Orthologs and in-paralogs are typically detected with phylogenetic methods, but these are slow and difficult to automate. Automatic clustering methods based on two-way best genome-wide matches on the other hand, have so far not separated in-paralogs from out-paralogs effectively. We present a fully automatic method for finding orthologs and in-paralogs from two species. Ortholog clusters are seeded with a two-way best pairwise match, after which an algorithm for adding in-paralogs is applied. The method bypasses multiple alignments and phylogenetic trees, which can be slow and error-prone steps in classical ortholog detection. Still, it robustly detects complex orthologous relationships and assigns confidence values for both orthologs and in-paralogs. The program, called INPARANOID, was tested on all completely sequenced eukaryotic genomes. To assess the quality of INPARANOID results, ortholog clusters were generated from a dataset of worm and mammalian transmembrane proteins, and were compared to clusters derived by manual tree-based ortholog detection methods. This study led to the identification with a high degree of confidence of over a dozen novel worm-mammalian ortholog assignments that were previously undetected because of shortcomings of phylogenetic methods.A WWW server that allows searching for orthologs between human and several fully sequenced genomes is installed at http://www.cgb.ki.se/inparanoid/. This is the first comprehensive resource with orthologs of all fully sequenced eukaryotic genomes. Programs and tables of orthology assignments are available from the same location."
359,75,4963,1,The third dimension for protein interactions and complexes.,"Interaction discovery methods, such as the two-hybrid system and affinity purification, suggest thousands of proteinâprotein interactions. Structural biology provides atomic details for many interactions but, to date, there has been limited discussion of how these two fields complement each other. Here, we apply a structural perspective to interpret interactions discovered by different techniques. This perspective reveals indirect interactions in two-hybrid systems, instances where molecular labels might obstruct interfaces, and possible explanations for why certain promiscuous proteins interact with many others. It also highlights that some methods favour tight complexes whereas others favour interactions of a more transient nature. We conclude by discussing how a combination of interaction discovery and structural biology will enhance our understanding of complex cellular processes."
360,75,7305,1,"Orthologs, paralogs, and evolutionary genomics.","Orthologs and paralogs are two fundamentally different types of homologous genes that evolved, respectively, by vertical descent from a single ancestral gene and by duplication. Orthology and paralogy are key concepts of evolutionary genomics. A clear distinction between orthologs and paralogs is critical for the construction of a robust evolutionary classification of genes and reliable functional annotation of newly sequenced genomes. Genome comparisons show that orthologous relationships with genes from taxonomically distant species can be established for the majority of the genes from each sequenced genome. This review examines in depth the definitions and subtypes of orthologs and paralogs, outlines the principal methodological approaches employed for identification of orthology and paralogy, and considers evolutionary and functional implications of these concepts. Expected online publication date for the Annual Review of Genetics Volume 39 is November 10, 2005. Please see http://www.annualreviews.org/catalog/pub_dates.asp for revised estimates."
361,75,10205,1,Relating three-dimensional structures to protein networks provides evolutionary insights.,"Most studies of protein networks operate on a high level of abstraction, neglecting structural and chemical aspects of each interaction. Here, we characterize interactions by using atomic-resolution information from three-dimensional protein structures. We find that some previously recognized relationships between network topology and genomic features (e.g., hubs tending to be essential proteins) are actually more reflective of a structural quantity, the number of distinct binding interfaces. Subdividing hubs with respect to this quantity provides insight into their evolutionary rate and indicates that additional mechanisms of network growth are active in evolution (beyond effective preferential attachment through gene duplication)."
362,75,12688,1,Choosing BLAST options for better detection of orthologs as reciprocal best hits,"Motivation: The analyses of the increasing number of genome sequences requires shortcuts for the detection of orthologs, such as Reciprocal Best Hits (RBH), where orthologs are assumed if two genes each in a different genome find each other as the best hit in the other genome. Two BLAST options seem to affect alignment scores the most, and thus the choice of a best hit: the filtering of low information sequence segments and the algorithm used to produce the final alignment. Thus, we decided to test whether such options would help better detect orthologs.  Results: Using Escherichia coli K12 as an example, we compared the number and quality of orthologs detected as RBH. We tested four different conditions derived from two options: filtering of low-information segments, hard (default) versus soft; and alignment algorithm, default (based on matching words) versus Smith-Waterman. All options resulted in significant differences in the number of orthologs detected, with the highest numbers obtained with the combination of soft filtering with Smith-Waterman alignments. We compared these results with those of Reciprocal Shortest Distances (RSD), supposed to be superior to RBH because it uses an evolutionary measure of distance, rather than BLAST statistics, to rank homologs and thus detect orthologs. RSD barely increased the number of orthologs detected over those found with RBH. Error estimates, based on analyses of conservation of gene order, found small differences in the quality of orthologs detected using RBH. However, RSD showed the highest error rates. Thus, RSD have no advantages over RBH.  Availability: Orthologs detected as Reciprocal Best Hits using soft masking and Smith-Waterman alignments can be downloaded from http://popolvuh.wlu.ca/Orthologs.  Contact: gmoreno@wlu.ca 10.1093/bioinformatics/btm585"
363,75,13867,1,InteroPORC: automated inference of highly conserved protein interaction networks.,"MOTIVATION: Protein-protein interaction networks provide insights into the relationships between the proteins of an organism thereby contributing to a better understanding of cellular processes. Nevertheless, large-scale interaction networks are available for only a few model organisms. Thus, interologs are useful for a systematic transfer of protein interaction networks between organisms. However, no standard tool is available so far for that purpose. RESULTS: In this study, we present an automated prediction tool developed for all sequenced genomes available in Integr8. We also have developed a second method to predict protein-protein interactions in the widely used cyanobacterium Synechocystis. Using these methods, we have constructed a new network of 8783 inferred interactions for Synechocystis. AVAILABILITY: InteroPORC is open-source, downloadable and usable through a web interface at http://biodev.extra.cea.fr/interoporc/."
364,75,15264,1,TopHat: discovering splice junctions with RNA-Seq,"Motivation: A new protocol for sequencing the messenger RNA in a cell, known as RNA-Seq, generates millions of short sequence fragments in a single run. These fragments, or Ã¢ÂÂreadsÃ¢ÂÂ, can be used to measure levels of gene expression and to identify novel splice variants of genes. However, current software for aligning RNA-Seq data to a genome relies on known splice junctions and cannot identify novel ones. TopHat is an efficient read-mapping algorithm designed to align reads from an RNA-Seq experiment to a reference genome without relying on known splice sites.Results: We mapped the RNA-Seq reads from a recent mammalian RNA-Seq experiment and recovered more than 72% of the splice junctions reported by the annotation-based software from that study, along with nearly 20 000 previously unreported junctions. The TopHat pipeline is much faster than previous systems, mapping nearly 2.2 million reads per CPU hour, which is sufficient to process an entire RNA-Seq experiment in less than a day on a standard desktop computer. We describe several challenges unique to ab initio splice site discovery from RNA-Seq reads that will require further algorithm development.Availability: TopHat is free, open-source software available from http://tophat.cbcb.umd.eduContact: cole@cs.umd.eduSupplementary information: Supplementary data are available at Bioinformatics online."
365,76,3886,1,Mining Version Histories to Guide Software Changes,"We apply data mining to version histories in order to guide programmers along related changes: ""Programmers who changed these functions also changed....â Given a set of existing changes, the mined association rules 1) suggest and predict likely further changes, 2) show up item coupling that is undetectable by program analysis, and 3) can prevent errors due to incomplete changes. After an initial change, our ROSE prototype can correctly predict further locations to be changed; the best predictive power is obtained for changes to existing software. In our evaluation based on the history of eight popular open source projects, ROSE's topmost three suggestions contained a correct location with a likelihood of more than 70 percent."
366,77,1887,1,Protein-protein interactions: structurally conserved residues distinguish between binding sites and exposed protein surfaces.,"Polar residue hot spots have been observed at protein-protein binding sites. Here we show that hot spots occur predominantly at the interfaces of macromolecular complexes, distinguishing binding sites from the remainder of the surface. Consequently, hot spots can be used to define binding epitopes. We further show a correspondence between energy hot spots and structurally conserved residues. The number of structurally conserved residues, particularly of high ranking energy hot spots, increases with the binding site contact size. This finding may suggest that effectively dispersing hot spots within a large contact area, rather than compactly clustering them, may be a strategy to sustain essential key interactions while still allowing certain protein flexibility at the interface. Thus, most conserved polar residues at the binding interfaces confer rigidity to minimize the entropic cost on binding, whereas surrounding residues form a flexible cushion. Furthermore, our finding that similar residue hot spots occur across different protein families suggests that affinity and specificity are not necessarily coupled: higher affinity does not directly imply greater specificity. Conservation of Trp on the protein surface indicates a highly likely binding site. To a lesser extent, conservation of Phe and Met also imply a binding site. For all three residues, there is a significant conservation in binding sites, whereas there is no conservation on the exposed surface. A hybrid strategy, mapping sequence alignment onto a single structure illustrates the possibility of binding site identification around these three residues. [Journal Article; In English; United States]"
367,77,11651,1,Interaction-site prediction for protein complexes: a critical assessment.,"MOTIVATION: Proteins function through interactions with other proteins and biomolecules. Protein-protein interfaces hold key information toward molecular understanding of protein function. In the past few years, there have been intensive efforts in developing methods for predicting protein interface residues. A review that presents the current status of interface prediction and an overview of its applications and project future developments is in order. SUMMARY: Interface prediction methods rely on a wide range of sequence, structural and physical attributes that distinguish interface residues from non-interface surface residues. The input data are manipulated into either a numerical value or a probability representing the potential for a residue to be inside a protein interface. Predictions are now satisfactory for complex-forming proteins that are well represented in the Protein Data Bank, but less so for under-represented ones. Future developments will be directed at tackling problems such as building structural models for multi-component structural complexes."
368,78,10540,1,"First, Break All the Rules: What the World's Greatest Managers Do Differently","{Marcus Buckingham and Curt Coffman expose the fallacies of standard management thinking in <i>First, Break All the Rules: What the World's Greatest Managers Do Differently</i>. In seven chapters, the two consultants for the Gallup Organization debunk some dearly held notions about management, such as ""treat people as you like to be treated""; ""people are capable of almost anything""; and ""a manager's role is diminishing in today's economy."" ""Great managers are revolutionaries,"" the authors write. ""This book will take you inside the minds of these managers to explain why they have toppled conventional wisdom and reveal the new truths they have forged in its place.""<p>  The authors have culled their observations from more than 80,000 interviews conducted by Gallup during the past 25 years. Quoting leaders such as basketball coach Phil Jackson, Buckingham and Coffman outline ""four keys"" to becoming an excellent manager: Finding the right fit for employees, focusing on strengths of employees, defining the right results, and selecting staff for talent--not just knowledge and skills. <I>First, Break All the Rules</I> offers specific techniques for helping people perform better on the job. For instance, the authors show ways to structure a trial period for a new worker and how to create a pay plan that rewards people for their expertise instead of how fast they climb the company ladder. ""The point is to focus people toward performance,"" they write. ""The manager is, and should be, totally responsible for this."" Written in plain English and well organized, this book tells you exactly how to improve as a supervisor. <i>--Dan Ring</i>} {<P> The greatest managers in the world seem to have little in common. They differ in sex, age, and race. They employ vastly different styles and focus on different goals. Yet despite their differences, great managers share one common trait: They do not hesitate to break virtually every rule held sacred by conventional wisdom. They do not believe that, with enough training, a person can achieve anything he sets his mind to. They do not try to help people overcome their weaknesses. They consistently disregard the golden rule. And, yes, they even play favorites. This amazing book explains why. <P> Marcus Buckingham and Curt Coffman of the Gallup Organization present the remarkable findings of their massive in-depth study of great managers across a wide variety of situations. Some were in leadership positions. Others were front-line supervisors. Some were in Fortune 500 companies; others were key players in small, entrepreneurial companies. Whatever their situations, the managers who ultimately became the focus of Gallup's research were invariably those who excelled at turning each employee's talent into performance. <P> In today's tight labor markets, companies compete to find and keep the best employees, using pay, benefits, promotions, and training. But these well-intentioned efforts often miss the mark. The front-line manager is the key to attracting and retaining talented employees. No matter how generous its pay or how renowned its training, the company that lacks great front-line managers will suffer. Buckingham and Coffman explain how the best managers select an employee for talent rather than for skills or experience; how they set expectations for him or her -- they define the right outcomes rather than the right steps; how they motivate people -- they build on each person's unique strengths rather than trying to fix his weaknesses; and, finally, how great managers develop people -- they find the right fit for each person, not the next rung on the ladder. And perhaps most important, this research -- which initially generated thousands of different survey questions on the subject of employee opinion -- finally produced the twelve simple questions that work to distinguish the strongest departments of a company from all the rest. This book is the first to present this essential measuring stick and to prove the link between employee opinions and productivity, profit, customer satisfaction, and the rate of turnover. <P> There are vital performance and career lessons here for managers at every level, and, best of all, the book shows you how to apply them to your own situation.}"
369,78,13636,1,The Artist's Way,"{NOW AVAILABLE _ Digitally remastered, and on CD for the first time <br><br> Read by the author} {With the basic principle that creative expression is the natural direction of life, Julia Cameron and Mark Bryan lead you through a comprehensive twelve-week program to recover your creativity from a variety of blocks, including limiting beliefs, fear, self-sabotage, jealousy, guilt, addictions, and other inhibiting forces, replacing them with artistic confidence and productivity.<p> This book links creativity to spirituality by showing how to connect with the creative energies of the universe, and has, in the four years since its publication, spawned a remarkable number of support groups for artists dedicated to practicing the exercises it contains. }"
370,78,13653,1,A Wolf at the Table: A Memoir of My Father,"{<DIV><DIV><DIV>When Augusten Burroughs was small, his father was a shadowy presence in his life: a form on the stairs, a cough from the basement, a silent figure smoking a cigarette in the dark. As Augusten grew older, something sinister within his father began to unfurl. Something dark and secretive that could not be named.Betrayal after shocking betrayal ensued, and Augustenâs childhood was over. The kind of father he wanted didnât exist for him. This father was distant, aloof, uninterested.<BR></DIV><DIV>With A Wolf at the Table, Augusten Burroughs makes a quantum leap into untapped emotional terrain: the radical pendulum swing between love and hate, the unspeakably terrifying relationship between father and son. Told with scorching honesty and penetrating insight, it is a story for anyone who has ever longed for unconditional love from a parent. <BR><BR>In an innovative collaboration, four musiciansâ<B>Patti Smith</B>, <B>Ingrid Michaelson</B>, <B>Tegan Quin</B>, and <B>Sea Wolf</B>âcomposed and recorded original and exclusive songs for the audiobook of <I>A Wolf at the Table</I>. Augusten himself was deeply involved in the scoring and sound design for the audiobook program, explaining his vision for the project as âa truly cinematic, theatrical experience for the listener in which the musical score reflects what is happening in the story throughout.â Though harrowing and brutal, it will ultimately leave you buoyed with the profound joy of simply being alive. Itâs a listening experience that is not to be missed.</DIV></DIV></DIV>} {Amazon Significant Seven, April 2008:  When I started reading <i>A Wolf at the Table</i>, I thought I knew what to expect. Augusten Burroughs captures intense experience with an inexplicably cool remove, imparting a stillness and purity to emotions that would likely run amok in anyone else's hands. I love this quality of his writing, and it's present in full force in this memoir of a childhood spent in thrall to a predatory and deeply unpredictable father. What I wasn't prepared for was the suspense--the dread-filled, nearly sonorous waiting for the worst to happen. An artful sort of bait-and-switch happens in the telling: Burroughs brings you to the brink of a terrible catharsis more than once, but the break in tension never comes. It is profoundly sad, remarkably tender, and fueled by a sense of love and reverence that only a child knows. <i>--Anne Bartholomew</i><br/><br/>}"
371,78,13662,1,The Essays of Warren Buffett : Lessons for Corporate America,"{The definitive work concerning Warren Buffett and intelligent investment philosophy, this is a collection of Buffett's letters to the shareholders of Berkshire Hathaway written over the past few decades that together furnish an enormously valuable informal education. The letters distill in plain words all the basic principles of sound business practices. They are arranged and introduced by a leading apostle of the ""value"" school and noted author, Lawrence Cunningham. Here in one place are the priceless pearls of business and investment wisdom, woven into a delightful narrative on the major topics concerning both managers and investors. These timeless lessons are ever-more important in the current environment.} {Buffett, the Bard of Omaha, is a genuine American folk hero, if folk heroes are allowed to build fortunes worth upward of \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\$15 billion. He's great at homespun metaphor, but behind those catchy phrases is a reservoir of financial acumen that's generally considered the best of his generation. For example, in an essay on CEO stock options, he writes, ""Negotiating with one's self seldom produces a barroom brawl."" This is his way of saying that an executive who can give himself compensation totally disproportionate to his performance surely will. There are uncountable gems of financial wisdom to be harvested from these essays, taken from the annual reports he writes for Berkshire Hathaway, his holding company. Just to pick one more, here's a now-famous line about those he competes with when making stock-market investments: ""What could be more advantageous in an intellectual contest--whether it be chess, bridge, or stock selection--than to have opponents who have been taught that thinking is a waste of energy?""<p> While Buffett has a policy of seldom commenting on stocks he owns--he feels public pronouncements will only lead to the public's expectation of more public pronouncements, and he likes to keep his cards close to his vest--he loves to discuss the principles behind his investments. These come primarily from Ben Graham, under whom Buffett studied at Columbia University and for whom he worked in the 1950s. First among them is the idea that price is what you pay and value is what you get--and if you're a smart investor, the first will always be less than the second. In that sense, the value of the lessons learned from Buffett's <I>Essays</I> could be far greater than the book's price. <I>--Lou Schuler</I>}"
372,78,13671,1,The Very Hungry Caterpillar (MINIATURE EDITION),"{the beloved caterpillar once again eats its way through the pages on its way to becoming a beautiful butterfly.} {With its lovely, humorous illustrations and wonderful narrative about a hungry caterpillar growing up to be a beautiful butterfly, Eric Carle's story touches anyone who still has some growing to do. Along with reassuring repetition--""He was still hungry ...""--the book includes some wonderful interactive moments: what youngster can resist sticking a finger through that hole in the page as his ravenous friend makes his way through various delicacies?}"
373,78,13680,1,Read All About It!,"{<p> Tyrone rules the school! </p> <p> He's king of the monkey bars, a math machine, and a science whiz. </p> <p> The only thing he doesn't like about school is reading. Books are so boring! But when strange visitors start dropping by the classroom for story hour, Tyrone discovers there's more to books than just words on pages. </p> <p> Tyrone and his friends are swept up in a mysterious adventure that lands them in a most unexpected place. Mrs. Laura Bush and her daughter Jenna create a classroom adventure that will leave readers racing to the shelves! </p> <p> <small>A portion of proceeds to benefit Teach for America and The New Teacher Project. </small> </p>}"
374,78,13689,1,Mac OS X Leopard: The Missing Manual,"{With Leopard, Apple has unleashed the greatest version of Mac OS X yet, and David Pogue is back with another meticulous Missing Manual to cover the operating system with a wealth of detail. The new Mac OS X 10.5, better known as Leopard, is faster than its predecessors, but nothing's too fast for Pogue and this Missing Manual. It's just one of reasons this is the most popular computer book of all time. Mac OS X: The Missing Manual, Leopard Edition is the authoritative book for Mac users of all technical levels and experience. If you're new to the Mac, this book gives you a crystal-clear, jargon-free introduction to the Dock, the Mac OS X folder structure, and the Mail application. There are also mini-manuals on iLife applications such as iMovie, iDVD, and iPhoto, and a tutorial for Safari, Mac's web browser. This Missing Manual book is amusing and fun to read, but Pogue doesn't take his subject lightly. Which new Leopard features work well and which do not? What should you look for? What should you avoid? Mac OS X: The Missing Manual, Leopard Edition offers an objective and straightforward instruction for using: Leopard's totally revamped Finder Spaces to group your windows and organize your Mac tasks Quick Look to view files before you open them The Time Machine, Leopard's new backup feature Spotlight to search for and find anything in your Mac Front Row, a new way to enjoy music, photos, and videos Enhanced Parental Controls that come with Leopard Quick tips for setting up and configuring your Mac to make it your own There's something new on practically every page of this new edition, and David Pogue brings his celebrated wit and expertise to every one of them. Mac's brought a new catto town and Mac OS X: The Missing Manual, Leopard Edition is a great new way to tame it.}"
375,78,13698,1,On Food And Cooking,"{<P> <I>On Food and Cooking</I> is a unique blend of culinary lore and scientific explanation that examines food -- its history, its make-up, and its behavior when we cook it, cool it, dice it, age it, or otherwise prepare it for eating. Generously spiced with historical and literary anecdote, it covers all the major food categories, from meat and potatoes to sauce bÃ©arnaise and champagne. Easy-to-understand scientific explanations throw light on such mysteries as why you can whip cream but not milk; what makes white meat white; whether searing really seals in flavor; how to tell stale eggs from fresh; why ""fruits"" ripen and ""vegetables"" don't; how to save a sauce; what hops do; and what happens when you knead dough. A chapter on nutrition reveals that Americans have been obsessed with their diet since the 1800s and exposes the fallacies behind food fads past and present. There's a section on additives -- a not-so-new addition to food -- and taste and smell, our two pleasure-giving versions of the oldest sense on earth. With more than 200 illustrations, including extraordinary photographs of food taken through the electron microscope, this book will delight and fascinate anyone who has ever cooked, savored, or wondered about food.} {A classic tome of gastronomic science and lore, <B>On Food and Cooking</B> delivers an erudite discussion of table ingredients and their interactions with our bodies. Following the historical, literary, scientific and practical treatment of foodstuffs from dairy to meat to vegetables, McGee explains the nature of digestion and hunger before tackling basic ingredient components, cooking methods and utensils. He explains what happens when food spoils, why eggs are so nutritious and how alcohol makes us drunk. As fascinating as it is comprehensive, this is as practical, interesting and necessary for the cook as for the scholar.}"
376,78,13707,1,Mary Bell's Complete Dehydrator Cookbook,"{<p>Far from being a fad, food dyhydrating is one of the most ancient, effective, and nutritous ways of preserving food. Now, at last, there is a book that teaches absolutely everything there is to know about using an electric food dyhydrator to dry foods at home -- and gives more than 100 foolproof recipes for scrumptious snacks and meals made from dried foods.</p> <p>With this extraordinary book, you can learn how to cross junk food and expensive store-bought snacks off your family's shopping list -- and add to your cupboard homemade, preservative-free fruit leathers, candied apricots, beef (and fish) jerkies, ""sun"" dried tomotoes, corn chips, banana chips, and so much more!</p> <p>Mary Bell gives specific techniques and instructions for preparing every kind of fruit (from apples to watermelon) and vegetable (from asparagus to zucchini). She also provides important shopping tips for buying an electric food dehydrator. The recipes for cooked meals (including mushroom soup, sloppy joes, pesto, and moist banana bread) will make this book a kitchen classic. And recipes for lightweight, filling trail snacks mean that the book will travel, too.</p> <p>Additional chapters explain to how make herb seasonings, granolas, celery powder, cosmetics, dried fruit sugars, potpourri -- and even pet treats!</p> <p>Food drying is an excellent way for gardeners to preserve their produce. It is a great way to make healthful snacks for the kids. It's perfect for the new wave of thrifty consumers who can't bear to spend dollars at health food stores for treats they cold make for pennies themselves. And food drying doesn't use chemicals or preservativesâso it's great for you and for the planet, too!</p>}"
377,78,13716,1,Pain Free: A Revolutionary Method for Stopping Chronic Pain,"{Starting today, you don't have to live in pain.<br><br>That is the revolutionary message of this breakthrough system for eliminating chronic pain without drugs, surgery, or expensive physical therapy. Developed by Pete Egoscue, a nationally renowned physiologist and sports injury consultant to some of today's top athletes, the Egoscue Method has an astounding 95 percent success rate. The key is a series of gentle exercises and carefully constructed stretches called E-cises. Inside you'll find detailed photographs and step-by-step instructions for dozens of e-cizes specifically designed to provide quick and lasting relief of:<br><br> Lower back pain, hip problems, sciatica, and bad knees<br> Carpal tunnel syndrome and even some forms of arthritis<br> Migraines and other headaches, stiff neck, fatigue, sinus problems, vertigo, and TMJ<br> Shin splints, varicose veins, sprained or weak ankles, and many foot ailments<br> Bursitis, tendinitis, and rotator cuff problems<br> Plus special preventive programs for maintaining health through the entire body.<br><br>With this book in hand, you're on your way to regaining the greatest gift of all: a pain-free body!<br><br>the help of Pete Egoscue's revolutionary program of quick <br>stretches and strength-building exercises, you can cure <br>chronic pain, and do it naturally. <br><br>Pete Egoscue has shown thousands of individuals, corporations, schools, and championship sports teams how to eliminate pain without investing in expensive ergonomic devices or resorting to surgery or drug therapies.Â Â His groundbreaking book, with nearly 50,000 hardcover copies sold, shows readers how to:<br><br>Relieve lower back pain<br>Improve hip problems, sciatica, and bad knees<br>Relieve migraines and other headaches, stiff neck, fatigue, sinus problems, vertigo, and TMJ<br>Relieve painful problems, like carpal tunnel syndrome, often misdiagnosed as arthritis<br>Prevent injuries and maintain health through stretching programs for the entire body<br><br>Filled with easy instructions, photos, and line illustrations throughout, this book will provide quick, effective pain relief. -->} {Pete Egoscue learned a lot about pain when, as a Marine officer, he was  wounded in Vietnam. He segued from patient to physical therapist, and now runs a  famous clinic in San Diego, where he claims he's helped 95 percent of his patients cure  chronic pain--including Jack Nicklaus and Charles Barkley, whose athletic careers he  helped prolong. At the heart of his program are stretches and motion exercises to restore  proper function to muscles and joints. His methods are often surprising and  counterintuitive. For example, for foot pain, he suggests a series of hip exercises. In fact,  this is one of the most startling books you'll read about the human organism. Egoscue has  strong opinions about how modern life is changing the way our bodies function, reducing  the tasks we must perform and thus reducing the functional range of motion of our  muscles and joints. Fortunately, he offers movement exercises to restore what nature  meant us to have.}"
378,78,13749,1,"The China Study: The Most Comprehensive Study of Nutrition Ever Conducted and the Startling Implications for Diet, Weight Loss and Long-Term Health","This exhaustive presentation of the findings from the China Study conclusively demonstrates the link between nutrition and heart disease, diabetes, and cancer. Referred to as the ""Grand Prix of epidemiology"" by _The New York Times_, this study examines more than 350 variables of health and nutrition with surveys from 6,500 adults in 65 counties, representing 2,500 counties across rural China and Taiwan. While revealing that proper nutrition can have a dramatic effect on reducing and reversing these ailments as well as obesity, this text calls into question the practices of many of the current dietary programs, such as the Atkins diet, that enjoy widespread popularity in the West. The impact of the politics of nutrition and the efforts of special interest groups on the creation and dissemination of public information on nutrition are also discussed."
379,78,13758,1,Eat to Live: The Revolutionary Formula for Fast and Sustained Weight Loss,"When Mehmet Oz or any of New York's leading doctors has a patient whose life depends on losing weight, they call on Joel Fuhrman, M.D. In EAT TO LIVE, Dr. Fuhrman offers his healthy, effective, and scientifically proven plan for shedding radical amounts of weight quickly, and keeping it off.Losing weight under Dr. Fuhrman's plan is not about willpower, it is about knowledge. The key to this revolutionary diet is the idea of nutrient density, as expressed by the simple formula Health=Nutrients/Calories. When the ratio of nutrients to calories is high, fat melts away and health is restored. Losing 20 pounds in two to three weeks is just the beginning. The more high-nutrient food Dr. Fuhrman's patients consume, the more they are satisfied with fewer calories, and the less they crave fat and high-calorie foods. Designed for people who must lose 50 pounds or more in a hurry, EAT TO LIVE works for every dieter, even those who want to lose as little as 10 pounds quickly. No willpower required-just knowledge!"
380,78,13771,1,"How to Retire Happy, Wild, and Free: Retirement Wisdom That You Won't Get from Your Financial Advisor","Retirement does not have to mean the end of lifeâin fact it can mean a whole new beginning to the life you never had time to explore. In HOW TO RETIRE HAPPY, WILD, AND FREE, best-selling author Ernie J. Zelinksi shows that the key to enjoying an active and satisfying retirement is dependent on much more than just having adequate financial resources. It means paying attention to all aspects of life, including leisure activities, creative pursuits, physical and mental well-being, and solid social support. With its friendly format, lively cartoons, and captivating quotations, Zelinskiâs guide offers inspirational advice on how to follow your dreams instead of someone elseâs, how to put your retirement in proper perspective, and how to enjoy life after work."
381,78,13780,1,Stillness Speaks,"Expanding on his mantra--""Get out of your head and into the moment""--Eckhart Tolle offers this new book on living in the now. _Stillness Speaks_ emphasises the art of ""inner stillness""--the place where thoughts, ego and attachments fall always and we are left only with what the moment has to offer: ""When you lose touch with inner stillness, you lose touch with yourself. When you lose touch with yourself, you lose touch with the world"".  Don't expect this to be a quick skim or even a straight-through read. Like his previous bestselling book _The Power of Now_, Tolle uses brief entries and numerous white spaces to give readers easy in-and-out access into enticing spiritual insights that expound on inner stillness, such as learning the difference between surrender and resignation, overcoming the fear death, and how to end suffering. In fact, this is designed to be an ongoing conversation. Pick it up any time or any place, but be sure to allow for plenty of breaks for serious contemplation. Even as you occasionally abandon the book, don't abandon the teachings, pleads Tolle. Embracing and practicing inner stillness is no longer a luxury, he writes,  > but a necessity if humankind is not to destroy itself. At the present time the dysfunction of the old consciousness and the arising of the new are both accelerating. Paradoxically things are getting worse and better at the same time, although ""the worse"" is more apparent because it makes so much noise.  Devotees who have read all of Tolle's books and audio tapes probably won't find new ideas or information here. But they may appreciate the refresher course--revisiting familiar concepts in a slightly different package. --_Gail Hudson, Amazon.com_"
382,78,14041,1,The Law of Attraction: The Basics Of The Teachings Of Abraham,"This book presents the powerful basics of the original Teachings of Abraham_._ Within these pages, youâll learn how all things, wanted and unwanted, are brought to you by this most powerful law of the universe, the_ Law of Attraction. _(that which is like unto itself is drawn). Youâve most likely heard the saying âLike attracts like,â âBirds of a feather flock together,â or âIt is done unto you as you believeâ (a belief is only a thought you keep thinking); and although the _Law of Attraction _has been alluded to by some of the greatest teachers in history, it has never before been explained in as clear and practical terms as in this latest book by _New York Times _best-selling authors, Esther and Jerry Hicks. Â Â Â Â Learn here about the omnipresent Laws that govern this Universe and how to make them work to your advantage. The understanding that youâll achieve by reading this book will take all the guesswork out of daily living. Youâll finally understand everything thatâs happening in your own life as well as in the lives of those youâre interacting with. This book will help you to joyously be, do, or have anything that you desire."
383,79,2795,1,Amazon.com Recommendations: Item-to-Item Collaborative Filtering,"Recommendation algorithms are best known for their use on e-commerce Web sites, where they use input about a customer's interests to generate a list of recommended items. Many applications use only the items that customers purchase and explicitly rate to represent their interests, but they can also use other attributes, including items viewed, demographic data, subject interests, and favorite artists. At Amazon.com, we use recommendation algorithms to personalize the online store for each customer. The store radically changes based on customer interests, showing programming titles to a software engineer and baby toys to a new mother. There are three common approaches to solving the recommendation problem: traditional collaborative filtering, cluster models, and search-based methods. Here, we compare these methods with our algorithm, which we call item-to-item collaborative filtering. Unlike traditional collaborative filtering, our algorithm's online computation scales independently of the number of customers and number of items in the product catalog. Our algorithm produces recommendations in real-time, scales to massive data sets, and generates high quality recommendations."
384,80,3776,1,A Global Geometric Framework for Nonlinear Dimensionality Reduction,"Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs-30,000 auditory nerve fibers or 10(6) optic nerve fibers-a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure."
385,81,857,1,Analysis of Panel Data,"{Panel data models have become increasingly popular among applied researchers due to their heightened capacity for capturing the complexity of human behavior, as compared to cross-sectional or time series data models.  This second edition represents a substantial revision of the highly successful first edition (1986). Recent advances in panel data research are presented in an accessible manner and are carefully integrated with the older material.  The thorough discussion of theory and the judicious use of empirical examples make this book useful to graduate students and advanced researchers in economics, business, sociology and political science.} {This book reviews the basic econometric methods that have been used to analyze panel data--data collected by observing a number of individuals over time. In analyzing this data, difficulties can arise due to unobserved individuals or time characteristics.  The focus of the book is twofold: correcting for these difficulties, and providing methods for improvement of the efficiency of the estimates.  The author draws together work on panel data from a number of different perspectives, including the econometric literature on specification analysis, the time series literature on dynamic models, and discrete choice literature.}"
386,81,10687,1,Principles of Critical Discourse Analysis,"This paper discusses some principles of critical discourse analysis, such as the explicit sociopolitical stance of discourse analysts, and a focus on dominance relations by elite groups and institutions as they are being enacted, legitimated or otherwise reproduced by text and talk. One of the crucial elements of this analysis of the relations between power and discourse is the patterns of access to (public) discourse for different social groups. Theoretically it is shown that in order to be able to relate power and discourse in an explicit way, we need the `cognitive interface' of models, knowledge, attitudes and ideologies and other social representations of the social mind, which also relate the individual and the social, and the micro- and the macro-levels of social structure. Finally, the argument is illustrated with an analysis of parliamentary debates about ethnic affairs. 10.1177/0957926593004002006"
387,82,563,1,A representation of the hazard rate of elapsed time in macaque area LIP.,"The capacity to anticipate the timing of environmental cues allows us to allocate sensory resources at the right time and prepare actions. Such anticipation requires knowledge of elapsed time and of the probability that an event will occur. Here we show that neurons in the parietal cortex represent the probability, as a function of time, that a salient event is likely to occur. Rhesus monkeys were trained to make eye movements to peripheral targets after a light dimmed. Within a block of trials, the 'go' times were drawn from either a bimodal or unimodal distribution of random numbers. Neurons in the lateral intraparietal area showed anticipatory activity that revealed an internal representation of both elapsed time and the probability that the 'go' signal was about to occur (termed the hazard rate). The results indicate that the parietal cortex contains circuitry for representing the time structure of environmental cues over a range of seconds."
388,82,604,1,Neural control of voluntary movement initiation.,"When humans respond to sensory stimulation, their reaction times tend to be long and variable relative to neural transduction and transmission times. The neural processes responsible for the duration and variability of reaction times are not understood. Single-cell recordings in a motor area of the cerebral cortex in behaving rhesus monkeys (Macaca mulatta) were used to evaluate two alternative mathematical models of the processes that underlie reaction times. Movements were initiated if and only if the neural activity reached a specific and constant threshold activation level. Stochastic variability in the rate at which neural activity grew toward that threshold resulted in the distribution of reaction times. This finding elucidates a specific link between motor behavior and activation of neurons in the cerebral cortex."
389,82,615,1,Phase locking of single neuron activity to theta oscillations during working memory in monkey extrastriate visual cortex.,"Working memory has been linked to elevated single neuron discharge in monkeys and to oscillatory changes in the human EEG, but the relation between these effects has remained largely unexplored. We addressed this question by measuring local field potentials and single unit activity simultaneously from multiple electrodes placed in extrastriate visual cortex while monkeys were performing a working memory task. We describe a significant enhancement in theta band energy during the delay period. Theta oscillations had a systematic effect on single neuron activity, with neurons emitting more action potentials near their preferred angle of each theta cycle. Sample-selective delay activity was enhanced if only action potentials emitted near the preferred theta angle were considered. Our results suggest that extrastriate visual cortex is involved in short-term maintenance of information and that theta oscillations provide a mechanism for structuring the recurrent interaction between neurons in different brain regions that underlie working memory."
390,82,627,1,Retinal ganglion cells act largely as independent encoders.,"Correlated firing among neurons is widespread in the visual system. Neighbouring neurons, in areas from retina to cortex, tend to fire together more often than would be expected by chance. The importance of this correlated firing for encoding visual information is unclear and controversial. Here we examine its importance in the retina. We present the retina with natural stimuli and record the responses of its output cells, the ganglion cells. We then use information theoretic techniques to measure the amount of information about the stimuli that can be obtained from the cells under two conditions: when their correlated firing is taken into account, and when their correlated firing is ignored. We find that more than 90% of the information about the stimuli can be obtained from the cells when their correlated firing is ignored. This indicates that ganglion cells act largely independently to encode information, which greatly simplifies the problem of decoding their activity."
391,82,646,1,"The Variable Discharge of Cortical Neurons: Implications for Connectivity, Computation, and Information Coding","Cortical neurons exhibit tremendous variability in the number and temporal distribution of spikes in their discharge patterns. Furthermore, this variability appears to be conserved over large regions of the cerebral cortex, suggesting that it is neither reduced nor expanded from stage to stage within a processing pathway. To investigate the principles underlying such statistical homogeneity, we have analyzed a model of synaptic integration incorporating a highly simplified integrate and fire mechanism with decay. We analyzed a ""high-input regime"" in which neurons receive hundreds of excitatory synaptic inputs during each interspike interval. To produce a graded response in this regime, the neuron must balance excitation with inhibition. We find that a simple integrate and fire mechanism with balanced excitation and inhibition produces a highly variable interspike interval, consistent with experimental data. Detailed information about the temporal pattern of synaptic inputs cannot be recovered from the pattern of output spikes, and we infer that cortical neurons are unlikely to transmit information in the temporal pattern of spike discharge. Rather, we suggest that quantities are represented as rate codes in ensembles of 50-100 neurons. These column-like ensembles tolerate large fractions of common synaptic input and yet covary only weakly in their spike discharge. We find that an ensemble of 100 neurons provides a reliable estimate of rate in just one interspike interval (10-50 msec). Finally, we derived an expression for the variance of the neural spike count that leads to a stable propagation of signal and noise in networks of neurons[---]that is, conditions that do not impose an accumulation or diminution of noise. The solution implies that single neurons perform simple algebra resembling averaging, and that more sophisticated computations arise by virtue of the anatomical convergence of novel combinations of inputs to the cortical column from external sources."
392,82,728,1,Role of experience and oscillations in transforming a rate code into a temporal code.,"In the vast majority of brain areas, the firing rates of neurons, averaged over several hundred milliseconds to several seconds, can be strongly modulated by, and provide accurate information about, properties of their inputs. This is referred to as the rate code. However, the biophysical laws of synaptic plasticity require precise timing of spikes over short timescales (<10 ms)1, 2. Hence it is critical to understand the physiological mechanisms that can generate precise spike timing in vivo, and the relationship between such a temporal code and a rate code. Here we propose a mechanism by which a temporal code can be generated through an interaction between an asymmetric rate code and oscillatory inhibition. Consistent with the predictions of our model, the rate3, 4 and temporal5-7 codes of hippocampal pyramidal neurons are highly correlated. Furthermore, the temporal code becomes more robust with experience. The resulting spike timing satisfies the temporal order constraints of hebbian learning. Thus, oscillations and receptive field asymmetry may have a critical role in temporal sequence learning."
393,82,761,1,Judgment under Uncertainty: Heuristics and Biases,"The thirty-five chapters in this book describe various judgmental heuristics and the biases they produce, not only in laboratory experiments but in important social, medical, and political situations as well. Individual chapters discuss the representativeness and availability heuristics, problems in judging covariation and control, overconfidence, multistage inference, social perception, medical diagnosis, risk perception, and methods for correcting and improving judgments under uncertainty. About half of the chapters are edited versions of classic articles; the remaining chapters are newly written for this book. Most review multiple studies or entire subareas of research and application rather than describing single experimental studies. This book will be useful to a wide range of students and researchers, as well as to decision makers seeking to gain insight into their judgments and to improve them."
394,82,813,1,Different time courses of learning-related activity in the prefrontal cortex and striatum,"To navigate our complex world, our brains have evolved a sophisticated ability to quickly learn arbitrary rules such as 'stop at red'. Studies in monkeys using a laboratory test of this capacity--conditional association learning--have revealed that frontal lobe structures (including the prefrontal cortex) as well as subcortical nuclei of the basal ganglia are involved in such learning. Neural correlates of associative learning have been observed in both brain regions, but whether or not these regions have unique functions is unclear, as they have typically been studied separately using different tasks. Here we show that during associative learning in monkeys, neural activity in these areas changes at different rates: the striatum (an input structure of the basal ganglia) showed rapid, almost bistable, changes compared with a slower trend in the prefrontal cortex that was more in accordance with slow improvements in behavioural performance. Also, pre-saccadic activity began progressively earlier in the striatum but not in the prefrontal cortex as learning took place. These results support the hypothesis that rewarded associations are first identified by the basal ganglia, the output of which 'trains' slower learning mechanisms in the frontal cortex."
395,82,1385,1,Adaptive Coding of Reward Value by Dopamine Neurons,"It is important for animals to estimate the value of rewards as accurately as possible. Because the number of potential reward values is very large, it is necessary that the brain's limited resources be allocated so as to discriminate better among more likely reward outcomes at the expense of less likely outcomes. We found that midbrain dopamine neurons rapidly adapted to the information provided by reward-predicting stimuli. Responses shifted relative to the expected reward value, and the gain adjusted to the variance of reward value. In this way, dopamine neurons maintained their reward sensitivity over a large range of reward values."
396,82,2111,1,Shifts in Selective Visual Attention: Towards the Underlying Neural Circuitry,"Psychophysical and physiological evidence indicates that the visual system of primates and humans has evolved a specialized processing focus moving across the visual scene. This study addresses the question of how simple networks of neuron-like elements can account for a variety of phenomena associated with this shift of selective visual attention. Specifically, we propose the following: (1) A number of elementary features, such as color, orientation, direction of movement, disparity etc. are represented in parallel in different topographical maps, called the early representation. (2) There exists a selective mapping from the early topographic representation into a more central non-topographic representation, such that at any instant the central representation contains the properties of only a single location in the visual scene, the selected location. We suggest that this mapping is the principal expression of early selective visual attention. One function of selective attention is to fuse information from different maps into one coherent whole. (3) Certain selection rules determine which locations will be mapped into the central representation. The major rule, using the conspicuity of locations in the early representation, is implemented using a so-called Winner-Take-All network. Inhibiting the selected location in this network causes an automatic shift towards the next most conspicious location. Additional rules are proximity and similarity preferences. We discuss how these rules can be implemented in neuron-like networks and suggest a possible role for the extensive back-projection from the visual cortex to the LGN."
397,82,2516,1,Choosing the greater of two goods: neural currencies for valuation and decision making,"To make adaptive decisions, animals must evaluate the costs and benefits of available options. The nascent field of neuroeconomics has set itself the ambitious goal of understanding the brain mechanisms that are responsible for these evaluative processes. A series of recent neurophysiological studies in monkeys has begun to address this challenge using novel methods to manipulate and measure an animalâs internal valuation of competing alternatives. By emphasizing the behavioural mechanisms and neural signals that mediate decision making under conditions of uncertainty, these studies might lay the foundation for an emerging neurobiology of choice behaviour."
398,82,3025,1,Action plans used in action observation,"How do we understand the actions of others? According to the direct matching hypothesis, action understanding results from a mechanism that maps an observed action onto motor representations of that action. Although supported by neurophysiological and brain-imaging studies, direct evidence for this hypothesis is sparse. In visually guided actions, task-specific proactive eye movements are crucial for planning and control. Because the eyes are free to move when observing such actions, the direct matching hypothesis predicts that subjects should produce eye movements similar to those produced when they perform the tasks. If an observer analyses action through purely visual means, however, eye movements will be linked reactively to the observed action. Here we show that when subjects observe a block stacking task, the coordination between their gaze and the actor's hand is predictive, rather than reactive, and is highly similar to the gaze-hand coordination when they perform the task themselves. These results indicate that during action observation subjects implement eye motor programs directed by motor representations of manual actions and thus provide strong evidence for the direct matching hypothesis."
399,82,3652,1,Midbrain dopamine neurons encode a quantitative reward prediction error signal.,"The midbrain dopamine neurons are hypothesized to provide a physiological correlate of the reward prediction error signal required by current models of reinforcement learning. We examined the activity of single dopamine neurons during a task in which subjects learned by trial and error when to make an eye movement for a juice reward. We found that these neurons encoded the difference between the current reward and a weighted average of previous rewards, a reward prediction error, but only for outcomes that were better than expected. Thus, the firing rate of midbrain dopamine neurons is quantitatively predicted by theoretical descriptions of the reward prediction error signal used in reinforcement learning models for circumstances in which this signal has a positive value. We also found that the dopamine system continued to compute the reward prediction error even when the behavioral policy of the animal was only weakly influenced by this computation."
400,82,4246,1,The neural basis of financial risk taking.,"Summary Investors systematically deviate from rationality when making financial decisions, yet the mechanisms responsible for these deviations have not been identified. Using event-related fMRI, we examined whether anticipatory neural activity would predict optimal and suboptimal choices in a financial decision-making task. We characterized two types of deviations from the optimal investment strategy of a rational risk-neutral agent as risk-seeking mistakes and risk-aversion mistakes. Nucleus accumbens activation preceded risky choices as well as risk-seeking mistakes, while anterior insula activation preceded riskless choices as well as risk-aversion mistakes. These findings suggest that distinct neural circuits linked to anticipatory affect promote different types of financial choices and indicate that excessive activation of these circuits may lead to investing mistakes. Thus, consideration of anticipatory neural mechanisms may add predictive power to the rational actor model of economic decision making."
401,82,4899,1,Optimal experimental design for event-related fMRI.,"An important challenge in the design and analysis of event-related or single-trial functional magnetic resonance imaging (fMRI) experiments is to optimize statistical efficiency, i.e., the accuracy with which the event-related hemodynamic response to different stimuli can be estimated for a given amount of imaging time. Several studies have suggested that using a fixed inter-stimulus-interval (ISI) of at least 15 sec results in optimal statistical efficiency or power and that using shorter ISIs results in a severe loss of power. In contrast, recent studies have demonstrated the feasibility of using ISIs as short as 500 ms while still maintaining considerable efficiency or power. Here, we attempt to resolve this apparent contradiction by a quantitative analysis of the relative efficiency afforded by different event-related experimental designs. This analysis shows that statistical efficiency falls off dramatically as the ISI gets sufficiently short, if the ISI is kept fixed for all trials. However, if the ISI is properly jittered or randomized from trial to trial, the efficiency improves monotonically with decreasing mean ISI. Importantly, the efficiency afforded by such variable ISI designs can be more than 10 times greater than that which can be achieved by fixed ISI designs. These results further demonstrate the feasibility of using identical experimental designs with fMRI and electro-/magnetoencephalography (EEG/MEG) without sacrificing statistical power or efficiency of either technique, thereby facilitating comparison and integration across imaging modalities. Hum. Brain Mapping 8:109-114, 1999. Â© 1999 Wiley-Liss, Inc."
402,82,5575,1,"Predicting How People Play Games: Reinforcement Learning in Experimental Games with Unique, Mixed Strategy Equilibria","We examine learning in all experiments we could locate involving 100 periods or more of games with a unique equilibnâum in mixed strategies, and in a new experiment. We study both the ex post ( âbest fitâ) akscrtptive power of learning models, and their ex ante predictive power, by simulating each experiment using parameters estimated from the other experiments. Even a one-parameter reinforcement learning model robustly outpelfonns the equilibrium predictions. Predictive power is improved by adding â yorgemngâ and âexperimentation, â or by allowing greater rationality as in probabilistic jictitious play. Implications for developing a low-rationality, cognitive game theory are discussed (JEL C72. C92 )"
403,82,6302,1,Neural Systems Responding to Degrees of Uncertainty in Human Decision-Making,"Much is known about how people make decisions under varying levels of probability (risk). Less is known about the neural basis of decision-making when probabilities are uncertain because of missing information (ambiguity). In decision theory, ambiguity about probabilities should not affect choices. Using functional brain imaging, we show that the level of ambiguity in choices correlates positively with activation in the amygdala and orbitofrontal cortex, and negatively with a striatal system. Moreover, striatal activity correlates positively with expected reward. Neurological subjects with orbitofrontal lesions were insensitive to the level of ambiguity and risk in behavioral choices. These data suggest a general neural circuit responding to degrees of uncertainty, contrary to decision theory."
404,82,6680,1,"Emotion and motivation: the role of the amygdala, ventral striatum, and prefrontal cortex.","Emotions are multifaceted, but a key aspect of emotion involves the assessment of the value of environmental stimuli. This article reviews the many psychological representations, including representations of stimulus value, which are formed in the brain during Pavlovian and instrumental conditioning tasks. These representations may be related directly to the functions of cortical and subcortical neural structures. The basolateral amygdala (BLA) appears to be required for a Pavlovian conditioned stimulus (CS) to gain access to the current value of the specific unconditioned stimulus (US) that it predicts, while the central nucleus of the amygdala acts as a controller of brainstem arousal and response systems, and subserves some forms of stimulus-response Pavlovian conditioning. The nucleus accumbens, which appears not to be required for knowledge of the contingency between instrumental actions and their outcomes, nevertheless influences instrumental behaviour strongly by allowing Pavlovian CSs to affect the level of instrumental responding (Pavlovian-instrumental transfer), and is required for the normal ability of animals to choose rewards that are delayed. The prelimbic cortex is required for the detection of instrumental action-outcome contingencies, while insular cortex may allow rats to retrieve the values of specific foods via their sensory properties. The orbitofrontal cortex, like the BLA, may represent aspects of reinforcer value that govern instrumental choice behaviour. Finally, the anterior cingulate cortex, implicated in human disorders of emotion and attention, may have multiple roles in responding to the emotional significance of stimuli and to errors in performance, preventing responding to inappropriate stimuli. Â© 2002 Elsevier Science Ltd. All rights reserved."
405,82,7052,1,Spike count reliability and the Poisson hypothesis.,"The variability of cortical activity in response to repeated presentations of a stimulus has been an area of controversy in the ongoing debate regarding the evidence for fine temporal structure in nervous system activity. We present a new statistical technique for assessing the significance of observed variability in the neural spike counts with respect to a minimal Poisson hypothesis, which avoids the conventional but troubling assumption that the spiking process is identically distributed across trials. We apply the method to recordings of inferotemporal cortical neurons of primates presented with complex visual stimuli. On this data, the minimal Poisson hypothesis is rejected: the neuronal responses are too reliable to be fit by a typical firing-rate model, even allowing for sudden, time-varying, and trial-dependent rate changes after stimulus onset. The statistical evidence favors a tightly regulated stimulus response in these neurons, close to stimulus onset, although not further away."
406,82,7934,1,Neurobiological Substrates of Dread,"Given the choice of waiting for an adverse outcome or getting it over with quickly, many people choose the latter. Theoretical models of decision-making have assumed that this occurs because there is a cost to waiting--i.e., dread. Using functional magnetic resonance imaging, we measured the neural responses to waiting for a cutaneous electric shock. Some individuals dreaded the outcome so much that, when given a choice, they preferred to receive more voltage rather than wait. Even when no decision was required, these extreme dreaders were distinguishable from those who dreaded mildly by the rate of increase of neural activity in the posterior elements of the cortical pain matrix. This suggests that dread derives, in part, from the attention devoted to the expected physical response and not simply from fear or anxiety. Although these differences were observed during a passive waiting procedure, they correlated with individual behavior in a subsequent choice paradigm, providing evidence for a neurobiological link between the experienced disutility of dread and subsequent decisions about unpleasant outcomes. 10.1126/science.1123721"
407,82,8576,1,Bayesian theories of conditioning in a changing world.,"The recent flowering of Bayesian approaches invites the re-examination of classic issues in behavior, even in areas as venerable as Pavlovian conditioning. A statistical account can offer a new, principled interpretation of behavior, and previous experiments and theories can inform many unexplored aspects of the Bayesian enterprise. Here we consider one such issue: the finding that surprising events provoke animals to learn faster. We suggest that, in a statistical account of conditioning, surprise signals change and therefore uncertainty and the need for new learning. We discuss inference in a world that changes and show how experimental results involving surprise can be interpreted from this perspective, and also how, thus understood, these phenomena help constrain statistical theories of animal and human learning."
408,82,9265,1,Chaos in neuronal networks with balanced excitatory and inhibitory activity,"Neurons in the cortex of behaving animals show temporally irregular spiking patterns. The origin of this irregularity and its implications for neural processing are unknown. The hypothesis that the temporal variability in the firing of a neuron results from an approximate balance between its excitatory and inhibitory inputs was investigated theoretically. Such a balance emerges naturally in large networks of excitatory and inhibitory neuronal populations that are sparsely connected by relatively strong synapses. The resulting state is characterized by strongly chaotic dynamics, even when the external inputs to the network are constant in time. Such a network exhibits a linear response, despite the highly nonlinear dynamics of single neurons, and reacts to changing external stimuli on time scales much smaller than the integration time constant of a single neuron."
409,82,10346,1,Solving the Distal Reward Problem through Linkage of STDP and Dopamine Signaling,"In Pavlovian and instrumental conditioning, reward typically comes seconds after reward-triggering actions, creating an explanatory conundrum known as Ã¢ÂÂdistal reward problemÃ¢ÂÂ: How does the brain know what firing patterns of what neurons are responsible for the reward if 1) the patterns are no longer there when the reward arrives and 2) all neurons and synapses are active during the waiting period to the reward? Here, we show how the conundrum is resolved by a model network of cortical spiking neurons with spike-timingÃ¢ÂÂdependent plasticity (STDP) modulated by dopamine (DA). Although STDP is triggered by nearly coincident firing patterns on a millisecond timescale, slow kinetics of subsequent synaptic plasticity is sensitive to changes in the extracellular DA concentration during the critical period of a few seconds. Random firings during the waiting period to the reward do not affect STDP and hence make the network insensitive to the ongoing activityÃ¢ÂÂthe key feature that distinguishes our approach from previous theoretical studies, which implicitly assume that the network be quiet during the waiting period or that the patterns be preserved until the reward arrives. This study emphasizes the importance of precise firing patterns in brain dynamics and suggests how a global diffusive reinforcement signal in the form of extracellular DA can selectively influence the right synapses at the right time."
410,82,11247,1,Use of simulated data sets to evaluate the fidelity of metagenomic processing methods,"Metagenomics is a rapidly emerging field of research for studying microbial communities. To evaluate methods presently used to process metagenomic sequences, we constructed three simulated data sets of varying complexity by combining sequencing reads randomly selected from 113 isolate genomes. These data sets were designed to model real metagenomes in terms of complexity and phylogenetic composition. We assembled sampled reads using three commonly used genome assemblers (Phrap, Arachne and JAZZ), and predicted genes using two popular gene-finding pipelines (fgenesb and CRITICA/GLIMMER). The phylogenetic origins of the assembled contigs were predicted using one sequence similarityâbased (blast hit distribution) and two sequence compositionâbased (PhyloPythia, oligonucleotide frequencies) binning methods. We explored the effects of the simulated community structure and method combinations on the fidelity of each processing step by comparison to the corresponding isolate genomes. The simulated data sets are available online to facilitate standardized benchmarking of tools for metagenomic analysis."
411,82,11715,1,When choice is demotivating: can one desire too much of a good thing?,"Current psychological theory and research affirm the positive affective and motivational consequences of having personal choice. These findings have led to the popular notion that the more choice, the better-that the human ability to manage, and the human desire for, choice is unlimited. Findings from 3 experimental studies starkly challenge this implicit assumption that having more choices is necessarily more intrinsically motivating than having fewer. These experiments, which were conducted in both field and laboratory settings, show that people are more likely to purchase gourmet jams or chocolates or to undertake optional class essay assignments when offered a limited array of 6 choices rather than a more extensive array of 24 or 30 choices. Moreover, participants actually reported greater subsequent satisfaction with their selections and wrote better essays when their original set of options had been limited. Implications for future research are discussed."
412,83,11200,1,The internet and social life,"The Internet is the latest in a series of technological breakthroughs in interpersonal communication, following the telegraph, telephone, radio, and television. It combines innovative features of its predecessors, such as bridging great distances and reaching a mass audience. However, the Internet has novel features as well, most critically the relative anonymity afforded to users and the provision of group venues in which to meet others with similar interests and values. We place the Internet in its historical context, and then examine the effects of Internet use on the user's psychological well-being, the formation and maintenance of personal relationships, group memberships and social identity, the workplace, and community involvement. The evidence suggests that while these effects are largely dependent on the particular goals that users bring to the interactionâsuch as self-expression, affiliation, or competitionâthey also interact in important ways with the unique qualities of the Internet communication situation."
413,84,2624,1,Information and Communication Technologies in Everyday Life : A Concise Introduction and Research Guide (New Technologies/New Cultures),"The internet, television, mobile phones, computer-based devices and other new forms of information technology are changing at a rapid pace with potentially profound but also subtle influences on social life. Yet, they also pose challenges and have to be managed. This book offers a succinct introduction to both the experience and implications of these information and communication technologies (ICTs) in everyday life.  Bringing together empirical and theoretical research in a coherent way, the author offers a fresh approach to understanding ICTs and everyday life. He covers topics in key areas, such as âthe digital divideâ; children, youth and ICTs; the dynamics of ICTs within households; social networks and ICTs; and time, movement and public space in relationship to ICTs. Drawing on a broad variety of studies from different countries, the author considers the potential, or feared, social consequences of ICTs. Throughout, he analyzes what factors are shaping the debates surrounding information and communication technologies in daily life.  With its concise, international approach, this book will be invaluable to professionals, policymakers and students who work in the field of ICTs. About the author(s) Leslie Haddon Visiting Research Associate,Media@LSE  Contents 1. Introduction 2. Uneven patterns of adoption and use of ICTs 3. Children, youth and ICTs 4. Changing life circumstances and ICTs 5. Managing relationships through and around ICTs 6. Social networks and ICTs 7. Time and ICTs 8. Movement, public spaces and ICTs 9. The careers of ICTs 10. Conclusions"
414,84,3233,1,Virtual Culture : Identity and Communication in Cybersociety,"{Not long after William Gibson hit the charts with his cyberpunk fiction, especially the groundbreaking (or Web-busting) <i>Neuromancer</i>, discussions were buzzing with ideas about how technology affects our culture and our beliefs. The essays that Steven Jones has collected explore cybersociety, online cultures, and their relationship not only to one another but also to traditional societies. The experiences of typically marginalized cultures--""cyberhate,"" Third World representation, gay identity in cyberspace, and punishment of ""virtual offenders""--are also explored, as in Ananda Mitra's essay, ""Virtual Commonality: Looking for India on the Internet."" <i>Virtual Culture</i> is a cutting-edge book that addresses the effects and defects of discourse and community on the Web.  } {Virtual Culture provides a unique analysis of a previously undocumented aspect of the cybersociety. Until now, the debate about participation in cyberculture has tended to focus on the ways that certain segments of the population are denied access to communications technologies. By contrast, the contributors to this volume scrutinize the way in which under-represented groups&#249;gay men, women, and special interest groups&#249;are exploiting the opportunities that the Internet provides for social and political change. Virtual Culture presents contributions from a range of subject disciplines, including communication, sociology, and anthropology in order to reflect on the diverse paradigms currently engaged in the study of electronic communities and networks. It sets out the definitions, boundaries, and approaches to the studies of these topics while demonstrating the theoretical and practical possibilities for cybersociety as an identity-structured space. Virtual Culture will be required reading for all students of communication, media and technology.}"
415,84,5539,1,"A Theory of Structure: Duality, Agency, and Transformation","""Structure"" is one of the most important, elusive, and undertheorized concepts in the social sciences. Setting out from a critique and reformulation of Anthony Giddens's notion of the duality of structure and Pierre Boundieu's notion of habitus, this article attempts to develop a theory of structure that restores human agency to social actors, builds the possibility of change into the concept of structure, and overcomes the divide between semiotic and materialist vision of structure."
416,84,11503,1,Staying connected while on the move: Cell phone use and social connectedness,"As people integrate use of the cell phone into their lives, do they view it as just an update of the fixed telephone or assign it special values? This study explores   that question in the framework of gratifications sought and their relationship both to differential cell phone use and to social connectedness. Based on a survey of Taiwanese college students, we found that the cell phone supplements the fixed telephone as a means of strengthening users' family bonds, expanding their psychological neighborhoods, and facilitating symbolic proximity to the people they call. Thus, the cell phone has evolved from a luxury for businesspeople into an important facilitator of many users' social relationships. For the poorly connected socially, the cell phone offers a unique advantage: it confers instant membership in a community. Finally, gender was found to mediate how users exploit   the cell phone to maintain social ties. 10.1177/1461444806059870"
417,85,1168,1,Multi-interval discretization of continuous-valued attributes for classification learning,"Since most real-world applications of classification learning involve continuous-valued attributes, properly addressing the discretization process is an important problem. This paper addresses the use of the entropy minimization heuristic for discretizing the range of a continuous-valued attribute into multiple intervals. We briefly present theoretical evidence for the appropriateness of this heuristic for use in the binary discretization algorithm used in ID3, C4, CART, and other learning algorithms. The results serve to justify extending the algorithm to derive multiple intervals. We formally derive a criterion based on the minimum description length principle for deciding the partitioning of intervals. We demonstrate via empirical evaluation on several real-world data sets that better decision trees are obtained using the new multi-interval algorithm."
418,85,9656,1,Statistical fraud detection: A review,"Abstract. Fraud is increasing dramatically with the expansion of modern technology and the global superhighways of communication, resulting in the loss of billions of dollars worldwide each year. Although prevention technologies are the best way to reduce fraud, fraudsters are adaptive and, given time, will usually find ways to circumvent such measures. Methodologies for the detection of fraud are essential if we are to catch fraudsters once fraud prevention has failed. Statistics and machine learning provide effective technologies for fraud detection and have been applied successfully to detect activities such as money laundering, e-commerce credit card fraud, telecommunications fraud and computer intrusion, to name but a few. We describe the tools available for statistical fraud detection and the areas in which fraud detection technologies are most used. Key words and phrases: Fraud detection, fraud prevention, statistics, machine learning, money laundering, computer intrusion, e-commerce, credit cards, telecommunications."
419,86,677,1,Toward a basic framework for webometrics,"In this article, we define webometrics within the framework of informetric studies and bibliometrics, as belonging to library and information science, and as associated with cybermetrics as a generic subfield. We develop a consistent and detailed link typology and terminology and make explicit the distinction among different Web node levels when using the proposed conceptual framework. As a consequence, we propose a novel diagram notation to fully appreciate and investigate link structures between Web nodes in webometric analyses. We warn against taking the analogy between citation analyses and link analyses too far."
420,86,3096,1,Inside the search process: Information seeking from the user's perspective,"The article discusses the users' perspective of information seeking. A model of the information search process is presented derived from a series of five studies investigating common experiences of users in information seeking situations. The cognitive and affective aspects of the process of information seeking suggest a gap between the users' natural process of information use and the information system and intermediaries' traditional patterns of information provision. ï¿½ 1991 John Wiley & Sons, Inc."
421,86,6748,1,The Probability Ranking Principle in IR,"The principle that, for optimal retrieval, documents should be ranked in order of the probability of relevance or usefulness has been brought into question by Cooper. It is shown that the principle can be justified under certain assumptions, but that in cases where these assumptions do not hold, the principle is not valid. The major problem appears to lie in the way the principle considers each document independently of the rest. The nature of the information on the basis of which the system decides whether or not to retrieve the documents determines whether the document-by-document approach is valid."
422,86,9786,1,Information science,"The purpose of this entry is to provide an overview of information science as a field or discipline, including a historical perspective to illustrate the events and forces that shaped it. Information science is a field of professional practice and scientific inquiry dealing with effective communication of information and information objects, particularly knowledge records, among humans in the context of social, organizational, and individual need for and use of information. Information science emerged in the aftermath of the Second World War, as did a number of other fields, addressing the problem of information explosion and using technology as a solution. Presently, information science deals with the same problems in the Web and digital environments. This entry covers problems addressed by information science, the intellectual structure of the field, and the description of main areasâinformation retrieval, human information behavior, metric studies, and digital libraries. This entry also includes an account of education related to information science and conclusions about major characteristics."
423,86,12671,1,Does the h index have predictive power?,"10.1073/pnas.0707962104 Bibliometric measures of individual scientific achievement are of particular interest if they can be used to predict future achievement. Here we report results of an empirical study of the predictive power of the  index compared with other indicators. Our findings indicate that the  index is better than other indicators considered (total citation count, citations per paper, and total paper count) in predicting future scientific achievement. We discuss reasons for the superiority of the  index."
424,86,16915,1,Google Scholar's Dramatic Coverage Improvement Five Years after Debut,"This article reports a 2010 empirical study using a 2005 study as a base to compare Google Scholar's coverage of scholarly journals with commercial services. Through random samples of eight databases, the author finds that, as of 2010, Google Scholar covers 98 to 100 percent of scholarly journals from both publicly accessible Web contents and from subscription-based databases that Google Scholar partners with. In 2005 the coverage of the same databases ranged from 30 to 88 percent. The author explores de-duplication of search results by Google Scholar and discusses its impacts on searches and library resources. With the dramatic improvement of Google Scholar, the uniqueness and effectiveness of subscription-based abstracts and indexes have dramatically changed."
425,87,1824,1,Trends in cooperative distributed problem solving,"The authors present an overview of cooperative distributed problem solving (CDPS), an emerging research area that combines aspects of AI (artificial intelligence) and distributed processing. CDPS can be used to study how a loosely coupled network of sophisticated problem-solving nodes can solve a complex problem which consists of a set of interdependent subproblems. Subproblems arise because of spatial, temporal, and functional distribution of data, knowledge, and processing capabilities. Application areas include distributed interpretation, distributed planning and control, cooperating expert systems, and computer-supported human cooperation. The authors survey the important approaches and empirical investigations that have been developed. The approaches covered include negotiation, functionally accurate cooperation, organizational structuring, multiagent planning, sophisticated local control, and theoretical frameworks."
426,87,1833,1,The control of discrete event systems,"A discrete event system (DES) is a dynamic system that evolves in accordance with the abrupt occurrence, at possibly unknown irregular intervals, of physical events. Such systems arise in a variety of contexts ranging from computer operating systems to the control of complex multimode processes. A control theory for the logical aspects of such DESs is surveyed. The focus is on the qualitative aspects of control, but computation and the related issue of computational complexity are also considered. Automata and formal language models for DESs are surveyed"
427,87,1842,1,Auction protocols for decentralized scheduling,"Decentralized scheduling is the problem of allocating resources to alternative possible uses over time, where competing uses are represented by autonomous agents. Market mechanisms use prices derived through distributing bidding protocols to determine schedules. We investigate the existence of equilibrium prices for some general classes of scheduling problems, the quality of equilibrium solutions, and the behavior of an ascending auction mechanism and bidding protocol. To remedy the potential nonexistence of price equilibria due to complementarities in preference, we introduce additional markets in combinations of basic goods. Finally, we consider direct revelation mechanisms and compare to the market-based approach. Journal of Economic Literature Classification Numbers: C62, C70, D44."
428,88,1479,1,"Diffusion of Innovations, 5th Edition","<P>Since the first edition of this landmark book was published in 1962, Everett Rogers's name has become ""virtually synonymous with the study of diffusion of innovations,"" according to Choice. The second and third editions of Diffusion of Innovations became the standard textbook and reference on diffusion studies. Now, in the fourth edition, Rogers presents the culmination of more than thirty years of research that will set a new standard for analysis and inquiry. <P>The fourth edition is (1) a revision of the theoretical framework and the research evidence supporting this model of diffusion, and (2) a new intellectual venture, in that new concepts and new theoretical viewpoints are introduced. This edition differs from its predecessors in that it takes a much more critical stance in its review and synthesis of 5,000 diffusion publications. During the past thirty years or so, diffusion research has grown to be widely recognized, applied and admired, but it has also been subjected to both constructive and destructive criticism. This criticism is due in large part to the stereotyped and limited ways in which many diffusion scholars have defined the scope and method of their field of study. Rogers analyzes the limitations of previous diffusion studies, showing, for example, that the convergence model, by which participants create and share information to reach a mutual understanding, more accurately describes diffusion in most cases than the linear model. <P>Rogers provides an entirely new set of case examples, from the Balinese Water Temple to Nintendo videogames, that beautifully illustrate his expansive research, as well as a completely revised bibliography covering all relevant diffusion scholarship in the past decade. Most important, he discusses recent research and current topics, including social marketing, forecasting the rate of adoption, technology transfer, and more. This all-inclusive work will be essential reading for scholars and students in the fields of communications, marketing, geography, economic development, political science, sociology, and other related fields for generations to come. ""Now in its fifth edition, Diffusion of Innovations is a classic work on the spread of new ideas. It has sold 30,000 copies in each edition and will continue to reach a huge academic audience. In this renowned book, Everett M. Rogers, professor and chair of the Department of Communication \\{\\&} Journalism at the University of New Mexico, explains how new ideas spread via communication channels over time. Such innovations are initially perceived as uncertain and even risky. To overcome this uncertainty, most people seek out others like themselves who have already adopted the new idea. Thus the diffusion process consists of a few individuals who first adopt an innovation, then spread the word among their circle of acquaintances--a process which typically takes months or years. But there are exceptions: use of the Internet in the 1990s, for example, may have spread more rapidly than any other innovation in the history of humankind. Furthermore, the Internet is changing the very nature of diffusion by decreasing the importance of physical distance between people. The fifth edition addresses the spread of the Internet, and how it has transformed the way human beings communicate and adopt new ideas."""
429,89,100,1,EDUTELLA: A P2P Networking Infrastructure Based on RDF,"Metadata for the World Wide Web is important, but metadata for Peer-to-Peer (P2P) networks is absolutely crucial. In this paper we discuss the open source project Edutella which builds upon metadata standards defined for the WWW and aims to provide an RDF-based metadata infrastructure for P2P applications, building on the recently announced JXTA Framework. We describe the goals and main services this infrastructure will provide and the architecture to connect Edutella Peers based on exchange of RDF metadata. As the query service is one of the core services of Edutella, upon which other services are built, we specify in detail the Edutella Common Data Model (ECDM) as basis for the Edutella query exchange language (RDF-QEL-i) and format implementing distributed queries over the Edutella network. Finally, we shortly discuss registration and mediation services, and introduce the prototype and application scenario for our current Edutella aware peers."
430,89,2902,1,On Agent-based Software Engineering,"Agent-based computing represents an exciting new synthesis both for Artificial Intelligence (AI) and, more generally, Computer Science. It has the potential to significantly improve the theory and the practice of modeling, designing, and implementing computer systems. Yet, to date, there has been little systematic analysis of what makes the agent-based approach such an appealing and powerful computational model. Moreover, even less effort has been devoted to discussing the inherent disadvantages that stem from adopting an agent-oriented view. Here both sets of issues are explored. The standpoint of this analysis is the role of agent-based software in solving complex, real-world problems. In particular, it will be argued that the development of robust and scalable software systems requires autonomous agents that can complete their objectives while situated in a dynamic and uncertain environment, that can engage in rich, high-level social interactions, and that can operate within flexible organisational structures."
431,89,6937,1,SHARP: an architecture for secure resource peering,"This paper presents Sharp, a framework for secure distributed resource management in an Internet-scale computing infrastructure. The cornerstone of Sharp is a construct to represent cryptographically protected resource claims--- promises or rights to control resources for designated time intervals---together with secure mechanisms to subdivide and  delegate claims across a network of resource managers. These  mechanisms enable flexible resource peering: sites may trade their resources with..."
432,90,20,1,Spectra of random graphs with given expected degrees,"In the study of the spectra of power-law graphs, there are basically two  competing approaches. One is to prove analogues of Wigner's semicircle law,  whereas the other predicts that the eigenvalues follow a power-law  distribution. Although the semicircle law and the power law have nothing in  common, we will show that both approaches are essentially correct if one  considers the appropriate matrices. We will prove that (under certain mild  conditions) the eigenvalues of the (normalized) Laplacian of a random  power-law graph follow the semicircle law, whereas the spectrum of the  adjacency matrix of a power-law graph obeys the power law. Our results are  based on the analysis of random graphs with given expected degrees and their  relations to several key invariants. Of interest are a number of (new) values  for the exponent Î², where phase transitions for eigenvalue distributions  occur. The spectrum distributions have direct implications to numerous graph  algorithms such as, for example, randomized algorithms that involve rapidly  mixing Markov chains."
433,90,486,1,Community structure in social and biological networks,"10.1073/pnas.122653799 A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well knownâa collaboration network and a food webâand find that it detects significant and informative community divisions in both cases."
434,90,1010,1,The Elements of Statistical Learning,"During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics.  Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry.  The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting--the first comprehensive treatment of this topic in any book.  Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit.  FROM THE REVIEWS:  TECHNOMETRICS ""[This] is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."""
435,90,1651,1,{DIP: the database of interacting proteins},"The Database of Interacting Proteins (DIP; http://dip.doe-mbi.ucla.edu ) is a database that documents experimentally determined protein-protein interactions. This database is intended to provide the scientific community with a comprehensive and integrated tool for browsing and efficiently extracting information about protein interactions and interaction networks in biological processes. Beyond cataloging details of protein-protein interactions, the DIP is useful for understanding protein function and protein-protein relationships, studying the properties of networks of interacting proteins, benchmarking predictions of protein-protein interactions, and studying the evolution of protein-protein interactions. 10.1093/nar/28.1.289"
436,90,3695,1,The Skip Quadtree: A Simple Dynamic Data Structure for Multidimensional Data,"We present a new multi-dimensional data structure, which we call the skip quadtree (for point data in R^2) or the skip octree (for point data in R^d, with constant d&gt;2). Our data structure combines the best features of two well-known data structures, in that it has the well-defined box-shaped regions of region quadtrees and the logarithmic-height search and update hierarchical structure of skip lists. Indeed, the bottom level of our structure is exactly a region quadtree (or octree for higher dimensional data). We describe efficient algorithms for inserting and deleting points in a skip quadtree, as well as fast methods for performing point location and approximate range queries."
437,90,5454,1,"Visualizing a discipline: An author co-citation analysis of information science, 1972-1995","This study presents an extensive domain analysis of a discipline&nbsp;-&nbsp;information science&nbsp;-&nbsp;in terms of its authors. Names of those most frequently cited in 12 key journals from 1972 through 1995 were retrieved from Social Scisearch via DIALOG. The top 120 were submitted to author co-citation analyses, yielding automatic classifications relevant to histories of the field. Tables and graphics reveal: (1) The disciplinary and institutional affiliations of contributors to information science; (2) the specialty structure of the discipline over 24 years; (3) authors' memberships in 1 or more specialties; (4) inertia and change in authors' positions on 2-dimensional subject maps over 3 8-year subperiods, 1972-1979, 1980-1987, 1988-1995; (5) the 2 major subdisciplines of information science and their evolving memberships; (6) <IMG SRC=""/giflibrary/12/ldquo.gif"" BORDER=""0"">canonical<IMG SRC=""/giflibrary/12/rdquo.gif"" BORDER=""0""> authors who are in the top 100 in all three subperiods; (7) changes in authors' eminence and influence over the subperiods, as shown by mean co-citation counts; (8) authors with marked changes in their mapped positions over the subperiods; (9) the axes on which authors are mapped, with interpretations; (10) evidence of a paradigm shift in information science in the 1980s; and (11) evidence on the general nature and state of integration of information science. Statistical routines include ALSCAL, INDSCAL, factor analysis, and cluster analysis with SPSS; maps and other graphics were made with DeltaGraph. Theory and methodology are sufficiently detailed to be usable by other researchers. &copy; 1998 John Wiley &amp; Sons, Inc."
438,90,7601,1,Weak pairwise correlations imply strongly correlated network states in a neural population,"Biological networks have so many possible states that exhaustive sampling is impossible. Successful analysis thus depends on simplifying hypotheses, but experiments on many systems hint that complicated, higher-order interactions among large groups of elements have an important role. Here we show, in the vertebrate retina, that weak correlations between pairs of neurons coexist with strongly collective behaviour in the responses of ten or more neurons. We find that this collective behaviour is described quantitatively by models that capture the observed pairwise correlations but assume no higher-order interactions. These maximum entropy models are equivalent to Ising models, and predict that larger networks are completely dominated by correlation effects. This suggests that the neural code has associative or error-correcting properties, and we provide preliminary evidence for such behaviour. As a first test for the generality of these ideas, we show that similar results are obtained from networks of cultured cortical neurons."
439,90,8304,1,Modularity and community structure in networks,"Many networks of interest in the sciences, including a variety of social and biological networks, are found to divide naturally into communities or modules. The problem of detecting and characterizing this community structure has attracted considerable recent attention. One of the most sensitive detection methods is optimization of the quality function known as ""modularity"" over the possible divisions of a network, but direct application of this method using, for instance, simulated annealing is computationally costly. Here we show that the modularity can be reformulated in terms of the eigenvectors of a new characteristic matrix for the network, which we call the modularity matrix, and that this reformulation leads to a spectral algorithm for community detection that returns results of better quality than competing methods in noticeably shorter running times. We demonstrate the algorithm with applications to several network data sets."
440,90,9804,1,Generalized Search Trees for Database Systems,"This paper introduces the Generalized Search Tree (GiST), an index structure supporting an extensible set of queries and data types. The GiST allows new data types to be indexed in a manner supporting queries natural to the types; this is in contrast to previous work on tree extensibility which only supported the traditional set of equality and range predicates. In a single data structure, the GiST provides all the basic search tree logic required by a database system, thereby unifying disparate structures such as B+-trees and R-trees in a single piece of code, and opening the application of search trees to general extensibility. To illustrate the flexibility of the GiST, we provide simple method implementations that allow it to behave like a B+-tree, an R-tree, and an RD-tree, a new index for data with set-valued attributes. We also present a preliminary performance analysis of RD-trees, which leads to discussion on the nature of tree indices and how they behave for various datasets."
441,90,10620,1,The Evolution of Cooperation: Revised Edition,"{Updated for the first time, the classic book on why cooperation is not only natural but also the best survival strategy <P> <I>The Evolution of Cooperation</I> addresses a simple yet age-old question: If living things evolve through competition, how can cooperation ever emerge? Despite the abundant evidence of cooperation all around us, there existed no purely naturalistic answer to this question until 1979, when Robert Axelrod famously ran a computer tournament featuring a standard game-theory exercise called The Prisoner's Dilemma. To everyone's surprise, the program that won the tournament, named Tit for Tat, was not only the simplest but the most ""cooperative"" entrant. This unexpected victory proved that cooperation--one might even say altruism--is <I>mathematically</I> possible and therefore needs no hidden hand or divine agent to create and sustain it. A great roadblock to the understanding of all sorts of behavior was at last removed. The updated edition includes an extensive new chapter on cooperation in cancer cells and among terrorist organizations. <P> ""This book, if read, grasped and applied, could have a profound effect."" (<I>Wall Street Journal</I>) <P> ""A fascinating, provocative, and important book."" (Douglas R. Hofstadter, author of <I>Godel, Escher, Bach</I>) <P>}"
442,90,11886,1,The spread of obesity in a large social network over 32 years.,"BACKGROUND: The prevalence of obesity has increased substantially over the past 30 years. We performed a quantitative analysis of the nature and extent of the person-to-person spread of obesity as a possible factor contributing to the obesity epidemic. METHODS: We evaluated a densely interconnected social network of 12,067 people assessed repeatedly from 1971 to 2003 as part of the Framingham Heart Study. The body-mass index was available for all subjects. We used longitudinal statistical models to examine whether weight gain in one person was associated with weight gain in his or her friends, siblings, spouse, and neighbors. RESULTS: Discernible clusters of obese persons (body-mass index [the weight in kilograms divided by the square of the height in meters], >/=30) were present in the network at all time points, and the clusters extended to three degrees of separation. These clusters did not appear to be solely attributable to the selective formation of social ties among obese persons. A person's chances of becoming obese increased by 57% (95% confidence interval [CI], 6 to 123) if he or she had a friend who became obese in a given interval. Among pairs of adult siblings, if one sibling became obese, the chance that the other would become obese increased by 40% (95% CI, 21 to 60). If one spouse became obese, the likelihood that the other spouse would become obese increased by 37% (95% CI, 7 to 73). These effects were not seen among neighbors in the immediate geographic location. Persons of the same sex had relatively greater influence on each other than those of the opposite sex. The spread of smoking cessation did not account for the spread of obesity in the network. CONCLUSIONS: Network phenomena appear to be relevant to the biologic and behavioral trait of obesity, and obesity appears to spread through social ties. These findings have implications for clinical and public health interventions. Copyright 2007 Massachusetts Medical Society."
443,91,121,1,Semantic blogging and decentralized knowledge management,"In the Semantic Web research group at Hewlett-Packard Laboratories, Bristol, we frequently circulate items of interest (such as news articles, software tools, links to Web sites, and competitor information). We call them snippets, or information nuggets, we would like to store, annotate, and share. Email is not the ideal medium for these tasks; its transient nature means the snippets are effectively lost over time. Yet the risk from using a more formal process, like a centralized database, is that it is both cumbersome to use (a barrier to entry) and overly rigid in its data model (not amenable to storing different types of information). Our need illustrates what I call decentralized, informal knowledge management."
444,91,2266,1,The Family Video Archive: an annotation and browsing environment for home movies,We present the Family Video Archive as a tool to give consumers the ability to annotate and browse large collections of informal family movies. The informal nature of home movies makes it difficult to use fully-automated techniques for scene detection and annotation. Our system explores the symbiosis between automated and manual techniques for annotation. We also explore the use of a zooming interaction paradigm for browsing and filtering large collections of video scenes.
445,92,6417,1,Molecular dynamics with coupling to an external bath,"In molecular dynamics (MD) simulations the need often arises to maintain such parameters as temperature or pressure rather than energy and volume, or to impose gradients for studying transport properties in nonequilibrium MD. A method is described to realize coupling to an external bath with constant temperature or pressure with adjustable time constants for the coupling. The method is easily extendable to other variables and to gradients, and can be applied also to polyatomic molecules involving internal constraints. The influence of coupling time constants on dynamical variables is evaluated. A leap-frog algorithm is presented for the general case involving constraints with coupling to both a constant temperature and a constant pressure bath. [Journal Article; U.S. Copyright Clearance Center Code: 0021-9606/84/203684-07$ 02.10; 37 Refs]"
446,93,98,1,Shannon Information and Kolmogorov Complexity,"We compare the elementary theories of Shannon information and Kolmogorov complexity, the extent to which they have a common purpose, and where they are fundamentally different. We discuss and relate the basic notions of both theories: Shannon entropy versus Kolmogorov complexity, the relation of both to universal coding, Shannon mutual information versus Kolmogorov (`algorithmic') mutual information, probabilistic sufficient statistic versus algorithmic sufficient statistic (related to lossy compression in the Shannon theory versus meaningful information in the Kolmogorov theory), and rate distortion theory versus Kolmogorov's structure function. Part of the material has appeared in print before, scattered through various publications, but this is the first comprehensive systematic comparison. The last mentioned relations are new."
447,93,1160,1,An Introduction to Kolmogorov Complexity and its Applications,"Written by two experts in the field, this is the only comprehensive and unified treatment of the central ideas and their applications of Kolmogorov complexity. The book presents a thorough treatment of the subject with a wide range of illustrative applications. Such applications include the randomness of finite objects or infinite sequences, Martin-Loef tests for randomness, information theory, computational learning theory, the complexity of algorithms, and the thermodynamics of computing. In this new edition the authors have added new material on circuit theory, distributed algorithms, data compression, and other topics."
448,93,3982,1,The Structure of Collaborative Tagging Systems,"Collaborative tagging describes the process by which many users add metadata in the form of keywords to shared content.  Recently, collaborative tagging has grown in popularity on the web, on sites that allow users to tag bookmarks, photographs and other content.  In this paper we analyze the structure of collaborative tagging systems as well as their dynamical aspects. Specifically, we discovered regularities in user activity, tag frequencies, kinds of tags used, bursts of popularity in bookmarking and a remarkable stability in the relative proportions of tags within a given url. We also present a dynamical model of collaborative tagging that predicts these stable patterns and relates them to imitation and shared knowledge."
449,93,6382,1,Executable object modeling with statecharts,"This paper reports on an effort to develop an integrated set of diagrammatic languages for modeling object-oriented systems, and to construct a supporting tool. The goal is for models to be intuitive and well-structured, yet fully executable and analyzable, enabling automatic synthesis of usable and efficient code in object-oriented languages such as C++. At the heart of the modeling method is the language of statecharts for specifying object behavior, and a hierarchical OMT-like language for describing the structure of classes and their inter-relationships, that we call O-charts. Objects can interact by event generation, or by direct invocation of operations. In the interest of keeping the exposition manageable, we leave out some technically involved topics, such as multiple-thread concurrency and active objects"
450,93,8722,1,The Semantic Web Revisited,"The original Scientific American article on the Semantic Web appeared in 2001. It described the evolution of a Web that consisted largely of documents for humans to read to one that included data and information for computers to manipulate. The Semantic Web is a Web of actionable information--information derived from data through a semantic theory for interpreting the symbols.This simple idea, however, remains largely unrealized. Shopbots and auction bots abound on the Web, but these are essentially handcrafted for particular tasks; they have little ability to interact with heterogeneous data and information types. Because we haven't yet delivered large-scale, agent-based mediation, some commentators argue that the Semantic Web has failed to deliver. We argue that agents can only flourish when standards are well established and that the Web standards for expressing shared meaning have progressed steadily over the past five years. Furthermore, we see the use of ontologies in the e-science community presaging ultimate success for the Semantic Web--just as the use of HTTP within the CERN particle physics community led to the revolutionary success of the original Web. This article is part of a special issue on the Future of AI."
451,94,175,1,Proper tail recursion and space efficiency,"The IEEE/ANSI standard for Scheme requires implementations to be properly tail recursive. This ensures that portable code can rely upon the space efficiency of continuation-passing style and other idioms. On its face, proper tail recursion concerns the efficiency of procedure calls that occur within a tail context. When examined closely, proper tail recursion also depends upon the fact that garbage collection can be asymptotically more space-efficient than Algol-like stack allocation. Proper..."
452,94,379,1,Theorems for Free!,"From the type of a polymorphic function we can derive a theorem that it satisfies. Every function of the same type satisfies the same theorem. This provides a free source of useful theorems, courtesy of Reynolds' abstraction theorem for the polymorphic lambda calculus.  1 Introduction  Write down the definition of a polymorphic function on a piece of paper. Tell me its type, but be careful not to let me see the function's definition. I will tell you a theorem that the function satisfies. The..."
453,94,4287,1,The Glasgow Haskell compiler: a technical overview,"We give an overview of the Glasgow Haskell compiler, focusing especially on way in which we have been able to exploit the rich theory of functional languages to give very practical improvements in the compiler. The compiler is portable, modular, generates good code, and is freely available. 1 Introduction Computer Science is both a scientific and an engineering discipline. As a scientific discipline, it seeks to establish generic principles and theories that can be used to explain or underpin a variety of particular applications. As an engineering discipline, it constructs substantial artefacts of software and hardware, sees where they fail and where they work, and develops new theory to underpin areas that are inadequately supported. (Milner [1991] eloquently argues for this dual approach in Computer Science. ) Functional programming is a research area that offers an unusually close interplay between these two aspects (Peyton Jones [1992b]). Theory often has immediate practical appl..."
454,95,747,1,The Strength of Weak Ties,"Analysis of social networks is suggested as a tool for linking micro and macro levels of sociological theory. The procedure is illustrated by elaboration of the macro implications of one aspect of small-scale interaction: the strength of dyadic ties. It is argued that the degree of overlap of two individuals' friendship networks varies directly with the strength of their tie to one another. The impact of this principle on diffusion of influence and information, mobility opportunity, and community organization is explored. Stress is laid on the cohesive power of weak ties. Most network models deal, implicitly, with strong ties, thus confining their applicability to small, well-defined groups. Emphasis on weak ties lends itself to discussion of relations between groups and to analysis of segments of social structure not easily defined in terms of primary groups."
455,95,4999,1,Professional Vision.,"Using as data videotapes of archaeologists making maps, and lawyers animating  events visible on the Rodney King videotape, this article investigates the discursive practices used by members of a profession to shape events in the phenomenal environment they focus their attention upon, the domain of their professional scrutiny, into the objects of knowledge that become the insignia of their profession: the theories, artifacts and bodies of expertise that are its special domain of competence and set it apart from other groups. Seeing is investigated as a socially situated, historically constituted body of practices through which the objects of knowledge which animate the discourse of a profession are constructed and shaped. Analysis focuses on three practices, coding schemes, highlighting, and the articulation of graphic representations, which  are articulated in a work relevant way within sequences of human interaction. Through the structure of talk in interaction members of a profession hold accountable for, and contest, the proper constitution and perception of the objects that define their professional competence."
456,95,10508,1,"The concept of ""Ba"": Building a Foundation for Knowledge Creation","A paper introduces the Japanese concept of ""Ba"" to organizational theory. Ba (equivalent to ""place"" in English) is a shared space for emerging relationships. It can be a physical, virtual, or mental space. Knowledge, in contrast to information, cannot be separated from the context - it is embedded in ba. To support the process of knowledge creation, a foundation in ba is required. The paper develops and explains 4 specific platforms and their relationships to knowledge creation. Each of the knowledge conversion modes is promoted by a specific ba. A self-transcending process of knowledge creation can be supported by providing ba on different organizational levels. This article presents case studies of 3 companies that employ ba on the team, division, and corporate level to enhance knowledge creation."
457,96,1920,1,Software Engineering,"From the Book: Preface&#58;    Software systems are now ubiquitous. Virtually all electrical equipment now includes some kind of software; software is used to help run manufacturing industry, schools and universities, health care, finance and government; many people now use software of different kinds for entertainment and education. The specification, development, management and evolution of these software systems make up the discipline of software engineering.   Even simple software systems have a high inherent complexity so engineering principles have to be used in their development. Software engineering is therefore an engineering discipline where software engineers use methods and theory from computer science and apply this cost-effectively to solve difficult problems. These difficult problems have meant that many software development projects have not been successful. However, most modern software provides good service to its users; we should not let high-profile failures obscure the real successes of software engineers over the past 30 years.   Books inevitably reflect the opinions and prejudices of their authors. Some readers will inevitably disagree with my opinions and with the choice of material which I include. Such disagreement is a healthy reflection of the diversity of the discipline and is essential for its evolution. Nevertheless, I hope that all software engineers and software engineering students can find something of interest here.   Although the book is intended as a general introduction to software engineering, it is biased, to some extent, towards my own interests in system requirements engineering and critical systems. I think these areparticularlyimportant for software engineering in the 21st century where the challenge we face is to ensure that our software meets the real needs of its users without causing damage to them or to the environment.   I dislike zealots of any kind, whether they are academics preaching the benefits of formal methods or salesmen trying to convince me that some tool or method is the answer to software development problems. There are no simple solutions to the problems of software engineering and we need a wide spectrum of tools and techniques to solve software engineering problems. I therefore don't describe commercial design methods or CASE systems but paint a broad picture of software engineering methods and tools.   Software engineering research has made tremendous strides over the past 15 years but there has been a relatively slow diffusion of this research into industrial practice. The principal challenge which we now face is not the development of new techniques and methods but the transfer of advanced software engineering research into everyday use. I see this book as a contributor to this process. I therefore discuss some techniques, such as viewpoints for requirements engineering, which are reasonably well developed but which are not yet widely used in industry.   Finally, it is impossible to over-emphasize the importance of people in the software engineering process. People specify, design and implement systems which help other people with their work. Most of the difficulties of very large system engineering are not technical problems but are the problems of managing large numbers of people with diverse priorities, abilities and interests. Software engineering techniques and tools are only effective when applied in a context which respects these different skills and abilities.   Changes from the fourth edition   Like many software systems, this book has grown and changed since its first edition was published in 1982. This latest edition started as a relatively minor update of the fourth edition but, in the course of writing the book, I decided that more significant revision and re-engineering was necessary. Although much of the material in the fourth edition has been retained, the following changes have been made&#58;     There are five completely new chapters covering computer-based system engineering, requirements analysis, architectural design, process improvement and software re-engineering.   The book has been restructured into eight parts covering an introduction to software engineering, requirements and specification, design, dependable systems development, verification and validation, CASE, management, and software evolution.   There have been radical revisions of the material on requirements engineering, object-oriented and functional design, and CASE.   Project management is introduced in the first part of the book then covered in more detail in a separate section which incorporates previous material on human factors. There is more emphasis on quality management."
458,96,3812,1,Finding bugs is easy,"Many techniques have been developed over the years to automatically find bugs in software. Often, these techniques rely on formal methods and sophisticated program analysis. While these techniques are valuable, they can be difficult to apply, and they arenât always effective in finding real bugs. Bug patterns are code idioms that are often errors. We have implemented automatic detectors for a variety of bug patterns found in Java programs. In this extended abstract1, we describe how we have used bug pattern detectors to find serious bugs in several widely used Java applications and libraries. We have found that the effort required to implement a bug pattern detector tends to be low, and that even extremely simple detectors find bugs in real applications. From our experience applying bug pattern detectors to real programs, we have drawn several interesting conclusions. First, we have found that even well tested code written by experts contains a surprising number of obvious bugs. Second, Java (and similar languages) have many language features and APIs which are prone to misuse. Finally, that simple automatic techniques can be effective at countering the impact of both ordinary mistakes and misunderstood language features."
459,96,4791,1,Object-Oriented Design Heuristics,"Object-Oriented Design Heuristics offers insight into object-oriented design improvement. The more than sixty guidelines presented in this book are language-independent and allow you to rate the integrity of a software design. The heuristics are not written as hard and fast rules; they are meant to serve as warning mechanisms which allow the flexibility of ignoring the heuristic as necessary. This tutorial-based approach, born out of the author's extensive experience developing software, teaching thousands of students, and critiquing designs in a variety of domains, allows you to apply the guidelines in a personalized manner. The heuristics cover important topics ranging from classes and objects (with emphasis on their relationships including association, uses, containment, and both single and multiple inheritance) to physical object-oriented design. You will gain an understanding of the synergy that exists between design heuristics and the popular concept of design patterns; heuristics can highlight a problem in one facet of a design while patterns can provide the solution. Programmers of all levels will find value in this book. The newcomer will discover a fast track to understanding the concepts of object-oriented programming. At the same time, experienced programmers seeking to strengthen their object-oriented development efforts will appreciate the insightful analysis. In short, with Object-Oriented Design Heuristics as your guide, you have the tools to become a better software developer."
460,97,547,1,Intelligent Agents: Theory and Practice,"The concept of an \\emph{agent} has become important in both Artificial Intelligence (AI) and mainstream computer science. Our aim in this paper is to point the reader at what we perceive to be the most important theoretical and practical issues associated with the design and construction of intelligent agents. For convenience, we divide these issues into three areas (though as the reader will see, the divisions are at times somewhat arbitrary). \\emph{Agent theory} is concerned with the question of what an agent is, and the use of mathematical formalisms for representing and reasoning about the properties of agents. \\emph{Agent architectures} can be thought of as software engineering models of agents; researchers in this area are primarily concerned with the problem of designing software or hardware systems that will satisfy the properties specified by agent theorists. Finally, \\emph{agent languages} are software systems for programming and experimenting with agents; these languages may embody principles proposed by theorists. The paper is \\emph{not} intended to serve as a tutorial introduction to all the issues mentioned; we hope instead simply to identify the most important issues, and point to work that elaborates on them. The article includes a short review of current and potential applications of agent technology."
461,97,3224,1,A Roadmap of Agent Research and Development,". This paper provides an overview of research and development activities in the field of autonomous agents and multi-agent systems. It aims to identify key concepts and applications, and to indicate how they relate to one-another. Some historical context to the field of agent-based computing is given, and contemporary research directions are presented. Finally, a range of open issues and future challenges are highlighted. Keywords: autonomous agents, multi-agent systems, history 1. ..."
462,97,6323,1,Matchmaking: Distributed Resource Management for High Throughput Computing,"Conventional resource management systems use a system model to describe resources and a centralized scheduler to control their allocation. We argue that this paradigm does not adapt well to distributed systems, particularly those built to support high throughput computing. Obstacles include heterogeneity of resources, which make uniform allocation algorithms difficult to formulate, and distributed ownership, leading to widely varying allocation policies. Faced with these problems, we developed and implemented the classified advertisement (classad) matchmaking framework, a flexible and general approach to resource management in distributed environment with decentralized ownership of resources. Novel aspects of the framework include a semi structured data model that combines schema, data, and query in a simple but powerful specification language, and a clean separation of the matching and claiming phases of resource allocation. The representation and protocols result in a robust, scalable and flexible framework that can evolve with changing resources. The framework was designed to solve real problems encountered in the deployment of Condor, a high throughput computing system developed at the University of Wisconsin-Madison. Condor is heavily used by scientists at numerous sites around the world. It derives much of its robustness and efficiency from the matchmaking architecture"
463,97,10030,1,A taxonomy and survey of grid resource management systems for distributed computing,"Model  Experience with large scale network computing systems has shown that efficient application and system performances are not necessarily the same [17]. More specifically, it may not be possible for the same scheduler to optimize application and system performances. One solution is to have a multi-layer RMS [18]. For example, to use an application scheduler such as AppLeS [19] in conjunction with a resource scheduler such as Globus [20] to form a two-layer RMS. Further, due to the expected scale of Grid systems, a Grid RMS is most likely to be an interconnection of various RMSs that are cooperating with one another within an accepted framework. Figure 2 shows a block diagram for a system with multiple interconnected RMSs and each RMS having multiple levels. User applications use the services of grid toolkits to implement their functionality. The grid toolkits use the RMS service interfaces to present suitable application layer abstractions. In the above figure, the same abstract model is used to represent the different instances of the RMSs and the different layers within an instance. However, the different instances may implement the abstract model distinctly."
464,97,15098,1,Above the Clouds: A Berkeley View of Cloud Computing,"Provided certain obstacles are overcome, we believe Cloud Computing has the potential to transform a large part of the IT industry, making software even more attractive as a service and shaping the way IT hardware is designed and purchased.  Developers with innovative ideas for new interactive Internet services no longer require the large capital outlays in hardware to deploy their service or the human expense to operate it. They need not be concerned about over-provisioning for a service whose popularity does not meet their predictions, thus wasting costly resources, or under-provisioning for one that becomes wildly popular, thus missing potential customers and revenue. Moreover, companies with large batch-oriented tasks can get their results as quickly as their programs can scale, since using 1000 servers for one hour costs no more than using one server for 1000 hours. This elasticity of resources, without paying a premium for large scale, is unprecedented in the history of IT.  The economies of scale of very large-scale datacenters combined with  ``pay-as-you-go'' resource usage has heralded the rise of Cloud Computing. It is now attractive to deploy an innovative new Internet service on a third party's Internet Datacenter rather than your own infrastructure, and to gracefully scale its resources as it grows or declines in popularity and revenue. Expanding and shrinking daily in response to normal diurnal patterns could lower costs even further. Cloud Computing transfers the risks of over-provisioning or under-provisioning to the Cloud Computing provider, who mitigates that risk by statistical multiplexing over a much larger set of users and who offers relatively low prices due better utilization and from the economy of purchasing at a larger scale. We define terms, present an economic model that quantifies the key buy vs. pay-as-you-go decision, offer a spectrum to classify Cloud Computing providers, and give our view of the top 10 obstacles and opportunities to the growth of Cloud Computing."
465,98,4080,1,"Designing for Virtual Communities in the Service of Learning (Learning in Doing: Social, Cognitive & Computational Perspectives)","{This volume explores the theoretical, design, learning, and methodological questions relevant to designing for and researching web-based communities to support the learning process. Coming from diverse academic backgrounds, the authors examine what we do and do not know about the processes and practices of designing communities to support educational processes. Taken as a collection, the chapters point to the challenges and complex tensions that emerge when designing for a web-supported community, especially when the focal practice of the community is learning.}"
466,98,10664,1,Learning by tagging: group knowledge formation in a self-organizing learning community,This research explores the use of Social Tagging as a means by which group knowledge is formed within a learning community. A study of an undergraduate Business School class that utilizes Social Tagging is undertaken to analyze the patterns and evolution of use of tags in order to make a case for Social Tagging as a viable means to visualize and facilitate group knowledge formation.
467,99,3982,1,The Structure of Collaborative Tagging Systems,"Collaborative tagging describes the process by which many users add metadata in the form of keywords to shared content.  Recently, collaborative tagging has grown in popularity on the web, on sites that allow users to tag bookmarks, photographs and other content.  In this paper we analyze the structure of collaborative tagging systems as well as their dynamical aspects. Specifically, we discovered regularities in user activity, tag frequencies, kinds of tags used, bursts of popularity in bookmarking and a remarkable stability in the relative proportions of tags within a given url. We also present a dynamical model of collaborative tagging that predicts these stable patterns and relates them to imitation and shared knowledge."
468,99,7923,1,Collaborative Tagging and Semiotic Dynamics,"Collaborative tagging has been quickly gaining ground because of its ability to recruit the activity of web users into effectively organizing and sharing vast amounts of information. Here we collect data from a popular system and investigate the statistical properties of tag co-occurrence. We introduce a stochastic model of user behavior embodying two main aspects of collaborative tagging: (i) a frequency-bias mechanism related to the idea that users are exposed to each other's tagging activity; (ii) a notion of memory - or aging of resources - in the form of a heavy-tailed access to the past state of the system. Remarkably, our simple modeling is able to account quantitatively for the observed experimental features, with a surprisingly high accuracy. This points in the direction of a universal behavior of users, who - despite the complexity of their own cognitive processes and the uncoordinated and selfish nature of their tagging activity - appear to follow simple activity patterns."
469,100,2463,1,Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions,"This paper presents an overview of the field of recommender systems and describes the current generation of recommendation methods that are usually classified into the following three main categories: content-based, collaborative, and hybrid recommendation approaches. This paper also describes various limitations of current recommendation methods and discusses possible extensions that can improve recommendation capabilities and make recommender systems applicable to an even broader range of applications. These extensions include, among others, an improvement of understanding of users and items, incorporation of the contextual information into the recommendation process, support for multicriteria ratings, and a provision of more flexible and less intrusive types of recommendations."
470,100,5292,1,Privacy Risks in Recommender Systems,"The authors explore the conflict between personalization and privacy that arises from the existence of straddlers - users with eclectic tastes who rates products across several different types or domains -- in recommender systems. While straddlers enable serendipitous recommendations, information about their existence could be used in conjunction with other data sources to uncover identities and reveal personal details. This article discusses a graph?theoretic model for studying the benefit for and risk to straddlers."
471,101,1863,1,A saliency-based search mechanism for overt and covert shifts of visual attention,"Most models of visual search, whether involving overt eye movements or covert shifts of attention, are based on the concept of a saliency map, that is, an explicit two-dimensional map that encodes the saliency or conspicuity of objects in the visual environment. Competition among neurons in this map gives rise to a single winning location that corresponds to the next attended target. Inhibiting this location automatically allows the system to attend to the next most salient location. We describe a detailed computer implementation of such a scheme, focusing on the problem of combining information across modalities, here orientation, intensity and color information, in a purely stimulus-driven manner. The model is applied to common psychophysical stimuli as well as to a very demanding visual search task. Its successful performance is used to address the extent to which the primate visual system carries out visual search via one or more such saliency maps and how this can be tested."
472,101,3495,1,A Random Walks View of Spectral Segmentation,We present a new view of clustering and segmentation  by pairwise similarities. We interpret the  similarities as edge flows in a Markov random walk  and study the eigenvalues and eigenvectors of the  walk's transition matrix. This view shows that  spectral methods for clustering and segmentation  have a probabilistic foundation. We prove that the  Normalized Cut method arises naturally from our  framework and we provide a complete characterization  of the cases when the Normalized Cut...
473,101,6935,1,Mirror Neurons and the Simulation Theory of Mind-Reading,"A new class of visuomotor neuron has been recently discovered in the monkeyâs premotor cortex: mirror neurons. These neurons respond both when a particular action is performed by the recorded monkey and when the same action, performed by another individual, is observed. Mirror neurons appear to form a cortical system matching observation and execution of goal-related motor actions. Experimental evidence suggests that a similar matching system also exists in humans. What might be the functional role of this matching system? One possible function is to enable an organism to detect certain mental states of observed conspecifics. This function might be part of, or a precursor to, a more general mind-reading ability. Two different accounts of mindreading have been suggested. According to âtheory theoryâ, mental states are represented as inferred posits of a naive theory. According to âsimulation theoryâ, other peopleâs mental states are represented by adopting their perspective: by tracking or matching their states with resonant states of oneâs own. The activity of mirror neurons, and the fact that observers undergo motor facilitation in the same muscular groups as those utilized by target agents, are findings that accord well with simulation theory but would not be predicted by theory theory."
474,101,12100,1,Programming Collective Intelligence: Building Smart Web 2.0 Applications,"{Want to tap the power behind search rankings, product recommendations, social bookmarking, and online matchmaking? This fascinating book demonstrates how you can build Web 2.0 applications to mine the enormous amount of data created by people on the Internet. With the sophisticated algorithms in this book, you can write smart programs to access interesting datasets from other web sites, collect data from users of your own applications, and analyze and understand the data once you've found it.<br /> <br /> <em>Programming Collective Intelligence</em> takes you into the world of machine learning and statistics, and explains how to draw conclusions about user experience, marketing, personal tastes, and human behavior in general -- all from information that you and others collect every day. Each algorithm is described clearly and concisely with code that can immediately be used on your web site, blog, Wiki, or specialized application. This book explains: <ul> <li>Collaborative filtering techniques that enable online retailers to recommend products or media</li> <li>Methods of clustering to detect groups of similar items in a large dataset</li> <li>Search engine features -- crawlers, indexers, query engines, and the PageRank algorithm</li> <li>Optimization algorithms that search millions of possible solutions to a problem and choose the best one</li> <li>Bayesian filtering, used in spam filters for classifying documents based on word types and other features</li> <li>Using decision trees not only to make predictions, but to model the way decisions are made</li> <li>Predicting numerical values rather than classifications to build price models</li> <li>Support vector machines to match people in online dating sites</li> <li>Non-negative matrix factorization to find the independent features in a dataset</li> <li>Evolving intelligence for problem solving -- how a computer develops its skill by improving its own code the more it plays a game</li> </ul> Each chapter includes exercises for extending the algorithms to make them more powerful. Go beyond simple database-backed applications and put the wealth of Internet data to work for you.<br /> <br /> ""Bravo! I cannot think of a better way for a developer to first learn these algorithms and methods, nor can I think of a better way for me (an old AI dog) to reinvigorate my knowledge of the details.""<br /> -- Dan Russell, Google <br /> <br /> ""Toby's book does a great job of breaking down the complex subject matter of machine-learning algorithms into practical, easy-to-understand examples that can be directly applied to analysis of social interaction across the Web today. If I had this book two years ago, it would have saved precious time going down some fruitless paths.""<br /> -- Tim Wolters, CTO, Collective Intellect}"
475,102,3118,1,Linking {UML} models of design and requirement,"In this paper, both a UML model of requirement and a UML model of a design are defined as a pair of class diagram and a family of sequence diagrams. We then give an unified semantics for models of requirements and designs. We define the consistency between a design class diagram and the interaction diagrams and show how the removal of inconsistency can be treated as a model refinement. We then formally define the correctness of UML model of design with respect to the model of requirement."
476,102,7835,1,MDA: revenge of the modelers or UML utopia?,"Modeling is at the core of many disciplines, but it is especially important in engineering because it facilitates communication and constructs complex things from smaller parts. Model engineering or model-driven development, treats software development as a set of transformations between successive models from requirements to analysis, to design, to implementation, to deployment. We discuss UML and MDA tools for developing software models. We also discuss domain specific languages, domain-oriented programming, platform-specific models and model engineering."
477,102,12753,1,Transforming Models with ATL,"This paper presents ATL (ATLAS Transformation Language): a hybrid model transformation language that allows both declarative and imperative constructs to be used in transformation definitions. The paper describes the language syntax and semantics by using examples. ATL is supported by a set of development tools such as an editor, a compiler, a virtual machine, and a debugger. A case study shows the applicability of the language constructs. Alternative ways for implementing the case study are outlined. In addition to the current features, the planned future ATL features are briefly discussed."
478,103,12424,1,How {Flickr} helps us make sense of the world: context and content in community-contributed media collections,"The advent of media-sharing sites like Flickr and YouTube has drastically increased the volume of community-contributed multimedia resources available on the web. These collections have a previously unimagined depth and breadth, and have generated new opportunities â and new challenges â to multimedia research. How do we analyze, understand and extract patterns from these new collections? How can we use these unstructured, unrestricted community contributions of media (and annotation) to generate âknowledgeâ? As a test case, we study Flickr â a popular photo sharing website. Flickr supports photo, time and location metadata, as well as a light-weight annotation model. We extract information from this dataset using two different approaches. First, we employ a location-driven approach to generate aggregate knowledge in the form of ârepresentative tags â for arbitrary areas in the world. Second, we use a tag-driven approach to automatically extract place and event semantics for Flickr tags, based on each tagâs metadata patterns. With the patterns we extract from tags and metadata, vision algorithms can be employed with greater precision. In particular, we demonstrate a location-tag-vision-based approach to retrieving images of geography-related landmarks and features from the Flickr dataset. The results suggest that community-contributed media and annotation can enhance and improve our access to multimedia resources â and our understanding of the world."
479,104,292,1,Term identification in the biomedical literature,"Sophisticated information technologies are needed for effective data acquisition and integration from a growing body of the biomedical literature. Successful term identification is key to getting access to the stored literature information, as it is the terms (and their relationships) that convey knowledge across scientific articles. Due to the complexities of a dynamically changing biomedical terminology, term identification has been recognized as the current bottleneck in text mining, andâas a consequenceâhas become an important research topic both in natural language processing and biomedical communities. This article overviews state-of-the-art approaches in term identification. The process of identifying terms is analysed through three steps: term recognition, term classification, and term mapping. For each step, main approaches and general trends, along with the major problems, are discussed. By assessing previous work in context of the overall term identification process, the review also tries to delineate needs for future work in the field."
480,104,8295,1,{The Unified Medical Language System: an informatics research collaboration.},"In 1986, the National Library of Medicine (NLM) assembled a large multidisciplinary, multisite team to work on the Unified Medical Language System (UMLS), a collaborative research project aimed at reducing fundamental barriers to the application of computers to medicine. Beyond its tangible products, the UMLS Knowledge Sources, and its influence on the field of informatics, the UMLS project is an interesting case study in collaborative research and development. It illustrates the strengths and challenges of substantive collaboration among widely distributed research groups. Over the past decade, advances in computing and communications have minimized the technical difficulties associated with UMLS collaboration and also facilitated the development, dissemination, and use of the UMLS Knowledge Sources. The spread of the World Wide Web has increased the visibility of the information access problems caused by multiple vocabularies and many information sources which are the focus of UMLS work. The time is propitious for building on UMLS accomplishments and making more progress on the informatics research issues first highlighted by the UMLS project more than 10 years ago."
481,104,11335,1,Survey of semantic annotation platforms,"The realization of the Semantic Web requires the widespread availability of semantic annotations for existing and new documents on the Web. Semantic annotations are to tag ontology class instance data and map it into ontology classes. The fully automatic creation of semantic annotations is an unsolved problem. Instead, current systems focus on the semi-automatic creation of annotations. The Semantic Web also requires facilities for the storage of annotations and ontologies, user interfaces, access APIs, and other features to fully support annotation usage. This paper examines current Semantic Web annotation platforms that provide annotation and related services, and reviews their architecture, approaches and performance."
482,105,1385,1,Adaptive Coding of Reward Value by Dopamine Neurons,"It is important for animals to estimate the value of rewards as accurately as possible. Because the number of potential reward values is very large, it is necessary that the brain's limited resources be allocated so as to discriminate better among more likely reward outcomes at the expense of less likely outcomes. We found that midbrain dopamine neurons rapidly adapted to the information provided by reward-predicting stimuli. Responses shifted relative to the expected reward value, and the gain adjusted to the variance of reward value. In this way, dopamine neurons maintained their reward sensitivity over a large range of reward values."
483,105,3652,1,Midbrain dopamine neurons encode a quantitative reward prediction error signal.,"The midbrain dopamine neurons are hypothesized to provide a physiological correlate of the reward prediction error signal required by current models of reinforcement learning. We examined the activity of single dopamine neurons during a task in which subjects learned by trial and error when to make an eye movement for a juice reward. We found that these neurons encoded the difference between the current reward and a weighted average of previous rewards, a reward prediction error, but only for outcomes that were better than expected. Thus, the firing rate of midbrain dopamine neurons is quantitatively predicted by theoretical descriptions of the reward prediction error signal used in reinforcement learning models for circumstances in which this signal has a positive value. We also found that the dopamine system continued to compute the reward prediction error even when the behavioral policy of the animal was only weakly influenced by this computation."
484,106,16659,1,Transcript assembly and quantification by RNA-Seq reveals unannotated transcripts and isoform switching during cell differentiation.,"High-throughput mRNA sequencing (RNA-Seq) promises simultaneous transcript discovery and abundance estimation. However, this would require algorithms that are not restricted by prior gene annotations and that account for alternative transcription and splicing. Here we introduce such algorithms in an open-source software program called Cufflinks. To test Cufflinks, we sequenced and analyzed >430 million paired 75-bp RNA-Seq reads from a mouse myoblast cell line over a differentiation time series. We detected 13,692 known transcripts and 3,724 previously unannotated ones, 62% of which are supported by independent expression data or by homologous genes in other species. Over the time series, 330 genes showed complete switches in the dominant transcription start site (TSS) or splice isoform, and we observed more subtle shifts in 1,304 other genes. These results suggest that Cufflinks can illuminate the substantial regulatory flexibility and complexity in even this well-studied model of muscle development and that it can improve transcriptome-based genome annotation."
485,107,133,1,A survey of approaches to automatic schema matching,"Abstract. Schema matching is a basic problem in many database application domains, such as data integration, E-business, data warehousing, and semantic query processing. In current implementations, schema matching is typically performed manually, which has significant limitations. On the other hand, previous research papers have proposed many techniques to achieve a partial automation of the match operation for specific application domains. We present a taxonomy that covers many of these existing approaches, and we describe the approaches in some detail. In particular, we distinguish between schema-level and instance-level, element-level and structure-level, and language-based and constraint-based matchers. Based on our classification we review some previous match implementations thereby indicating which part of the solution space they cover. We intend our taxonomy and review of past work to be useful when comparing different approaches to schema matching, when developing a new match algorithm, and when implementing a schema matching component."
486,107,4499,1,{SPARQL Query Language for {RDF}},"RDF is a flexible, extensible way to represent information about World Wide Web resources. It is used to represent, among other things, personal information, social networks, metadata about digital artifacts like music and images, as well as provide a means of integration over disparate sources of information. A standardized query language for RDF data with multiple implementations offers developers and end users a way to write and to consume the results of queries across this wide range of information. This document describes a query language for RDF, called SPARQL, for querying RDF data. This document describes the query language part of SPARQL for easy access to RDF stores. It is designed to meet the requirements and design objectives described in the W3C RDF Data Access Working Group (DAWG) document ""RDF Data Access Use Cases and Requirements""."
487,107,8133,1,Exploring the computing literature with visualization and stepping stones & pathways,"This article focuses on effective aids to assist computing professionals explore computing literature. Several efforts have been made to serve this community, including the Networked Computer Technical Reference Library, which grew out of efforts to collect technical reports from academic and research departments. The purpose of the paper is to provide illustrations of how the computing community can be assisted by richer support for exploratory search, related to the authors' work with the Computing and Information Technology Interactive Digital Educational Library (CITIDEL)."
488,107,12110,1,Data Integration with Uncertainty,"This paper reports our first set of results on managing uncertainty in data integration. We posit that data-integration systems need to handle uncertainty at three levels and do so in a principled fashion. First, the semantic mappings between the data sources and the mediated schema may be approximate because there may be too many of them to be created and maintained or because in some domains (e.g., bioinformatics) it is not clear what the mappings should be. Second, the data from the sources may be extracted using information extraction techniques and so may yield erroneous data. Third, queries to the system may be posed with keywords rather than in a structured form. As a first step to building such a system, we introduce the concept of probabilistic schema mappings and analyze their formal foundations. We show that there are two possible semantics for such mappings:  by-table  semantics assumes that there exists a correct mapping but we do not know what it is;  by-tuple  semantics assumes that the correct mapping may depend on the particular tuple in the source data. We present the query complexity and algorithms for answering queries in the presence of probabilistic schema mappings, and we describe an algorithm for efficiently computing the top- k  answers to queries in such a setting. Finally, we consider using probabilistic mappings in the scenario of data exchange."
489,108,793,1,The challenge of information visualization evaluation,"As the field of information visualization matures, the tools and ideas described in our research publications are reaching users. The reports of usability studies and controlled experiments are helpful to understand the potential and limitations of our tools, but we need to consider other evaluation approaches that take into account the long exploratory nature of users tasks, the value of potential discoveries or the benefits of overall awareness. We need better metrics and benchmark repositories to compare tools, and we should also seek reports of successful adoption and demonstrated utility."
490,108,2033,1,The Small World Web,"I show that the World Wide Web is a small world, in the sense that sites are highly clustered yet the path length between them is small. I also demonstrate the advantages of a search engine which makes use of the fact that pages corresponding to a particular search query can form small world networks. In a further application, the search engine uses the small-worldness of its search results to measure the connectedness between communities on the Web."
491,108,3084,1,The Simultaneous Evolution of Author and Paper Networks,"There has been a long history of research into the structure and evolution of mankind's scientific endeavor. However, recent progress in applying the tools of science to understand science itself has been unprecedented because only recently has there been access to high-volume and high-quality data sets of scientific output (e.g., publications, patents, grants), as well as computers and algorithms capable of handling this enormous stream of data. This paper reviews major work on models that aim to capture and recreate the structure and dynamics of scientific evolution. We then introduce a general process model that simultaneously grows co-author and paper-citation networks. The statistical and dynamic properties of the networks generated by this model are validated against a 20-year data set of articles published in the Proceedings of the National Academy of Science. Systematic deviations from a power law distribution of citations to papers are well fit by a model that incorporates a partitioning of authors and papers into topics, a bias for authors to cite recent papers, and a tendency for authors to cite papers cited by papers that they have read. In this TARL model (for Topics, Aging, and Recursive Linking), the number of topics is linearly related to the clustering coefficient of the simulated paper citation network."
492,109,50,1,Metabolomics and systems biology: making sense of the soup.,"Novel techniques for acquiring metabolomics data continue to emerge. Such data require proper storage in suitably configured databases, which then permit one to establish the size of microbial metabolomes (hundreds of major metabolites) and allow the nature, organisation and control of metabolic networks to be investigated. A variety of algorithms for metabolic network reconstruction coupled to suitable modelling algorithms are the ground substances for the development of metabolic network and systems biology. Even qualitative models of metabolic networks, when subject to stoichiometric constraints, can prove highly informative, and are the first step to the quantitative models, which alone can allow the true representation of complex biochemical systems."
493,109,5589,1,Text mining and ontologies in biomedicine: Making sense of raw text,"The volume of biomedical literature is increasing at such a rate that it is becoming difficult to locate, retrieve and manage the reported information without text mining, which aims to automatically distill information, extract facts, discover implicit links and generate hypotheses relevant to user needs. Ontologies, as conceptual models, provide the necessary framework for semantic representation of textual information. The principal link between text and an ontology is terminology, which maps terms to domain-specific concepts. This paper summarises different approaches in which ontologies have been used for text-mining applications in biomedicine. 10.1093/bib/6.3.239"
494,110,7823,1,Phylogenies and the Comparative Method,"Comparative studies of the relationship between two phenotypes, or between a phenotype and an environment, are frequently carried out by invalid statistical methods. Most regression, correlation, and contingency table methods, including nonparametric methods, assume that the points are drawn independently from a common distribution. When species are taken from a branching phylogeny, they are manifestly nonindependent. Use of a statistical method that assumes independence will cause overstatement of the significance in hypothesis tests. Some illustrative examples of these phenomena have been given, and limitations of previous proposals of ways to correct for the nonindependence have been discussed. A method of correcting for the phylogeny has been proposed. It requires that we know both the tree topology and the branch lengths, and that we be willing to allow the characters to be modeled by Brownian motion on a linear scale. Given these conditions, the phylogeny specifies a set of contrasts among species, contrasts that are statistically independent and can be used in regression or correlation studies. The considerable barriers to making practical use of this technique have been discussed."
495,111,6413,1,Spread of Epidemic Disease on Networks,"The study of social networks, and in particular the spread of disease on networks, has attracted considerable recent attention in the physics community. In this paper, we show that a large class of standard epidemiological models, the so-called susceptible/infective/removed Ã´ÂÂ°ÂSIRÃ´ÂÂ°Â models can be solved exactly on a wide variety of networks. In addition to the standard but unrealistic case of fixed infectiveness time and fixed and uncorrelated probability of transmission between all pairs of individuals, we solve cases in which times and probabilities are nonuniform and correlated. We also consider one simple case of an epidemic in a structured population, that of a sexually transmitted disease in a population divided into men and women. We confirm the correctness of our exact solutions with numerical simulations of SIR epidemics on networks."
496,111,11589,1,Modulation of neuronal interactions through neuronal synchronization.,"Brain processing depends on the interactions between neuronal groups. Those interactions are governed by the pattern of anatomical connections and by yet unknown mechanisms that modulate the effective strength of a given connection. We found that the mutual influence among neuronal groups depends on the phase relation between rhythmic activities within the groups. Phase relations supporting interactions between the groups preceded those interactions by a few milliseconds, consistent with a mechanistic role. These effects were specific in time, frequency, and space, and we therefore propose that the pattern of synchronization flexibly determines the pattern of neuronal interactions."
497,112,140,1,Toward an Understanding of the Motivation of Open Source Software Developers,"An Open Source Software (OSS) project is unlikely to  be successful unless there is an accompanied community  that provides the platform for developers and users to  collaborate. Members of such communities are volunteers  whose motivation to participate and contribute is of  essential importance to the success of OSS projects. In  this paper, we aim to create an understanding of what  motivates people to participate in OSS communities. We  theorize that learning is one of the motivational forces.  Our theory is grounded in the learning theory of  Legitimate Peripheral Participation, and is supported by  analyzing the social structure of OSS communities and the  co-evolution between OSS systems and communities. We  also discuss practical implications of our theory for  creating and maintaining sustainable OSS communities as  well as for software engineering research and education."
498,112,281,1,Modeling and performance analysis of BitTorrent-like peer-to-peer networks,"In this paper, we develop simple models to study the performance of BitTorrent, a second generation peer-to-peer (P2P) application. We first present a simple fluid model and study the scalability, performance and efficiency of such a file-sharing mechanism. We then consider the built-in incentive mechanism of BitTorrent and study its effect on network performance. We also provide numerical results based on both simulations and real traces obtained from the Internet."
499,112,528,1,The impact of web-logs (blogs) on student perceptions of isolation and alienation in a web-based distance-learning environment,"In the rush to promote the use of computer-mediated technologies for both traditional and distance learning, relatively little research has been conducted about learner feelings of isolation, alienation and frustration. More recent technologies such as web-logs (blogs) may provide a wider range of tools for bridging learners' feelings of isolation. The purpose of this research is to investigate the impact of using blogs in a web-based learning environment. This qualitative investigation presents an interpretive case study of student perceptions of using blogs in a web-based technology integration course for K-12 pre-service teacher education students. Findings indicate that the use of blogs helped prevent feelings of isolation and alienation for distance learners"
500,112,1264,1,The nonsense of 'knowledge management',"Abstract  Examines critically the origins and basis of 'knowledge management', its components and its development as a field of consultancy practice. Problems in the distinction between 'knowledge' and 'information' are explored, as well as Polanyi's concept of 'tacit knowing'. The concept is examined in the journal literature, the Web sites of consultancy companies, and in the presentation of business schools. The conclusion is reached that 'knowledge management' is an umbrella term for a variety of organizational activities, none of which are concerned with the management of knowledge. Those activities that are not concerned with the management of information are concerned with the management of work practices, in the expectation that changes in such areas as communication practice will enable information sharing."
501,112,1402,1,Distributed cognition: Toward a new foundation for human-computer interaction research,"We are quickly passing through the historical moment when people work in front of a single computer, dominated by a small CRT and focused on tasks involving only local information. Networked computers are becoming ubiquitous and are playing increasingly significant roles in our lives and in the basic infrastructures of science, business, and social interaction. For human-computer interaction to advance in the new millennium we need to better understand the emerging dynamic of interaction in which the focus task is no longer confined to the desktop but reaches into a complex networked world of information and computer-mediated interactions. We think the theory of distributed cognition has a special role to play in understanding interactions between people and technologies, for its  focus has always been on whole environments: what we really do in them and how we coordinate our activity in them. Distributed cognition provides a radical reorientation of how to think about designing and supporting human-computer interaction. As a theory it is specifically tailored to understanding interactions among people and technologies. In this article we propose distributed cognition as a new foundation for human-computer interaction, sketch an integrated research framework, and use selections from our earlier work to suggest how this framework can provide new opportunities in the design of digital work materials."
502,112,2083,1,How does the Internet Affect Social Capital,"this article. One is the dramatic increase in Internet use since the 1990s, affecting the way people live, work, and play in the developed world. Approximately 60 percent of North American adult households are online, with growing percentages in other countries (Howard, Rainie, &amp; Jones, 2002; Reddick, Boucher, &amp; Groseillers, 2000). For a large proportion of the population of Internet users, Internet access is a daily activity, with more than half of Internet users reporting having been online..."
503,112,2632,1,Formally citing the Web,"How do authors refer to Web-based information sources in their formal scientific publications? It is not yet well known how scientists and scholars actually include new types of information sources, available through the new media, in their published work. This article reports on a comparative study of the lists of references in 38 scientific journals in five different scientific and social scientific fields. The fields are sociology, library and information science, biochemistry and biotechnology, neuroscience, and the mathematics of computing. As is well known, references, citations, and hyperlinks play different roles in academic publishing and communication. Our study focuses on hyperlinks as attributes of references in formal scholarly publications. The study developed and applied a method to analyze the differential roles of publishing media in the analysis of scientific and scholarly literature references. The present secondary databases that include reference and citation data (the Web of Science) cannot be used for this type of research. By the automated processing and analysis of the full text of scientific and scholarly articles, we were able to extract the references and hyperlinks contained in these references in relation to other features of the scientific and scholarly literature. Our findings show that hyperlinking references are indeed, as expected, abundantly present in the formal literature. They also tend to cite more recent literature than the average reference. The large majority of the references are to Web instances of traditional scientific journals. Other types of Web-based information sources are less well represented in the lists of references, except in the case of pure e-journals. We conclude that this can be explained by taking the role of the publisher into account. Indeed, it seems that the shift from print-based to electronic publishing has created new roles for the publisher. By shaping the way scientific references are hyperlinking to other information sources, the publisher may have a large impact on the availability of scientific and scholarly information."
504,112,3234,1,Doing Internet Research: Critical Issues and Methods for Examining the Net,"{<P>Whether or not one believes the hyperbolic claims about the Internet being the biggest thing since the invention of the wheel, the Internet is a medium with great consequences for social and economic life. <B>Doing Internet Research</B> is written to help people discern in what ways it has commanded the public imagination, and the methodological issues that arise when one tries to study and understand the social processes occurring within the Internet. Each contributor to the volume offers original responses in the search for, and critique of, methods with which to study the Internet and the social, political, economic, artistic, communicative phenomena occurring within and around it. This book provides encouragement for readers getting started with Internet research and also provides perspective on this new and ubiquitous communication medium. </P> }"
505,112,4074,1,Teaching and learning online with wikis,"Wikis are fully editable websites; any user can read or add content to a wiki site. This functionality means that wikis are an excellent tool for collaboration in an online environment. This paper presents wikis as a useful tool for facilitating online education. Basic wiki functionality is outlined and different wikis are reviewed to highlight the features that make them a valuable technology for teaching and learning online. Finally, the paper discuses a wiki project underway at Deakin University. This project uses a wiki to host an icebreaker exercise which aims to facilitate ongoing interaction between members of online learning groups. Wiki projects undertaken in America are outlined and future wiki research plans are also discussed. These wiki projects illustrate how e-learning practitioners can and are moving beyond their comfort zone by using wikis to enhance the process of teaching and learning online."
506,112,4954,1,Living in a smart environment - implications for the coming ubiquitous information society,"A good instrument for understanding the possible blessings and perils of new technology in general and ubiquitous computing in particular is the development and analysis of scenarios of the future. This paper presents some of the consequences implied by several such scenarios that have been developed in the interdisciplinary research project ""Living in a Smart Environment - Implications of Ubiquitous Computing"". To show how manifold and far-reaching such consequences might be, the paper emphasizes two particularly relevant implications. First, it discusses some of the deep economic paradigm shifts that could arise from a large-scale deployment of ubiquitous and pervasive computing technologies. Second, it investigates issues of social compatibility and dependability of future ubiquitous computing applications, considering both impending pitfalls and emerging opportunities. 2004 IEEE."
507,112,6391,1,Increasing participation in online communities: A framework for human-computer interaction,"Online communities are becoming an accepted part of the lives of Internet users, although participation in these communities is dependent on the types of people that form them. Some of the online community's members do not participate, people referred to as lurkers, whereas others who have been in the community for a long time, referred to as elders, participate regularly and support others. Understanding what drives these individuals and how they chose whether or not to participate will lead to online communities that thrive. This paper proposes a conceptual framework to describe what drives such individuals to carry out actions such as posting messages and adding content (level 1), the cognitions they use to determine whether or not to take such actions (level 2) and the means by which they go about carrying out the action in the environment (level 3). Finally, the framework is applied to the problem of encouraging members to participate by discussing the methods by which people can be persuaded to participate by changing the way they interpret their desires and their environment. (PsycINFO Database Record (c) 2008 APA, all rights reserved) (journal abstract)"
508,112,7887,1,"Why Minimal Guidance During Instruction Does Not Work: An Analysis of the Failure of Constructivist, Discovery, Problem-Based, Experiential, and Inquiry-Based Teaching","Evidence for the superiority of guided instruction is explained in the context of our knowledge of human cognitive architecture, expertânovice differences, and cognitive load. Although unguided or minimally guided instructional approaches are very popular and intuitively appealing, the point is made that these approaches ignore both the structures that constitute human cognitive architecture and evidence from empirical studies over the past half-century that consistently indicate that minimally guided instruction is less effective and less efficient than instructional approaches that place a strong emphasis on guidance of the student learning process. The advantage of guidance begins to recede only when learners have sufficiently high prior knowledge to provide ""internal"" guidance. Recent developments in instructional research and instructional design models that support guidance during instruction are briefly described. [ABSTRACT FROM AUTHOR]; Copyright of Educational Psychologist is the property of Lawrence Erlbaum Associates and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This abstract may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full abstract. (Copyright applies to all Abstracts.)"
509,112,10738,1,A review of research on the impact of professional learning communities on teaching practice and student learning,"After an overview of the characteristics of professional learning communities (PLCs), this manuscript presents a review of 10 American studies and one English study on the impact of PLCs on teaching practices and student learning. Although, few studies move beyond self-reports of positive impact, a small number of empirical studies explore the impact on teaching practice and student learning. The collective results of these studies suggest that well-developed PLCs have positive impact on both teaching practice and student achievement. Implications of this research and suggestions for next steps in the efforts to document the impact of PLCs on teaching and learning are included."
510,113,4190,1,Mobile genetic elements: the agents of open source evolution.,"Horizontal genomics is a new field in prokaryotic biology that is focused on the analysis of DNA sequences in prokaryotic chromosomes that seem to have originated from other prokaryotes or eukaryotes. However, it is equally important to understand the agents that effect DNA movement: plasmids, bacteriophages and transposons. Although these agents occur in all prokaryotes, comprehensive genomics of the prokaryotic mobile gene pool or 'mobilome' lags behind other genomics initiatives owing to challenges that are distinct from cellular chromosomal analysis. Recent work shows promise of improved mobile genetic element (MGE) genomics and consequent opportunities to take advantage â and avoid the dangers â of these 'natural genetic engineers'. This review describes MGEs, their properties that are important in horizontal gene transfer, and current opportunities to advance MGE genomics."
511,113,11139,1,Error bars in experimental biology.,"Error bars commonly appear in figures in publications, but experimental biologists are often unsure how they should be used and interpreted. In this article we illustrate some basic features of error bars and explain how they can help communicate data and assist correct interpretation. Error bars may show confidence intervals, standard errors, standard deviations, or other quantities. Different types of error bars give quite different information, and so figure legends must make clear what error bars represent. We suggest eight simple rules to assist with effective use and interpretation of error bars."
512,113,16756,1,The ecological coherence of high bacterial taxonomic ranks," The species is a fundamental unit of biological organization, but its relevance for Bacteria and Archaea is still hotly debated. Even more controversial is whether the deeper branches of the ribosomal RNA-derived phylogenetic tree, such as the phyla, have ecological importance. Here, we discuss the ecological coherence of high bacterial taxa in the light of genome analyses and present examples of niche differentiation between deeply diverging groups in terrestrial and aquatic systems. The ecological relevance of high bacterial taxa has implications for bacterial taxonomy, evolution and ecology."
513,114,1485,1,"""GrabCut"": interactive foreground extraction using iterated graph cuts","Figure 1: Three examples of GrabCut. The user drags a rectangle loosely around an object. The object is then extracted automatically. The problem of efficient, interactive foreground/background segmentation in still images is of great practical importance in image editing. Classical image segmentation tools use either texture (colour) information, e.g. Magic Wand, or edge (contrast) information, e.g. Intelligent Scissors. Recently, an approach based on optimization by graph-cut has been developed which successfully combines both types of information. In this paper we extend the graph-cut approach in three respects. First, we have developed a more powerful, iterative version of the optimisation. Secondly, the power of the iterative algorithm is used to simplify substantially the user interaction needed for a given quality of result. Thirdly, a robust algorithm for âborder matting â has been developed to estimate simultaneously the alpha-matte around an object boundary and the colours of foreground pixels. We show that for moderately difficult examples the proposed method outperforms competitive tools."
514,114,4808,1,Probabilistic Models in Information Retrieval,"In this paper, an introduction and survey over probabilistic information retrieval (IR) is given. First, the basic concepts of this approach are described: the probability-ranking principle shows that optimum retrieval quality can be achieved under certain assumptions; a conceptual model for IR along with the corresponding event space clarify the interpretation of the probabilistic parameters involved. For the estimation of these parameters, three different learning strategies are distinguished, namely query-related, document-related and description-related learning. As a representative for each of these strategies, a specific model is described. A new approach regards IR as uncertain inference; here, imaging is used as a new technique for estimating the probabilistic parameters, and probabilistic inference networks support more complex forms of inference. Finally, the more general problems of parameter estimations, query expansion and the development of models for advanced document representations are discussed. 10.1093/comjnl/35.3.243"
515,114,7448,1,Class-Based n-gram Models of Natural Language,"We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their cooccurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics. 1 ..."
516,114,9828,1,Automatic linguistic indexing of pictures by a statistical modeling approach,"Automatic linguistic indexing of pictures is an important but highly challenging problem for researchers in computer vision and content-based image retrieval. In this paper, we introduce a statistical modeling approach to this problem. Categorized images are used to train a dictionary of hundreds of statistical models each representing a concept. Images of any given concept are regarded as instances of a stochastic process that characterizes the concept. To measure the extent of association between an image and the textual description of a concept, the likelihood of the occurrence of the image based on the characterizing stochastic process is computed. A high likelihood indicates a strong association. In our experimental implementation, we focus on a particular group of stochastic processes, that is, the two-dimensional multiresolution hidden Markov models (2D MHMMs). We implemented and tested our ALIP (Automatic Linguistic Indexing of Pictures) system on a photographic image database of 600 different concepts, each with about 40 training images. The system is evaluated quantitatively using more than 4,600 images outside the training database and compared with a random annotation scheme. Experiments have demonstrated the good accuracy of the system and its high potential in linguistic indexing of photographic images."
517,115,904,1,A unified framework for model-based clustering,"Model-based clustering techniques have been widely used and have shown promising results in many applications involving complex data. This paper presents a unified framework for probabilistic model-based clustering based on a bipartite graph view of data and models that highlights the commonalities and differences among existing model-based clustering algorithms. In this view, clusters are represented as probabilistic models in a model space that is conceptually separate from the data space. For partitional clustering, the view is conceptually similar to the Expectation-Maximization (EM) algorithm. For hierarchical clustering, the graph-based view helps to visualize critical/important distinctions between similarity-based approaches and model-based approaches. The framework also suggests several useful variations of existing clustering algorithms. Two new variationsâbalanced model-based clustering and hybrid model-based clusteringâare discussed and empirically evaluated on a variety of data types."
518,115,6470,1,A framework and methodology for studying the causes of software errors in programming systems,"An essential aspect of programmersâ work is the correctness of their code. This makes current HCI techniques ill-suited to analyze and design the programming systems that programmers use everyday, since these techniques focus more on problems with learnability and efficiency of use, and less on error-proneness. We propose a framework and methodology that focuses specifically on errors by supporting the description and identification of the causes of software errors in terms of chains of cognitive breakdowns. The framework is based on both old and new studies of programming, as well as general research on the mechanisms of human error. Our experiences using the framework and methodology to study the Alice programming system have directly inspired the design of several new programming tools and interfaces. This includes the Whyline debugging interface, which we have shown to reduce debugging time by a factor of 8 and help programmers get 40% further through their tasks. We discuss the framework's and methodology's implications for programming system design, software engineering, and the psychology of programming."
519,116,15,1,Collective dynamics of 'small-world' networks.,"Networks of coupled dynamical systems have been used to model biological oscillators1, 2, 3, 4, Josephson junction arrays5,6, excitable media7, neural networks8, 9, 10, spatial games11, genetic control networks12 and many other self-organizing systems. Ordinarily, the connection topology is assumed to be either completely regular or completely random. But many biological, technological and social networks lie somewhere between these two extremes. Here we explore simple models of networks that can be tuned through this middle ground: regular networks 'rewired' to introduce increasing amounts of disorder. We find that these systems can be highly clustered, like regular lattices, yet have small characteristic path lengths, like random graphs. We call them 'small-world' networks, by analogy with the small-world phenomenon13,14 (popularly known as six degrees of separation15). The neural network of the worm Caenorhabditis elegans, the power grid of the western United States, and the collaboration graph of film actors are shown to be small-world networks. Models of dynamical systems with small-world coupling display enhanced signal-propagation speed, computational power, and synchronizability. In particular, infectious diseases spread more easily in small-world networks than in regular lattices."
520,116,401,1,The Small-World Phenomenon: An Algorithmic Perspective,"Long a matter of folklore, the &amp;quot;small-world phenomenon &amp;quot;--- the principle that we are all linked by short chains of acquaintances--- was inaugurated as an area of experimental study in the social sciences through the pioneering work of Stanley Milgram in the 1960&#039;s. This work was among the first to make the phenomenon quantitative, allowing people to speak of the &amp;quot;six degrees of separation &amp;quot; between any two people in the United States. Since then, a number of network models have been proposed as frameworks in which to study the problem analytically. One of the most refined of these models was formulated in recent work of Watts and Strogatz; their framework provided compelling evidence that the small-world phenomenon is pervasive in a range of networks arising in nature and technology, and a fundamental ingredient in the evolution of the World Wide Web. But existing models are insu#cient to explain the striking algorithmic component of Milgram&#039;s original findings: that individuals using local information are collectively very e#ective at actually constructing short paths between two points in a social network. Although recently proposed network models are rich in short paths, we prove that no decentralized algorithm, operating with local information only, can construct short paths in these networks with non-negligible probability. We then define an infinite family of network models that naturally generalizes the Watts-Strogatz model, and show that for one of these models, there is a decentralized algorithm capable of finding short paths with high probability. More generally, we provide a strong characterization of this family of network models, showing that there is in fact a unique model within the family for which decentralized algorithms are e#ective."
521,116,1091,1,Artificial intelligence : a modern approach,"The long-anticipated revision of this best-selling book offers the most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For those interested in artificial intelligence."
522,116,1611,1,{Evolving protein interaction networks through gene duplication},"The topology of the proteome map revealed by recent large-scale hybridization methods has shown that the distribution of protein-protein interactions is highly heterogeneous, with many proteins having few edges while a few of them are heavily connected. This particular topology is shared by other cellular networks, such as metabolic pathways, and it has been suggested to be responsible for the high mutational homeostasis displayed by the genome of some organisms. In this paper we explore a recent model of proteome evolution that has been shown to reproduce many of the features displayed by its real counterparts. The model is based on gene duplication plus re-wiring of the newly created genes. The statistical features displayed by the proteome of well-known organisms are reproduced and suggest that the overall topology of the protein maps naturally emerges from the two leading mechanisms considered by the model."
523,116,2605,1,"Foundations of Language: Brain, Meaning, Grammar, Evolution","Already hailed as a masterpiece, Foundations of Language offers a brilliantoverhaul of the last thirty-five years of research in generative linguisticsand related fields. ""Few books really deserve the cliche 'this should be readby every researcher in the field,'"" writes Steven Pinker, author of TheLanguage Instinct, ""But Ray Jackendoff's Foundations of Language does.""Foundations of Language offers a radically new understanding of how language,the brain, and perception intermesh. The book renews the promise of earlygenerative linguistics: that language can be a valuable entree intounderstanding the human mind and brain. The approach is remarkablyinterdisciplinary. Behind its innovations is Jackendoff's fundamental proposalthat the creativity of language derives from multiple parallel generativesystems linked by interface components. this shift in basic architecture makespossible a radical reconception of mental grammar and how it is learned. As aconsequence, Jackendoff is able to reintegrate linguistics with philosophy ofmind, cognitive and developmental psychology, evolutionary biology,neuroscience, and computational linguistics. Among the major topics treatedare language processing, the relation of language to perception, theinnateness of language, and the evolution of the language capacity, as well asmore standard issues in linguistic theory such as the roles of syntax and thelexicon. In addition, Jackendoff offers a sophisticated theory of semanticsthat incorporates insights from philosophy of language, logic and formalsemantics, lexical semantics of various stripes, cognitive grammar,psycholinguistic and neurolinguistic approaches, and the author's ownconceptual semantics.Here then is the most fundamental contribution to linguistic theory in overthree decades."
524,116,3761,1,"The Faculty of Language: What Is It, Who Has It, and How Did It Evolve?","We argue that an understanding of the faculty of language requires substantial interdisciplinary cooperation. We suggest how current developments in linguistics can be profitably wedded to work in evolutionary biology, anthropology, psychology, and neuroscience. We submit that a distinction should be made between the faculty of language in the broad sense (FLB) and in the narrow sense (FLN). FLB includes a sensory-motor system, a conceptual-intentional system, and the computational mechanisms for recursion, providing the capacity to generate an infinite range of expressions from a finite set of elements. We hypothesize that FLN only includes recursion and is the only uniquely human component of the faculty of language. We further argue that FLN may have evolved for reasons other than language, hence comparative studies might look for evidence of such computations outside of the domain of communication (for example, number, navigation, and social relations)."
525,116,5489,1,The Major Transitions in Evolution,"{Over the history of life there have been several major changes in the way genetic information is organized and transmitted from one generation to the next. These transitions include the origin of life itself, the first eukaryotic cells, reproduction by sexual means, the appearance of<br>multicellular plants and animals, the emergence of cooperation and of animal societies, and the unique language ability of humans. This ambitious book provides the first unified discussion of the full range of these transitions. The authors highlight the similarities between different<br>transitions--between the union of replicating molecules to form chromosomes and of cells to form multicellular organisms, for example--and show how understanding one transition sheds light on others. They trace a common theme throughout the history of evolution: after a major transition some<br>entities lose the ability to replicate independently, becoming able to reproduce only as part of a larger whole. The authors investigate this pattern and why selection between entities at a lower level does not disrupt selection at more complex levels. Their explanation encompasses a compelling<br>theory of the evolution of cooperation at all levels of complexity. Engagingly written and filled with numerous illustrations, this book can be read with enjoyment by anyone with an undergraduate training in biology. It is ideal for advanced discussion groups on evolution and includes accessible<br>discussions of a wide range of topics, from molecular biology and linguistics to insect societies.}"
526,116,7147,1,Meaningful Information,"The information in an individual finite object (like a binary string) is commonly measured by its Kolmogorov complexity. One can divide that information into two parts: the information accounting for the useful regularity present in the object and the information accounting for the remaining accidental information. There can be several ways (model classes) in which the regularity is expressed. Kolmogorov has proposed the model class of finite sets, generalized later to computable probability mass functions. The resulting theory, known as Algorithmic Statistics, analyzes the algorithmic sufficient statistic when the statistic is restricted to the given model class. However, the most general way to proceed is perhaps to express the useful information as a recursive function. The resulting measure has been called the ``sophistication'' of the object. We develop the theory of recursive functions statistic, the maximum and minimum value, the existence of absolutely nonstochastic objects (that have maximal sophistication--all the information in them is meaningful and there is no residual randomness), determine its relation with the more restricted model classes of finite sets, and computable probability distributions, in particular with respect to the algorithmic (Kolmogorov) minimal sufficient statistic, the relation to the halting problem and further algorithmic properties."
527,116,8197,1,Robust Real-Time Face Detection,"This paper describes a face detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the {âIntegral} Imageâ which allows the features used by our detector to be computed very quickly. The second is a simple and efficient classifier which is built using the {AdaBoost} learning algorithm {(Freund} and Schapire, 1995) to select a small number of critical visual features from a very large set of potential features. The third contribution is a method for combining classifiers in a âcascadeâ which allows background regions of the image to be quickly discarded while spending more computation on promising face-like regions. A set of experiments in the domain of face detection is presented. The system yields face detection performance comparable to the best previous systems {(Sung} and Poggio, 1998; Rowley et al., 1998; Schneiderman and Kanade, 2000; Roth et al., 2000). Implemented on a conventional desktop, face detection proceeds at 15 frames per second."
528,116,9400,1,Language within our grasp,"In monkeys, the rostral part of ventral premotor cortex (area F5) contains neurons that discharge, both when the monkey grasps or manipulates objects and when it observes the experimenter making similar actions. These neurons (mirror neurons) appear to represent a system that matches observed events to similar, internally generated actions, and in this way forms a link between the observer and the actor. Transcranial magnetic stimulation and positron emission tomography (PET) experiments suggest that a mirror system for gesture recognition also exists in humans and includes Broca's area. We propose here that such an observation/execution matching system provides a necessary bridge from'doing' to'communicating',as the link between actor and observer becomes a link between the sender and the receiver of each message Type: JOURNAL ARTICLE Type: REVIEW Type: REVIEW, TUTORIAL Language: Eng"
529,116,10565,1,Collective minds,"By tapping into social cues, individuals in a group may gain access to higher-order computational capacities that mirror the group's responses to its environment. In 1905 the field naturalist Edmund Selous, a confirmed Darwinian and meticulous observer of bird behaviour, wrote of his wonderment when observing tens of thousands of starlings coming together to roost: ""they circle; now dense like a polished roof, now disseminated like the meshes of some vast all-heaven-sweeping net...wheeling, rending, darting...a madness in the sky"".Throughout his life Selous struggled to explain the remarkable synchrony and coherence of motion during flocking, and he concluded that somehow a connectivity of individual minds and transference of thoughts must underlie such behaviour."
530,116,11123,1,A General Theory of Bibliometric and Other Cumulative Advantage Processes,"Abstract 10.1002/asi.4630270505.abs A Cumulative Advantage Distribution is proposed which models statistically the situation in which success breeds success. It differs from the Negative Binomial Distribution in that lack of success, being a non-event, is not punished by increased chance of failure. It is shown that such a stochastic law is governed by the Beta Function, containing only one free parameter, and this is approximated by a skew or hyperbolic distribution of the type that is widespread in bibliometrics and diverse social science phenomena. In particular, this is shown to be an appropriate underlying probabilistic theory for the Bradford Law, the Lotka Law, the Pareto and Zipf Distributions, and for all the empirical results of citation frequency analysis. As side results one may derive also the obsolescence factor for literature use. The Beta Function is peculiarly elegant for these manifold purposes because it yields both the actual and the cumulative distributions in simple form, and contains a limiting case of an inverse square law to which many empirical distributions conform."
531,117,14592,1,Assessing the relevance of node features for network structure,"10.1073/pnas.0811511106 Networks describe a variety of interacting complex systems in social science, biology, and information technology. Usually the nodes of real networks are identified not only by their connections but also by some other characteristics. Examples of characteristics of nodes can be age, gender, or nationality of a person in a social network, the abundance of proteins in the cell taking part in protein-interaction networks, or the geographical position of airports that are connected by directed flights. Integrating the information on the connections of each node with the information about its characteristics is crucial to discriminating between the essential and negligible characteristics of nodes for the structure of the network. In this paper we propose a general indicator Î, based on entropy measures, to quantify the dependence of a network's structure on a given set of features. We apply this method to social networks of friendships in U.S. schools, to the protein-interaction network of  and to the U.S. airport network, showing that the proposed measure provides information that complements other known measures."
532,117,15639,1,The Energy Landscape of Social Balance,"We model a close-knit community of friends and enemies as a fully connected network with positive and negative signs on its edges. Theories from social psychology suggest that certain sign patterns are more stable than others. This notion of social ""balance"" allows us to define an energy landscape for such networks. Its structure is complex: numerical experiments reveal a landscape dimpled with local minima of widely varying energy levels. We derive rigorous bounds on the energies of these local minima and prove that they have a modular structure that can be used to classify them."
533,117,15789,1,Economic Networks: The New Challenges,"The current economic crisis illustrates a critical need for new and fundamental understanding of the structure and dynamics of economic networks. Economic systems are increasingly built on interdependencies, implemented through trans-national credit and investment networks, trade relations, or supply chains that have proven difficult to predict and control. We need, therefore, an approach that stresses the systemic complexity of economic networks and that can be used to revise and extend established paradigms in economic theory. This will facilitate the design of policies that reduce conflicts between individual interests and global efficiency, as well as reduce the risk of global failure by making economic networks more robust. 10.1126/science.1173644"
534,117,15928,1,Cooperative Behavior Cascades in Human Social Networks,"Theoretical models suggest that social networks influence the evolution ofcooperation, but to date there have been few experimental studies.Observational data suggest that a wide variety of behaviors may spread in humansocial networks, but subjects in such studies can choose to befriend peoplewith similar behaviors, posing difficulty for causal inference. Here, weexploit a seminal set of laboratory experiments that originally showed thatvoluntary costly punishment can help sustain cooperation. In these experiments,subjects were randomly assigned to a sequence of different groups in order toplay a series of single-shot public goods games with strangers; this featureallowed us to draw networks of interactions to explore how cooperative anduncooperative behavior spreads from person to person to person. We show that,in both an ordinary public goods game and in a public goods game withpunishment, focal individuals are influenced by fellow group members'contribution behavior in future interactions with other individuals who werenot a party to the initial interaction. Furthermore, this influence persistsfor multiple periods and spreads up to three degrees of separation (from personto person to person to person). The results suggest that each additionalcontribution a subject makes to the public good in the first period is tripledover the course of the experiment by other subjects who are directly orindirectly influenced to contribute more as a consequence. These are the firstresults to show experimentally that cooperative behavior cascades in humansocial networks."
535,117,16389,1,Rules for Biologically Inspired Adaptive Network Design,"Transport networks are ubiquitous in both social and biological systems. Robust network performance involves a complex trade-off involving cost, transport efficiency, and fault tolerance. Biological networks have been honed by many cycles of evolutionary selection pressure and are likely to yield reasonable solutions to such combinatorial optimization problems. Furthermore, they develop without centralized control and may represent a readily scalable solution for growing networks in general. We show that the slime mold Physarum polycephalum forms networks with comparable efficiency, fault tolerance, and cost to those of real-world infrastructure networks--in this case, the Tokyo rail system. The core mechanisms needed for adaptive network formation can be captured in a biologically inspired mathematical model that may be useful to guide network construction in other domains. 10.1126/science.1177894"
536,118,248,1,Data clustering: a review,"Clustering is the unsupervised classification of patterns (observations, data items, or feature vectors) into groups (clusters). The clustering problem has been addressed in many contexts and by researchers in many disciplines; this reflects its broad appeal and usefulness as one of the steps in exploratory data analysis. However, clustering is a difficult problem combinatorially, and differences in assumptions and contexts in different communities has made the transfer of useful generic concepts and methodologies slow to occur. This paper presents an overview of pattern clustering methods from a statistical pattern recognition perspective, with a goal of providing useful advice and references to fundamental concepts accessible to the broad community of clustering practitioners. We present a taxonomy of clustering techniques, and identify  cross-cutting themes and recent advances. We also describe some important applications of clustering algorithms such as image segmentation, object recognition, and information retrieval."
537,118,2222,1,Latent {D}irichlet Allocation,"We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of  discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each  item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in  turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of  text modeling, the topic probabilities provide an explicit representation of a document. We present  efficient approximate inference techniques based on variational methods and an EM algorithm for  empirical Bayes parameter estimation. We report results in document modeling, text classification,  and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI  model."
538,118,2804,1,Content-based image retrieval at the end of the early years,"The paper presents a review of 200 references in content-based image retrieval. The paper starts with discussing the working conditions of content-based retrieval: patterns of use, types of pictures, the role of semantics, and the sensory gap. Subsequent sections discuss computational steps for image retrieval systems. Step one of the review is image processing for retrieval sorted by color, texture, and local geometry. Features for retrieval are discussed next, sorted by: accumulative and global features, salient points, object and shape features, signs, and structural combinations thereof. Similarity of pictures and objects in pictures is reviewed for each of the feature types, in close connection to the types and means of feedback the user of the systems is capable of giving by interaction. We briefly discuss aspects of system engineering: databases, system architecture, and evaluation. In the concluding section, we present our view on: the driving force of the field, the heritage from computer vision, the influence on computer vision, the role of similarity and of interaction, the need for databases, the problem of evaluation, and the role of the semantic gap."
539,118,4683,1,Effective personalization based on association rule discovery from web usage data,"To engage visitors to a Web site at a very early stage (i.e., before registration or authentication), personalization tools must rely primarily on clickstream data captured in Web server logs. The lack of explicit user ratings as well as the sparse nature and the large volume of data in such a setting poses serious challenges to standard collaborative filtering techniques in terms of scalability and performance. Web usage mining techniques such as clustering that rely on offline pattern discovery from user transactions can be used to improve the scalability of collaborative filtering, however, this is often at the cost of reduced recommendation accuracy. In this paper we propose effective and scalable techniques for Web personalization based on association rule discovery from usage data. Through detailed experimental evaluation on real usage data, we show that the proposed methodology can achieve better recommendation effectiveness, while maintaining a computational advantage over direct approaches to collaborative filtering such as the  k -nearest-neighbor strategy."
540,118,8326,1,Object categories and expertise: Is the basic level in the eye of the beholder?,"Classic research on conceptual hierarchies has shown that the interaction between the human perceiver and objects in the environment specifies one level of abstraction for categorizing objects, called the basic level, which plays a primary role in cognition. The question of whether the special psychological status of the basic level can be modified by experience was addressed in three experiments comparing the performance of subjects in expert and novice domains. The main findings were that in the domain of expertise (a) subordinate-level categories were as differentiated as the basic-level categories, (b) subordinate-level names were used as frequently as basic-level names for identifying objects, and (c) subordinate-level categorizations were as fast as basic-level categorizations. Taken together, these results demonstrate that individual differences in domain-specific knowledge affect the extent that the basic level is central to categorization."
541,118,9718,1,Scaling to Very Very Large Corpora for Natural Language Disambiguation,"The amount of readily available on-line text has reached hundreds of billions of words and continues to grow. Yet for most core natural language tasks, algorithms continue to be optimized, tested and compared after training on corpora consisting of only one million words or less. In this paper, we evaluate the performance of different learning methods on a prototypical natural language disambiguation task, confusion set disambiguation, when trained on orders of magnitude more labeled data than has previously been used. We are fortunate that for this particular application, correctly labeled training data is free. Since this will often not be the case, we examine methods for effectively exploiting very large corpora when labeled data comes at a cost."
542,118,11220,1,Frequent Pattern Mining: Current Status and Future Directions,"Frequent pattern mining has been a focused theme in data mining research for over a decade. Abundant literature has been dedicated to this research and tremendous progress has been made, ranging from efficient and scalable algorithms for frequent itemset mining in transaction databases to numerous research frontiers, such as sequential pattern mining, structured pattern mining, correlation mining, associative classification, and frequent pattern-based clustering, as well as their broad applications. In this article, we provide a brief overview of the current status of frequent pattern mining and discuss a few promising research directions. We believe that frequent pattern mining research has substantially broadened the scope of data analysis and will have deep impact on data mining methodologies and applications in the long run. However, there are still some challenging research issues that need to be solved before frequent pattern mining can claim a cornerstone approach in data mining applications."
543,118,14846,1,Novelty and diversity in information retrieval evaluation,"Evaluation measures act as objective functions to be optimized by information retrieval systems. Such objective functions must accurately reflect user requirements, particularly when tuning IR systems and learning ranking functions. Ambiguity in queries and redundancy in retrieved documents are poorly reflected by current evaluation measures. In this paper, we present a framework for evaluation that systematically rewards novelty and diversity. We develop this framework into a specific evaluation measure, based on cumulative gain. We demonstrate the feasibility of our approach using a test collection based on the TREC question answering track."
544,119,454,1,Indexing by Latent Semantic Analysis,"A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (&#034;semantic structure&#034;) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. Deerwester - 1 - 1. Introduction We describe here a new approach to automatic indexing and retrieval. It is designed to overcome a fundamental problem that plagu..."
545,119,9456,1,From Data Mining to Knowledge Discovery in Databases,"â  Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field. Across a wide variety of fields, data are"
546,120,12934,1,Measuring emotion: the Self-Assessment Manikin and the Semantic Differential.,"The Self-Assessment Manikin (SAM) is a non-verbal pictorial assessment technique that directly measures the pleasure, arousal, and dominance associated with a person's affective reaction to a wide variety of stimuli. In this experiment, we compare reports of affective experience obtained using SAM, which requires only three simple judgments, to the Semantic Differential scale devised by Mehrabian and Russell (An approach to environmental psychology, 1974) which requires 18 different ratings. Subjective reports were measured to a series of pictures that varied in both affective valence and intensity. Correlations across the two rating methods were high both for reports of experienced pleasure and felt arousal. Differences obtained in the dominance dimension of the two instruments suggest that SAM may better track the personal response to an affective stimulus. SAM is an inexpensive, easy method for quickly assessing reports of affective response in many contexts."
547,121,8860,1,Pfam: a comprehensive database of protein domain families based on seed alignments.,"Databases of multiple sequence alignments are a valuable aid to protein sequence classification and analysis. One of the main challenges when constructing such a database is to simultaneously satisfy the conflicting demands of completeness on the one hand and quality of alignment and domain definitions on the other. The latter properties are best dealt with by manual approaches, whereas completeness in practice is only amenable to automatic methods. Herein we present a database based on hidden Markov model profiles (HMMs), which combines high quality and completeness. Our database, Pfam, consists of parts A and B. Pfam-A is curated and contains well-characterized protein domain families with high quality alignments, which are maintained by using manually checked seed alignments and HMMs to find and align all members. Pfam-B contains sequence families that were generated automatically by applying the Domainer algorithm to cluster and align the remaining protein sequences after removal of Pfam-A domains. By using Pfam, a large number of previously unannotated proteins from the Caenorhabditis elegans genome project were classified. We have also identified many novel family memberships in known proteins, including new kazal, Fibronectin type III, and response regulator receiver domains. Pfam-A families have permanent accession numbers and form a library of HMMs available for searching and automatic annotation of new protein sequences. Proteins: 28:405-420, 1997. Â© 1997 Wiley-Liss, Inc."
548,122,6236,1,Illuminating drug discovery with biological pathways.,"Systems biology promises to impact significantly on the drug discovery process. One of its ultimate goals is to provide an understanding of the complete set of molecular mechanisms describing an organism. Although this goal is a long way off, many useful insights can already come from currently available information and technology. One of the biggest challenges in drug discovery today is the high attrition rate: many promising candidates prove ineffective or toxic owing to a poor understanding of the molecular mechanisms of biological systems they target. A ""systems"" approach can help identify pathways related to a disease and can suggest secondary effects of drugs that might cause these problems and thus ultimately improve the drug discovery pipeline."
549,122,12336,1,Drugâtarget network,"The global set of relationships between protein targets of all drugs and all disease-gene products in the human proteinâprotein interaction or 'interactome' network remains uncharacterized. We built a bipartite graph composed of US Food and Drug Administrationâapproved drugs and proteins linked by drugâtarget binary associations. The resulting network connects most drugs into a highly interlinked giant component, with strong local clustering of drugs of similar types according to Anatomical Therapeutic Chemical classification. Topological analyses of this network quantitatively showed an overabundance of 'follow-on' drugs, that is, drugs that target already targeted proteins. By including drugs currently under investigation, we identified a trend toward more functionally diverse targets improving polypharmacology. To analyze the relationships between drug targets and disease-gene products, we measured the shortest distance between both sets of proteins in current models of the human interactome network. Significant differences in distance were found between etiological and palliative drugs. A recent trend toward more rational drug design was observed."
550,122,14610,1,Network pharmacology: the next paradigm in drug discovery,"The dominant paradigm in drug discovery is the concept of designing maximally selective ligands to act on individual drug targets. However, many effective drugs act via modulation of multiple proteins rather than single targets. Advances in systems biology are revealing a phenotypic robustness and a network structure that strongly suggests that exquisitely selective compounds, compared with multitarget drugs, may exhibit lower than desired clinical efficacy. This new appreciation of the role of polypharmacology has significant implications for tackling the two major sources of attrition in drug developmentâefficacy and toxicity. Integrating network biology and polypharmacology holds the promise of expanding the current opportunity space for druggable targets. However, the rational design of polypharmacology faces considerable challenges in the need for new methods to validate target combinations and optimize multiple structure-activity relationships while maintaining drug-like properties. Advances in these areas are creating the foundation of the next paradigm in drug discovery: network pharmacology."
551,122,15488,1,Drug discovery chemistry: a primer for the non-specialist," Like all scientific disciplines, drug discovery chemistry is rife with terminology and methodology that can seem intractable to those outside the sphere of synthetic chemistry. Derived from a successful in-house workshop, this Foundation Review aims to demystify some of this inherent terminology, providing the non-specialist with a general insight into the nomenclature, terminology and workflow of medicinal chemists within the pharmaceutical industry."
552,122,16090,1,Structure-Based Predictive Models for Allosteric Hot Spots,"In allostery, a binding event at one site in a protein modulates the behavior of a distant site. Identifying residues that relay the signal between sites remains a challenge. We have developed predictive models using support-vector machines, a widely used machine-learning method. The training data set consisted of residues classified as either hotspots or non-hotspots based on experimental characterization of point mutations from a diverse set of allosteric proteins. Each residue had an associated set of calculated features. Two sets of features were used, one consisting of dynamical, structural, network, and informatic measures, and another of structural measures defined by Daily and Gray [1] . The resulting models performed well on an independent data set consisting of hotspots and non-hotspots from five allosteric proteins. For the independent data set, our top 10 models using Feature Set 1 recalled 68Ã¢â¬â81% of known hotspots, and among total hotspot predictions, 58Ã¢â¬â67% were actual hotspots. Hence, these models have precision P = 58Ã¢â¬â67% and recall R = 68Ã¢â¬â81%. The corresponding models for Feature Set 2 had P = 55Ã¢â¬â59% and R = 81Ã¢â¬â92%. We combined the features from each set that produced models with optimal predictive performance. The top 10 models using this hybrid feature set had R = 73Ã¢â¬â81% and P = 64Ã¢â¬â71%, the best overall performance of any of the sets of models. Our methods identified hotspots in structural regions of known allosteric significance. Moreover, our predicted hotspots form a network of contiguous residues in the interior of the structures, in agreement with previous work. In conclusion, we have developed models that discriminate between known allosteric hotspots and non-hotspots with high accuracy and sensitivity. Moreover, the pattern of predicted hotspots corresponds to known functional motifs implicated in allostery, and is consistent with previous work describing sparse networks of allosterically important residues."
553,123,3114,1,Weblogs as a bridging genre,"Abstract:  Purpose â Aims to describe systematically the characteristics of weblogs (blogs) â frequently modified web pages in which dated entries are listed in reverse chronological sequence and which are the latest genre of internet communication to attain widespread popularity.  Design/methodology/approach â This paper presents the results of a quantitative content analysis of 203 randomly selected blogs, comparing the empirically observable features of the corpus with popular claims about the nature of blogs, and finding them to differ in a number of respects.  Findings â Notably, blog authors, journalists and scholars alike exaggerate the extent to which blogs are interlinked, interactive, and oriented towards external events, and underestimate the importance of blogs as individualistic, intimate forms of self-expression.  Originality/value â Based on the profile generated by the empirical analysis, considers the likely antecedents of the blog genre, situates it with respect to the dominant forms of digital communication on the internet today, and suggests possible developments of the use of blogs over time in response to changes in user behavior, technology, and the broader ecology of internet genres."
554,124,406,1,Notable design patterns for domain-specific languages,"The realisation of domain-specific languages ( s) differs in fundamental ways from that of traditional programming languages. We describe eight recurring patterns that we have identified as being used for design and implementation. Existing languages can be extended, restricted, partially used, or become hosts for s. Simple s can be implemented by lexical processing. In addition, s can be used to create front-ends to existing systems or to express complicated data structures. Finally, s can be combined using process pipelines. The patterns described form a pattern language that can be used as a building block for a systematic view of the software development process involving s."
