{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WORKING_the_mean_squares.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "bHfUHx4ygQZ9",
        "colab_type": "code",
        "outputId": "0ebcc475-c25a-443e-8c48-13e5999c0d60",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision torchtext\n",
        "import torch\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tcmalloc: large alloc 1073750016 bytes == 0x5906a000 @  0x7f3b899532a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dG7ACwjqEwJQ",
        "colab_type": "code",
        "cellView": "form",
        "outputId": "f62f363f-85cf-4c0a-b4be-9f92def34e06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1448
        }
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!python -m spacy download en\n",
        "!pip install msgpack==0.5.6\n",
        "!pip install spacy==2.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Requirement already satisfied: msgpack==0.5.6 in /usr/local/lib/python3.6/dist-packages (0.5.6)\n",
            "Collecting spacy==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/84/e53592c3fce989a5c917dce6a45cb40d8c6724441d218c9682d3988aba39/spacy-2.0.0.tar.gz (13.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 13.2MB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (1.14.6)\n",
            "Collecting murmurhash<0.29,>=0.28 (from spacy==2.0.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/82/55/7f050e9f73c9a58df219c63e77304b0ff01676847061dc99abb484cff3a8/murmurhash-0.28.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting cymem<1.32,>=1.30 (from spacy==2.0.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/a5/0f/d29aa68c55db37844c77e7e96143bd96651fd0f4453c9f6ee043ac846b77/cymem-1.31.2-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting preshed<2.0.0,>=1.0.0 (from spacy==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/88/57a818051f3d71e800bfb7ba4df56d3ea5793482ef11f1d2109b726f3bac/preshed-1.0.1-cp36-cp36m-manylinux1_x86_64.whl (80kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 22.6MB/s \n",
            "\u001b[?25hCollecting thinc<6.11.0,>=6.10.0 (from spacy==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/b1/47a88072d0a38b3594c0a638a62f9ef7c742b8b8a87f7b105f7ed720b14b/thinc-6.10.3.tar.gz (1.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 1.2MB 13.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (0.9.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (1.11.0)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (1.35)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (0.2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (2.18.4)\n",
            "Collecting regex==2017.4.5 (from spacy==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K    100% |████████████████████████████████| 604kB 20.9MB/s \n",
            "\u001b[?25hCollecting ftfy<5.0.0,>=4.4.2 (from spacy==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/5d/9385540977b00df1f3a0c0f07b7e6c15b5e7a3109d7f6ae78a0a764dab22/ftfy-4.4.3.tar.gz (50kB)\n",
            "\u001b[K    100% |████████████████████████████████| 51kB 19.1MB/s \n",
            "\u001b[?25hCollecting msgpack-python (from spacy==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8a/20/6eca772d1a5830336f84aca1d8198e5a3f4715cd1c7fc36d3cc7f7185091/msgpack-python-0.5.6.tar.gz (138kB)\n",
            "\u001b[K    100% |████████████████████████████████| 143kB 26.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack-numpy in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (0.4.3.2)\n",
            "Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.0->spacy==2.0.0) (0.5.6)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.0->spacy==2.0.0) (0.9.0.1)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.0->spacy==2.0.0) (1.10.11)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.0->spacy==2.0.0) (4.28.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.0) (2018.11.29)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.0) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.0) (1.22)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.0->spacy==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy==2.0.0) (0.5.1)\n",
            "Building wheels for collected packages: spacy, thinc, regex, ftfy, msgpack-python\n",
            "  Running setup.py bdist_wheel for spacy ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ae/7a/c1/48ed01646ebbacb5cce1cec6cc708708093c37a09ae1d67f77\n",
            "  Running setup.py bdist_wheel for thinc ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/db/bc/e1/9b321b6b203288cf636a56e668ed5700076af4ed66062278ca\n",
            "  Running setup.py bdist_wheel for regex ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "  Running setup.py bdist_wheel for ftfy ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/37/54/00/d320239bfc8aad1455314f302dd82a75253fc585e17b81704e\n",
            "  Running setup.py bdist_wheel for msgpack-python ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d5/de/86/7fa56fda12511be47ea0808f3502bc879df4e63ab168ec0406\n",
            "Successfully built spacy thinc regex ftfy msgpack-python\n",
            "Installing collected packages: murmurhash, cymem, preshed, thinc, regex, ftfy, msgpack-python, spacy\n",
            "  Found existing installation: murmurhash 1.0.1\n",
            "    Uninstalling murmurhash-1.0.1:\n",
            "      Successfully uninstalled murmurhash-1.0.1\n",
            "  Found existing installation: cymem 2.0.2\n",
            "    Uninstalling cymem-2.0.2:\n",
            "      Successfully uninstalled cymem-2.0.2\n",
            "  Found existing installation: preshed 2.0.1\n",
            "    Uninstalling preshed-2.0.1:\n",
            "      Successfully uninstalled preshed-2.0.1\n",
            "  Found existing installation: thinc 6.12.1\n",
            "    Uninstalling thinc-6.12.1:\n",
            "      Successfully uninstalled thinc-6.12.1\n",
            "  Found existing installation: regex 2018.1.10\n",
            "    Uninstalling regex-2018.1.10:\n",
            "      Successfully uninstalled regex-2018.1.10\n",
            "  Found existing installation: spacy 2.0.18\n",
            "    Uninstalling spacy-2.0.18:\n",
            "      Successfully uninstalled spacy-2.0.18\n",
            "Successfully installed cymem-1.31.2 ftfy-4.4.3 msgpack-python-0.5.6 murmurhash-0.28.0 preshed-1.0.1 regex-2017.4.5 spacy-2.0.0 thinc-6.10.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5u8at9kYHTBd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip show spacy\n",
        "!pip show msgpack"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UYE1-KYPYGrY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DATA LOADER\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from torchtext import data, vocab\n",
        "import torch\n",
        "import spacy\n",
        "from random import randint\n",
        "\n",
        "spacy_en = spacy.load('en')\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def tokenizer(text):  # create a tokenizer function\n",
        "  turncate = 100\n",
        "  \n",
        "  tokens = [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "  tokens = list(filter(lambda token: token not in STOP_WORDS, tokens))\n",
        "  if 0 < turncate < len(tokens):\n",
        "    return tokens[:turncate]\n",
        "  return tokens\n",
        "\n",
        "\n",
        "class citeulike:\n",
        "    \"\"\"\n",
        "    Class to handle the Cite-U-Like data\n",
        "\n",
        "    Predict:\n",
        "    match_status\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, batch_size=100):\n",
        "        print('Device: ' + str(device))\n",
        "\n",
        "        self.user = data.Field(sequential=False, use_vocab=False, dtype=torch.int64)\n",
        "        self.doc_title = data.Field(sequential=True, lower=True, tokenize=tokenizer, include_lengths=True)\n",
        "        self.ratings = data.Field(sequential=False, use_vocab=False)\n",
        "        # self.doc_abstract = data.Field(sequential=True, tokenize=tokenizer, lower=True)\n",
        "\n",
        "        self.train_set, self.validation_set, self.test_set = data.TabularDataset.splits(\n",
        "            path='./gdrive/My Drive/Colab Notebooks/data/',\n",
        "            train='train_data.csv',\n",
        "            validation='val_data.csv',\n",
        "            test='test_data.csv',\n",
        "            format='csv',\n",
        "            fields=[\n",
        "                ('index', None),\n",
        "                ('user', self.user),\n",
        "                ('doc_id', None),\n",
        "                ('ratings', self.ratings),\n",
        "                # ('REAL_DOC_TITLE', None), # comment this line out to use the doc title\n",
        "                ('doc_title', self.doc_title), \n",
        "                # ('doc_abstract', self.doc_abstract)\n",
        "            ],\n",
        "            skip_header=True,\n",
        "        )\n",
        "\n",
        "        self.train_iter, self.validation_iter, self.test_iter = data.BucketIterator.splits(\n",
        "            (self.train_set, self.validation_set, self.test_set),\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            device=device,\n",
        "            sort_key=lambda x: len(x.doc_title),\n",
        "            sort_within_batch=True,\n",
        "            repeat=False)\n",
        "\n",
        "        self.user.build_vocab(self.train_set)\n",
        "        self.ratings.build_vocab(self.train_set)\n",
        "        url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
        "        # self.doc_abstract.build_vocab(self.train_set, max_size=None, vectors=vocab.Vectors('wiki.simple.vec', url=url))\n",
        "        self.doc_title.build_vocab(self.train_set, max_size=None, vectors=vocab.Vectors('wiki.simple.vec', url=url))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nvDFnPDH7FTQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# MODEL\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.utils.rnn as rnn\n",
        "\n",
        "\n",
        "class CollaborativeFilteringNet(nn.Module):\n",
        "\n",
        "    def __init__(self, article_field, user_field, author_dim=20, p1=0, p2=0):\n",
        "\n",
        "        super(CollaborativeFilteringNet, self).__init__()\n",
        "\n",
        "        article_vectors = article_field.vocab.vectors\n",
        "        num_embeddings = article_vectors.size()[0]\n",
        "        self.embedding_dim = article_vectors.size()[1]\n",
        "\n",
        "        self.article_embeddings = nn.Embedding(num_embeddings, self.embedding_dim)\n",
        "        self.article_embeddings.weight.data.copy_(article_vectors)\n",
        "        self.article_embeddings.requires_grad = False\n",
        "\n",
        "        num_author = len(user_field.vocab.freqs)\n",
        "        self.author_embedding = nn.Embedding(num_author, author_dim)\n",
        "        self.author_embedding.weight.data.uniform_(0, 0.01)\n",
        "        self.author_embedding.requires_grad = False\n",
        "\n",
        "        self.l_text = nn.Sequential(\n",
        "            nn.Dropout(p1),\n",
        "            nn.Linear(in_features=(self.embedding_dim),\n",
        "                   out_features=author_dim,\n",
        "                   bias=True),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.l_out = nn.Sequential(\n",
        "            nn.Dropout(p2),\n",
        "            nn.Linear(in_features=author_dim * 2,\n",
        "                   out_features=1,\n",
        "                   bias=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        author = self.author_embedding(x.user)\n",
        "        text = x.doc_title\n",
        "        \n",
        "        numpy_text = text.cpu().data.numpy()\n",
        "        num_non_ones = np.count_nonzero(np.subtract(numpy_text, np.ones(numpy_text.shape)), axis=0)\n",
        "        num_non_ones = np.repeat(np.expand_dims(num_non_ones, 1), self.embedding_dim, 1)\n",
        "        num_non_ones = torch.tensor(num_non_ones).to(device).float()\n",
        "        \n",
        "        text = self.article_embeddings(text)\n",
        "        text = torch.sum(text, 0).to(device) / num_non_ones\n",
        "        text = self.l_text(text)\n",
        "        \n",
        "        x = torch.cat((author, text), 1)\n",
        "        x = self.l_out(x)\n",
        "                \n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class LstmNet(nn.Module):\n",
        "    def __init__(self, article_field, user_field, user_dim=50, hidden_dim=50, lstm_layers=5):\n",
        "        super(LstmNet, self).__init__()\n",
        "        article_vectors = article_field.vocab.vectors\n",
        "        num_embeddings = article_vectors.size()[0]\n",
        "        embedding_dim = article_vectors.size()[1]\n",
        "\n",
        "        self.article_embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
        "        self.article_embeddings.weight.data.copy_(article_vectors)\n",
        "\n",
        "        num_author = len(user_field.vocab.freqs)\n",
        "        self.author_embedding = nn.Embedding(num_author, user_dim)\n",
        "        self.author_embedding.weight.data.uniform_(0, 0.01)\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm_layers = lstm_layers\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=lstm_layers)\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(in_features=(user_dim + hidden_dim),\n",
        "                      out_features=1,\n",
        "                      bias=True),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = len(x.user)\n",
        "        user = self.author_embedding(x.user)\n",
        "        text = self.article_embeddings(x.doc_title)\n",
        "\n",
        "        ## Packing and padding\n",
        "        # packed = rnn.pack_padded_sequence(text, lengths)\n",
        "        lstm_out, (lstm_hidden, lstm_state) = self.lstm(text)\n",
        "        # padded, lengths = rnn.pad_packed_sequence(lstm_out)\n",
        "\n",
        "\n",
        "        # x = torch.cat((user, lstm_state[-1]), 1).cuda()\n",
        "        # x = self.linear(x)\n",
        "\n",
        "        x = (user * lstm_state[-1]).sum(1)\n",
        "\n",
        "        out = torch.sigmoid(x)\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mF95z_C-hW3S",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TRAIN\n",
        "\n",
        "from types import SimpleNamespace\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "\n",
        "\n",
        "def train_with_negative_sampling(train_iter, val_iter, net, test_iter, optimizer, criterion, num_epochs=5):\n",
        "    train_loss = []\n",
        "    train_error = []\n",
        "    train_accs = []\n",
        "    val_res = []\n",
        "    for i in range(num_epochs):\n",
        "        net.train()\n",
        "        for batch in train_iter:\n",
        "            users,(docs, lengths), ratings = batch.user, batch.doc_title, batch.ratings\n",
        "            net.train()\n",
        "\n",
        "            batch_with_negative_sampling = {'user': users, 'doc_title': docs}\n",
        "            output = net(SimpleNamespace(**batch_with_negative_sampling)).reshape(-1)\n",
        "            targets = ratings.float().to(device)\n",
        "            batch_loss = criterion(output, targets)\n",
        "\n",
        "            train_loss.append(get_numpy(batch_loss))\n",
        "            train_error.append(accuracy_sigmoid(output, targets))\n",
        "            train_accs.append(accuracy(output, targets))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            batch_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print(\n",
        "            #     \"Train loss: {:.3f}, Train avg error: {:.3f}\"\n",
        "            #         .format(criterion(output, target), accuracy_sigmoid(output, target)))\n",
        "\n",
        "\n",
        "        net.eval()\n",
        "        val_loss, val_accs, val_err, val_length = [0, 0, 0, 0]\n",
        "\n",
        "        for val_batch in val_iter:\n",
        "            users, (docs, lengths), ratings = val_batch.user, val_batch.doc_title, val_batch.ratings\n",
        "            batch_without_negative_sampling = {'user': users, 'doc_title': docs}\n",
        "            val_output = net(SimpleNamespace(**batch_without_negative_sampling),).reshape(-1)\n",
        "            val_target = ratings.float().to(device)\n",
        "            val_loss += criterion(val_output, val_target) * val_batch.batch_size\n",
        "            val_err += accuracy_sigmoid(val_output, val_target) * val_batch.batch_size\n",
        "            val_accs += accuracy_sigmoid(val_output, val_target) * val_batch.batch_size\n",
        "            val_length += val_batch.batch_size\n",
        "\n",
        "        val_loss /= val_length\n",
        "        val_err /= val_length\n",
        "        val_accs /= val_length\n",
        "        val_res.append(val_accs)\n",
        "\n",
        "        print(\n",
        "            \"Epoch {}: Train loss: {:.2f},  Train accs: {:.2f}, Train avg error: {:.2f}\"\n",
        "                .format(i+1, np.mean(train_loss), 1.0 - np.mean(train_accs), np.mean(train_error)))\n",
        "        print(\n",
        "            \"          Validation loss: {:.2f}, Validation accs: {:.2f}, Validation avg error: {:.2f}\"\n",
        "                .format(val_loss, 1.0 - val_accs, val_err))\n",
        "        print()\n",
        "\n",
        "\n",
        "def accuracy_sigmoid(output, target):\n",
        "    return torch.mean(torch.abs(output - target).float()).cpu().data.numpy()\n",
        "\n",
        "\n",
        "def accuracy(output, target):\n",
        "    return torch.mean(torch.abs(torch.round(output) - target)).cpu().data.numpy()\n",
        "\n",
        "\n",
        "def get_numpy(loss):\n",
        "    return loss.cpu().data.numpy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KpVwL5MTLpDS",
        "colab_type": "code",
        "outputId": "b5d49981-07f6-4a07-e401-691a625140f9",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# MAIN\n",
        "\n",
        "from torch import optim, nn\n",
        "\n",
        "citeulike = citeulike(batch_size=20)\n",
        "\n",
        "train_iter = citeulike.train_iter\n",
        "test_iter = citeulike.test_iter\n",
        "validation_iter = citeulike.validation_iter\n",
        "\n",
        "user_field = citeulike.user\n",
        "title_field = citeulike.doc_title\n",
        "\n",
        "print('Data loaded.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda:0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/wiki.simple.vec: 293MB [00:04, 62.9MB/s]                           \n",
            "  0%|          | 0/111051 [00:00<?, ?it/s]Skipping token b'111051' with 1-dimensional vector [b'300']; likely a header\n",
            "100%|█████████▉| 110663/111051 [00:14<00:00, 7823.40it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Data loaded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Trsl3N6Bf93",
        "colab_type": "code",
        "outputId": "583e9e83-4c6d-48e7-aaae-082969a2d229",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1547
        }
      },
      "cell_type": "code",
      "source": [
        "#Simple Collaborative filtering model\n",
        "\n",
        "net = CollaborativeFilteringNet(article_field=title_field, user_field=user_field, author_dim=50).to(device)\n",
        "#opt = optim.Adam(net.parameters(), lr=1e-4, weight_decay=1e-6)\n",
        "opt = optim.SGD(net.parameters(), lr=1e-2, momentum=0.8)\n",
        "criterion = nn.BCELoss()\n",
        "train_with_negative_sampling(train_iter=train_iter, test_iter=test_iter, val_iter=validation_iter,\n",
        "                                net=net, optimizer=opt, criterion=criterion, num_epochs=30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train loss: 0.69,  Train accs: 0.57, Train avg error: 0.50\n",
            "          Validation loss: 0.67, Validation accs: 0.52, Validation avg error: 0.48\n",
            "\n",
            "Epoch 2: Train loss: 0.65,  Train accs: 0.62, Train avg error: 0.47\n",
            "          Validation loss: 0.63, Validation accs: 0.57, Validation avg error: 0.43\n",
            "\n",
            "Epoch 3: Train loss: 0.64,  Train accs: 0.64, Train avg error: 0.45\n",
            "          Validation loss: 0.62, Validation accs: 0.58, Validation avg error: 0.42\n",
            "\n",
            "Epoch 4: Train loss: 0.63,  Train accs: 0.65, Train avg error: 0.44\n",
            "          Validation loss: 0.61, Validation accs: 0.58, Validation avg error: 0.42\n",
            "\n",
            "Epoch 5: Train loss: 0.63,  Train accs: 0.65, Train avg error: 0.44\n",
            "          Validation loss: 0.60, Validation accs: 0.58, Validation avg error: 0.42\n",
            "\n",
            "Epoch 6: Train loss: 0.62,  Train accs: 0.66, Train avg error: 0.44\n",
            "          Validation loss: 0.62, Validation accs: 0.58, Validation avg error: 0.42\n",
            "\n",
            "Epoch 7: Train loss: 0.62,  Train accs: 0.66, Train avg error: 0.43\n",
            "          Validation loss: 0.61, Validation accs: 0.58, Validation avg error: 0.42\n",
            "\n",
            "Epoch 8: Train loss: 0.62,  Train accs: 0.66, Train avg error: 0.43\n",
            "          Validation loss: 0.62, Validation accs: 0.58, Validation avg error: 0.42\n",
            "\n",
            "Epoch 9: Train loss: 0.62,  Train accs: 0.66, Train avg error: 0.43\n",
            "          Validation loss: 0.61, Validation accs: 0.58, Validation avg error: 0.42\n",
            "\n",
            "Epoch 10: Train loss: 0.62,  Train accs: 0.66, Train avg error: 0.43\n",
            "          Validation loss: 0.59, Validation accs: 0.59, Validation avg error: 0.41\n",
            "\n",
            "Epoch 11: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.43\n",
            "          Validation loss: 0.62, Validation accs: 0.58, Validation avg error: 0.42\n",
            "\n",
            "Epoch 12: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.43\n",
            "          Validation loss: 0.58, Validation accs: 0.60, Validation avg error: 0.40\n",
            "\n",
            "Epoch 13: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.58, Validation accs: 0.59, Validation avg error: 0.41\n",
            "\n",
            "Epoch 14: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.63, Validation accs: 0.57, Validation avg error: 0.43\n",
            "\n",
            "Epoch 15: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.60, Validation accs: 0.59, Validation avg error: 0.41\n",
            "\n",
            "Epoch 16: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.56, Validation accs: 0.61, Validation avg error: 0.39\n",
            "\n",
            "Epoch 17: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.59, Validation accs: 0.59, Validation avg error: 0.41\n",
            "\n",
            "Epoch 18: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.60, Validation accs: 0.59, Validation avg error: 0.41\n",
            "\n",
            "Epoch 19: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.59, Validation accs: 0.59, Validation avg error: 0.41\n",
            "\n",
            "Epoch 20: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.61, Validation accs: 0.58, Validation avg error: 0.42\n",
            "\n",
            "Epoch 21: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.61, Validation accs: 0.58, Validation avg error: 0.42\n",
            "\n",
            "Epoch 22: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.63, Validation accs: 0.57, Validation avg error: 0.43\n",
            "\n",
            "Epoch 23: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.62, Validation accs: 0.58, Validation avg error: 0.42\n",
            "\n",
            "Epoch 24: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.59, Validation accs: 0.59, Validation avg error: 0.41\n",
            "\n",
            "Epoch 25: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.60, Validation accs: 0.59, Validation avg error: 0.41\n",
            "\n",
            "Epoch 26: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.61, Validation accs: 0.58, Validation avg error: 0.42\n",
            "\n",
            "Epoch 27: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.62, Validation accs: 0.58, Validation avg error: 0.42\n",
            "\n",
            "Epoch 28: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.59, Validation accs: 0.59, Validation avg error: 0.41\n",
            "\n",
            "Epoch 29: Train loss: 0.61,  Train accs: 0.67, Train avg error: 0.42\n",
            "          Validation loss: 0.61, Validation accs: 0.58, Validation avg error: 0.42\n",
            "\n",
            "Epoch 30: Train loss: 0.61,  Train accs: 0.68, Train avg error: 0.42\n",
            "          Validation loss: 0.61, Validation accs: 0.58, Validation avg error: 0.42\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AhOHl8xBG0fr",
        "colab_type": "code",
        "outputId": "c0b27913-ea8c-4973-8240-de25dc86b399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1547
        }
      },
      "cell_type": "code",
      "source": [
        "# Collaborative filtering with LSTM\n",
        "\n",
        "net = LstmNet(article_field=title_field, user_field=user_field).to(device)\n",
        "opt = optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-6)\n",
        "#opt = optim.SGD(net.parameters(), lr=1e-1, momentum=0.8)\n",
        "criterion = nn.BCELoss()\n",
        "train_with_negative_sampling(train_iter=train_iter, test_iter=test_iter, val_iter=validation_iter,\n",
        "                                net=net, optimizer=opt, criterion=criterion, num_epochs=30)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1: Train loss: 0.59,  Train accs: 0.68, Train avg error: 0.41\n",
            "          Validation loss: 0.42, Validation accs: 0.70, Validation avg error: 0.30\n",
            "\n",
            "Epoch 2: Train loss: 0.54,  Train accs: 0.72, Train avg error: 0.36\n",
            "          Validation loss: 0.38, Validation accs: 0.73, Validation avg error: 0.27\n",
            "\n",
            "Epoch 3: Train loss: 0.50,  Train accs: 0.75, Train avg error: 0.33\n",
            "          Validation loss: 0.37, Validation accs: 0.74, Validation avg error: 0.26\n",
            "\n",
            "Epoch 4: Train loss: 0.47,  Train accs: 0.77, Train avg error: 0.31\n",
            "          Validation loss: 0.33, Validation accs: 0.77, Validation avg error: 0.23\n",
            "\n",
            "Epoch 5: Train loss: 0.45,  Train accs: 0.78, Train avg error: 0.29\n",
            "          Validation loss: 0.32, Validation accs: 0.79, Validation avg error: 0.21\n",
            "\n",
            "Epoch 6: Train loss: 0.43,  Train accs: 0.80, Train avg error: 0.28\n",
            "          Validation loss: 0.30, Validation accs: 0.81, Validation avg error: 0.19\n",
            "\n",
            "Epoch 7: Train loss: 0.41,  Train accs: 0.81, Train avg error: 0.26\n",
            "          Validation loss: 0.31, Validation accs: 0.81, Validation avg error: 0.19\n",
            "\n",
            "Epoch 8: Train loss: 0.39,  Train accs: 0.82, Train avg error: 0.25\n",
            "          Validation loss: 0.30, Validation accs: 0.82, Validation avg error: 0.18\n",
            "\n",
            "Epoch 9: Train loss: 0.37,  Train accs: 0.83, Train avg error: 0.24\n",
            "          Validation loss: 0.29, Validation accs: 0.83, Validation avg error: 0.17\n",
            "\n",
            "Epoch 10: Train loss: 0.36,  Train accs: 0.84, Train avg error: 0.23\n",
            "          Validation loss: 0.31, Validation accs: 0.83, Validation avg error: 0.17\n",
            "\n",
            "Epoch 11: Train loss: 0.35,  Train accs: 0.84, Train avg error: 0.22\n",
            "          Validation loss: 0.31, Validation accs: 0.83, Validation avg error: 0.17\n",
            "\n",
            "Epoch 12: Train loss: 0.33,  Train accs: 0.85, Train avg error: 0.21\n",
            "          Validation loss: 0.31, Validation accs: 0.84, Validation avg error: 0.16\n",
            "\n",
            "Epoch 13: Train loss: 0.32,  Train accs: 0.86, Train avg error: 0.20\n",
            "          Validation loss: 0.33, Validation accs: 0.83, Validation avg error: 0.17\n",
            "\n",
            "Epoch 14: Train loss: 0.31,  Train accs: 0.86, Train avg error: 0.19\n",
            "          Validation loss: 0.34, Validation accs: 0.83, Validation avg error: 0.17\n",
            "\n",
            "Epoch 15: Train loss: 0.30,  Train accs: 0.87, Train avg error: 0.19\n",
            "          Validation loss: 0.35, Validation accs: 0.83, Validation avg error: 0.17\n",
            "\n",
            "Epoch 16: Train loss: 0.29,  Train accs: 0.87, Train avg error: 0.18\n",
            "          Validation loss: 0.36, Validation accs: 0.83, Validation avg error: 0.17\n",
            "\n",
            "Epoch 17: Train loss: 0.28,  Train accs: 0.88, Train avg error: 0.18\n",
            "          Validation loss: 0.37, Validation accs: 0.83, Validation avg error: 0.17\n",
            "\n",
            "Epoch 18: Train loss: 0.27,  Train accs: 0.88, Train avg error: 0.17\n",
            "          Validation loss: 0.37, Validation accs: 0.83, Validation avg error: 0.17\n",
            "\n",
            "Epoch 19: Train loss: 0.27,  Train accs: 0.89, Train avg error: 0.17\n",
            "          Validation loss: 0.38, Validation accs: 0.83, Validation avg error: 0.17\n",
            "\n",
            "Epoch 20: Train loss: 0.26,  Train accs: 0.89, Train avg error: 0.16\n",
            "          Validation loss: 0.38, Validation accs: 0.84, Validation avg error: 0.16\n",
            "\n",
            "Epoch 21: Train loss: 0.25,  Train accs: 0.89, Train avg error: 0.16\n",
            "          Validation loss: 0.40, Validation accs: 0.83, Validation avg error: 0.17\n",
            "\n",
            "Epoch 22: Train loss: 0.25,  Train accs: 0.90, Train avg error: 0.15\n",
            "          Validation loss: 0.40, Validation accs: 0.84, Validation avg error: 0.16\n",
            "\n",
            "Epoch 23: Train loss: 0.24,  Train accs: 0.90, Train avg error: 0.15\n",
            "          Validation loss: 0.40, Validation accs: 0.84, Validation avg error: 0.16\n",
            "\n",
            "Epoch 24: Train loss: 0.24,  Train accs: 0.90, Train avg error: 0.15\n",
            "          Validation loss: 0.44, Validation accs: 0.83, Validation avg error: 0.17\n",
            "\n",
            "Epoch 25: Train loss: 0.23,  Train accs: 0.90, Train avg error: 0.14\n",
            "          Validation loss: 0.41, Validation accs: 0.84, Validation avg error: 0.16\n",
            "\n",
            "Epoch 26: Train loss: 0.22,  Train accs: 0.91, Train avg error: 0.14\n",
            "          Validation loss: 0.44, Validation accs: 0.83, Validation avg error: 0.17\n",
            "\n",
            "Epoch 27: Train loss: 0.22,  Train accs: 0.91, Train avg error: 0.14\n",
            "          Validation loss: 0.44, Validation accs: 0.83, Validation avg error: 0.17\n",
            "\n",
            "Epoch 28: Train loss: 0.22,  Train accs: 0.91, Train avg error: 0.13\n",
            "          Validation loss: 0.42, Validation accs: 0.84, Validation avg error: 0.16\n",
            "\n",
            "Epoch 29: Train loss: 0.21,  Train accs: 0.91, Train avg error: 0.13\n",
            "          Validation loss: 0.46, Validation accs: 0.83, Validation avg error: 0.17\n",
            "\n",
            "Epoch 30: Train loss: 0.21,  Train accs: 0.91, Train avg error: 0.13\n",
            "          Validation loss: 0.44, Validation accs: 0.84, Validation avg error: 0.16\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-BWkAOZKGmeL",
        "colab_type": "code",
        "outputId": "414d929e-7fec-4f7c-cd3d-b2f11f7ac06d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(title_field.vocab.freqs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13483"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    }
  ]
}