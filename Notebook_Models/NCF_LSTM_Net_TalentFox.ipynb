{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NCF_LSTM_Net_TalentFox.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "bHfUHx4ygQZ9",
        "colab_type": "code",
        "outputId": "90ca7343-2201-42d6-ae70-9d3b55933bc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1924
        }
      },
      "cell_type": "code",
      "source": [
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision torchtext\n",
        "import torch\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "!python -m spacy download en\n",
        "\n",
        "#####################################################\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "\n",
        "######################################################\n",
        "\n",
        "!python -m spacy download en\n",
        "!pip install msgpack==0.5.6\n",
        "!pip install spacy==2.0.0\n",
        "\n",
        "# WHAT YOU NEED TO DO\n",
        "# This piece of code will install everything to colab. However, you need to upload the csv files to your drive\n",
        "# to the My Drive/Colab Notebooks/data/' path. When you run this part, it will ask you to connect your own google drive\n",
        "# so you will need to give access to it - it seems i can't share my drive through Colab unfortunately\n",
        "\n",
        "# import torch.backends.cudnn as cudnn\n",
        "# cudnn.enabled = False"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 37.4MB 63.5MB/s \n",
            "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): en-core-web-sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: spacy>=2.0.0a18 in /usr/local/lib/python3.6/dist-packages (from en-core-web-sm==2.0.0) (2.0.0)\n",
            "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (4.4.3)\n",
            "Requirement already satisfied: msgpack-numpy in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.4.3.2)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.35)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.9.6)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (2017.4.5)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.14.6)\n",
            "Requirement already satisfied: thinc<6.11.0,>=6.10.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (6.10.3)\n",
            "Requirement already satisfied: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.31.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.11.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (2.18.4)\n",
            "Requirement already satisfied: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.28.0)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.2.8.2)\n",
            "Requirement already satisfied: msgpack-python in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.5.6)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: msgpack>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from msgpack-numpy->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.5.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (4.28.1)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.10.11)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.9.0.1)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (2018.11.29)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (2.6)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.5.1)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.9.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 37.4MB 59.8MB/s \n",
            "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): en-core-web-sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: spacy>=2.0.0a18 in /usr/local/lib/python3.6/dist-packages (from en-core-web-sm==2.0.0) (2.0.0)\n",
            "Requirement already satisfied: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.28.0)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.14.6)\n",
            "Requirement already satisfied: msgpack-python in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.5.6)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.35)\n",
            "Requirement already satisfied: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.31.2)\n",
            "Requirement already satisfied: msgpack-numpy in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.4.3.2)\n",
            "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (2017.4.5)\n",
            "Requirement already satisfied: thinc<6.11.0,>=6.10.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (6.10.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (2.18.4)\n",
            "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (4.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.11.0)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.2.8.2)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.9.6)\n",
            "Requirement already satisfied: msgpack>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from msgpack-numpy->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.5.6)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.10.11)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.9.0.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (4.28.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (2018.11.29)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (2.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.22)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.0->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy>=2.0.0a18->en-core-web-sm==2.0.0) (0.5.1)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Requirement already satisfied: msgpack==0.5.6 in /usr/local/lib/python3.6/dist-packages (0.5.6)\n",
            "Requirement already satisfied: spacy==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied: msgpack-numpy in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (0.4.3.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (2.18.4)\n",
            "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (2017.4.5)\n",
            "Requirement already satisfied: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (1.31.2)\n",
            "Requirement already satisfied: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (0.28.0)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (0.9.6)\n",
            "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: thinc<6.11.0,>=6.10.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (6.10.3)\n",
            "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (4.4.3)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (1.35)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (1.11.0)\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (0.2.8.2)\n",
            "Requirement already satisfied: msgpack-python in /usr/local/lib/python3.6/dist-packages (from spacy==2.0.0) (0.5.6)\n",
            "Requirement already satisfied: msgpack>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from msgpack-numpy->spacy==2.0.0) (0.5.6)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.0) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.0) (2018.11.29)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.0) (1.22)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.0->spacy==2.0.0) (1.10.11)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.0->spacy==2.0.0) (0.9.0.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.0->spacy==2.0.0) (4.28.1)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy==2.0.0) (1.0.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.0->spacy==2.0.0) (0.9.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from html5lib->ftfy<5.0.0,>=4.4.2->spacy==2.0.0) (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hOeHBff7gXsa",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Data pre-processing file\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from torchtext import data, vocab\n",
        "import torch\n",
        "import spacy\n",
        "from random import randint\n",
        "\n",
        "spacy_en = spacy.load('en')\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TalentFox:\n",
        "    \"\"\"\n",
        "    Class to handle the TalentFox data\n",
        "\n",
        "    Predict:\n",
        "    match_status\n",
        "\n",
        "    Columns for candidate:\n",
        "    candidate_city, candidate_state, candidate_country, candidate_title, candidate_birth_date,\n",
        "    candidate_current_fixed_salary, candidate_current_bonus_salary, candidate_in_job_market_since,\n",
        "    candidate_other_languages, candidate_is_looking_for_new_job, candidate_wish_2, candidate_wish_3, candidate_wish_1,\n",
        "    candidate_education, candidate_language_negotiative, candidate_language_basic, candidate_language_fluent,\n",
        "    candidate_highest_degree, candidate_career_type, candidate_industries, candidate_professions, candidate_resume,\n",
        "    candidate_feedback, candidate_professions_global, candidate_industries_global, candidate_relocation_ready,\n",
        "\n",
        "    Columns for job:\n",
        "    job_fixed_salary, job_bonus_salary, job_title, job_vacation_days, job_needed_experience, job_language,\n",
        "    job_description, job_daily_tasks_of_job, job_required_experience_of_candidate,\n",
        "    job_preferred_experience_of_candidate, job_preferred_education_of_candidate, job_max_candidate_age,\n",
        "    job_min_candidate_age, job_company_structure, job_language_skills_negotiative, job_language_skills_basic,\n",
        "    job_candidate_radius, job_candidate_relocation, job_city, job_state, job_country, job_time_model, job_max_salary,\n",
        "    job_questions_for_candidate, match_employer_feedback\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, batch_size=100):\n",
        "        print('Device: ' + str(device))\n",
        "\n",
        "        self.candidate_title = data.Field(sequential=True, lower=True, tokenize=tokenizer, include_lengths=True, use_vocab=True)\n",
        "        self.candidate_resume = data.Field(sequential=True, lower=True, include_lengths=True, use_vocab=True)\n",
        "        self.job_title = data.Field(sequential=True, lower=True, tokenize=tokenizer, include_lengths=True, use_vocab=True)\n",
        "        self.job_description = data.Field(sequential=True, lower=True, include_lengths=True, use_vocab=True)\n",
        "        self.match_status = data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "        self.train_set, self.validation_set = data.TabularDataset.splits(\n",
        "            path='./gdrive/My Drive/Colab Notebooks/data/TalentFox/',\n",
        "            train='train_data.csv',\n",
        "            validation='val_data.csv',\n",
        "            format='csv',\n",
        "            fields=[\n",
        "                ('index', None),\n",
        "                ('job_title', self.job_title),\n",
        "                ('job_description', self.job_description),\n",
        "                ('candidate_title', self.candidate_title),\n",
        "                ('candidate_resume', self.candidate_resume),\n",
        "                ('match_status', self.match_status)\n",
        "            ],\n",
        "            skip_header=True,\n",
        "        )\n",
        "\n",
        "        self.train_iter, self.validation_iter = data.BucketIterator.splits(\n",
        "            (self.train_set, self.validation_set),\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            device=device,\n",
        "            sort_key=lambda x: len(x.job_description),\n",
        "            sort_within_batch=True,\n",
        "            repeat=True)\n",
        "\n",
        "        self.match_status.build_vocab(self.train_set)\n",
        "        url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.de.vec'\n",
        "        self.job_title.build_vocab(self.train_set, vectors=vocab.Vectors('wiki.de.vec', url=url))\n",
        "        self.job_description.build_vocab(self.train_set, vectors=vocab.Vectors('wiki.de.vec', url=url))\n",
        "        self.candidate_title.build_vocab(self.train_set, vectors=vocab.Vectors('wiki.de.vec', url=url))\n",
        "        self.candidate_resume.build_vocab(self.train_set, vectors=vocab.Vectors('wiki.de.vec', url=url))\n",
        "        \n",
        "STOP_WORDS = {'(', ')', '/', 'm', 'w', '-', ' ', '.', '\\t'}\n",
        "\n",
        "def tokenizer(text):  # create a tokenizer function\n",
        "    tokens = [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "    tokens = list(filter(lambda token: token not in STOP_WORDS, tokens))\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nvDFnPDH7FTQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Model file\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.utils.rnn as rnn\n",
        "\n",
        "max_rating = 5.0\n",
        "min_rating = 0.5\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class TalentNetExperimental(nn.Module):\n",
        "    def __init__(self, job_title, job_description, candidate_title, candidate_resume, p1=0.2, p2=0.2, p3=0.2):\n",
        "        super(TalentNetExperimental, self).__init__()\n",
        "        self.job_title_vectors = job_title.vocab.vectors\n",
        "        self.job_title_num_embeddings = self.job_title_vectors.size()[0]\n",
        "        self.job_title_embedding_dim = self.job_title_vectors.size()[1]\n",
        "        \n",
        "        self.job_description_vectors = job_description.vocab.vectors\n",
        "        self.job_description_num_embeddings = self.job_description_vectors.size()[0]\n",
        "        self.job_description_embedding_dim = self.job_description_vectors.size()[1]\n",
        "\n",
        "        self.candidate_title_vectors = candidate_title.vocab.vectors\n",
        "        self.candidate_title_num_embeddings = self.candidate_title_vectors.size()[0]\n",
        "        self.candidate_title_embedding_dim = self.candidate_title_vectors.size()[1]\n",
        "\n",
        "        self.candidate_resume_vectors = candidate_resume.vocab.vectors\n",
        "        self.candidate_resume_num_embeddings = self.candidate_resume_vectors.size()[0]\n",
        "        self.candidate_resume_embedding_dim = self.candidate_resume_vectors.size()[1]\n",
        "\n",
        "        self.job_title_embeddings = nn.Embedding(self.job_title_num_embeddings, self.job_title_embedding_dim)\n",
        "        self.job_title_embeddings.weight.data.copy_(self.job_title_vectors)\n",
        "\n",
        "        self.job_description_embeddings = nn.Embedding(self.job_description_num_embeddings, self.job_description_embedding_dim)\n",
        "        self.job_description_embeddings.weight.data.copy_(self.job_description_vectors)\n",
        "\n",
        "        self.candidate_title_embeddings = nn.Embedding(self.candidate_title_num_embeddings, self.candidate_title_embedding_dim)\n",
        "        self.candidate_title_embeddings.weight.data.copy_(self.candidate_title_vectors)\n",
        "\n",
        "        self.candidate_resume_embeddings = nn.Embedding(self.candidate_resume_num_embeddings, self.candidate_resume_embedding_dim)\n",
        "        self.candidate_resume_embeddings.weight.data.copy_(self.candidate_resume_vectors)\n",
        "\n",
        "        self.hidden_dim = 50\n",
        "        self.lstm_layers = 5\n",
        "        self.lstm_job_description = nn.LSTM(input_size=self.job_description_embedding_dim, hidden_size=self.hidden_dim, num_layers=self.lstm_layers)\n",
        "        self.lstm_candidate_resume = nn.LSTM(input_size=self.candidate_resume_embedding_dim, hidden_size=self.hidden_dim, num_layers=self.lstm_layers)\n",
        "        \n",
        "        self.l1 = nn.Sequential(\n",
        "            nn.Dropout(p1),\n",
        "            nn.Linear(in_features=300,\n",
        "                      out_features=50),\n",
        "        )\n",
        "        \n",
        "        self.l2 = nn.Sequential(\n",
        "            nn.Dropout(p2),\n",
        "            nn.Linear(in_features=300,\n",
        "                      out_features=50),\n",
        "        )\n",
        "          \n",
        "        self.l3 = nn.Sequential(\n",
        "            nn.Dropout(p3),\n",
        "            nn.Linear(in_features=300,\n",
        "                      out_features=50),\n",
        "        )\n",
        "\n",
        "    def forward(self, data, job_description_lengths, candidate_resume_lengths):\n",
        "        job_title = data.job_title\n",
        "        job_description = self.job_description_embeddings(data.job_description)\n",
        "        candidate_title = data.candidate_title\n",
        "        candidate_resume = data.candidate_resume\n",
        "\n",
        "        numpy_job = job_title.cpu().data.numpy()\n",
        "        num_non_ones = np.count_nonzero(np.subtract(numpy_job, np.ones(numpy_job.shape)), axis=0)\n",
        "        num_non_ones = np.repeat(np.expand_dims(num_non_ones, 1), self.job_title_embedding_dim, 1)\n",
        "        num_non_ones = torch.tensor(num_non_ones).to(device).float()\n",
        "\n",
        "        job_title = self.job_title_embeddings(job_title)\n",
        "        job_title = torch.sum(job_title, 0).to(device) / num_non_ones\n",
        "        \n",
        "        ## Packing and padding\n",
        "        packed = rnn.pack_padded_sequence(job_description, job_description_lengths)\n",
        "        lstm_out, (lstm_hidden, lstm_state) = self.lstm_job_description(packed)\n",
        "        #padded, lengths = rnn.pad_packed_sequence(lstm_out)\n",
        "        last_hidden_job = lstm_hidden[-1]\n",
        "        \"\"\"\n",
        "        numpy_job = job_description.cpu().data.numpy()\n",
        "        num_non_ones = np.count_nonzero(np.subtract(numpy_job, np.ones(numpy_job.shape)), axis=0)\n",
        "        num_non_ones = np.repeat(np.expand_dims(num_non_ones, 1), self.job_description_embedding_dim, 1)\n",
        "        num_non_ones = torch.tensor(num_non_ones).to(device).float()\n",
        "\n",
        "        job_description = self.job_description_embeddings(job_description)\n",
        "        job_description = torch.sum(job_description, 0).to(device) / num_non_ones\n",
        "        \"\"\"\n",
        "        \n",
        "        numpy_candidate = candidate_title.cpu().data.numpy()\n",
        "        num_non_ones = np.count_nonzero(np.subtract(numpy_candidate, np.ones(numpy_candidate.shape)), axis=0)\n",
        "        num_non_ones = np.repeat(np.expand_dims(num_non_ones, 1), self.candidate_title_embedding_dim, 1)\n",
        "        num_non_ones = torch.tensor(num_non_ones).to(device).float()\n",
        "\n",
        "        candidate_title = self.candidate_title_embeddings(candidate_title)\n",
        "        candidate_title = torch.sum(candidate_title, 0).to(device) / num_non_ones\n",
        "\n",
        "        numpy_candidate = candidate_resume.cpu().data.numpy()\n",
        "        num_non_ones = np.count_nonzero(np.subtract(numpy_candidate, np.ones(numpy_candidate.shape)), axis=0)\n",
        "        num_non_ones = np.repeat(np.expand_dims(num_non_ones, 1), self.candidate_resume_embedding_dim, 1)\n",
        "        num_non_ones = torch.tensor(num_non_ones).to(device).float()\n",
        "\n",
        "        candidate_resume = self.candidate_resume_embeddings(candidate_resume)\n",
        "        candidate_resume = torch.sum(candidate_resume, 0).to(device) / num_non_ones\n",
        "        \n",
        "        job_title = self.l1(job_title)\n",
        "        candidate_title = self.l2(candidate_title)\n",
        "        candidate_resume = self.l3(candidate_resume)\n",
        "\n",
        "        x = (job_title * last_hidden_job * candidate_title * candidate_resume).sum(1)\n",
        "\n",
        "        \"\"\"\n",
        "        catted = torch.cat([job_title, job_description, candidate_title, candidate_resume], dim=1)\n",
        "\n",
        "        x = self.lin1(catted)\n",
        "        x = self.lin2(x)\n",
        "        x = self.lin3(x)\n",
        "        \"\"\"\n",
        "        out = torch.sigmoid(x)\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mF95z_C-hW3S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from types import SimpleNamespace\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def talent_fox_train(train_iter, val_iter, net, optimizer, criterion, ratio, num_epochs=5):\n",
        "    net.train()\n",
        "    prev_epoch = 0\n",
        "    train_loss = []\n",
        "    train_accs = []\n",
        "    train_accs_pos = 0\n",
        "    train_sum = 0\n",
        "    val_res = []\n",
        "    for batch in train_iter:\n",
        "        (job_title, job_title_lengths) = batch.job_title\n",
        "        (job_description, job_description_lengths) = batch.job_description\n",
        "        (candidate_title, candidate_title_lengths) = batch.candidate_title\n",
        "        (candidate_resume, candidate_resume_lengths) = batch.candidate_resume\n",
        "        match_status = batch.match_status\n",
        "\n",
        "        net.train()\n",
        "\n",
        "        batch_sampling = {'job_title': job_title, 'job_description': job_description, 'candidate_title': candidate_title, 'candidate_resume': candidate_resume}\n",
        "        output = net(SimpleNamespace(**batch_sampling), job_description_lengths, candidate_resume_lengths).reshape(-1)\n",
        "        targets = match_status.float().to(device)\n",
        "        criterion.weight = weights(targets, ratio)\n",
        "        batch_loss = criterion(output, targets)\n",
        "\n",
        "        train_loss.append(get_numpy(batch_loss))\n",
        "        train_accs.append(accuracy_sigmoid(output, targets))\n",
        "        train_accs_pos += accuracy_talent(output, targets)\n",
        "        train_sum += sum_targets(targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if train_iter.epoch != prev_epoch:\n",
        "            net.eval()\n",
        "            val_loss, val_accs, val_accs_pos, val_length, val_sum = 0, 0, 0, 0, 0\n",
        "\n",
        "            for val_batch in val_iter:\n",
        "                if val_iter.epoch != train_iter.epoch-1:\n",
        "                    break\n",
        "                (job_title, job_title_lengths) = val_batch.job_title\n",
        "                (job_description, job_description_lengths) = val_batch.job_description\n",
        "                (candidate_title, candidate_title_lengths) = val_batch.candidate_title\n",
        "                (candidate_resume, candidate_resume_lengths) = val_batch.candidate_resume\n",
        "                match_status = val_batch.match_status\n",
        "\n",
        "                batch_sampling = {'job_title': job_title, 'job_description': job_description,\n",
        "                                  'candidate_title': candidate_title, 'candidate_resume': candidate_resume}\n",
        "                val_output = net(SimpleNamespace(**batch_sampling), job_description_lengths, candidate_resume_lengths).reshape(-1)\n",
        "                val_target = match_status.float().to(device)\n",
        "                val_loss += criterion(val_output, val_target) * val_batch.batch_size\n",
        "                val_accs += accuracy_sigmoid(val_output, val_target) * val_batch.batch_size\n",
        "                val_accs_pos += accuracy_talent(val_output, val_target)\n",
        "                val_length += val_batch.batch_size\n",
        "                val_sum += sum_targets(val_target)\n",
        "\n",
        "            val_loss /= val_length\n",
        "            val_accs /= val_length\n",
        "            val_res.append(val_accs)\n",
        "\n",
        "            print(\n",
        "                \"Epoch {}: Train loss: {:.3f},  Train accs total: {:.3f}, Train accs positive: {:.3f}\"\n",
        "                    .format(train_iter.epoch, np.mean(train_loss), 1.0 - np.mean(train_accs), train_accs_pos/train_sum))\n",
        "            print(\n",
        "                \"          Validation loss: {:.3f}, Validation accs total: {:.3f}, Validation accs positive: {:.3f}\"\n",
        "                    .format(val_loss, 1.0 - val_accs, val_accs_pos/val_sum))\n",
        "            print()\n",
        "            train_loss = []\n",
        "            train_accs = []\n",
        "            train_accs_pos = 0\n",
        "            train_sum = 0\n",
        "            net.train()\n",
        "\n",
        "        prev_epoch = train_iter.epoch\n",
        "        if train_iter.epoch == num_epochs:\n",
        "            break\n",
        "\n",
        "def negative_sampling(users, docs, num_user):\n",
        "    if torch.cuda.is_available():\n",
        "        random_user = torch.tensor(\n",
        "        [randint(0, num_user) for _ in range(len(users))]\n",
        "        ).to(device)\n",
        "    else:\n",
        "        random_user = torch.tensor(\n",
        "            [randint(0, num_user-1) for _ in range(len(users))]\n",
        "        ).to(device)\n",
        "\n",
        "    author = torch.cat((users, random_user), 0).to(device)\n",
        "    doc_title = torch.cat((docs, docs), 1).to(device)\n",
        "\n",
        "    batch_with_negative_sampling = {'user': author, 'doc_title': doc_title}\n",
        "    return SimpleNamespace(**batch_with_negative_sampling)\n",
        "\n",
        "def plot_res(train_res, val_res, num_res):\n",
        "    x_vals = np.arange(num_res)\n",
        "    plt.figure()\n",
        "    plt.plot(x_vals, train_res, 'r', x_vals, val_res, 'b')\n",
        "    plt.legend(['Train Accucary', 'Validation Accuracy'])\n",
        "    plt.xlabel('Updates'), plt.ylabel('Acc')\n",
        "\n",
        "def accuracy_one_hot(output, target):\n",
        "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
        "    correct_prediction = torch.eq(torch.max(output, 1)[1], target)\n",
        "    # averaging the one-hot encoded vector\n",
        "    return torch.mean(correct_prediction.float())\n",
        "\n",
        "def sum_targets(targets):\n",
        "    return torch.sum(targets)\n",
        "\n",
        "def accuracy_talent(output, targets):\n",
        "    correct_predictions = 0\n",
        "    for idx, val in enumerate(output):\n",
        "        if val > 0.5 and targets[idx] == torch.tensor(1.0):\n",
        "            correct_predictions += 1\n",
        "    return correct_predictions\n",
        "\n",
        "def accuracy_sigmoid(output, target):\n",
        "    return torch.mean(torch.abs(output - target).float()).to(device).data.numpy()\n",
        "\n",
        "\n",
        "def accuracy(output, target):\n",
        "    return torch.mean(torch.abs(torch.round(output) - target)).to(device).data.numpy()\n",
        "\n",
        "def weights(target, ratio):\n",
        "    weight = []\n",
        "    for val in target:\n",
        "        if val == torch.tensor(1.):\n",
        "            weight.append(ratio)\n",
        "        else:\n",
        "            weight.append(1.)\n",
        "    return torch.tensor(weight)\n",
        "\n",
        "def print_params(net):\n",
        "    for name, param in net.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            print(name, param.data)\n",
        "\n",
        "\n",
        "def get_numpy(loss):\n",
        "    return loss.to(device).data.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KpVwL5MTLpDS",
        "colab_type": "code",
        "outputId": "69ae42bd-ebfa-48c3-912d-ef73ce90c408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1999
        }
      },
      "cell_type": "code",
      "source": [
        "from torch import optim, nn\n",
        "\n",
        "tf = TalentFox(batch_size=100)\n",
        "\n",
        "train_iter = tf.train_iter\n",
        "val_iter = tf.validation_iter\n",
        "\n",
        "job_title = tf.job_title\n",
        "job_description = tf.job_description\n",
        "candidate_title = tf.candidate_title\n",
        "candidate_resume = tf.candidate_resume\n",
        "\n",
        "ratio = (train_iter.dataset.fields['match_status'].vocab.freqs['0']/train_iter.dataset.fields['match_status'].vocab.freqs['1'])\n",
        "\n",
        "net = TalentNetExperimental(job_title=job_title, job_description=job_description, candidate_title=candidate_title, candidate_resume=candidate_resume).to(device)\n",
        "opt = optim.Adam(net.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "talent_fox_train(train_iter=train_iter, val_iter=val_iter, net=net, optimizer=opt, criterion=criterion, ratio=ratio, num_epochs=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "Epoch 1: Train loss: 1.337,  Train accs total: 0.501, Train accs positive: 0.411\n",
            "          Validation loss: 1.044, Validation accs total: 0.500, Validation accs positive: 0.357\n",
            "\n",
            "Epoch 2: Train loss: 1.317,  Train accs total: 0.507, Train accs positive: 0.432\n",
            "          Validation loss: 1.396, Validation accs total: 0.547, Validation accs positive: 0.000\n",
            "\n",
            "Epoch 3: Train loss: 1.311,  Train accs total: 0.521, Train accs positive: 0.378\n",
            "          Validation loss: 1.429, Validation accs total: 0.530, Validation accs positive: 0.071\n",
            "\n",
            "Epoch 4: Train loss: 1.238,  Train accs total: 0.546, Train accs positive: 0.494\n",
            "          Validation loss: 1.215, Validation accs total: 0.568, Validation accs positive: 0.321\n",
            "\n",
            "Epoch 5: Train loss: 1.203,  Train accs total: 0.614, Train accs positive: 0.751\n",
            "          Validation loss: 0.728, Validation accs total: 0.656, Validation accs positive: 0.268\n",
            "\n",
            "Epoch 6: Train loss: 0.923,  Train accs total: 0.690, Train accs positive: 0.813\n",
            "          Validation loss: 0.621, Validation accs total: 0.785, Validation accs positive: 0.250\n",
            "\n",
            "Epoch 7: Train loss: 0.909,  Train accs total: 0.751, Train accs positive: 0.888\n",
            "          Validation loss: 1.396, Validation accs total: 0.584, Validation accs positive: 0.536\n",
            "\n",
            "Epoch 8: Train loss: 0.695,  Train accs total: 0.778, Train accs positive: 0.896\n",
            "          Validation loss: 0.995, Validation accs total: 0.782, Validation accs positive: 0.304\n",
            "\n",
            "Epoch 9: Train loss: 0.610,  Train accs total: 0.842, Train accs positive: 0.892\n",
            "          Validation loss: 0.677, Validation accs total: 0.798, Validation accs positive: 0.304\n",
            "\n",
            "Epoch 10: Train loss: 0.458,  Train accs total: 0.860, Train accs positive: 0.942\n",
            "          Validation loss: 1.172, Validation accs total: 0.804, Validation accs positive: 0.321\n",
            "\n",
            "Epoch 11: Train loss: 0.712,  Train accs total: 0.840, Train accs positive: 0.950\n",
            "          Validation loss: 1.897, Validation accs total: 0.814, Validation accs positive: 0.250\n",
            "\n",
            "Epoch 12: Train loss: 0.802,  Train accs total: 0.856, Train accs positive: 0.925\n",
            "          Validation loss: 1.137, Validation accs total: 0.709, Validation accs positive: 0.446\n",
            "\n",
            "Epoch 13: Train loss: 0.526,  Train accs total: 0.869, Train accs positive: 0.938\n",
            "          Validation loss: 1.197, Validation accs total: 0.838, Validation accs positive: 0.232\n",
            "\n",
            "Epoch 14: Train loss: 0.444,  Train accs total: 0.891, Train accs positive: 0.963\n",
            "          Validation loss: 1.196, Validation accs total: 0.809, Validation accs positive: 0.321\n",
            "\n",
            "Epoch 15: Train loss: 0.335,  Train accs total: 0.909, Train accs positive: 0.959\n",
            "          Validation loss: 1.091, Validation accs total: 0.814, Validation accs positive: 0.321\n",
            "\n",
            "Epoch 16: Train loss: 0.276,  Train accs total: 0.918, Train accs positive: 0.967\n",
            "          Validation loss: 0.875, Validation accs total: 0.878, Validation accs positive: 0.196\n",
            "\n",
            "Epoch 17: Train loss: 0.230,  Train accs total: 0.929, Train accs positive: 0.975\n",
            "          Validation loss: 0.651, Validation accs total: 0.872, Validation accs positive: 0.179\n",
            "\n",
            "Epoch 18: Train loss: 0.370,  Train accs total: 0.941, Train accs positive: 0.988\n",
            "          Validation loss: 1.229, Validation accs total: 0.895, Validation accs positive: 0.179\n",
            "\n",
            "Epoch 19: Train loss: 0.196,  Train accs total: 0.944, Train accs positive: 0.979\n",
            "          Validation loss: 0.777, Validation accs total: 0.897, Validation accs positive: 0.179\n",
            "\n",
            "Epoch 20: Train loss: 0.551,  Train accs total: 0.892, Train accs positive: 0.975\n",
            "          Validation loss: 1.639, Validation accs total: 0.870, Validation accs positive: 0.250\n",
            "\n",
            "Epoch 21: Train loss: 0.293,  Train accs total: 0.935, Train accs positive: 0.963\n",
            "          Validation loss: 0.950, Validation accs total: 0.877, Validation accs positive: 0.161\n",
            "\n",
            "Epoch 22: Train loss: 0.695,  Train accs total: 0.865, Train accs positive: 0.971\n",
            "          Validation loss: 1.117, Validation accs total: 0.826, Validation accs positive: 0.196\n",
            "\n",
            "Epoch 23: Train loss: 0.305,  Train accs total: 0.916, Train accs positive: 0.950\n",
            "          Validation loss: 0.854, Validation accs total: 0.850, Validation accs positive: 0.196\n",
            "\n",
            "Epoch 24: Train loss: 0.202,  Train accs total: 0.937, Train accs positive: 0.992\n",
            "          Validation loss: 0.761, Validation accs total: 0.871, Validation accs positive: 0.179\n",
            "\n",
            "Epoch 25: Train loss: 0.178,  Train accs total: 0.947, Train accs positive: 0.983\n",
            "          Validation loss: 0.791, Validation accs total: 0.887, Validation accs positive: 0.161\n",
            "\n",
            "Epoch 26: Train loss: 0.378,  Train accs total: 0.916, Train accs positive: 0.983\n",
            "          Validation loss: 0.999, Validation accs total: 0.824, Validation accs positive: 0.232\n",
            "\n",
            "Epoch 27: Train loss: 0.429,  Train accs total: 0.921, Train accs positive: 0.975\n",
            "          Validation loss: 1.189, Validation accs total: 0.869, Validation accs positive: 0.161\n",
            "\n",
            "Epoch 28: Train loss: 0.236,  Train accs total: 0.940, Train accs positive: 0.983\n",
            "          Validation loss: 0.537, Validation accs total: 0.867, Validation accs positive: 0.214\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-d022587ede00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtalent_fox_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-0eb940c4b805>\u001b[0m in \u001b[0;36mtalent_fox_train\u001b[0;34m(train_iter, val_iter, net, optimizer, criterion, ratio, num_epochs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}